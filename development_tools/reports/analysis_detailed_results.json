{
  "generated_by": "run_development_tools.py - AI Development Tools Runner",
  "last_generated": "2025-12-04 07:37:12",
  "source": "python development_tools/run_development_tools.py audit --full",
  "audit_tier": 3,
  "note": "This file is auto-generated. Do not edit manually.",
  "timestamp": "2025-12-04T07:37:12.812815",
  "successful": [
    "analyze_functions",
    "analyze_function_registry",
    "analyze_documentation",
    "analyze_documentation_sync",
    "analyze_error_handling",
    "analyze_module_dependencies",
    "analyze_unused_imports",
    "analyze_legacy_references",
    "quick_status"
  ],
  "failed": [],
  "results": {
    "analyze_functions": {
      "success": true,
      "data": {
        "total_functions": 1490,
        "moderate_complexity": 153,
        "high_complexity": 138,
        "critical_complexity": 128,
        "undocumented": 23,
        "handlers": 836,
        "tests": 60,
        "utilities": 0
      },
      "timestamp": "2025-12-04T07:33:39.451302"
    },
    "analyze_function_registry": {
      "success": true,
      "data": {
        "analysis": {
          "duplicate_count": 117,
          "duplicate_sample": [
            {
              "files": [
                "communication/__init__.py",
                "communication/core/__init__.py",
                "core/__init__.py"
              ],
              "name": "__getattr__"
            },
            {
              "files": [
                "ai/cache_manager.py",
                "ai/chatbot.py",
                "ai/context_builder.py",
                "ai/conversation_history.py",
                "ai/lm_studio_manager.py",
                "ai/prompt_manager.py",
                "communication/communication_channels/base/base_channel.py",
                "communication/communication_channels/base/command_registry.py",
                "communication/communication_channels/base/rich_formatter.py",
                "communication/communication_channels/discord/account_flow_handler.py",
                "communication/communication_channels/discord/api_client.py",
                "communication/communication_channels/discord/bot.py",
                "communication/communication_channels/discord/checkin_view.py",
                "communication/communication_channels/discord/event_handler.py",
                "communication/communication_channels/discord/task_reminder_view.py",
                "communication/communication_channels/discord/webhook_server.py",
                "communication/communication_channels/discord/welcome_handler.py",
                "communication/communication_channels/email/bot.py",
                "communication/core/channel_monitor.py",
                "communication/core/channel_orchestrator.py",
                "communication/core/retry_manager.py",
                "communication/message_processing/command_parser.py",
                "communication/message_processing/conversation_flow_manager.py",
                "communication/message_processing/interaction_manager.py",
                "communication/message_processing/message_router.py",
                "core/backup_manager.py",
                "core/checkin_analytics.py",
                "core/checkin_dynamic_manager.py",
                "core/config.py",
                "core/error_handling.py",
                "core/file_auditor.py",
                "core/headless_service.py",
                "core/logger.py",
                "core/message_analytics.py",
                "core/scheduler.py",
                "core/service.py",
                "core/service_utilities.py",
                "core/user_data_manager.py",
                "ui/dialogs/account_creator_dialog.py",
                "ui/dialogs/admin_panel.py",
                "ui/dialogs/category_management_dialog.py",
                "ui/dialogs/channel_management_dialog.py",
                "ui/dialogs/checkin_management_dialog.py",
                "ui/dialogs/message_editor_dialog.py",
                "ui/dialogs/process_watcher_dialog.py",
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/dialogs/task_completion_dialog.py",
                "ui/dialogs/task_crud_dialog.py",
                "ui/dialogs/task_edit_dialog.py",
                "ui/dialogs/task_management_dialog.py",
                "ui/dialogs/user_analytics_dialog.py",
                "ui/dialogs/user_profile_dialog.py",
                "ui/ui_app_qt.py",
                "ui/widgets/category_selection_widget.py",
                "ui/widgets/channel_selection_widget.py",
                "ui/widgets/checkin_settings_widget.py",
                "ui/widgets/dynamic_list_container.py",
                "ui/widgets/dynamic_list_field.py",
                "ui/widgets/period_row_widget.py",
                "ui/widgets/tag_widget.py",
                "ui/widgets/task_settings_widget.py",
                "ui/widgets/user_profile_settings_widget.py",
                "user/context_manager.py",
                "user/user_preferences.py"
              ],
              "name": "__init__"
            },
            {
              "files": [
                "ai/chatbot.py",
                "communication/core/channel_orchestrator.py",
                "user/user_context.py"
              ],
              "name": "__new__"
            },
            {
              "files": [
                "ai/context_builder.py",
                "ai/conversation_history.py",
                "communication/communication_channels/base/base_channel.py",
                "communication/communication_channels/base/command_registry.py",
                "communication/communication_channels/discord/event_handler.py",
                "ui/widgets/dynamic_list_container.py"
              ],
              "name": "__post_init__"
            },
            {
              "files": [
                "communication/command_handlers/analytics_handler.py",
                "communication/command_handlers/interaction_handlers.py"
              ],
              "name": "_get_field_scale"
            }
          ],
          "duplicates": {
            "__getattr__": [
              "communication/__init__.py",
              "communication/core/__init__.py",
              "core/__init__.py"
            ],
            "__init__": [
              "ai/cache_manager.py",
              "ai/chatbot.py",
              "ai/context_builder.py",
              "ai/conversation_history.py",
              "ai/lm_studio_manager.py",
              "ai/prompt_manager.py",
              "communication/communication_channels/base/base_channel.py",
              "communication/communication_channels/base/command_registry.py",
              "communication/communication_channels/base/rich_formatter.py",
              "communication/communication_channels/discord/account_flow_handler.py",
              "communication/communication_channels/discord/api_client.py",
              "communication/communication_channels/discord/bot.py",
              "communication/communication_channels/discord/checkin_view.py",
              "communication/communication_channels/discord/event_handler.py",
              "communication/communication_channels/discord/task_reminder_view.py",
              "communication/communication_channels/discord/webhook_server.py",
              "communication/communication_channels/discord/welcome_handler.py",
              "communication/communication_channels/email/bot.py",
              "communication/core/channel_monitor.py",
              "communication/core/channel_orchestrator.py",
              "communication/core/retry_manager.py",
              "communication/message_processing/command_parser.py",
              "communication/message_processing/conversation_flow_manager.py",
              "communication/message_processing/interaction_manager.py",
              "communication/message_processing/message_router.py",
              "core/backup_manager.py",
              "core/checkin_analytics.py",
              "core/checkin_dynamic_manager.py",
              "core/config.py",
              "core/error_handling.py",
              "core/file_auditor.py",
              "core/headless_service.py",
              "core/logger.py",
              "core/message_analytics.py",
              "core/scheduler.py",
              "core/service.py",
              "core/service_utilities.py",
              "core/user_data_manager.py",
              "ui/dialogs/account_creator_dialog.py",
              "ui/dialogs/admin_panel.py",
              "ui/dialogs/category_management_dialog.py",
              "ui/dialogs/channel_management_dialog.py",
              "ui/dialogs/checkin_management_dialog.py",
              "ui/dialogs/message_editor_dialog.py",
              "ui/dialogs/process_watcher_dialog.py",
              "ui/dialogs/schedule_editor_dialog.py",
              "ui/dialogs/task_completion_dialog.py",
              "ui/dialogs/task_crud_dialog.py",
              "ui/dialogs/task_edit_dialog.py",
              "ui/dialogs/task_management_dialog.py",
              "ui/dialogs/user_analytics_dialog.py",
              "ui/dialogs/user_profile_dialog.py",
              "ui/ui_app_qt.py",
              "ui/widgets/category_selection_widget.py",
              "ui/widgets/channel_selection_widget.py",
              "ui/widgets/checkin_settings_widget.py",
              "ui/widgets/dynamic_list_container.py",
              "ui/widgets/dynamic_list_field.py",
              "ui/widgets/period_row_widget.py",
              "ui/widgets/tag_widget.py",
              "ui/widgets/task_settings_widget.py",
              "ui/widgets/user_profile_settings_widget.py",
              "user/context_manager.py",
              "user/user_preferences.py"
            ],
            "__new__": [
              "ai/chatbot.py",
              "communication/core/channel_orchestrator.py",
              "user/user_context.py"
            ],
            "__post_init__": [
              "ai/context_builder.py",
              "ai/conversation_history.py",
              "communication/communication_channels/base/base_channel.py",
              "communication/communication_channels/base/command_registry.py",
              "communication/communication_channels/discord/event_handler.py",
              "ui/widgets/dynamic_list_container.py"
            ],
            "_get_field_scale": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_add_schedule_period": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/schedule_handler.py"
            ],
            "_handle_add_schedule_period__parse_time_format": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/schedule_handler.py"
            ],
            "_handle_checkin_history": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_checkin_status": [
              "communication/command_handlers/checkin_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_complete_task": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_complete_task__find_task_by_identifier": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_completion_rate": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_continue_checkin": [
              "communication/command_handlers/checkin_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_create_task": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_create_task__parse_relative_date": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_delete_task": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_delete_task__find_task_by_identifier": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_edit_schedule_period": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/schedule_handler.py"
            ],
            "_handle_edit_schedule_period__parse_time_format": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/schedule_handler.py"
            ],
            "_handle_energy_trends": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_habit_analysis": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_list_tasks": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__apply_filters": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__build_filter_info": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__build_response": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__create_rich_data": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__format_due_date": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__format_list": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__generate_suggestions": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__get_suggestion": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__no_tasks_response": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_list_tasks__sort_tasks": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_mood_trends": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_profile_stats": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/profile_handler.py"
            ],
            "_handle_quant_summary": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_schedule_status": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/schedule_handler.py"
            ],
            "_handle_show_analytics": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_show_profile": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/profile_handler.py"
            ],
            "_handle_show_schedule": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/schedule_handler.py"
            ],
            "_handle_sleep_analysis": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_start_checkin": [
              "communication/command_handlers/checkin_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_handle_task_stats": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_update_profile": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/profile_handler.py"
            ],
            "_handle_update_schedule": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/schedule_handler.py"
            ],
            "_handle_update_task": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_update_task__find_task_by_identifier": [
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/task_handler.py"
            ],
            "_handle_wellness_score": [
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/interaction_handlers.py"
            ],
            "_is_valid_intent": [
              "communication/message_processing/command_parser.py",
              "communication/message_processing/interaction_manager.py"
            ],
            "accept": [
              "ui/dialogs/account_creator_dialog.py",
              "ui/dialogs/schedule_editor_dialog.py"
            ],
            "add_message": [
              "ai/conversation_history.py",
              "core/message_management.py"
            ],
            "add_new_period": [
              "ui/dialogs/schedule_editor_dialog.py",
              "ui/widgets/checkin_settings_widget.py",
              "ui/widgets/task_settings_widget.py"
            ],
            "can_handle": [
              "communication/command_handlers/account_handler.py",
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/base_handler.py",
              "communication/command_handlers/checkin_handler.py",
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/profile_handler.py",
              "communication/command_handlers/schedule_handler.py",
              "communication/command_handlers/task_handler.py",
              "core/error_handling.py"
            ],
            "cancel": [
              "ui/dialogs/schedule_editor_dialog.py",
              "ui/dialogs/user_profile_dialog.py"
            ],
            "center_dialog": [
              "ui/dialogs/account_creator_dialog.py",
              "ui/dialogs/schedule_editor_dialog.py",
              "ui/dialogs/user_profile_dialog.py"
            ],
            "channel_type": [
              "communication/communication_channels/base/base_channel.py",
              "communication/communication_channels/discord/bot.py",
              "communication/communication_channels/email/bot.py"
            ],
            "cleanup_task_reminders": [
              "core/scheduler.py",
              "tasks/task_management.py"
            ],
            "clear_welcomed_status": [
              "communication/communication_channels/discord/welcome_handler.py",
              "communication/core/welcome_manager.py"
            ],
            "closeEvent": [
              "ui/dialogs/process_watcher_dialog.py",
              "ui/ui_app_qt.py"
            ],
            "close_dialog": [
              "ui/dialogs/account_creator_dialog.py",
              "ui/dialogs/schedule_editor_dialog.py"
            ],
            "create_new_user": [
              "core/user_management.py",
              "ui/ui_app_qt.py"
            ],
            "critical": [
              "core/file_auditor.py",
              "core/logger.py"
            ],
            "debug": [
              "core/file_auditor.py",
              "core/logger.py"
            ],
            "error": [
              "core/file_auditor.py",
              "core/logger.py"
            ],
            "find_lowest_available_period_number": [
              "ui/dialogs/schedule_editor_dialog.py",
              "ui/widgets/checkin_settings_widget.py",
              "ui/widgets/task_settings_widget.py"
            ],
            "get_all_user_ids": [
              "core/user_data_handlers.py",
              "core/user_management.py"
            ],
            "get_available_data_types": [
              "core/checkin_analytics.py",
              "core/user_management.py"
            ],
            "get_available_tags": [
              "ui/widgets/tag_widget.py",
              "ui/widgets/task_settings_widget.py"
            ],
            "get_checkin_settings": [
              "ui/dialogs/checkin_management_dialog.py",
              "ui/widgets/checkin_settings_widget.py"
            ],
            "get_command_definitions": [
              "communication/message_processing/interaction_manager.py",
              "communication/message_processing/message_router.py"
            ],
            "get_examples": [
              "communication/command_handlers/account_handler.py",
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/base_handler.py",
              "communication/command_handlers/checkin_handler.py",
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/profile_handler.py",
              "communication/command_handlers/schedule_handler.py",
              "communication/command_handlers/task_handler.py"
            ],
            "get_help": [
              "communication/command_handlers/account_handler.py",
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/base_handler.py",
              "communication/command_handlers/checkin_handler.py",
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/profile_handler.py",
              "communication/command_handlers/schedule_handler.py",
              "communication/command_handlers/task_handler.py"
            ],
            "get_recent_messages": [
              "ai/conversation_history.py",
              "core/message_management.py"
            ],
            "get_registered_channels": [
              "communication/core/channel_orchestrator.py",
              "communication/core/factory.py"
            ],
            "get_selected_categories": [
              "ui/dialogs/category_management_dialog.py",
              "ui/widgets/category_selection_widget.py"
            ],
            "get_selected_channel": [
              "ui/dialogs/channel_management_dialog.py",
              "ui/widgets/channel_selection_widget.py"
            ],
            "get_slash_command_map": [
              "communication/message_processing/interaction_manager.py",
              "communication/message_processing/message_router.py"
            ],
            "get_statistics": [
              "ai/conversation_history.py",
              "ui/dialogs/task_management_dialog.py",
              "ui/widgets/task_settings_widget.py"
            ],
            "get_timestamp_for_sorting": [
              "core/message_management.py",
              "core/response_tracking.py"
            ],
            "get_welcome_message": [
              "communication/communication_channels/discord/welcome_handler.py",
              "communication/core/welcome_manager.py"
            ],
            "handle": [
              "communication/command_handlers/account_handler.py",
              "communication/command_handlers/analytics_handler.py",
              "communication/command_handlers/base_handler.py",
              "communication/command_handlers/checkin_handler.py",
              "communication/command_handlers/interaction_handlers.py",
              "communication/command_handlers/profile_handler.py",
              "communication/command_handlers/schedule_handler.py",
              "communication/command_handlers/task_handler.py"
            ],
            "handle_task_reminder": [
              "communication/core/channel_orchestrator.py",
              "core/scheduler.py"
            ],
            "has_been_welcomed": [
              "communication/communication_channels/discord/welcome_handler.py",
              "communication/core/welcome_manager.py"
            ],
            "info": [
              "core/file_auditor.py",
              "core/logger.py"
            ],
            "is_ready": [
              "ai/lm_studio_manager.py",
              "communication/communication_channels/base/base_channel.py"
            ],
            "is_schedule_period_active": [
              "core/schedule_management.py",
              "user/user_preferences.py"
            ],
            "is_service_running": [
              "core/service_utilities.py",
              "ui/ui_app_qt.py"
            ],
            "keyPressEvent": [
              "ui/dialogs/account_creator_dialog.py",
              "ui/dialogs/user_profile_dialog.py"
            ],
            "load_existing_data": [
              "ui/dialogs/schedule_editor_dialog.py",
              "ui/widgets/checkin_settings_widget.py",
              "ui/widgets/task_settings_widget.py",
              "ui/widgets/user_profile_settings_widget.py"
            ],
            "main": [
              "core/headless_service.py",
              "core/service.py",
              "run_headless_service.py",
              "run_mhm.py",
              "run_tests.py",
              "ui/generate_ui_files.py",
              "ui/ui_app_qt.py"
            ],
            "mark_as_welcomed": [
              "communication/communication_channels/discord/welcome_handler.py",
              "communication/core/welcome_manager.py"
            ],
            "open_personalization_dialog": [
              "ui/dialogs/account_creator_dialog.py",
              "ui/dialogs/user_profile_dialog.py"
            ],
            "open_schedule_editor": [
              "ui/dialogs/schedule_editor_dialog.py",
              "ui/ui_app_qt.py"
            ],
            "populate_timezones": [
              "ui/widgets/channel_selection_widget.py",
              "ui/widgets/user_profile_settings_widget.py"
            ],
            "refresh_tags": [
              "ui/widgets/tag_widget.py",
              "ui/widgets/task_settings_widget.py"
            ],
            "register_data_loader": [
              "core/user_data_handlers.py",
              "core/user_management.py"
            ],
            "remove_period_row": [
              "ui/dialogs/schedule_editor_dialog.py",
              "ui/widgets/checkin_settings_widget.py",
              "ui/widgets/task_settings_widget.py"
            ],
            "save_user_data": [
              "core/user_data_handlers.py",
              "user/user_context.py"
            ],
            "send_test_message": [
              "core/headless_service.py",
              "ui/ui_app_qt.py"
            ],
            "set_checkin_settings": [
              "ui/dialogs/checkin_management_dialog.py",
              "ui/widgets/checkin_settings_widget.py"
            ],
            "set_schedule_period_active": [
              "core/schedule_management.py",
              "user/user_preferences.py"
            ],
            "set_selected_categories": [
              "ui/dialogs/category_management_dialog.py",
              "ui/widgets/category_selection_widget.py"
            ],
            "set_selected_channel": [
              "ui/dialogs/channel_management_dialog.py",
              "ui/widgets/channel_selection_widget.py"
            ],
            "setup_connections": [
              "ui/dialogs/account_creator_dialog.py",
              "ui/dialogs/message_editor_dialog.py",
              "ui/dialogs/task_completion_dialog.py",
              "ui/dialogs/task_crud_dialog.py",
              "ui/dialogs/task_edit_dialog.py",
              "ui/dialogs/user_analytics_dialog.py",
              "ui/widgets/checkin_settings_widget.py",
              "ui/widgets/tag_widget.py",
              "ui/widgets/task_settings_widget.py"
            ],
            "setup_functionality": [
              "ui/dialogs/schedule_editor_dialog.py",
              "ui/widgets/period_row_widget.py"
            ],
            "setup_initial_state": [
              "ui/dialogs/message_editor_dialog.py",
              "ui/dialogs/user_analytics_dialog.py"
            ],
            "setup_ui": [
              "ui/dialogs/admin_panel.py",
              "ui/dialogs/message_editor_dialog.py",
              "ui/dialogs/process_watcher_dialog.py",
              "ui/dialogs/task_completion_dialog.py",
              "ui/dialogs/task_crud_dialog.py",
              "ui/dialogs/task_edit_dialog.py",
              "ui/dialogs/user_profile_dialog.py",
              "ui/widgets/tag_widget.py"
            ],
            "showEvent": [
              "ui/widgets/checkin_settings_widget.py",
              "ui/widgets/task_settings_widget.py"
            ],
            "sort_key": [
              "core/schedule_management.py",
              "ui/dialogs/schedule_editor_dialog.py"
            ],
            "start": [
              "communication/communication_channels/discord/webhook_server.py",
              "core/file_auditor.py",
              "core/service.py"
            ],
            "stop": [
              "communication/communication_channels/discord/webhook_server.py",
              "core/file_auditor.py"
            ],
            "undo_last_tag_delete": [
              "ui/widgets/tag_widget.py",
              "ui/widgets/task_settings_widget.py"
            ],
            "update_channel_preferences": [
              "core/user_data_handlers.py",
              "core/user_management.py"
            ],
            "update_user_account": [
              "core/user_data_handlers.py",
              "core/user_management.py"
            ],
            "update_user_context": [
              "core/user_data_handlers.py",
              "core/user_management.py"
            ],
            "update_user_schedules": [
              "core/user_data_handlers.py",
              "core/user_management.py"
            ],
            "validate_configuration": [
              "core/service.py",
              "ui/ui_app_qt.py"
            ],
            "warning": [
              "core/file_auditor.py",
              "core/logger.py"
            ]
          },
          "high_complexity": [
            {
              "complexity": 2192,
              "file": "ai/chatbot.py",
              "has_docstring": true,
              "name": "_create_comprehensive_context_prompt"
            },
            {
              "complexity": 2115,
              "file": "communication/communication_channels/discord/bot.py",
              "has_docstring": true,
              "name": "initialize__register_events"
            },
            {
              "complexity": 2067,
              "file": "core/user_data_handlers.py",
              "has_docstring": true,
              "name": "get_user_data"
            },
            {
              "complexity": 1976,
              "file": "communication/message_processing/interaction_manager.py",
              "has_docstring": true,
              "name": "handle_message"
            },
            {
              "complexity": 1338,
              "file": "run_tests.py",
              "has_docstring": true,
              "name": "print_combined_summary"
            },
            {
              "complexity": 1209,
              "file": "run_tests.py",
              "has_docstring": true,
              "name": "main"
            },
            {
              "complexity": 1084,
              "file": "ai/chatbot.py",
              "has_docstring": true,
              "name": "_get_contextual_fallback"
            },
            {
              "complexity": 1002,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_show_profile"
            },
            {
              "complexity": 885,
              "file": "communication/core/channel_orchestrator.py",
              "has_docstring": true,
              "name": "_send_predefined_message"
            },
            {
              "complexity": 825,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_update_profile"
            },
            {
              "complexity": 819,
              "file": "communication/command_handlers/profile_handler.py",
              "has_docstring": true,
              "name": "_handle_update_profile"
            },
            {
              "complexity": 814,
              "file": "communication/message_processing/command_parser.py",
              "has_docstring": true,
              "name": "_extract_entities_rule_based"
            },
            {
              "complexity": 793,
              "file": "core/logger.py",
              "has_docstring": true,
              "name": "doRollover"
            },
            {
              "complexity": 780,
              "file": "core/checkin_analytics.py",
              "has_docstring": true,
              "name": "get_quantitative_summaries"
            },
            {
              "complexity": 774,
              "file": "ai/chatbot.py",
              "has_docstring": true,
              "name": "generate_response"
            },
            {
              "complexity": 751,
              "file": "core/service.py",
              "has_docstring": true,
              "name": "run_service_loop"
            },
            {
              "complexity": 751,
              "file": "communication/communication_channels/discord/webhook_handler.py",
              "has_docstring": true,
              "name": "handle_application_authorized"
            },
            {
              "complexity": 742,
              "file": "communication/command_handlers/profile_handler.py",
              "has_docstring": true,
              "name": "_format_profile_text"
            },
            {
              "complexity": 742,
              "file": "run_tests.py",
              "has_docstring": true,
              "name": "run_command"
            },
            {
              "complexity": 713,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "refresh_user_list"
            },
            {
              "complexity": 708,
              "file": "ai/chatbot.py",
              "has_docstring": true,
              "name": "generate_contextual_response"
            },
            {
              "complexity": 702,
              "file": "communication/message_processing/conversation_flow_manager.py",
              "has_docstring": true,
              "name": "_parse_reminder_periods_from_text"
            },
            {
              "complexity": 696,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "validate_configuration"
            },
            {
              "complexity": 691,
              "file": "ui/widgets/user_profile_settings_widget.py",
              "has_docstring": true,
              "name": "__init__"
            },
            {
              "complexity": 691,
              "file": "run_headless_service.py",
              "has_docstring": true,
              "name": "main"
            },
            {
              "complexity": 686,
              "file": "ui/dialogs/account_creator_dialog.py",
              "has_docstring": true,
              "name": "validate_input"
            },
            {
              "complexity": 677,
              "file": "ui/widgets/user_profile_settings_widget.py",
              "has_docstring": true,
              "name": "load_existing_data"
            },
            {
              "complexity": 661,
              "file": "core/logger.py",
              "has_docstring": false,
              "name": "__init__"
            },
            {
              "complexity": 657,
              "file": "core/user_data_handlers.py",
              "has_docstring": true,
              "name": "save_user_data"
            },
            {
              "complexity": 655,
              "file": "communication/message_processing/conversation_flow_manager.py",
              "has_docstring": true,
              "name": "_handle_task_reminder_followup"
            },
            {
              "complexity": 639,
              "file": "core/user_data_validation.py",
              "has_docstring": true,
              "name": "validate_user_update"
            },
            {
              "complexity": 618,
              "file": "ui/dialogs/task_edit_dialog.py",
              "has_docstring": true,
              "name": "render_reminder_period_row"
            },
            {
              "complexity": 615,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_show_schedule"
            },
            {
              "complexity": 615,
              "file": "communication/command_handlers/schedule_handler.py",
              "has_docstring": true,
              "name": "_handle_show_schedule"
            },
            {
              "complexity": 611,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_edit_schedule_period"
            },
            {
              "complexity": 597,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "stop_service"
            },
            {
              "complexity": 596,
              "file": "communication/communication_channels/email/bot.py",
              "has_docstring": true,
              "name": "_receive_emails_sync"
            },
            {
              "complexity": 584,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "system_health_check"
            },
            {
              "complexity": 564,
              "file": "communication/message_processing/interaction_manager.py",
              "has_docstring": true,
              "name": "get_user_suggestions"
            },
            {
              "complexity": 563,
              "file": "communication/command_handlers/schedule_handler.py",
              "has_docstring": true,
              "name": "_handle_edit_schedule_period"
            },
            {
              "complexity": 560,
              "file": "core/user_data_handlers.py",
              "has_docstring": true,
              "name": "_save_user_data__merge_single_type"
            },
            {
              "complexity": 557,
              "file": "core/schedule_management.py",
              "has_docstring": true,
              "name": "get_schedule_time_periods"
            },
            {
              "complexity": 557,
              "file": "communication/communication_channels/discord/webhook_server.py",
              "has_docstring": true,
              "name": "do_POST"
            },
            {
              "complexity": 553,
              "file": "tasks/task_management.py",
              "has_docstring": true,
              "name": "create_task"
            },
            {
              "complexity": 544,
              "file": "core/user_management.py",
              "has_docstring": true,
              "name": "create_new_user"
            },
            {
              "complexity": 533,
              "file": "communication/command_handlers/profile_handler.py",
              "has_docstring": true,
              "name": "_handle_show_profile"
            },
            {
              "complexity": 529,
              "file": "ai/context_builder.py",
              "has_docstring": true,
              "name": "create_context_prompt"
            },
            {
              "complexity": 516,
              "file": "communication/communication_channels/discord/bot.py",
              "has_docstring": true,
              "name": "initialize__register_commands"
            },
            {
              "complexity": 515,
              "file": "ai/chatbot.py",
              "has_docstring": true,
              "name": "_extract_command_from_response"
            },
            {
              "complexity": 510,
              "file": "core/user_data_manager.py",
              "has_docstring": true,
              "name": "update_user_index"
            },
            {
              "complexity": 510,
              "file": "communication/command_handlers/account_handler.py",
              "has_docstring": true,
              "name": "_handle_link_account"
            },
            {
              "complexity": 508,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_create_task"
            },
            {
              "complexity": 495,
              "file": "communication/command_handlers/analytics_handler.py",
              "has_docstring": true,
              "name": "_handle_checkin_analysis"
            },
            {
              "complexity": 491,
              "file": "core/user_data_manager.py",
              "has_docstring": true,
              "name": "rebuild_full_index"
            },
            {
              "complexity": 490,
              "file": "core/headless_service.py",
              "has_docstring": true,
              "name": "start_headless_service"
            },
            {
              "complexity": 490,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "_check_discord_status"
            },
            {
              "complexity": 485,
              "file": "core/logger.py",
              "has_docstring": true,
              "name": "get_component_logger"
            },
            {
              "complexity": 477,
              "file": "communication/core/channel_orchestrator.py",
              "has_docstring": true,
              "name": "handle_task_reminder"
            },
            {
              "complexity": 475,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "connect_signals"
            },
            {
              "complexity": 468,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "schedule_all_users_immediately"
            },
            {
              "complexity": 466,
              "file": "core/checkin_analytics.py",
              "has_docstring": true,
              "name": "get_sleep_analysis"
            },
            {
              "complexity": 464,
              "file": "core/checkin_dynamic_manager.py",
              "has_docstring": true,
              "name": "_parse_numerical_response"
            },
            {
              "complexity": 455,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "send_task_reminder"
            },
            {
              "complexity": 451,
              "file": "ui/dialogs/channel_management_dialog.py",
              "has_docstring": false,
              "name": "save_channel_settings"
            },
            {
              "complexity": 448,
              "file": "communication/core/channel_orchestrator.py",
              "has_docstring": true,
              "name": "_email_polling_loop"
            },
            {
              "complexity": 446,
              "file": "core/user_data_validation.py",
              "has_docstring": true,
              "name": "validate_schedule_periods"
            },
            {
              "complexity": 445,
              "file": "core/message_management.py",
              "has_docstring": true,
              "name": "get_recent_messages"
            },
            {
              "complexity": 435,
              "file": "core/service.py",
              "has_docstring": true,
              "name": "start"
            },
            {
              "complexity": 435,
              "file": "communication/command_handlers/analytics_handler.py",
              "has_docstring": true,
              "name": "_handle_task_analytics"
            },
            {
              "complexity": 432,
              "file": "core/user_data_handlers.py",
              "has_docstring": true,
              "name": "_save_user_data__validate_data"
            },
            {
              "complexity": 431,
              "file": "ui/dialogs/user_analytics_dialog.py",
              "has_docstring": true,
              "name": "load_mood_data"
            },
            {
              "complexity": 430,
              "file": "core/user_data_manager.py",
              "has_docstring": true,
              "name": "update_message_references"
            },
            {
              "complexity": 429,
              "file": "communication/command_handlers/task_handler.py",
              "has_docstring": true,
              "name": "_handle_create_task"
            },
            {
              "complexity": 429,
              "file": "tasks/task_management.py",
              "has_docstring": true,
              "name": "update_task"
            },
            {
              "complexity": 425,
              "file": "communication/message_processing/conversation_flow_manager.py",
              "has_docstring": true,
              "name": "_handle_checkin"
            },
            {
              "complexity": 424,
              "file": "communication/communication_channels/discord/bot.py",
              "has_docstring": true,
              "name": "_start_ngrok_tunnel"
            },
            {
              "complexity": 423,
              "file": "communication/message_processing/command_parser.py",
              "has_docstring": true,
              "name": "_ai_enhanced_parse"
            },
            {
              "complexity": 423,
              "file": "ui/dialogs/task_edit_dialog.py",
              "has_docstring": true,
              "name": "load_task_data"
            },
            {
              "complexity": 422,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "schedule_message_at_random_time"
            },
            {
              "complexity": 418,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "send_checkin_prompt"
            },
            {
              "complexity": 413,
              "file": "ui/dialogs/user_analytics_dialog.py",
              "has_docstring": true,
              "name": "load_overview_data"
            },
            {
              "complexity": 408,
              "file": "ui/dialogs/process_watcher_dialog.py",
              "has_docstring": true,
              "name": "show_process_details"
            },
            {
              "complexity": 407,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "clear_all_accumulated_jobs"
            },
            {
              "complexity": 407,
              "file": "communication/core/channel_orchestrator.py",
              "has_docstring": true,
              "name": "send_message_sync"
            },
            {
              "complexity": 405,
              "file": "communication/communication_channels/discord/task_reminder_view.py",
              "has_docstring": true,
              "name": "get_task_reminder_view"
            },
            {
              "complexity": 403,
              "file": "core/message_management.py",
              "has_docstring": true,
              "name": "archive_old_messages"
            },
            {
              "complexity": 403,
              "file": "communication/communication_channels/discord/bot.py",
              "has_docstring": true,
              "name": "_create_discord_embed"
            },
            {
              "complexity": 402,
              "file": "core/checkin_analytics.py",
              "has_docstring": true,
              "name": "get_mood_trends"
            },
            {
              "complexity": 402,
              "file": "core/checkin_analytics.py",
              "has_docstring": true,
              "name": "get_energy_trends"
            },
            {
              "complexity": 400,
              "file": "core/user_data_handlers.py",
              "has_docstring": true,
              "name": "update_user_preferences"
            },
            {
              "complexity": 400,
              "file": "ui/dialogs/category_management_dialog.py",
              "has_docstring": true,
              "name": "save_category_settings"
            },
            {
              "complexity": 400,
              "file": "tasks/task_management.py",
              "has_docstring": true,
              "name": "_create_next_recurring_task_instance"
            },
            {
              "complexity": 399,
              "file": "core/user_data_manager.py",
              "has_docstring": true,
              "name": "backup_user_data"
            },
            {
              "complexity": 397,
              "file": "core/file_operations.py",
              "has_docstring": true,
              "name": "load_json_data"
            },
            {
              "complexity": 397,
              "file": "tasks/task_management.py",
              "has_docstring": true,
              "name": "complete_task"
            },
            {
              "complexity": 394,
              "file": "core/error_handling.py",
              "has_docstring": true,
              "name": "handle_error"
            },
            {
              "complexity": 389,
              "file": "core/user_data_manager.py",
              "has_docstring": true,
              "name": "export_user_data"
            },
            {
              "complexity": 388,
              "file": "core/message_analytics.py",
              "has_docstring": true,
              "name": "get_message_frequency"
            },
            {
              "complexity": 388,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_add_schedule_period"
            },
            {
              "complexity": 388,
              "file": "communication/command_handlers/schedule_handler.py",
              "has_docstring": true,
              "name": "_handle_add_schedule_period"
            },
            {
              "complexity": 386,
              "file": "communication/message_processing/conversation_flow_manager.py",
              "has_docstring": true,
              "name": "_select_checkin_questions_with_weighting"
            },
            {
              "complexity": 382,
              "file": "ui/dialogs/process_watcher_dialog.py",
              "has_docstring": true,
              "name": "update_mhm_processes"
            },
            {
              "complexity": 382,
              "file": "ui/widgets/dynamic_list_field.py",
              "has_docstring": true,
              "name": "__init__"
            },
            {
              "complexity": 381,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "cleanup_task_reminders"
            },
            {
              "complexity": 380,
              "file": "core/checkin_dynamic_manager.py",
              "has_docstring": true,
              "name": "validate_answer"
            },
            {
              "complexity": 380,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "get_random_time_within_period"
            },
            {
              "complexity": 380,
              "file": "core/user_data_validation.py",
              "has_docstring": true,
              "name": "validate_new_user_data"
            },
            {
              "complexity": 380,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "view_all_users_summary"
            },
            {
              "complexity": 376,
              "file": "ui/dialogs/task_edit_dialog.py",
              "has_docstring": true,
              "name": "save_task"
            },
            {
              "complexity": 375,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "run_daily_scheduler"
            },
            {
              "complexity": 375,
              "file": "ui/dialogs/user_analytics_dialog.py",
              "has_docstring": true,
              "name": "load_sleep_data"
            },
            {
              "complexity": 374,
              "file": "ui/dialogs/process_watcher_dialog.py",
              "has_docstring": true,
              "name": "update_all_processes"
            },
            {
              "complexity": 372,
              "file": "communication/communication_channels/discord/bot.py",
              "has_docstring": true,
              "name": "_check_dns_resolution"
            },
            {
              "complexity": 371,
              "file": "communication/message_processing/command_parser.py",
              "has_docstring": false,
              "name": "__init__"
            },
            {
              "complexity": 368,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "_check_email_status"
            },
            {
              "complexity": 367,
              "file": "core/logger.py",
              "has_docstring": true,
              "name": "_get_log_paths_for_environment"
            },
            {
              "complexity": 366,
              "file": "core/user_management.py",
              "has_docstring": true,
              "name": "_get_user_data__load_account"
            },
            {
              "complexity": 366,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_complete_task"
            },
            {
              "complexity": 366,
              "file": "ui/dialogs/channel_management_dialog.py",
              "has_docstring": true,
              "name": "__init__"
            },
            {
              "complexity": 364,
              "file": "communication/command_handlers/analytics_handler.py",
              "has_docstring": true,
              "name": "_handle_task_stats"
            },
            {
              "complexity": 361,
              "file": "communication/__init__.py",
              "has_docstring": true,
              "name": "__getattr__"
            },
            {
              "complexity": 361,
              "file": "ui/dialogs/task_management_dialog.py",
              "has_docstring": true,
              "name": "save_task_settings"
            },
            {
              "complexity": 360,
              "file": "communication/core/channel_orchestrator.py",
              "has_docstring": true,
              "name": "_initialize_channel_with_retry_sync"
            },
            {
              "complexity": 360,
              "file": "ai/context_builder.py",
              "has_docstring": true,
              "name": "analyze_context"
            },
            {
              "complexity": 359,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "schedule_message_for_period"
            },
            {
              "complexity": 358,
              "file": "communication/communication_channels/discord/bot.py",
              "has_docstring": true,
              "name": "_stop_ngrok_tunnel"
            },
            {
              "complexity": 357,
              "file": "communication/communication_channels/discord/checkin_view.py",
              "has_docstring": true,
              "name": "get_checkin_view"
            },
            {
              "complexity": 355,
              "file": "ai/chatbot.py",
              "has_docstring": true,
              "name": "_detect_mode"
            },
            {
              "complexity": 353,
              "file": "core/file_operations.py",
              "has_docstring": true,
              "name": "save_json_data"
            },
            {
              "complexity": 349,
              "file": "ui/dialogs/checkin_management_dialog.py",
              "has_docstring": true,
              "name": "save_checkin_settings"
            },
            {
              "complexity": 345,
              "file": "core/config.py",
              "has_docstring": true,
              "name": "validate_logging_configuration"
            },
            {
              "complexity": 345,
              "file": "core/file_operations.py",
              "has_docstring": true,
              "name": "determine_file_path"
            },
            {
              "complexity": 345,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "schedule_all_task_reminders"
            },
            {
              "complexity": 345,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_complete_task__find_task_by_identifier"
            },
            {
              "complexity": 344,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_task_stats"
            },
            {
              "complexity": 342,
              "file": "core/message_management.py",
              "has_docstring": true,
              "name": "ensure_user_message_files"
            },
            {
              "complexity": 340,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_delete_task"
            },
            {
              "complexity": 339,
              "file": "core/backup_manager.py",
              "has_docstring": true,
              "name": "_cleanup_old_backups"
            },
            {
              "complexity": 339,
              "file": "core/message_management.py",
              "has_docstring": true,
              "name": "create_message_file_from_defaults"
            },
            {
              "complexity": 338,
              "file": "communication/command_handlers/task_handler.py",
              "has_docstring": true,
              "name": "_handle_task_stats"
            },
            {
              "complexity": 337,
              "file": "ui/dialogs/account_creator_dialog.py",
              "has_docstring": true,
              "name": "_validate_and_accept__update_user_index"
            },
            {
              "complexity": 336,
              "file": "ui/dialogs/user_analytics_dialog.py",
              "has_docstring": true,
              "name": "load_habits_data"
            },
            {
              "complexity": 335,
              "file": "core/user_data_manager.py",
              "has_docstring": true,
              "name": "get_user_summary"
            },
            {
              "complexity": 335,
              "file": "ui/dialogs/task_crud_dialog.py",
              "has_docstring": true,
              "name": "refresh_active_tasks"
            },
            {
              "complexity": 334,
              "file": "ui/dialogs/message_editor_dialog.py",
              "has_docstring": true,
              "name": "populate_table"
            },
            {
              "complexity": 332,
              "file": "ui/dialogs/task_edit_dialog.py",
              "has_docstring": true,
              "name": "collect_reminder_periods"
            },
            {
              "complexity": 330,
              "file": "core/user_management.py",
              "has_docstring": true,
              "name": "_get_user_data__load_preferences"
            },
            {
              "complexity": 330,
              "file": "ui/dialogs/account_creator_dialog.py",
              "has_docstring": true,
              "name": "setup_connections"
            },
            {
              "complexity": 328,
              "file": "communication/communication_channels/email/bot.py",
              "has_docstring": true,
              "name": "_receive_emails_sync__extract_body"
            },
            {
              "complexity": 327,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "schedule_daily_message_job"
            },
            {
              "complexity": 326,
              "file": "core/user_data_manager.py",
              "has_docstring": true,
              "name": "get_user_info_for_data_manager"
            },
            {
              "complexity": 325,
              "file": "core/user_data_validation.py",
              "has_docstring": true,
              "name": "validate_personalization_data"
            },
            {
              "complexity": 324,
              "file": "ui/widgets/tag_widget.py",
              "has_docstring": true,
              "name": "edit_tag"
            },
            {
              "complexity": 323,
              "file": "communication/command_handlers/task_handler.py",
              "has_docstring": true,
              "name": "_find_task_by_identifier"
            },
            {
              "complexity": 318,
              "file": "core/user_management.py",
              "has_docstring": true,
              "name": "_get_user_data__load_context"
            },
            {
              "complexity": 315,
              "file": "core/user_management.py",
              "has_docstring": true,
              "name": "_get_user_data__load_schedules"
            },
            {
              "complexity": 314,
              "file": "core/service_utilities.py",
              "has_docstring": true,
              "name": "get_service_processes"
            },
            {
              "complexity": 313,
              "file": "core/logger.py",
              "has_docstring": true,
              "name": "compress_old_logs"
            },
            {
              "complexity": 313,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_update_task"
            },
            {
              "complexity": 313,
              "file": "ui/widgets/dynamic_list_container.py",
              "has_docstring": true,
              "name": "__init__"
            },
            {
              "complexity": 312,
              "file": "core/scheduler.py",
              "has_docstring": false,
              "name": "scheduler_loop"
            },
            {
              "complexity": 312,
              "file": "communication/message_processing/conversation_flow_manager.py",
              "has_docstring": true,
              "name": "handle_inbound_message"
            },
            {
              "complexity": 312,
              "file": "communication/communication_channels/discord/bot.py",
              "has_docstring": true,
              "name": "_check_network_connectivity"
            },
            {
              "complexity": 312,
              "file": "ui/dialogs/task_crud_dialog.py",
              "has_docstring": true,
              "name": "refresh_completed_tasks"
            },
            {
              "complexity": 311,
              "file": "core/user_data_handlers.py",
              "has_docstring": true,
              "name": "_save_user_data__check_cross_file_invariants"
            },
            {
              "complexity": 310,
              "file": "ui/dialogs/user_profile_dialog.py",
              "has_docstring": true,
              "name": "add_loved_one_widget"
            },
            {
              "complexity": 309,
              "file": "ai/chatbot.py",
              "has_docstring": true,
              "name": "_clean_system_prompt_leaks"
            },
            {
              "complexity": 308,
              "file": "core/logger.py",
              "has_docstring": true,
              "name": "get_log_file_info"
            },
            {
              "complexity": 308,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "cleanup_orphaned_task_reminders"
            },
            {
              "complexity": 308,
              "file": "core/user_data_manager.py",
              "has_docstring": true,
              "name": "remove_from_index"
            },
            {
              "complexity": 308,
              "file": "communication/command_handlers/account_handler.py",
              "has_docstring": true,
              "name": "_handle_create_account"
            },
            {
              "complexity": 308,
              "file": "ui/widgets/period_row_widget.py",
              "has_docstring": true,
              "name": "__init__"
            },
            {
              "complexity": 307,
              "file": "ui/dialogs/user_profile_dialog.py",
              "has_docstring": true,
              "name": "create_custom_field_list"
            },
            {
              "complexity": 306,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_delete_task__find_task_by_identifier"
            },
            {
              "complexity": 306,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_update_task__find_task_by_identifier"
            },
            {
              "complexity": 304,
              "file": "core/user_data_manager.py",
              "has_docstring": true,
              "name": "delete_user_completely"
            },
            {
              "complexity": 303,
              "file": "core/user_management.py",
              "has_docstring": true,
              "name": "ensure_category_has_default_schedule"
            },
            {
              "complexity": 302,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "schedule_checkin_at_exact_time"
            },
            {
              "complexity": 302,
              "file": "communication/command_handlers/analytics_handler.py",
              "has_docstring": true,
              "name": "_handle_quant_summary"
            },
            {
              "complexity": 301,
              "file": "communication/message_processing/conversation_flow_manager.py",
              "has_docstring": true,
              "name": "_handle_command_during_checkin"
            },
            {
              "complexity": 301,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "view_cache_status"
            },
            {
              "complexity": 299,
              "file": "communication/message_processing/interaction_manager.py",
              "has_docstring": true,
              "name": "_enhance_response_with_ai"
            },
            {
              "complexity": 299,
              "file": "run_tests.py",
              "has_docstring": true,
              "name": "parse_junit_xml"
            },
            {
              "complexity": 298,
              "file": "core/error_handling.py",
              "has_docstring": true,
              "name": "handle_errors"
            },
            {
              "complexity": 298,
              "file": "ui/dialogs/task_management_dialog.py",
              "has_docstring": true,
              "name": "__init__"
            },
            {
              "complexity": 298,
              "file": "ui/widgets/tag_widget.py",
              "has_docstring": true,
              "name": "delete_tag"
            },
            {
              "complexity": 297,
              "file": "core/logger.py",
              "has_docstring": true,
              "name": "cleanup_old_logs"
            },
            {
              "complexity": 297,
              "file": "core/scheduler.py",
              "has_docstring": true,
              "name": "schedule_task_reminder_at_time"
            },
            {
              "complexity": 297,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_status"
            },
            {
              "complexity": 296,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_quant_summary"
            },
            {
              "complexity": 295,
              "file": "core/config.py",
              "has_docstring": true,
              "name": "validate_all_configuration"
            },
            {
              "complexity": 295,
              "file": "core/config.py",
              "has_docstring": true,
              "name": "print_configuration_report"
            },
            {
              "complexity": 295,
              "file": "communication/command_handlers/interaction_handlers.py",
              "has_docstring": true,
              "name": "_handle_show_analytics"
            },
            {
              "complexity": 295,
              "file": "communication/core/retry_manager.py",
              "has_docstring": true,
              "name": "_process_retry_queue"
            },
            {
              "complexity": 294,
              "file": "ui/widgets/dynamic_list_container.py",
              "has_docstring": false,
              "name": "_deduplicate_values"
            },
            {
              "complexity": 294,
              "file": "ui/widgets/user_profile_settings_widget.py",
              "has_docstring": true,
              "name": "set_checkbox_group"
            },
            {
              "complexity": 292,
              "file": "core/user_data_handlers.py",
              "has_docstring": true,
              "name": "_save_user_data__normalize_data"
            },
            {
              "complexity": 291,
              "file": "communication/command_handlers/analytics_handler.py",
              "has_docstring": true,
              "name": "_handle_show_analytics"
            },
            {
              "complexity": 289,
              "file": "ui/widgets/tag_widget.py",
              "has_docstring": true,
              "name": "add_tag"
            },
            {
              "complexity": 288,
              "file": "ui/ui_app_qt.py",
              "has_docstring": true,
              "name": "start_service"
            }
          ],
          "high_complexity_total": 1078,
          "undocumented_handlers": [
            {
              "file": "communication/command_handlers/account_handler.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/account_handler.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/analytics_handler.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/analytics_handler.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/analytics_handler.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/analytics_handler.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/checkin_handler.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/checkin_handler.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/checkin_handler.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/checkin_handler.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/profile_handler.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/profile_handler.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/profile_handler.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/profile_handler.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/schedule_handler.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/schedule_handler.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/schedule_handler.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/schedule_handler.py",
              "name": "handle"
            },
            {
              "file": "communication/command_handlers/task_handler.py",
              "name": "can_handle"
            },
            {
              "file": "communication/command_handlers/task_handler.py",
              "name": "get_examples"
            },
            {
              "file": "communication/command_handlers/task_handler.py",
              "name": "get_help"
            },
            {
              "file": "communication/command_handlers/task_handler.py",
              "name": "handle"
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "name": "create_view"
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "name": "create_view"
            },
            {
              "file": "core/schemas.py",
              "name": "_validate_email"
            },
            {
              "file": "core/schemas.py",
              "name": "_validate_timezone"
            },
            {
              "file": "core/schemas.py",
              "name": "validate_account_dict"
            },
            {
              "file": "core/schemas.py",
              "name": "validate_messages_file_dict"
            },
            {
              "file": "core/schemas.py",
              "name": "validate_preferences_dict"
            },
            {
              "file": "core/schemas.py",
              "name": "validate_schedules_dict"
            },
            {
              "file": "core/user_data_validation.py",
              "name": "validate_schedule_periods__validate_time_format"
            },
            {
              "file": "core/user_management.py",
              "name": "_ensure_default_loaders_once"
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "name": "on_personalization_save"
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "name": "get_selected_categories"
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "name": "set_selected_categories"
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "name": "get_selected_channel"
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "name": "save_channel_settings"
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "name": "set_selected_channel"
            },
            {
              "file": "ui/dialogs/checkin_management_dialog.py",
              "name": "on_enable_checkins_toggled"
            },
            {
              "file": "ui/dialogs/task_management_dialog.py",
              "name": "get_statistics"
            },
            {
              "file": "ui/dialogs/task_management_dialog.py",
              "name": "on_enable_task_management_toggled"
            },
            {
              "file": "ui/ui_app_qt.py",
              "name": "on_save"
            },
            {
              "file": "ui/widgets/category_selection_widget.py",
              "name": "get_selected_categories"
            },
            {
              "file": "ui/widgets/category_selection_widget.py",
              "name": "set_selected_categories"
            },
            {
              "file": "ui/widgets/channel_selection_widget.py",
              "name": "get_selected_channel"
            },
            {
              "file": "ui/widgets/channel_selection_widget.py",
              "name": "set_contact_info"
            },
            {
              "file": "ui/widgets/channel_selection_widget.py",
              "name": "set_selected_channel"
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "name": "_on_preset_toggled"
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "name": "_on_row_deleted"
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "name": "get_values"
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "name": "set_values"
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "name": "_on_delete"
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "name": "get_text"
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "name": "is_checked"
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "name": "set_checked"
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "name": "set_text"
            },
            {
              "file": "ui/widgets/task_settings_widget.py",
              "name": "load_existing_data"
            }
          ],
          "undocumented_handlers_total": 81,
          "undocumented_other": [
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "name": "__init__"
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "name": "__init__"
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "name": "__init__"
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "name": "__init__"
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "name": "__init__"
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "name": "__init__"
            },
            {
              "file": "communication/communication_channels/discord/checkin_view.py",
              "name": "__init__"
            },
            {
              "file": "communication/communication_channels/discord/task_reminder_view.py",
              "name": "__init__"
            },
            {
              "file": "communication/communication_channels/discord/welcome_handler.py",
              "name": "__init__"
            },
            {
              "file": "communication/message_processing/command_parser.py",
              "name": "__init__"
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "name": "start_analytics_flow"
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "name": "start_messages_flow"
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "name": "start_profile_flow"
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "name": "start_schedule_flow"
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "name": "__init__"
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "name": "_augment_suggestions"
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "name": "parse_due"
            },
            {
              "file": "core/error_handling.py",
              "name": "decorator"
            },
            {
              "file": "core/error_handling.py",
              "name": "wrapper"
            },
            {
              "file": "core/file_auditor.py",
              "name": "__init__"
            },
            {
              "file": "core/file_auditor.py",
              "name": "_classify_path"
            },
            {
              "file": "core/file_auditor.py",
              "name": "_split_env_list"
            },
            {
              "file": "core/file_auditor.py",
              "name": "critical"
            },
            {
              "file": "core/file_auditor.py",
              "name": "debug"
            },
            {
              "file": "core/file_auditor.py",
              "name": "error"
            },
            {
              "file": "core/file_auditor.py",
              "name": "info"
            },
            {
              "file": "core/file_auditor.py",
              "name": "start_auditor"
            },
            {
              "file": "core/file_auditor.py",
              "name": "stop_auditor"
            },
            {
              "file": "core/file_auditor.py",
              "name": "warning"
            },
            {
              "file": "core/logger.py",
              "name": "__init__"
            },
            {
              "file": "core/logger.py",
              "name": "__init__"
            },
            {
              "file": "core/logger.py",
              "name": "__init__"
            },
            {
              "file": "core/logger.py",
              "name": "__init__"
            },
            {
              "file": "core/logger.py",
              "name": "__init__"
            },
            {
              "file": "core/logger.py",
              "name": "__init__"
            },
            {
              "file": "core/logger.py",
              "name": "critical"
            },
            {
              "file": "core/logger.py",
              "name": "debug"
            },
            {
              "file": "core/logger.py",
              "name": "error"
            },
            {
              "file": "core/logger.py",
              "name": "filter"
            },
            {
              "file": "core/logger.py",
              "name": "filter"
            },
            {
              "file": "core/logger.py",
              "name": "format"
            },
            {
              "file": "core/logger.py",
              "name": "info"
            },
            {
              "file": "core/logger.py",
              "name": "warning"
            },
            {
              "file": "core/schedule_management.py",
              "name": "add_schedule_period"
            },
            {
              "file": "core/schedule_management.py",
              "name": "edit_schedule_period"
            },
            {
              "file": "core/schedule_management.py",
              "name": "sort_key"
            },
            {
              "file": "core/scheduler.py",
              "name": "scheduler_loop"
            },
            {
              "file": "core/schemas.py",
              "name": "_accept_legacy_shape"
            },
            {
              "file": "core/schemas.py",
              "name": "_coerce_bool"
            },
            {
              "file": "core/schemas.py",
              "name": "_normalize_contact"
            },
            {
              "file": "core/schemas.py",
              "name": "_normalize_days"
            },
            {
              "file": "core/schemas.py",
              "name": "_normalize_flags"
            },
            {
              "file": "core/schemas.py",
              "name": "_normalize_periods"
            },
            {
              "file": "core/schemas.py",
              "name": "_valid_days"
            },
            {
              "file": "core/schemas.py",
              "name": "_valid_time"
            },
            {
              "file": "core/schemas.py",
              "name": "to_dict"
            },
            {
              "file": "core/user_data_validation.py",
              "name": "is_valid_email"
            },
            {
              "file": "core/user_data_validation.py",
              "name": "is_valid_phone"
            },
            {
              "file": "ui/dialogs/schedule_editor_dialog.py",
              "name": "sort_key"
            },
            {
              "file": "ui/ui_app_qt.py",
              "name": "cleanup_old_requests"
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "name": "_add_blank_row"
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "name": "_deduplicate_values"
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "name": "_ensure_single_blank_row"
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "name": "_first_blank_index"
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "name": "_on_row_edited"
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "name": "is_blank"
            }
          ],
          "undocumented_other_total": 66
        },
        "coverage": 93.6,
        "errors": [],
        "extra": {
          "count": 0,
          "files": {}
        },
        "missing": {
          "count": 0,
          "files": {},
          "missing_files": []
        },
        "registry_sections": {},
        "totals": {
          "classes_found": 159,
          "files_scanned": 107,
          "functions_documented": 1404,
          "functions_found": 1500
        }
      },
      "timestamp": "2025-12-04T07:33:48.029875"
    },
    "analyze_documentation": {
      "success": true,
      "data": {
        "artifacts": [],
        "consolidation_recommendations": [
          {
            "category": "Development Workflow",
            "files": [
              "DOCUMENTATION_GUIDE.md",
              "DEVELOPMENT_WORKFLOW.md",
              "development_docs/CHANGELOG_DETAIL.md",
              "development_docs/PLANS.md",
              "ai_development_docs/AI_SESSION_STARTER.md",
              "ai_development_docs/AI_ARCHITECTURE.md",
              "ai_development_docs/AI_DOCUMENTATION_GUIDE.md",
              "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md",
              "ai_development_docs/AI_CHANGELOG.md",
              "ai_development_docs/AI_REFERENCE.md",
              "ai_development_docs/AI_LOGGING_GUIDE.md",
              "ai_development_docs/AI_TESTING_GUIDE.md",
              "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
              "ai_development_docs/AI_LEGACY_REMOVAL_GUIDE.md",
              "core/ERROR_HANDLING_GUIDE.md",
              "logs/LOGGING_GUIDE.md",
              "tests/TESTING_GUIDE.md",
              "tests/MANUAL_TESTING_GUIDE.md",
              "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md",
              "tests/MANUAL_DISCORD_TEST_GUIDE.md",
              "communication/COMMUNICATION_GUIDE.md",
              "communication/communication_channels/discord/DISCORD_GUIDE.md",
              "scripts/SCRIPTS_GUIDE.md",
              "ui/UI_GUIDE.md",
              "ai/SYSTEM_AI_GUIDE.md",
              "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
              "development_tools/DEVELOPMENT_TOOLS_GUIDE.md"
            ],
            "priority": "medium",
            "similarity_score": 0.1921182841899749,
            "suggestion": "Consider consolidating into a single DEVELOPMENT_WORKFLOW.md"
          },
          {
            "category": "Testing",
            "files": [
              "ai_development_docs/AI_TESTING_GUIDE.md",
              "tests/TESTING_GUIDE.md",
              "tests/MANUAL_TESTING_GUIDE.md"
            ],
            "priority": "medium",
            "similarity_score": 0.29697863023774945,
            "suggestion": "Consider consolidating testing documentation into a single TESTING_GUIDE.md"
          },
          {
            "category": "High Section Overlap",
            "files": [
              "DOCUMENTATION_GUIDE.md",
              "ai_development_docs/AI_DOCUMENTATION_GUIDE.md"
            ],
            "overlap_count": 8,
            "priority": "high",
            "suggestion": "These files share 8 sections - consider consolidating or clearly differentiating their purposes"
          },
          {
            "category": "High Section Overlap",
            "files": [
              "DEVELOPMENT_WORKFLOW.md",
              "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
            ],
            "overlap_count": 9,
            "priority": "high",
            "suggestion": "These files share 9 sections - consider consolidating or clearly differentiating their purposes"
          },
          {
            "category": "High Section Overlap",
            "files": [
              "ARCHITECTURE.md",
              "ai_development_docs/AI_ARCHITECTURE.md"
            ],
            "overlap_count": 7,
            "priority": "high",
            "suggestion": "These files share 7 sections - consider consolidating or clearly differentiating their purposes"
          },
          {
            "category": "High Section Overlap",
            "files": [
              "ai_development_docs/AI_CHANGELOG.md",
              "development_docs/CHANGELOG_DETAIL.md"
            ],
            "overlap_count": 94,
            "priority": "high",
            "suggestion": "These files share 94 sections - consider consolidating or clearly differentiating their purposes"
          },
          {
            "category": "High Section Overlap",
            "files": [
              "ai_development_docs/AI_LOGGING_GUIDE.md",
              "logs/LOGGING_GUIDE.md"
            ],
            "overlap_count": 10,
            "priority": "high",
            "suggestion": "These files share 10 sections - consider consolidating or clearly differentiating their purposes"
          },
          {
            "category": "High Section Overlap",
            "files": [
              "ai_development_docs/AI_TESTING_GUIDE.md",
              "tests/TESTING_GUIDE.md"
            ],
            "overlap_count": 8,
            "priority": "high",
            "suggestion": "These files share 8 sections - consider consolidating or clearly differentiating their purposes"
          },
          {
            "category": "High Section Overlap",
            "files": [
              "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
              "development_tools/DEVELOPMENT_TOOLS_GUIDE.md"
            ],
            "overlap_count": 7,
            "priority": "high",
            "suggestion": "These files share 7 sections - consider consolidating or clearly differentiating their purposes"
          },
          {
            "category": "High Section Overlap",
            "files": [
              "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
              "core/ERROR_HANDLING_GUIDE.md"
            ],
            "overlap_count": 11,
            "priority": "high",
            "suggestion": "These files share 11 sections - consider consolidating or clearly differentiating their purposes"
          }
        ],
        "documents": [
          "ARCHITECTURE.md",
          "DEVELOPMENT_WORKFLOW.md",
          "DOCUMENTATION_GUIDE.md",
          "HOW_TO_RUN.md",
          "PROJECT_VISION.md",
          "README.md",
          "TODO.md",
          "ai/SYSTEM_AI_GUIDE.md",
          "ai_development_docs/AI_ARCHITECTURE.md",
          "ai_development_docs/AI_CHANGELOG.md",
          "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md",
          "ai_development_docs/AI_DOCUMENTATION_GUIDE.md",
          "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
          "ai_development_docs/AI_LEGACY_REMOVAL_GUIDE.md",
          "ai_development_docs/AI_LOGGING_GUIDE.md",
          "ai_development_docs/AI_REFERENCE.md",
          "ai_development_docs/AI_SESSION_STARTER.md",
          "ai_development_docs/AI_TESTING_GUIDE.md",
          "communication/COMMUNICATION_GUIDE.md",
          "communication/communication_channels/discord/DISCORD_GUIDE.md",
          "core/ERROR_HANDLING_GUIDE.md",
          "development_docs/CHANGELOG_DETAIL.md",
          "development_docs/PLANS.md",
          "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
          "development_tools/DEVELOPMENT_TOOLS_GUIDE.md",
          "logs/LOGGING_GUIDE.md",
          "scripts/SCRIPTS_GUIDE.md",
          "tests/MANUAL_DISCORD_TEST_GUIDE.md",
          "tests/MANUAL_TESTING_GUIDE.md",
          "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md",
          "tests/TESTING_GUIDE.md",
          "ui/UI_GUIDE.md"
        ],
        "duplicates": [],
        "file_purposes": {
          "ARCHITECTURE.md": {
            "content_length": 17367,
            "header_info": "# MHM System Architecture\n\n> **File**: `ARCHITECTURE.md`\n> **Audience**: Human developers building or maintaining the platform  \n> **Purpose**: Explain system design, module responsibilities, and data",
            "section_count": 9,
            "sections": [
              "Introduction",
              "1. Directory Overview",
              "2. User Data Model",
              "2.1. User Data Flow Diagram",
              "3. Data Handling Patterns",
              "4. Key Modules and Responsibilities",
              "5. UI Architecture and Naming Conventions",
              "6. Channel-Agnostic Architecture",
              "7. Development Notes"
            ]
          },
          "DEVELOPMENT_WORKFLOW.md": {
            "content_length": 14260,
            "header_info": "# Development Workflow Guide\n\n> **File**: `DEVELOPMENT_WORKFLOW.md`  \n> **Audience**: Human developer (beginner programmer)  \n> **Purpose**: Safe development practices and day-to-day procedures  \n> **",
            "section_count": 32,
            "sections": [
              "Introduction",
              "Quick Reference",
              "1. Safety First",
              "1.1. Core Safety Rules",
              "1.2. Pre-Change Checklist",
              "2. Virtual Environment Best Practices",
              "2.1. Why It Matters",
              "2.2. Common Commands (PowerShell)",
              "2.3. Troubleshooting",
              "2.4. Configuration and .env"
            ]
          },
          "DOCUMENTATION_GUIDE.md": {
            "content_length": 30991,
            "header_info": "# Documentation Guide\n\n> **File**: `DOCUMENTATION_GUIDE.md`  \n> **Audience**: Developers and contributors  \n> **Purpose**: Organize, author, and maintain project documentation  \n> **Style**: Comprehen",
            "section_count": 32,
            "sections": [
              "Introduction",
              "Quick Reference",
              "1. Documentation Categories",
              "1.1. Human-facing documents",
              "1.2. AI-facing documents",
              "1.3. Generated and analytical documentation",
              "2. Standards and Templates",
              "2.1. Metadata for `.md` documentation files",
              "2.2. Metadata for `.mdc` rule files",
              "2.3. Structure guidelines for human-facing docs"
            ]
          },
          "HOW_TO_RUN.md": {
            "content_length": 7400,
            "header_info": "# How to Run MHM\n\n\n> **File**: `HOW_TO_RUN.md`\n> **Audience**: New users and developers setting up the project  \n> **Purpose**: Step-by-step setup and launch instructions  \n> **Style**: Clear, beginne",
            "section_count": 18,
            "sections": [
              "Introduction",
              "1. Quick Start (Recommended)",
              "1.1. Step 1: Set up Virtual Environment",
              "1.2. Step 2: Install Dependencies",
              "1.3. Step 3: Install the Project in Editable Mode",
              "1.4. Step 4: Configure Environment (Optional)",
              "1.5. Step 5: Launch the Application",
              "2. Alternative Commands",
              "3. Command Reference (Discord and Chat)",
              "4. Alternative Launch Methods"
            ]
          },
          "PROJECT_VISION.md": {
            "content_length": 9684,
            "header_info": "# MHM Project Vision\n\n\n> **File**: `PROJECT_VISION.md`\n> **Audience**: Human Developer, AI Collaborators, and Future Contributors  \n> **Purpose**: Overarching vision, mission, and long-term direction ",
            "section_count": 42,
            "sections": [
              "Introduction",
              "1. **Core Vision**",
              "2. **Mission Statement**",
              "3. **Core Values**",
              "3.1. **1. Personalized & Adaptive**",
              "3.2. **2. Hope-Focused & Supportive**",
              "3.3. **3. Executive Functioning Support**",
              "4. **Target Users**",
              "4.1. **Primary User: You**",
              "4.2. **Secondary Users: Similar Individuals**"
            ]
          },
          "README.md": {
            "content_length": 14109,
            "header_info": "# Mental Health Management (MHM)\n\n> **File**: `README.md`\n> **Audience**: Human developer (beginner-friendly)\n> **Purpose**: High-level overview, navigation, and quick start\n> **Style**: Beginner-frie",
            "section_count": 17,
            "sections": [
              "Introduction",
              "1. Navigation",
              "2. Features",
              "3. Quick Start",
              "3.1. Headless service commands",
              "4. Documentation",
              "5. Architecture",
              "6. AI Integration (Optional)",
              "7. Project Structure",
              "8. License and privacy"
            ]
          },
          "TODO.md": {
            "content_length": 23515,
            "header_info": "# TODO.md - MHM Project Tasks\n\n> **File**: `TODO.md`\n> **Audience**: Human Developer (Beginner Programmer) and AI collaborators\n> **Purpose**: Current development priorities and planned improvements  ",
            "section_count": 9,
            "sections": [
              "Introduction",
              "How to Add New TODOs",
              "High Priority",
              "Medium Priority",
              "User Experience Improvements",
              "Performance Optimizations",
              "Low Priority",
              "Documentation",
              "Testing"
            ]
          },
          "ai/SYSTEM_AI_GUIDE.md": {
            "content_length": 17916,
            "header_info": "# AI System Documentation\n\n> **File**: `ai/SYSTEM_AI_GUIDE.md`\n> **Audience**: Developers and AI collaborators working on MHM's AI system\n> **Purpose**: Explain how the AI subsystem is structured, how",
            "section_count": 26,
            "sections": [
              "Introduction",
              "1. Overview",
              "2. High-Level Architecture",
              "2.1. Core components",
              "2.2. Request / response flow (simplified)",
              "3. Entry Points and Modes",
              "3.1. Main runtime entry points",
              "3.2. Modes",
              "3.3. Command parsing integration",
              "4. Context and Conversation State"
            ]
          },
          "ai_development_docs/AI_ARCHITECTURE.md": {
            "content_length": 8754,
            "header_info": "# AI Architecture - Quick Reference\n\n> **File**: `ai_development_docs/AI_ARCHITECTURE.md`  \n> **Pair**: [ARCHITECTURE.md](ARCHITECTURE.md)  \n> **Audience**: AI collaborators and tools  \n> **Purpose**:",
            "section_count": 8,
            "sections": [
              "Introduction",
              "1. Directory Overview",
              "2. User Data Model",
              "3. Data Handling Patterns",
              "4. Key Modules and Responsibilities",
              "5. UI Architecture and Naming Conventions",
              "6. Channel-Agnostic Architecture",
              "7. Development Notes"
            ]
          },
          "ai_development_docs/AI_CHANGELOG.md": {
            "content_length": 123763,
            "header_info": "# AI Changelog - Brief Summary for AI Context\n\n\n> **File**: `ai_development_docs/AI_CHANGELOG.md`\n> **Audience**: AI collaborators (Cursor, Codex, etc.)\n> **Purpose**: Lightweight summary of recent ch",
            "section_count": 148,
            "sections": [
              "Introduction",
              "Overview",
              "How to Update This File",
              "YYYY-MM-DD - Brief Title **COMPLETED**",
              "Recent Changes (Most Recent First)",
              "2025-12-03 - Phase 9 Status Reports Improvements and Tool Integration **COMPLETED**",
              "2025-12-02 - Implemented Shared Mtime-Based Caching for Development Tools **COMPLETED**",
              "2025-12-02 - Fixed Path Drift and Non-ASCII Characters in Documentation **COMPLETED**",
              "2025-12-02 - Completed M7.4 Tool Decomposition Plan **COMPLETED**",
              "2025-12-01 - Development Tools Configuration Cleanup and Fixes **COMPLETED**"
            ]
          },
          "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md": {
            "content_length": 12042,
            "header_info": "# AI Development Workflow Guide\n\n> **File**: `ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md`  \n> **Pair**: [DEVELOPMENT_WORKFLOW.md](DEVELOPMENT_WORKFLOW.md)\n> **Audience**: AI collaborators and tool",
            "section_count": 12,
            "sections": [
              "Introduction",
              "Quick Reference",
              "1. Safety First",
              "2. Virtual Environment Best Practices",
              "3. Development Process",
              "4. Testing Strategy",
              "5. Common Tasks",
              "6. Emergency Procedures",
              "7. Learning Resources",
              "8. Success Tips"
            ]
          },
          "ai_development_docs/AI_DOCUMENTATION_GUIDE.md": {
            "content_length": 13641,
            "header_info": "# AI Documentation Guide\n\n> **File**: `ai_development_docs/AI_DOCUMENTATION_GUIDE.md`\n> **Pair**: [DOCUMENTATION_GUIDE.md](DOCUMENTATION_GUIDE.md)\n> **Audience**: AI collaborators and tools\n> **Purpos",
            "section_count": 19,
            "sections": [
              "Introduction",
              "Quick Reference",
              "1. Documentation Categories",
              "1.1. Detailed documentation",
              "1.2. AI-facing documents",
              "1.3. Generated and analytical docs",
              "2. Standards and Templates",
              "2.1. Heading numbering standard",
              "2.2. Example marking standards",
              "2.3. File path reference standards"
            ]
          },
          "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md": {
            "content_length": 8814,
            "header_info": "# AI Error Handling Guide\n\n> **File**: `ai_development_docs/AI_ERROR_HANDLING_GUIDE.md`  \n> **Pair**: [ERROR_HANDLING_GUIDE.md](core/ERROR_HANDLING_GUIDE.md)  \n> **Audience**: AI collaborators and too",
            "section_count": 12,
            "sections": [
              "Introduction",
              "1. Purpose and Design Principles",
              "2. Architecture Overview",
              "3. Usage Patterns",
              "4. Error Categories and Severity",
              "5. Error Message Guidelines",
              "6. Configuration and Integration",
              "7. Testing Error Handling",
              "8. Monitoring and Debugging",
              "9. Legacy and Migration"
            ]
          },
          "ai_development_docs/AI_LEGACY_REMOVAL_GUIDE.md": {
            "content_length": 6496,
            "header_info": "# AI Legacy Code Removal Guide\n\n> **File**: `ai_development_docs/AI_LEGACY_REMOVAL_GUIDE.md`\n> **Audience**: AI collaborators and developers cleaning up deprecated code paths\n> **Purpose**: Routing an",
            "section_count": 8,
            "sections": [
              "Introduction",
              "Quick Reference",
              "1. Standards",
              "2. Process",
              "3. Checklist",
              "4. Tools",
              "5. Best Practices",
              "6. Success Criteria"
            ]
          },
          "ai_development_docs/AI_LOGGING_GUIDE.md": {
            "content_length": 8089,
            "header_info": "# AI Logging Guide\n\n> **File**: `ai_development_docs/AI_LOGGING_GUIDE.md`  \n> **Pair**: [LOGGING_GUIDE.md](logs/LOGGING_GUIDE.md)  \n> **Audience**: AI assistants and automation tools  \n> **Purpose**: ",
            "section_count": 11,
            "sections": [
              "Introduction",
              "1. Purpose and Scope",
              "2. Logging Architecture",
              "3. Log Levels and When to Use Them",
              "4. Component Log Files and Layout",
              "5. Configuration (Environment Variables)",
              "6. Log Rotation, Backups, and Archival",
              "7. Legacy Compatibility Logging Standard",
              "8. Maintenance and Cleanup",
              "9. Best Practices"
            ]
          },
          "ai_development_docs/AI_REFERENCE.md": {
            "content_length": 3776,
            "header_info": "# AI Reference - Troubleshooting & System Understanding\n\n\n> **File**: `ai_development_docs/AI_REFERENCE.md`\n> **Purpose**: Troubleshooting patterns and deep system understanding for AI collaborators  ",
            "section_count": 13,
            "sections": [
              "Introduction",
              "1. Troubleshooting Patterns",
              "1.1. Documentation appears incomplete",
              "1.2. Code change introduced failures",
              "1.3. PowerShell command failed",
              "1.4. User questions accuracy",
              "2. Communication Patterns",
              "3. System Understanding",
              "3.1. Critical files (do not break)",
              "3.2. Data flow patterns"
            ]
          },
          "ai_development_docs/AI_SESSION_STARTER.md": {
            "content_length": 6935,
            "header_info": "# AI Session Starter\n\n> **File**: `ai_development_docs/AI_SESSION_STARTER.md`\n> **Audience**: AI collaborators working on the MHM codebase and docs\n> **Purpose**: Provide a compact starting point for ",
            "section_count": 8,
            "sections": [
              "Introduction",
              "1. Starting a Session",
              "2. Core AI References",
              "3. Safe Change Rules",
              "4. Typical Tasks and Where to Look",
              "4.1. \"Help me edit documentation\"",
              "4.2. \"Help me refactor or clean up code\"",
              "4.3. \"Help me understand the system\""
            ]
          },
          "ai_development_docs/AI_TESTING_GUIDE.md": {
            "content_length": 9655,
            "header_info": "# AI Testing Guide\n\n> **File**: `ai_development_docs/AI_TESTING_GUIDE.md`  \n> **Pair**: [TESTING_GUIDE.md](tests/TESTING_GUIDE.md)  \n> **Audience**: AI collaborators and tools  \n> **Purpose**: Routing",
            "section_count": 9,
            "sections": [
              "Introduction",
              "1. Purpose and Scope",
              "2. Test Layout and Types",
              "3. Fixtures, Utilities, and Safety",
              "4. Running Automated Tests",
              "5. Parallel Execution and Coverage",
              "6. Writing and Extending Tests",
              "7. Debugging and Troubleshooting",
              "8. Manual and Channel-Specific Testing Overview"
            ]
          },
          "communication/COMMUNICATION_GUIDE.md": {
            "content_length": 11634,
            "header_info": "# Communication Module - Channel-Agnostic Architecture\n\n> **File**: `communication/COMMUNICATION_GUIDE.md`  \n> **Audience**: Developers and maintainers working on communication channels and message fl",
            "section_count": 16,
            "sections": [
              "Introduction",
              "1. Core Principle",
              "2. Architecture Layers",
              "2.1. High-Level Flow",
              "2.2. Message Models",
              "2.3. Message Processing (`communication/message_processing/`)",
              "2.4. Channel Base and Adapters (`communication/communication_channels/`)",
              "3. Key Patterns",
              "3.1. Error Handling Boundaries",
              "3.2. Logging"
            ]
          },
          "communication/communication_channels/discord/DISCORD_GUIDE.md": {
            "content_length": 15924,
            "header_info": "# Discord Channel Guide\n\n> **File**: `communication/communication_channels/discord/DISCORD_GUIDE.md`  \n> **Audience**: Developers and maintainers working on the Discord channel  \n> **Purpose**: Docume",
            "section_count": 22,
            "sections": [
              "Introduction",
              "1. Purpose and Scope",
              "2. Discord Channel Architecture",
              "3. Bot Lifecycle and Connection Management",
              "3.1. Startup",
              "3.2. Shutdown and Disconnects",
              "4. Message and Interaction Flow",
              "4.1. Messages",
              "4.2. Reactions and Membership Events",
              "4.3. UI Views and Button Callbacks"
            ]
          },
          "core/ERROR_HANDLING_GUIDE.md": {
            "content_length": 17954,
            "header_info": "# Error Handling Guide\n\n> **File**: `core/ERROR_HANDLING_GUIDE.md`  \n> **Audience**: Developers and AI assistants working on MHM  \n> **Purpose**: Centralized error handling patterns, categories, and i",
            "section_count": 24,
            "sections": [
              "Introduction",
              "1. Purpose and Design Principles",
              "2. Architecture Overview",
              "2.1. Core types and hierarchy",
              "2.2. Recovery strategies",
              "2.3. Error handler and helpers",
              "2.4. Decorators and utilities",
              "2.5. Integration with logging",
              "3. Usage Patterns",
              "3.1. Decorated functions (preferred)"
            ]
          },
          "development_docs/CHANGELOG_DETAIL.md": {
            "content_length": 862141,
            "header_info": "# CHANGELOG_DETAIL.md - Complete Detailed Changelog History\n\n\n> **File**: `development_docs/CHANGELOG_DETAIL.md`\n> **Audience**: Developers and contributors  \n> **Purpose**: Full historical record of ",
            "section_count": 301,
            "sections": [
              "Introduction",
              "Overview",
              "How to Update This File",
              "How to Add Changes",
              "YYYY-MM-DD - Brief Title",
              "Recent Changes (Most Recent First)",
              "2025-12-03 - Phase 9 Status Reports Improvements and Tool Integration **COMPLETED**",
              "2025-12-02 - Implemented Shared Mtime-Based Caching for Development Tools **COMPLETED**",
              "2025-12-02 - Fixed Path Drift and Non-ASCII Characters in Documentation **COMPLETED**",
              "2025-12-02 - Completed M7.4 Tool Decomposition Plan and Additional Module Dependencies Decomposition **COMPLETED**"
            ]
          },
          "development_docs/PLANS.md": {
            "content_length": 58499,
            "header_info": "# MHM Development Plans\n\n\n> **File**: `development_docs/PLANS.md`\n> **Audience**: Human Developer & AI Collaborators  \n> **Purpose**: Consolidated development plans (grouped, interdependent work) with",
            "section_count": 45,
            "sections": [
              "Introduction",
              "[ACTIVE] **Current Active Plans**",
              "**Test Suite Performance Optimization Plan** **ON HOLD**",
              "**Error Handling Quality Improvement Plan** **IN PROGRESS**",
              "**AI Chatbot Actionability Sprint** **IN PROGRESS**",
              "**Discord App/Bot Capabilities Exploration** **PLANNING**",
              "**Account Management System Improvements** **MOSTLY COMPLETE**",
              "**Mood-Aware Support Calibration** **PLANNING**",
              "**2025-09-16 - High Complexity Function Refactoring - Phase 2** **IN PROGRESS**",
              "**Message Deduplication Advanced Features**"
            ]
          },
          "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md": {
            "content_length": 12965,
            "header_info": "# AI Development Tools Guide\n\n> **File**: `development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md`  \n> **Audience**: AI collaborators and automated tooling  \n> **Purpose**: Routing and constraints for all AI",
            "section_count": 11,
            "sections": [
              "Introduction",
              "1. Purpose and Scope",
              "2. Running the Tool Suite",
              "3. Audit Modes and Outputs",
              "3.1. Tier 1: Quick Audit - `audit --quick`",
              "3.2. Tier 2: Standard Audit - `audit` (default)",
              "3.3. Tier 3: Full Audit - `audit --full`",
              "4. Tool Catalog and Tiering",
              "4.1. Naming Conventions",
              "5. Operating Standards and Maintenance"
            ]
          },
          "development_tools/DEVELOPMENT_TOOLS_GUIDE.md": {
            "content_length": 24500,
            "header_info": "# Development Tools Guide\n\n> **File**: `development_tools/DEVELOPMENT_TOOLS_GUIDE.md`  \n> **Audience**: Human maintainers and contributors  \n> **Purpose**: Detailed reference for every development too",
            "section_count": 16,
            "sections": [
              "Introduction",
              "1. Purpose and Scope",
              "2. Running the Tool Suite",
              "2.1. Global Options",
              "2.2. Command Examples",
              "2.3. Command Summary",
              "2.4. Entry Point Expectations",
              "3. Audit Modes and Outputs",
              "4. Tool Catalog and Tiering",
              "4.1. Naming Conventions"
            ]
          },
          "logs/LOGGING_GUIDE.md": {
            "content_length": 11131,
            "header_info": "# Logging Guide\n\n> **File**: `logs/LOGGING_GUIDE.md`  \n> **Audience**: Developers and maintainers  \n> **Purpose**: Describe logging architecture, behavior, and maintenance tasks for MHM  \n> **Style**:",
            "section_count": 32,
            "sections": [
              "Introduction",
              "1. Purpose and Scope",
              "2. Logging Architecture",
              "2.1. Central logger module",
              "2.2. Component loggers",
              "2.3. Format",
              "3. Log Levels and When to Use Them",
              "3.1. DEBUG",
              "3.2. INFO",
              "3.3. WARNING"
            ]
          },
          "scripts/SCRIPTS_GUIDE.md": {
            "content_length": 6685,
            "header_info": "# Scripts Directory\n\n\n> **File**: `scripts/SCRIPTS_GUIDE.md`\n> **Audience**: Developers using MHM utility scripts and tools  \n> **Purpose**: Guide for utility scripts, migration tools, and testing scr",
            "section_count": 14,
            "sections": [
              "Introduction",
              "Quick Reference",
              "1. Structure",
              "2. Available Scripts",
              "2.1. Project Cleanup (`scripts/`)",
              "2.2. Testing Tools (`testing/`)",
              "2.3. AI Testing (`testing/ai/`)",
              "2.4. Debug Tools (`debug/`)",
              "2.5. Utilities (`utilities/`)",
              "2.6. Cleanup Tools (`utilities/cleanup/`)"
            ]
          },
          "tests/MANUAL_DISCORD_TEST_GUIDE.md": {
            "content_length": 7077,
            "header_info": "# Manual Discord Testing Guide\n\n> **File**: `tests/MANUAL_DISCORD_TEST_GUIDE.md`  \n> **Audience**: Developers and AI assistants performing manual Discord testing  \n> **Purpose**: Manual testing proced",
            "section_count": 10,
            "sections": [
              "Introduction",
              "1. Prerequisites",
              "2. Task Reminder Follow-up Flow Testing",
              "2.1. Basic Test Scenarios",
              "2.2. Edge Cases",
              "3. Verification Commands",
              "4. Success Criteria",
              "5. Known Issues to Watch For",
              "6. Reporting Results",
              "7. Quick Test Checklist"
            ]
          },
          "tests/MANUAL_TESTING_GUIDE.md": {
            "content_length": 11513,
            "header_info": "# Manual Testing Guide\n\n> **File**: `tests/MANUAL_TESTING_GUIDE.md`  \n> **Audience**: Developers and AI assistants performing manual testing  \n> **Purpose**: Canonical manual testing flows and checkli",
            "section_count": 26,
            "sections": [
              "Introduction",
              "1. Purpose and Scope",
              "2. Core Manual Flows",
              "2.1. Application startup and shutdown",
              "2.2. Basic configuration and environment",
              "3. UI Manual Testing",
              "3.1. Layout and visual checks",
              "3.2. Navigation and interaction",
              "3.3. Validation and error feedback",
              "4. Scheduling & Reminder Manual Tests"
            ]
          },
          "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md": {
            "content_length": 4945,
            "header_info": "# System AI Functionality Testing Guide\n\n> **File**: `tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md`  \n> **Audience**: Developers and AI assistants working on MHM AI functionality  \n> **Purpose**: Quick",
            "section_count": 10,
            "sections": [
              "Introduction",
              "1. Quick Start",
              "2. Test Suite Structure",
              "3. Prerequisites",
              "4. Test Data Scenarios",
              "5. Logging Behavior",
              "6. Test Features",
              "7. Test Categories",
              "8. Known Issues",
              "9. Cleanup"
            ]
          },
          "tests/TESTING_GUIDE.md": {
            "content_length": 21189,
            "header_info": "# Testing Guide\n\n> **File**: `tests/TESTING_GUIDE.md`  \n> **Audience**: Developers and AI assistants working on MHM  \n> **Purpose**: Comprehensive testing framework focused on real behavior, integrati",
            "section_count": 35,
            "sections": [
              "Introduction",
              "1. Purpose and Scope",
              "2. Test Layout and Types",
              "2.1. Discovery rules",
              "2.2. Test types",
              "2.3. Where to put new tests",
              "3. Fixtures, Utilities, and Safety",
              "3.1. Shared fixtures and hooks",
              "3.2. Test utilities and helpers",
              "3.3. Filesystem safety"
            ]
          },
          "ui/UI_GUIDE.md": {
            "content_length": 7863,
            "header_info": "# UI Development Guide\n\n> **File**: `ui/UI_GUIDE.md`  \n> **Audience**: Developers working on MHM UI components  \n> **Purpose**: Guide for UI development, generation, and maintenance  \n> **Style**: Tec",
            "section_count": 5,
            "sections": [
              "Introduction",
              "Quick Reference",
              "1. UI Generation",
              "2. File Structure",
              "3. Usage Pattern"
            ]
          }
        },
        "missing": [],
        "placeholders": [],
        "section_overlaps": {
          "1. Directory Overview": [
            "ARCHITECTURE.md",
            "ai_development_docs/AI_ARCHITECTURE.md"
          ],
          "1. Documentation Categories": [
            "DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md"
          ],
          "1. Purpose and Design Principles": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "1. Purpose and Scope": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "ai_development_docs/AI_TESTING_GUIDE.md",
            "logs/LOGGING_GUIDE.md",
            "tests/TESTING_GUIDE.md",
            "tests/MANUAL_TESTING_GUIDE.md",
            "communication/communication_channels/discord/DISCORD_GUIDE.md",
            "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
            "development_tools/DEVELOPMENT_TOOLS_GUIDE.md"
          ],
          "1. Safety First": [
            "DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
          ],
          "1.2. AI-facing documents": [
            "DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md"
          ],
          "10. Examples": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "10. Usage Examples": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "logs/LOGGING_GUIDE.md"
          ],
          "11. Cross-References": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "2. Architecture Overview": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "2. Logging Architecture": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "logs/LOGGING_GUIDE.md"
          ],
          "2. Running the Tool Suite": [
            "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
            "development_tools/DEVELOPMENT_TOOLS_GUIDE.md"
          ],
          "2. Standards and Templates": [
            "DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md"
          ],
          "2. Test Layout and Types": [
            "ai_development_docs/AI_TESTING_GUIDE.md",
            "tests/TESTING_GUIDE.md"
          ],
          "2. User Data Model": [
            "ARCHITECTURE.md",
            "ai_development_docs/AI_ARCHITECTURE.md"
          ],
          "2. Virtual Environment Best Practices": [
            "DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
          ],
          "2025-01-09 - Test Coverage Expansion Session: Communication, UI, and Discord Bot Modules **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-01-14 - Test Coverage Expansion for User Preferences, UI Management, and Prompt Manager **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-01-16 - Pathlib Migration Completion and Test Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-01-27 - Pytest Markers Analysis and Standardization **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-01-XX - Error Handling Coverage Expansion to 92.9% **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-01-XX - Preferences Block Preservation When Features Disabled **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-01-XX - Test Isolation Improvements and Bug Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-01 - Chat Interaction Storage Testing Implementation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-01 - Comprehensive Error Handling Enhancement **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-01 - Comprehensive Quantitative Analytics Expansion **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-01 - Log Rotation Truncation Fix **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-01 - Test Suite Warnings Resolution and Coverage Improvements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-02 - AI Development Tools Audit and Documentation Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-11 - Critical Async Error Handling Fix for Discord Message Receiving **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-11 - Fixed Two Critical Errors: AttributeError and Invalid File Path **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-11 - Test Suite Fixes: All 50 Failures Resolved **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-12 - Fixed Test Logging: Headers, Isolation, and Rotation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-13 - Automated Weekly Backup System **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-13 - Consolidated Legacy Daily Checkin Files **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-13 - User Index Refactoring: Removed Redundant Data Cache **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-16 - Unused Imports Cleanup Phase 2.11 **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-17 - LM Studio Automatic Management System **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-18 - Comprehensive Unused Imports Cleanup - Final Phase **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-18 - Logging System Optimization and Redundancy Reduction **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-19 - Enhanced Checkin Response Parsing and Skip Functionality **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-19 - Unused Imports Analysis and Tool Improvement **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-21 - Documentation & Testing Improvements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-23 - UI Service Management Integration **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-24 - AI Quick Reference Cleanup **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-10-24 - Cursor Rules & Commands Refresh **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-02 - AI Response Quality Improvements & Documentation Updates **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-02 - Documentation Sync Fixes & Test Warning Resolution **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-02 - UI Validation Fixes, Import Detection Improvements, and AI Validator Enhancements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-03 - Package-Level Exports Migration (Phase 0-2 Complete) **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-03 - Package-Level Exports Migration (Phases 3-10 Complete) **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-03 - Test Artifact Cleanup Enhancements and Coverage Log Management **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-03 - Unused Imports Cleanup, AI Function Registry Dynamic Improvements, and Command File Syntax Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-04 - AI Documentation Generators Enhancement **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-04 - Documentation Maintenance, Error Messages, and AI Prompt Improvements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-05 - Communication Module Test Coverage Expansion **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-05 - Log File Rollover Fix and Infinite Error Loop Prevention **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-05 - Pytest Marks Audit and Test Categorization Improvements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-06 - AI Development Tools Component Logger Implementation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-06 - Test Coverage Expansion for Communication and UI Modules **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-06 - Test Coverage Expansion for Priority Modules **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-06 - Test Isolation Improvements and Coverage Regeneration Enhancements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-06 - UI Dialog Test Coverage Expansion and Test Quality Improvements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-09 - Analytics Scale Normalization and Conversion Helper Functions **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-09 - Critical Fix: Scheduler User Detection Bug **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-09 - Critical Fix: Test Users Creating Real Windows Scheduled Tasks **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-09 - Legacy Code Cleanup and Documentation Drift Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-09 - Legacy Compatibility Audit and Module Investigation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-09 - Test Coverage Expansion: Account Creator Dialog, Discord Bot, and Warning Fix **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-09 - Test Coverage Expansion: Communication Core, UI Dialogs, and Interaction Manager **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-10 - Plan Investigation, Test Fixes, Discord Validation, and Plan Cleanup **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-10 - Testing Infrastructure Improvements and Discord Retry Verification **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-11 - Audit System Performance Optimization and Test Suite Logging Improvements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-11 - User Data Flow Architecture Refactoring, Message Analytics Foundation, Plan Maintenance, and Test Suite Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-12 - Email Polling Timeout and Event Loop Fix **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-12 - Email Timeout Logging Reduction and Parallel Test Race Condition Fix **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-12 - Parallel Test Execution Stability: File Locking and Race Condition Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-13 - Discord Welcome Message System and Account Management Flow **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-13 - Documentation Sync Improvements and Tool Enhancements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-13 - Test Coverage Expansion and Test Suite Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-14 - Enhanced Discord Account Creation with Feature Selection **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-14 - Test Suite Stability Fixes and Race Condition Improvements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-15 - Discord Button UI Improvements and Test Isolation Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-15 - Documentation File Address Standard Implementation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-15 - UI Dialog Accuracy Improvements and Discord View Creation Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-16 - Flaky Test Detection Improvements and Parallel Execution Marker Application **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-16 - Test Stability Improvements, Coroutine Warning Suppression, and Parallel Execution Marker Application **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-17 - Logger Recursion Fix & Test Retry Logic Cleanup **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-18 - Comprehensive Documentation Review and Reorganization **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-18 - Documentation Heading Numbering Standardization Implementation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-20 - Error Handling Coverage Expansion to 100% **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-20 - PLANS.md and TODO.md Systematic Review and Documentation Consolidation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-20 - Test Infrastructure Robustness Improvements and Flaky Test Fixes **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-23 - Test Coverage Expansion and Test Suite Optimization **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-23 - Test Suite Stability Fixes and Logging Improvements **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-24 - Test Suite Performance Investigation and Reversion **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-25 - AI Dev Tools Phase 2: Core Infrastructure Stabilization **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-26 - AI Dev Tools Phase 3: Core Analysis Tools Hardening **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-26 - Command Reference Documentation Sync **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-26 - Documentation Quality Improvements and Path-to-Link Conversion **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-26 - Pathlib cleanup for Discord diagnostic **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-26 - Schedule Period Edit Cache Race Condition Fix **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-28 - Phase 6 Development Tools: Complete Portability Implementation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-28 - Phase 6 Development Tools: Core Tool Portability Implementation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-29 - User Profile Settings Widget Legacy Review **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-11-30 - Phase 7.2: Development Tools Directory Reorganization **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-12-02 - Fixed Path Drift and Non-ASCII Characters in Documentation **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-12-02 - Implemented Shared Mtime-Based Caching for Development Tools **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "2025-12-03 - Phase 9 Status Reports Improvements and Tool Integration **COMPLETED**": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "3. Audit Modes and Outputs": [
            "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
            "development_tools/DEVELOPMENT_TOOLS_GUIDE.md"
          ],
          "3. Data Handling Patterns": [
            "ARCHITECTURE.md",
            "ai_development_docs/AI_ARCHITECTURE.md"
          ],
          "3. Development Process": [
            "DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
          ],
          "3. Documentation Synchronization Checklist": [
            "DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md"
          ],
          "3. Fixtures, Utilities, and Safety": [
            "ai_development_docs/AI_TESTING_GUIDE.md",
            "tests/TESTING_GUIDE.md"
          ],
          "3. Log Levels and When to Use Them": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "logs/LOGGING_GUIDE.md"
          ],
          "3. Usage Patterns": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "3.1. Paired documentation files": [
            "DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md"
          ],
          "4. Component Log Files and Layout": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "logs/LOGGING_GUIDE.md"
          ],
          "4. Error Categories and Severity": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "4. Key Modules and Responsibilities": [
            "ARCHITECTURE.md",
            "ai_development_docs/AI_ARCHITECTURE.md"
          ],
          "4. Maintenance Guidelines": [
            "DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md"
          ],
          "4. Running Automated Tests": [
            "ai_development_docs/AI_TESTING_GUIDE.md",
            "tests/TESTING_GUIDE.md"
          ],
          "4. Testing Strategy": [
            "DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
          ],
          "4. Tool Catalog and Tiering": [
            "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
            "development_tools/DEVELOPMENT_TOOLS_GUIDE.md"
          ],
          "4.1. Naming Conventions": [
            "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
            "development_tools/DEVELOPMENT_TOOLS_GUIDE.md"
          ],
          "5. Common Tasks": [
            "DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
          ],
          "5. Configuration (Environment Variables)": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "logs/LOGGING_GUIDE.md"
          ],
          "5. Error Message Guidelines": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "5. Generated Documentation Standards": [
            "DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md"
          ],
          "5. Operating Standards and Maintenance": [
            "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
            "development_tools/DEVELOPMENT_TOOLS_GUIDE.md"
          ],
          "5. Parallel Execution and Coverage": [
            "ai_development_docs/AI_TESTING_GUIDE.md",
            "tests/TESTING_GUIDE.md"
          ],
          "5. UI Architecture and Naming Conventions": [
            "ARCHITECTURE.md",
            "ai_development_docs/AI_ARCHITECTURE.md"
          ],
          "6. Channel-Agnostic Architecture": [
            "ARCHITECTURE.md",
            "ai_development_docs/AI_ARCHITECTURE.md"
          ],
          "6. Configuration and Integration": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "6. Emergency Procedures": [
            "DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
          ],
          "6. Generated File Metadata Standards": [
            "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
            "development_tools/DEVELOPMENT_TOOLS_GUIDE.md"
          ],
          "6. Log Rotation, Backups, and Archival": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "logs/LOGGING_GUIDE.md"
          ],
          "6. Resources": [
            "DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md"
          ],
          "6. Writing and Extending Tests": [
            "ai_development_docs/AI_TESTING_GUIDE.md",
            "tests/TESTING_GUIDE.md"
          ],
          "7. Debugging and Troubleshooting": [
            "ai_development_docs/AI_TESTING_GUIDE.md",
            "tests/TESTING_GUIDE.md"
          ],
          "7. Development Notes": [
            "ARCHITECTURE.md",
            "ai_development_docs/AI_ARCHITECTURE.md"
          ],
          "7. Learning Resources": [
            "DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
          ],
          "7. Legacy Compatibility Logging Standard": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "logs/LOGGING_GUIDE.md"
          ],
          "7. Testing Error Handling": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "8. Maintenance and Cleanup": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "logs/LOGGING_GUIDE.md"
          ],
          "8. Manual and Channel-Specific Testing Overview": [
            "ai_development_docs/AI_TESTING_GUIDE.md",
            "tests/TESTING_GUIDE.md"
          ],
          "8. Monitoring and Debugging": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "8. Success Tips": [
            "DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
          ],
          "9. Best Practices": [
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "logs/LOGGING_GUIDE.md"
          ],
          "9. Git Workflow (PowerShell-Safe)": [
            "DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md"
          ],
          "9. Legacy and Migration": [
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md"
          ],
          "How to Update This File": [
            "development_docs/CHANGELOG_DETAIL.md",
            "ai_development_docs/AI_CHANGELOG.md"
          ],
          "Testing": [
            "TODO.md",
            "development_docs/CHANGELOG_DETAIL.md"
          ]
        }
      },
      "timestamp": "2025-12-04T07:33:43.401472"
    },
    "analyze_documentation_sync": {
      "success": true,
      "data": {
        "status": "FAIL",
        "total_issues": 1,
        "paired_doc_issues": 1,
        "path_drift_issues": 0,
        "ascii_compliance_issues": 0,
        "heading_numbering_issues": 0,
        "path_drift_files": [],
        "paired_docs": {
          "content_sync": [
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md has sections missing in DEVELOPMENT_WORKFLOW.md: {'10. Standard Audit Recipe'}"
          ]
        },
        "path_drift": {},
        "ascii_compliance": {},
        "heading_numbering": {}
      },
      "timestamp": "2025-12-04T07:33:39.467448"
    },
    "analyze_error_handling": {
      "success": true,
      "data": {
        "total_functions": 1475,
        "functions_with_try_except": 661,
        "functions_with_error_handling": 1473,
        "functions_with_decorators": 1384,
        "functions_missing_error_handling": 2,
        "error_handling_coverage": 99.86440677966102,
        "error_patterns": {
          "handle_errors_decorator": 1477,
          "try_except": 745,
          "handle_configuration_error": 4,
          "ValidationError": 16,
          "ConfigurationError": 6,
          "error_handler_usage": 13,
          "handle_file_error": 4,
          "handle_communication_error": 5,
          "handle_network_error": 5,
          "handle_validation_error": 2,
          "handle_ai_error": 16,
          "safe_file_operation": 1,
          "CommunicationError": 2,
          "AIError": 2,
          "FileOperationError": 10,
          "MHMError": 2,
          "DataError": 13
        },
        "missing_error_handling": [
          {
            "file": "communication\\core\\channel_orchestrator.py",
            "function": "handle_message_sending",
            "line": 1064,
            "quality": "none"
          },
          {
            "file": "communication\\message_processing\\interaction_manager.py",
            "function": "add_suggestion",
            "line": 591,
            "quality": "none"
          }
        ],
        "error_handling_quality": {
          "excellent": 1384,
          "basic": 89,
          "none": 2
        },
        "recommendations": [
          "Add error handling to 2 functions",
          "Replace basic try-except with @handle_errors decorator where appropriate",
          "Priority: Add error handling to 2 critical functions"
        ],
        "phase1_candidates": [
          {
            "file_path": "core\\error_handling.py",
            "function_name": "handle_errors",
            "line_start": 619,
            "line_end": 699,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "core\\error_handling.py",
            "function_name": "handle_error",
            "line_start": 443,
            "line_end": 522,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "core\\file_locking.py",
            "function_name": "safe_json_read",
            "line_start": 211,
            "line_end": 237,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\command_handlers\\interaction_handlers.py",
            "function_name": "_get_task_candidates",
            "line_start": 608,
            "line_end": 642,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\command_handlers\\interaction_handlers.py",
            "function_name": "_handle_complete_task__find_task_by_identifier",
            "line_start": 761,
            "line_end": 822,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\command_handlers\\interaction_handlers.py",
            "function_name": "_handle_complete_task__find_most_urgent_task",
            "line_start": 824,
            "line_end": 869,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\command_handlers\\interaction_handlers.py",
            "function_name": "_handle_delete_task__find_task_by_identifier",
            "line_start": 871,
            "line_end": 925,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\command_handlers\\interaction_handlers.py",
            "function_name": "_handle_update_task__find_task_by_identifier",
            "line_start": 927,
            "line_end": 981,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\command_handlers\\interaction_handlers.py",
            "function_name": "_handle_status",
            "line_start": 1695,
            "line_end": 1763,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\command_handlers\\interaction_handlers.py",
            "function_name": "_handle_messages",
            "line_start": 1765,
            "line_end": 1825,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\command_handlers\\interaction_handlers.py",
            "function_name": "_handle_quant_summary",
            "line_start": 2384,
            "line_end": 2419,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\core\\channel_monitor.py",
            "function_name": "_check_and_restart_stuck_channels",
            "line_start": 85,
            "line_end": 95,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\core\\channel_monitor.py",
            "function_name": "_attempt_channel_restart",
            "line_start": 97,
            "line_end": 129,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_handle_scheduled_checkin",
            "line_start": 1217,
            "line_end": 1254,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\base\\command_registry.py",
            "function_name": "discord_command_callback",
            "line_start": 143,
            "line_end": 150,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\discord\\bot.py",
            "function_name": "__init__",
            "line_start": 47,
            "line_end": 89,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\discord\\bot.py",
            "function_name": "_cleanup_session_with_timeout",
            "line_start": 319,
            "line_end": 329,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\discord\\bot.py",
            "function_name": "on_ready",
            "line_start": 687,
            "line_end": 774,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\discord\\bot.py",
            "function_name": "on_message",
            "line_start": 1000,
            "line_end": 1150,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\discord\\bot.py",
            "function_name": "_check_new_authorized_users",
            "line_start": 713,
            "line_end": 726,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\discord\\bot.py",
            "function_name": "_sync_app_cmds",
            "line_start": 699,
            "line_end": 704,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\discord\\bot.py",
            "function_name": "_app_cb",
            "line_start": 1171,
            "line_end": 1207,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\discord\\bot.py",
            "function_name": "_dynamic",
            "line_start": 1231,
            "line_end": 1243,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "communication\\communication_channels\\discord\\webhook_handler.py",
            "function_name": "_send_welcome_dm",
            "line_start": 148,
            "line_end": 176,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "ui\\ui_app_qt.py",
            "function_name": "run_full_scheduler",
            "line_start": 769,
            "line_end": 783,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "ui\\ui_app_qt.py",
            "function_name": "run_user_scheduler",
            "line_start": 785,
            "line_end": 803,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "ui\\ui_app_qt.py",
            "function_name": "run_category_scheduler",
            "line_start": 805,
            "line_end": 828,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "ui\\dialogs\\account_creator_dialog.py",
            "function_name": "on_feature_toggled",
            "line_start": 359,
            "line_end": 373,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "run_headless_service.py",
            "function_name": "main",
            "line_start": 12,
            "line_end": 128,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": true,
            "priority": "high"
          },
          {
            "file_path": "core\\error_handling.py",
            "function_name": "_log_error",
            "line_start": 524,
            "line_end": 569,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\error_handling.py",
            "function_name": "_show_user_error",
            "line_start": 571,
            "line_end": 587,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\error_handling.py",
            "function_name": "decorator",
            "line_start": 630,
            "line_end": 698,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\error_handling.py",
            "function_name": "async_wrapper",
            "line_start": 635,
            "line_end": 663,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\error_handling.py",
            "function_name": "wrapper",
            "line_start": 668,
            "line_end": 696,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\file_locking.py",
            "function_name": "file_lock",
            "line_start": 23,
            "line_end": 114,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\file_locking.py",
            "function_name": "file_lock",
            "line_start": 121,
            "line_end": 208,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\response_tracking.py",
            "function_name": "get_timestamp_for_sorting",
            "line_start": 85,
            "line_end": 102,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\scheduler.py",
            "function_name": "_select_task_for_reminder__calculate_due_date_weight",
            "line_start": 1002,
            "line_end": 1036,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\scheduler.py",
            "function_name": "select_task_for_reminder",
            "line_start": 1109,
            "line_end": 1142,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\scheduler.py",
            "function_name": "scheduler_loop",
            "line_start": 51,
            "line_end": 111,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\service.py",
            "function_name": "_check_test_message_requests__cleanup_request_file",
            "line_start": 688,
            "line_end": 694,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\service.py",
            "function_name": "check_test_message_requests",
            "line_start": 704,
            "line_end": 723,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\service.py",
            "function_name": "_cleanup_test_message_requests__remove_request_file",
            "line_start": 855,
            "line_end": 863,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\service.py",
            "function_name": "_check_reschedule_requests__cleanup_request_file",
            "line_start": 944,
            "line_end": 950,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\service.py",
            "function_name": "check_reschedule_requests",
            "line_start": 960,
            "line_end": 979,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\user_management.py",
            "function_name": "_load_presets_json",
            "line_start": 1221,
            "line_end": 1238,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\user_management.py",
            "function_name": "get_timezone_options",
            "line_start": 1247,
            "line_end": 1254,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_monitor.py",
            "function_name": "_restart_monitor_loop",
            "line_start": 75,
            "line_end": 83,
            "is_async": false,
            "operation_type": "validation",
            "is_entry_point": true,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_initialize_channel_with_retry",
            "line_start": 415,
            "line_end": 459,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_initialize_channel_with_retry_sync",
            "line_start": 604,
            "line_end": 654,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "send_message",
            "line_start": 656,
            "line_end": 746,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_check_logging_health",
            "line_start": 748,
            "line_end": 773,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "send_message_sync",
            "line_start": 775,
            "line_end": 866,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "broadcast_message",
            "line_start": 868,
            "line_end": 895,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_shutdown_all_async",
            "line_start": 942,
            "line_end": 952,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_shutdown_sync",
            "line_start": 1002,
            "line_end": 1041,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "receive_messages",
            "line_start": 1043,
            "line_end": 1062,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_should_send_checkin_prompt",
            "line_start": 1191,
            "line_end": 1215,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_send_checkin_prompt",
            "line_start": 1256,
            "line_end": 1308,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_send_ai_generated_message",
            "line_start": 1311,
            "line_end": 1351,
            "is_async": false,
            "operation_type": "network",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "_send_predefined_message",
            "line_start": 1353,
            "line_end": 1465,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\message_processing\\interaction_manager.py",
            "function_name": "_get_help_response",
            "line_start": 507,
            "line_end": 548,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\message_processing\\interaction_manager.py",
            "function_name": "get_user_suggestions",
            "line_start": 585,
            "line_end": 692,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\message_processing\\interaction_manager.py",
            "function_name": "_augment_suggestions",
            "line_start": 849,
            "line_end": 873,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "communication\\message_processing\\interaction_manager.py",
            "function_name": "parse_due",
            "line_start": 603,
            "line_end": 613,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\ui_app_qt.py",
            "function_name": "load_theme",
            "line_start": 362,
            "line_end": 379,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\ui_app_qt.py",
            "function_name": "load_user_categories",
            "line_start": 1008,
            "line_end": 1038,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\ui_app_qt.py",
            "function_name": "view_all_users_summary",
            "line_start": 2220,
            "line_end": 2281,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\ui_app_qt.py",
            "function_name": "system_health_check",
            "line_start": 2283,
            "line_end": 2383,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\ui_app_qt.py",
            "function_name": "cleanup_old_requests",
            "line_start": 1647,
            "line_end": 1655,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\channel_management_dialog.py",
            "function_name": "__init__",
            "line_start": 16,
            "line_end": 65,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\task_crud_dialog.py",
            "function_name": "refresh_active_tasks",
            "line_start": 109,
            "line_end": 147,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\task_crud_dialog.py",
            "function_name": "refresh_completed_tasks",
            "line_start": 149,
            "line_end": 186,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\task_crud_dialog.py",
            "function_name": "update_statistics",
            "line_start": 188,
            "line_end": 200,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\task_crud_dialog.py",
            "function_name": "add_new_task",
            "line_start": 212,
            "line_end": 221,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\task_crud_dialog.py",
            "function_name": "edit_selected_task",
            "line_start": 223,
            "line_end": 244,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\task_crud_dialog.py",
            "function_name": "complete_selected_task",
            "line_start": 246,
            "line_end": 279,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\task_crud_dialog.py",
            "function_name": "delete_selected_task",
            "line_start": 281,
            "line_end": 311,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\task_crud_dialog.py",
            "function_name": "restore_selected_task",
            "line_start": 313,
            "line_end": 345,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ui\\dialogs\\task_crud_dialog.py",
            "function_name": "delete_completed_task",
            "line_start": 347,
            "line_end": 377,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ai\\chatbot.py",
            "function_name": "test_system_prompt_integration",
            "line_start": 1180,
            "line_end": 1202,
            "is_async": false,
            "operation_type": "file_io",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "ai\\conversation_history.py",
            "function_name": "_cleanup_old_sessions",
            "line_start": 384,
            "line_end": 396,
            "is_async": false,
            "operation_type": "user_data",
            "is_entry_point": false,
            "priority": "medium"
          },
          {
            "file_path": "core\\backup_manager.py",
            "function_name": "__init__",
            "line_start": 27,
            "line_end": 41,
            "is_async": false,
            "operation_type": "configuration",
            "is_entry_point": false,
            "priority": "low"
          },
          {
            "file_path": "core\\config.py",
            "function_name": "validate_and_raise_if_invalid",
            "line_start": 560,
            "line_end": 597,
            "is_async": false,
            "operation_type": "validation",
            "is_entry_point": false,
            "priority": "low"
          },
          {
            "file_path": "communication\\core\\channel_monitor.py",
            "function_name": "get_channel_health_status",
            "line_start": 152,
            "line_end": 181,
            "is_async": false,
            "operation_type": "validation",
            "is_entry_point": false,
            "priority": "low"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "get_channel_status",
            "line_start": 897,
            "line_end": 906,
            "is_async": false,
            "operation_type": "configuration",
            "is_entry_point": false,
            "priority": "low"
          },
          {
            "file_path": "communication\\core\\channel_orchestrator.py",
            "function_name": "health_check_all",
            "line_start": 917,
            "line_end": 931,
            "is_async": false,
            "operation_type": "validation",
            "is_entry_point": false,
            "priority": "low"
          },
          {
            "file_path": "ui\\dialogs\\process_watcher_dialog.py",
            "function_name": "__init__",
            "line_start": 22,
            "line_end": 41,
            "is_async": false,
            "operation_type": "ui",
            "is_entry_point": false,
            "priority": "low"
          },
          {
            "file_path": "ui\\dialogs\\process_watcher_dialog.py",
            "function_name": "closeEvent",
            "line_start": 443,
            "line_end": 452,
            "is_async": false,
            "operation_type": "ui",
            "is_entry_point": false,
            "priority": "low"
          }
        ],
        "phase1_total": 89,
        "phase1_by_priority": {
          "high": 29,
          "medium": 53,
          "low": 7
        },
        "phase2_exceptions": [
          {
            "file_path": "ui\\dialogs\\admin_panel.py",
            "line_number": 85,
            "exception_type": "ValueError",
            "suggested_replacement": "DataError",
            "function_name": "set_admin_data",
            "function_line": 75,
            "line_content": "raise ValueError(\"Admin data must be a dictionary\")",
            "context": "            if not isinstance(data, dict):\n                logger.warning(f\"Admin data must be dict, got {type(data)}\")\n                raise ValueError(\"Admin data must be a dictionary\")\n            \n            # TODO: Implement actual data setting"
          }
        ],
        "phase2_total": 1,
        "phase2_by_type": {
          "ValueError": 1
        },
        "worst_modules": [
          {
            "module": "message_processing/interaction_manager.py",
            "coverage": 95.45454545454545,
            "missing": 1,
            "total": 22
          },
          {
            "module": "core/channel_orchestrator.py",
            "coverage": 98.07692307692307,
            "missing": 1,
            "total": 52
          },
          {
            "module": "auto_cleanup.py",
            "coverage": 100.0,
            "missing": 0,
            "total": 22
          },
          {
            "module": "backup_manager.py",
            "coverage": 100.0,
            "missing": 0,
            "total": 28
          },
          {
            "module": "checkin_analytics.py",
            "coverage": 100.0,
            "missing": 0,
            "total": 26
          }
        ]
      },
      "timestamp": "2025-12-04T07:33:43.478761"
    },
    "analyze_module_dependencies": {
      "success": true,
      "data": {
        "files_scanned": 328,
        "total_imports": 3778,
        "documented_dependencies": 327,
        "standard_library": 1281,
        "third_party": 562,
        "local_imports": 1935,
        "missing_dependencies": 1,
        "missing_sections": [
          "tests/development_tools/test_path_drift_detection.py"
        ]
      },
      "timestamp": "2025-12-04T07:33:50.393755"
    },
    "analyze_unused_imports": {
      "success": true,
      "data": {},
      "timestamp": "2025-12-04T07:36:43.003298"
    },
    "analyze_legacy_references": {
      "success": true,
      "data": {
        "findings": {
          "legacy_compatibility_markers": [
            [
              "development_tools\\shared\\mtime_cache.py",
              "#!/usr/bin/env python3\n\"\"\"\nMtime-based File Cache Utility\n\nProvides a reusable caching mechanism for file-based analyzers that checks\nfile modification times (mtime) to determine if cached results are still valid.\n\nUsage:\n    from development_tools.shared.mtime_cache import MtimeFileCache\n    \n    cache = MtimeFileCache(\n        cache_file=project_root / \"development_tools\" / \"docs\" / \".my_cache.json\",\n        project_root=project_root,\n        use_cache=True\n    )\n    \n    # Check if file is cached\n    cached_results = cache.get_cached(file_path)\n    if cached_results is not None:\n        # Use cached results\n        return cached_results\n    \n    # Process file and cache results\n    results = process_file(file_path)\n    cache.cache_results(file_path, results)\n    cache.save_cache()\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, TypeVar\n\nT = TypeVar('T')  # Generic type for cached results\n\ntry:\n    from core.logger import get_component_logger\n    logger = get_component_logger(\"development_tools\")\nexcept ImportError:\n    logger = None\n\n\nclass MtimeFileCache:\n    \"\"\"\n    Mtime-based file cache for analyzer results.\n    \n    Caches results keyed by file path, with validation based on file modification time.\n    Only re-processes files that have been modified since the last cache entry.\n    \"\"\"\n    \n    def __init__(\n        self,\n        cache_file: Path,\n        project_root: Path,\n        use_cache: bool = True,\n        tool_name: Optional[str] = None,\n        domain: Optional[str] = None\n    ):\n        \"\"\"\n        Initialize the cache.\n        \n        Args:\n            cache_file: Path to the JSON cache file (legacy - used if tool_name/domain not provided)\n            project_root: Root directory of the project (for relative path generation)\n            use_cache: Whether to use caching (if False, all operations are no-ops)\n            tool_name: Name of the tool (e.g., 'analyze_ascii_compliance') - if provided, uses standardized storage\n            domain: Domain directory (e.g., 'docs') - if provided with tool_name, uses standardized storage\n        \"\"\"\n        self.cache_file = cache_file\n        self.project_root = project_root.resolve()\n        self.use_cache = use_cache\n        self.cache_data: Dict[str, Dict[str, Any]] = {}\n        self.tool_name = tool_name\n        self.domain = domain\n        self.use_standardized_storage = tool_name is not None and domain is not None\n        \n        if self.use_cache:\n            self._load_cache()\n    \n    def _load_cache(self) -> None:\n        \"\"\"Load cache from disk if it exists.\"\"\"\n        if self.use_standardized_storage:\n            # Use standardized storage\n            try:\n                from .output_storage import load_tool_cache\n                loaded_data = load_tool_cache(self.tool_name, self.domain, project_root=self.project_root)\n                if loaded_data:\n                    # load_tool_cache already extracts data from metadata wrapper, so loaded_data is the cache content\n                    # Migrate old cache format (with 'issues' key) to new format (with 'results' key)\n                    migrated_data = {}\n                    for key, value in loaded_data.items():\n                        if isinstance(value, dict):\n                            # Check if it's old format with 'issues' key\n                            if 'issues' in value and 'results' not in value:\n                                migrated_data[key] = {\n                                    'mtime': value.get('mtime'),\n                                    'results': value.get('issues', [])\n                                }\n                            else:\n                                # Already in new format or has 'results' key\n                                migrated_data[key] = value\n                        else:\n                            # Invalid format, skip\n                            continue\n                    self.cache_data = migrated_data\n                    if logger:\n                        logger.debug(f\"Loaded cache from standardized storage ({self.tool_name}) with {len(self.cache_data)} entries\")\n                    return\n            except Exception as e:\n                if logger:\n                    logger.warning(f\"Failed to load cache from standardized storage: {e}\")\n        \n        # LEGACY COMPATIBILITY: Fallback to legacy file-based loading\n        # New standardized storage location: development_tools/{domain}/jsons/.{tool_name}_cache.json\n        # Removal plan: After all tools migrate to standardized storage and old cache files are removed, remove this fallback.\n        # Detection: Search for \"cache_file.exists()\" and \"MtimeFileCache\" without tool_name/domain parameters to find legacy usage.\n        if self.cache_file.exists():\n            if logger:\n                logger.debug(f\"LEGACY: Loading cache from legacy location: {self.cache_file}\")\n            try:\n                with open(self.cache_file, 'r', encoding='utf-8') as f:\n                    loaded_data = json.load(f)\n                    # Migrate old cache format (with 'issues' key) to new format (with 'results' key)\n                    # This handles migration from analyze_unused_imports.py old format\n                    migrated_data = {}\n                    for key, value in loaded_data.items():\n                        if isinstance(value, dict):\n                            # Check if it's old format with 'issues' key\n                            if 'issues' in value and 'results' not in value:\n                                migrated_data[key] = {\n                                    'mtime': value.get('mtime'),\n                                    'results': value.get('issues', [])\n                                }\n                            else:\n                                # Already in new format or has 'results' key\n                                migrated_data[key] = value\n                        else:\n                            # Invalid format, skip\n                            continue\n                    self.cache_data = migrated_data\n                if logger:\n                    logger.debug(f\"Loaded cache from {self.cache_file} with {len(self.cache_data)} entries\")\n            except Exception as e:\n                if logger:\n                    logger.warning(f\"Failed to load cache from {self.cache_file}: {e}\")\n                self.cache_data = {}\n    \n    def save_cache(self) -> None:\n        \"\"\"Save cache to disk.\"\"\"\n        if not self.use_cache:\n            return\n        \n        if self.use_standardized_storage:\n            # Use standardized storage\n            try:\n                from .output_storage import save_tool_cache\n                save_tool_cache(self.tool_name, self.domain, self.cache_data, project_root=self.project_root)\n                if logger:\n                    logger.debug(f\"Saved cache to standardized storage ({self.tool_name}) with {len(self.cache_data)} entries\")\n                return\n            except Exception as e:\n                if logger:\n                    logger.warning(f\"Failed to save cache to standardized storage: {e}\")\n        \n        # LEGACY COMPATIBILITY: Fallback to legacy file-based saving\n        # New standardized storage location: development_tools/{domain}/jsons/.{tool_name}_cache.json\n        # Removal plan: After all tools migrate to standardized storage and old cache files are removed, remove this fallback.\n        # Detection: Search for \"cache_file.parent.mkdir\" and \"MtimeFileCache\" without tool_name/domain parameters to find legacy usage.\n        if logger:\n            logger.debug(f\"LEGACY: Saving cache to legacy location: {self.cache_file}\")\n        try:\n            self.cache_file.parent.mkdir(parents=True, exist_ok=True)\n            with open(self.cache_file, 'w', encoding='utf-8') as f:\n                json.dump(self.cache_data, f, indent=2)\n            if logger:\n                logger.debug(f\"Saved cache to {self.cache_file} with {len(self.cache_data)} entries\")\n        except Exception as e:\n            if logger:\n                logger.warning(f\"Failed to save cache to {self.cache_file}: {e}\")\n    \n    def _get_file_cache_key(self, file_path: Path) -> str:\n        \"\"\"Generate cache key for a file (relative path from project root).\"\"\"\n        try:\n            rel_path = file_path.resolve().relative_to(self.project_root)\n            return str(rel_path).replace('\\\\', '/')\n        except ValueError:\n            # File is outside project root, use absolute path\n            return str(file_path.resolve())\n    \n    def _is_file_cached(self, file_path: Path) -> bool:\n        \"\"\"Check if file results are cached and still valid (mtime matches).\"\"\"\n        if not self.use_cache:\n            return False\n        \n        cache_key = self._get_file_cache_key(file_path)\n        if cache_key not in self.cache_data:\n            return False\n        \n        cached_mtime = self.cache_data[cache_key].get('mtime')\n        if cached_mtime is None:\n            return False\n        \n        try:\n            current_mtime = file_path.stat().st_mtime\n            return current_mtime == cached_mtime\n        except OSError:\n            return False\n    \n    def get_cached(self, file_path: Path) -> Optional[T]:\n        \"\"\"\n        Get cached results for a file if available and still valid.\n        \n        Args:\n            file_path: Path to the file to check\n            \n        Returns:\n            Cached results if available and valid, None otherwise\n        \"\"\"\n        if not self._is_file_cached(file_path):\n            return None\n        \n        cache_key = self._get_file_cache_key(file_path)\n        cached_data = self.cache_data[cache_key].get('results')\n        return cached_data\n    \n    def cache_results(self, file_path: Path, results: T) -> None:\n        \"\"\"\n        Cache results for a file.\n        \n        Args:\n            file_path: Path to the file being cached\n            results: Results to cache (must be JSON-serializable)\n        \"\"\"\n        if not self.use_cache:\n            return\n        \n        try:\n            cache_key = self._get_file_cache_key(file_path)\n            mtime = file_path.stat().st_mtime\n            self.cache_data[cache_key] = {\n                'mtime': mtime,\n                'results': results\n            }\n        except OSError:\n            # File doesn't exist or can't be accessed, skip caching\n            pass\n    \n    def clear_cache(self) -> None:\n        \"\"\"Clear all cached data (in memory only, call save_cache() to persist).\"\"\"\n        self.cache_data = {}\n    \n    def get_cache_stats(self) -> Dict[str, int]:\n        \"\"\"\n        Get statistics about the cache.\n        \n        Returns:\n            Dictionary with cache statistics (total_entries, etc.)\n        \"\"\"\n        return {\n            'total_entries': len(self.cache_data),\n            'cache_file': str(self.cache_file)\n        }\n\n",
              [
                {
                  "pattern": "# LEGACY COMPATIBILITY:",
                  "match": "# LEGACY COMPATIBILITY:",
                  "line": 112,
                  "line_content": "# LEGACY COMPATIBILITY: Fallback to legacy file-based loading",
                  "start": 4365,
                  "end": 4388
                },
                {
                  "pattern": "# LEGACY COMPATIBILITY:",
                  "match": "# LEGACY COMPATIBILITY:",
                  "line": 164,
                  "line_content": "# LEGACY COMPATIBILITY: Fallback to legacy file-based saving",
                  "start": 7203,
                  "end": 7226
                }
              ]
            ],
            [
              "development_tools\\shared\\operations.py",
              "#!/usr/bin/env python3\n# TOOL_TIER: core\n\n\n\"\"\"\n\nCore operations used by the AI development toolchain.\n\nThis module contains the reusable service layer that powers the CLI in\n\n`run_development_tools.py`. Each public method exposes a discrete workflow that\n\ncan be invoked programmatically or through a registered command.\n\n\"\"\"\n\nimport sys\n\nimport subprocess\n\nimport json\n\nimport os\n\nimport re\n\nimport argparse\n\nfrom pathlib import Path\n\nfrom datetime import datetime\n\nfrom dataclasses import dataclass\n\nfrom typing import Any, Dict, List, Optional, Sequence, Callable\n\nfrom collections import OrderedDict, defaultdict\n\nfrom .. import config\n# Load external config if path was provided (will be set by AIToolsService)\n\n# Add project root to path for core module imports\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nfrom core.logger import get_component_logger\n\nlogger = get_component_logger(\"development_tools\")\n\nSCRIPT_REGISTRY = {\n\n    'analyze_documentation': 'docs/analyze_documentation.py',\n\n    'analyze_function_registry': 'functions/analyze_function_registry.py',\n\n    'analyze_module_dependencies': 'imports/analyze_module_dependencies.py',\n\n    'analyze_config': 'config/analyze_config.py',\n\n    'decision_support': 'reports/decision_support.py',\n\n    'analyze_documentation_sync': 'docs/analyze_documentation_sync.py',\n    'analyze_path_drift': 'docs/analyze_path_drift.py',\n    'analyze_missing_addresses': 'docs/analyze_missing_addresses.py',\n    'analyze_ascii_compliance': 'docs/analyze_ascii_compliance.py',\n    'analyze_heading_numbering': 'docs/analyze_heading_numbering.py',\n    'analyze_unconverted_links': 'docs/analyze_unconverted_links.py',\n    'generate_directory_tree': 'docs/generate_directory_tree.py',\n\n    'analyze_error_handling': 'error_handling/analyze_error_handling.py',\n    'generate_error_handling_report': 'error_handling/generate_error_handling_report.py',\n    'generate_error_handling_recommendations': 'error_handling/generate_error_handling_recommendations.py',\n\n    'analyze_functions': 'functions/analyze_functions.py',\n    'analyze_function_patterns': 'functions/analyze_function_patterns.py',\n    'generate_function_registry': 'functions/generate_function_registry.py',\n\n    'generate_module_dependencies': 'imports/generate_module_dependencies.py',\n    'analyze_module_imports': 'imports/analyze_module_imports.py',\n    'analyze_dependency_patterns': 'imports/analyze_dependency_patterns.py',\n\n    'fix_legacy_references': 'legacy/fix_legacy_references.py',\n    'analyze_legacy_references': 'legacy/analyze_legacy_references.py',\n    'generate_legacy_reference_report': 'legacy/generate_legacy_reference_report.py',\n\n    'quick_status': 'reports/quick_status.py',\n\n    'generate_test_coverage': 'tests/generate_test_coverage.py',\n    'analyze_test_coverage': 'tests/analyze_test_coverage.py',\n    'generate_test_coverage_reports': 'tests/generate_test_coverage_reports.py',\n    'analyze_test_markers': 'tests/analyze_test_markers.py',\n\n    'analyze_unused_imports': 'imports/analyze_unused_imports.py',\n\n    'analyze_ai_work': 'ai_work/analyze_ai_work.py',\n\n    'fix_version_sync': 'docs/fix_version_sync.py',\n\n    'fix_documentation': 'docs/fix_documentation.py',\n\n    'system_signals': 'reports/system_signals.py',\n    \n    'cleanup_project': 'shared/fix_project_cleanup.py'\n\n}\n\nfrom ..shared.file_rotation import create_output_file\nfrom .common import COMMAND_TIERS\nfrom .output_storage import save_tool_result, load_tool_result, get_all_tool_results, _get_domain_from_tool_name\n\nclass AIToolsService:\n\n    \"\"\"Comprehensive AI tools runner optimized for AI collaboration.\"\"\"\n\n    def __init__(self, project_root: Optional[str] = None, config_path: Optional[str] = None, \n                 project_name: Optional[str] = None, key_files: Optional[List[str]] = None):\n\n        # Load external config if path provided, or try to auto-load from default location\n        if config_path:\n            config.load_external_config(config_path)\n        else:\n            # Try to auto-load from development_tools/config/development_tools_config.json or project root\n            config.load_external_config()\n        \n        # Use provided project_root or fall back to config\n        if project_root:\n            self.project_root = Path(project_root).resolve()\n        else:\n            self.project_root = Path(config.get_project_root()).resolve()\n        \n        # Store config_path for reference\n        self.config_path = config_path\n        \n        # Project-specific configuration (for portability)\n        # Can be overridden by external config\n        self.project_name = project_name or config.get_external_value('project.name', \"Project\")\n        self.key_files = key_files or config.get_external_value('project.key_files', [])\n\n        self.workflow_config = config.get_workflow_config() or {}\n        \n        # Store path validation result for status display\n        self.path_validation_result: Optional[Dict[str, Any]] = None\n\n        self.validation_config = config.get_ai_validation_config() or {}\n\n        self.ai_config = config.get_ai_collaboration_config() or {}\n\n        self.audit_config = config.get_quick_audit_config() or {}\n\n        self.results_cache = {}\n\n        self.docs_sync_results = None\n\n        self.system_signals = None\n        self.dev_tools_coverage_results = None\n        self.module_dependency_summary = None\n        self.todo_sync_result = None\n\n        self.exclusion_config = {\n\n            'include_tests': False,\n\n            'include_dev_tools': False\n\n        }\n\n        self.docs_sync_summary = None\n\n        self.legacy_cleanup_results = None\n\n        self.legacy_cleanup_summary = None\n\n        self.status_results = None\n\n        self.status_summary = None\n        \n        self.current_audit_tier = None  # Track current audit tier (1=quick, 2=standard, 3=full)\n\n    def set_exclusion_config(self, include_tests: bool = False, include_dev_tools: bool = False):\n\n        \"\"\"Set exclusion configuration for audit tools.\"\"\"\n\n        self.exclusion_config = {\n\n            'include_tests': include_tests,\n\n            'include_dev_tools': include_dev_tools\n\n        }\n\n    def run_script(self, script_name: str, *args, timeout: Optional[int] = 300) -> Dict:\n\n        \"\"\"Run a registered helper script from development_tools.\"\"\"\n\n        script_rel_path = SCRIPT_REGISTRY.get(script_name)\n\n        if not script_rel_path:\n\n            return {\n\n                'success': False,\n\n                'output': '',\n\n                'error': f\"Script '{script_name}' is not registered\"\n\n            }\n\n        script_path = Path(__file__).resolve().parent.parent / script_rel_path\n\n        if not script_path.exists():\n\n            return {\n\n                'success': False,\n\n                'output': '',\n\n                'error': f\"Registered script '{script_name}' not found at {script_rel_path}\"\n\n            }\n\n        cmd = [sys.executable, str(script_path)] + list(args)\n\n        try:\n\n            result = subprocess.run(\n\n                cmd,\n\n                capture_output=True,\n\n                text=True,\n\n                cwd=str(self.project_root),\n\n                timeout=timeout  # default 5 minute timeout\n\n            )\n\n            return {\n\n                'success': result.returncode == 0,\n\n                'output': result.stdout,\n\n                'error': result.stderr,\n\n                'returncode': result.returncode\n\n            }\n\n        except subprocess.TimeoutExpired:\n\n            return {\n\n                'success': False,\n\n                'output': '',\n\n                'error': f\"Script '{script_name}' timed out after {timeout // 60 if timeout else 'N/A'} minutes\",\n\n                'returncode': None\n\n            }\n\n    def run_analyze_documentation(self, include_overlap: bool = False) -> Dict:\n\n        \"\"\"Run analyze_documentation with structured JSON handling.\"\"\"\n\n        args = [\"--json\"]\n        if include_overlap:\n            args.append(\"--overlap\")\n        result = self.run_script(\"analyze_documentation\", *args)\n\n        output = result.get('output', '')\n\n        data = None\n\n        if output:\n\n            try:\n\n                data = json.loads(output)\n\n            except json.JSONDecodeError:\n\n                data = None\n\n        if data is not None:\n\n            result['data'] = data\n\n            self.results_cache['analyze_documentation'] = data\n\n            # Save to standardized storage\n            try:\n                save_tool_result('analyze_documentation', 'docs', data, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_documentation result: {e}\")\n\n            result['issues_found'] = bool(data.get('duplicates') or data.get('placeholders') or data.get('missing'))\n\n            result['success'] = True\n\n            result['error'] = ''\n\n        else:\n\n            lowered = output.lower() if isinstance(output, str) else ''\n\n            if not result.get('success') and (\"verbatim duplicate\" in lowered or \"placeholder\" in lowered):\n\n                result['issues_found'] = True\n\n                result['success'] = True\n\n                result['error'] = ''\n\n        return result\n\n    def run_analyze_function_registry(self) -> Dict:\n\n        \"\"\"Run analyze_function_registry with structured JSON handling.\"\"\"\n\n        result = self.run_script(\"analyze_function_registry\", \"--json\")\n\n        output = result.get('output', '')\n\n        data = None\n\n        if output:\n\n            try:\n\n                data = json.loads(output)\n\n            except json.JSONDecodeError:\n\n                data = None\n\n        if data is not None:\n\n            result['data'] = data\n\n            # Save to standardized storage\n            try:\n                save_tool_result('analyze_function_registry', 'functions', data, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_function_registry result: {e}\")\n\n            missing = data.get('missing', {}) if isinstance(data.get('missing'), dict) else data.get('missing')\n\n            extra = data.get('extra', {}) if isinstance(data.get('extra'), dict) else data.get('extra')\n\n            errors = data.get('errors') or []\n\n            missing_count = missing.get('count') if isinstance(missing, dict) else missing\n\n            extra_count = extra.get('count') if isinstance(extra, dict) else extra\n\n            result['issues_found'] = bool(missing_count or extra_count or errors)\n\n            result['success'] = True\n\n            result['error'] = ''\n\n        else:\n\n            lowered = output.lower() if isinstance(output, str) else ''\n\n            if 'missing from registry' in lowered or 'missing items' in lowered or 'extra functions' in lowered:\n\n                result['issues_found'] = True\n\n                result['success'] = True\n\n                result['error'] = ''\n\n        return result\n\n    def run_analyze_module_dependencies(self) -> Dict:\n\n        \"\"\"Run analyze_module_dependencies and capture dependency drift summary.\"\"\"\n\n        result = self.run_script(\"analyze_module_dependencies\")\n\n        output = result.get('output', '')\n\n        summary = self._parse_module_dependency_report(output)\n\n        if summary:\n\n            result['data'] = summary\n\n            # Save to standardized storage\n            try:\n                save_tool_result('analyze_module_dependencies', 'imports', summary, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_module_dependencies result: {e}\")\n\n            issues = summary.get('missing_dependencies', 0)\n\n            issues = issues or len(summary.get('missing_sections') or [])\n\n            result['issues_found'] = bool(issues)\n\n            # preserve success flag if script executed; default to True when stdout parsed\n\n            if 'success' not in result:\n\n                result['success'] = True\n\n            self.module_dependency_summary = summary\n\n            self.results_cache['analyze_module_dependencies'] = summary\n\n        return result\n\n    def run_analyze_functions(self) -> Dict:\n\n        \"\"\"Run analyze_functions with structured JSON handling.\"\"\"\n\n        # Build command line arguments based on exclusion configuration\n\n        args = [\"--json\"]  # Always request JSON output for parsing\n\n        if self.exclusion_config.get('include_tests', False):\n\n            args.append(\"--include-tests\")\n\n        if self.exclusion_config.get('include_dev_tools', False):\n\n            args.append(\"--include-dev-tools\")\n\n        result = self.run_script(\"analyze_functions\", *args)\n\n        # Parse JSON output if available\n        if result.get('success') and result.get('output'):\n            try:\n                import json\n                json_data = json.loads(result['output'])\n                result['data'] = json_data\n                \n                # Merge in examples extracted from text output (if available)\n                if 'analyze_functions' in self.results_cache:\n                    extracted_metrics = self.results_cache['analyze_functions']\n                    # Merge examples into json_data\n                    if 'critical_complexity_examples' in extracted_metrics:\n                        json_data['critical_complexity_examples'] = extracted_metrics['critical_complexity_examples']\n                    if 'high_complexity_examples' in extracted_metrics:\n                        json_data['high_complexity_examples'] = extracted_metrics['high_complexity_examples']\n                    if 'undocumented_examples' in extracted_metrics:\n                        json_data['undocumented_examples'] = extracted_metrics['undocumented_examples']\n                \n                # Save to standardized storage (with examples included)\n                try:\n                    save_tool_result('analyze_functions', 'functions', json_data, project_root=self.project_root)\n                    # Also update results_cache with merged data\n                    self.results_cache['analyze_functions'] = json_data\n                except Exception as e:\n                    logger.warning(f\"Failed to save analyze_functions result: {e}\")\n            except (json.JSONDecodeError, ValueError) as e:\n                logger.warning(f\"Failed to parse analyze_functions JSON output: {e}\")\n\n        return result\n\n    def run_decision_support(self) -> Dict:\n\n        \"\"\"Run decision_support with structured JSON handling.\"\"\"\n\n        # Build command line arguments based on exclusion configuration\n\n        args = []\n\n        if self.exclusion_config.get('include_tests', False):\n\n            args.append(\"--include-tests\")\n\n        if self.exclusion_config.get('include_dev_tools', False):\n\n            args.append(\"--include-dev-tools\")\n\n        result = self.run_script(\"decision_support\", *args)\n\n        return result\n\n    def run_analyze_error_handling(self) -> Dict:\n\n        \"\"\"Run analyze_error_handling with structured JSON handling.\"\"\"\n\n        # Build command line arguments based on exclusion configuration\n\n        args = [\"--json\"]\n\n        if self.exclusion_config.get('include_tests', False):\n\n            args.append(\"--include-tests\")\n\n        if self.exclusion_config.get('include_dev_tools', False):\n\n            args.append(\"--include-dev-tools\")\n\n        result = self.run_script(\"analyze_error_handling\", *args)\n\n        output = result.get('output', '')\n\n        data = None\n\n        if output:\n\n            try:\n\n                # Find the JSON part in the output (after the text)\n\n                lines = output.split('\\n')\n\n                json_start = -1\n\n                for i, line in enumerate(lines):\n\n                    if line.strip().startswith('{'):\n\n                        json_start = i\n\n                        break\n\n                if json_start >= 0:\n\n                    json_output = '\\n'.join(lines[json_start:])\n\n                    data = json.loads(json_output)\n\n                else:\n\n                    # Fallback: try to parse the entire output\n\n                    data = json.loads(output)\n\n            except json.JSONDecodeError:\n\n                data = None\n\n        # If JSON parsing from stdout failed, try reading from the file\n        # LEGACY COMPATIBILITY: Reading from old file location for backward compatibility\n        # New standardized storage location: error_handling/jsons/analyze_error_handling_results.json\n        # Removal plan: After analyze_error_handling.py is updated to use standardized storage, remove this fallback.\n        # Detection: Search for \"error_handling_details.json\" to find all references.\n        if data is None:\n            try:\n                # Try old file location first (backward compatibility)\n                json_file = self.project_root / 'development_tools' / 'error_handling' / 'error_handling_details.json'\n                if json_file.exists():\n                    with open(json_file, 'r', encoding='utf-8') as f:\n                        file_data = json.load(f)\n                    # Handle new structure with metadata wrapper\n                    if 'error_handling_results' in file_data:\n                        data = file_data['error_handling_results']\n                    else:\n                        # Fallback to old structure (direct results)\n                        data = file_data\n                else:\n                    # Try new standardized storage location\n                    from .output_storage import load_tool_result\n                    data = load_tool_result('analyze_error_handling', 'error_handling', project_root=self.project_root)\n            except (OSError, json.JSONDecodeError):\n                data = None\n\n        if data is not None:\n\n            result['data'] = data\n\n            # Save to standardized storage\n            try:\n                save_tool_result('analyze_error_handling', 'error_handling', data, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_error_handling result: {e}\")\n\n            # Check for issues in error handling coverage\n            # NOTE: 'error_handling_coverage' is a backward compatibility fallback for old JSON format\n            coverage = data.get('analyze_error_handling') or data.get('error_handling_coverage', 0)\n\n            missing_count = data.get('functions_missing_error_handling', 0)\n\n            result['issues_found'] = coverage < 80 or missing_count > 0\n\n            result['success'] = True\n\n            result['error'] = ''\n\n        else:\n\n            lowered = output.lower() if isinstance(output, str) else ''\n\n            if 'missing error handling' in lowered or 'coverage' in lowered:\n\n                result['issues_found'] = True\n\n                result['success'] = True\n\n                result['error'] = ''\n\n        return result\n\n    def run_analyze_documentation_sync(self) -> Dict:\n        \"\"\"Run analyze_documentation_sync with structured data handling.\"\"\"\n        try:\n            # Import and call the checker directly to get structured data\n            from ..docs.analyze_documentation_sync import DocumentationSyncChecker\n            \n            checker = DocumentationSyncChecker()\n            results = checker.run_checks()\n            \n            # Convert results to the expected format\n            summary = results.get('summary', {})\n            data = {\n                'status': summary.get('status', 'UNKNOWN'),\n                'total_issues': summary.get('total_issues', 0),\n                'paired_doc_issues': summary.get('paired_doc_issues', 0),\n                'path_drift_issues': summary.get('path_drift_issues', 0),\n                'ascii_compliance_issues': summary.get('ascii_compliance_issues', 0),\n                'heading_numbering_issues': summary.get('heading_numbering_issues', 0),\n                'path_drift_files': list(results.get('path_drift', {}).keys()),\n                # Store detailed issues for each category\n                'paired_docs': results.get('paired_docs', {}),\n                'path_drift': results.get('path_drift', {}),\n                'ascii_compliance': results.get('ascii_compliance', {}),\n                'heading_numbering': results.get('heading_numbering', {})\n            }\n            \n            # Generate text output for compatibility\n            import io\n            import sys\n            output_buffer = io.StringIO()\n            original_stdout = sys.stdout\n            sys.stdout = output_buffer\n            try:\n                checker.print_report(results)\n                output = output_buffer.getvalue()\n            finally:\n                sys.stdout = original_stdout\n            \n            return {\n                'success': True,\n                'output': output,\n                'error': '',\n                'returncode': 0,\n                'data': data\n            }\n        except Exception as e:\n            logger.error(f\"Error running documentation sync checker: {e}\")\n            # Fallback to subprocess method\n        result = self.run_script(\"analyze_documentation_sync\", \"--check\")\n        output = result.get('output', '')\n        data = None\n        \n        if output:\n            try:\n                # Try to parse JSON output if available\n                data = json.loads(output)\n            except json.JSONDecodeError:\n                # If not JSON, create structured data from text output\n                data = self._parse_documentation_sync_output(output)\n        \n        if data is not None:\n            result['data'] = data\n            \n            # Save to standardized storage\n            try:\n                save_tool_result('analyze_documentation_sync', 'docs', data, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_documentation_sync result: {e}\")\n            \n            result['success'] = True\n            result['error'] = ''\n        else:\n            result['success'] = False\n            result['error'] = f'Failed to parse documentation sync output: {e}'\n        \n        return result\n\n    def _parse_documentation_sync_output(self, output: str) -> Dict:\n        \"\"\"Parse documentation sync checker text output into structured data.\"\"\"\n        data = {\n            'path_drift_issues': 0,\n            'paired_doc_issues': 0,\n            'ascii_issues': 0,\n            'path_drift_files': [],\n            'status': 'UNKNOWN'\n        }\n        \n        lines = output.split('\\n')\n        for line in lines:\n            line = line.strip()\n            if 'Path drift issues:' in line:\n                try:\n                    data['path_drift_issues'] = int(line.split(':')[1].strip())\n                except (ValueError, IndexError):\n                    pass\n            elif 'Paired documentation issues:' in line:\n                try:\n                    data['paired_doc_issues'] = int(line.split(':')[1].strip())\n                except (ValueError, IndexError):\n                    pass\n            elif 'ASCII compliance issues:' in line:\n                try:\n                    data['ascii_issues'] = int(line.split(':')[1].strip())\n                except (ValueError, IndexError):\n                    pass\n            elif 'Overall status:' in line:\n                data['status'] = line.split(':')[1].strip()\n        \n        return data\n\n    def run_analyze_legacy_references(self) -> Dict:\n        \"\"\"Run analyze_legacy_references with structured data handling.\"\"\"\n        try:\n            # Import and call the analyzer directly to get structured data\n            from ..legacy.analyze_legacy_references import LegacyReferenceAnalyzer\n            \n            analyzer = LegacyReferenceAnalyzer(project_root=str(self.project_root))\n            findings = analyzer.scan_for_legacy_references()\n            \n            # Calculate summary statistics\n            total_files = sum(len(files) for files in findings.values())\n            total_markers = sum(len(matches) for files in findings.values() for _, _, matches in files)\n            \n            # Convert findings to JSON-serializable format\n            # Findings is Dict[str, List[Tuple[str, str, List[Dict]]]]\n            # We need to convert tuples to lists for JSON serialization\n            serializable_findings = {}\n            for pattern_type, file_list in findings.items():\n                serializable_findings[pattern_type] = [\n                    [file_path, content, matches] for file_path, content, matches in file_list\n                ]\n            \n            data = {\n                'findings': serializable_findings,\n                'files_with_issues': total_files,\n                'legacy_markers': total_markers,\n                'report_path': 'development_docs/LEGACY_REFERENCE_REPORT.md'\n            }\n            \n            # Save to standardized storage\n            try:\n                from .output_storage import save_tool_result\n                save_tool_result('analyze_legacy_references', 'legacy', data, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_legacy_references result: {e}\")\n            \n            # Store in results_cache and legacy_cleanup_summary\n            self.results_cache['analyze_legacy_references'] = data\n            self.legacy_cleanup_summary = data\n            \n            return {\n                'success': True,\n                'output': f\"Found {total_files} files with {total_markers} legacy markers\",\n                'error': '',\n                'returncode': 0,\n                'data': data\n            }\n        except Exception as e:\n            logger.error(f\"Failed to run analyze_legacy_references: {e}\")\n            return {\n                'success': False,\n                'output': '',\n                'error': str(e),\n                'returncode': 1,\n                'data': None\n            }\n\n    def run_analyze_unused_imports(self) -> Dict:\n\n        \"\"\"Run analyze_unused_imports with structured JSON handling.\"\"\"\n\n        # Use longer timeout for this script (10 minutes) as it runs pylint on many files\n\n        script_path = Path(__file__).resolve().parent.parent / 'imports' / 'analyze_unused_imports.py'\n\n        cmd = [sys.executable, str(script_path), '--json']\n\n        try:\n\n            result_proc = subprocess.run(\n\n                cmd,\n\n                capture_output=True,\n\n                text=True,\n\n                cwd=str(self.project_root),\n\n                timeout=600  # 10 minute timeout for pylint operations\n\n            )\n\n            result = {\n\n                'success': result_proc.returncode == 0,\n\n                'output': result_proc.stdout,\n\n                'error': result_proc.stderr,\n\n                'returncode': result_proc.returncode\n\n            }\n\n        except subprocess.TimeoutExpired:\n\n            return {\n\n                'success': False,\n\n                'output': '',\n\n                'error': 'Unused imports checker timed out after 10 minutes',\n\n                'returncode': None,\n\n                'issues_found': False\n\n            }\n\n        output = result.get('output', '')\n\n        data = None\n\n        if output:\n\n            try:\n\n                data = json.loads(output)\n\n            except json.JSONDecodeError:\n\n                data = None\n\n        if data is not None:\n\n            result['data'] = data\n\n            self.results_cache['unused_imports'] = data\n\n            # Check for issues\n\n            total_unused = data.get('total_unused', 0)\n\n            result['issues_found'] = total_unused > 0\n\n            result['success'] = True\n\n            result['error'] = ''\n\n        else:\n\n            lowered = output.lower() if isinstance(output, str) else ''\n\n            if 'unused import' in lowered:\n\n                result['issues_found'] = True\n\n                result['success'] = True\n\n                result['error'] = ''\n\n        return result\n\n    # ===== SIMPLE COMMANDS (for users) =====\n\n    def run_audit(self, quick: bool = False, full: bool = False, include_overlap: bool = False):\n\n        \"\"\"Run audit workflow with three-tier structure.\n\n        Args:\n            quick: If True, run Tier 1 (quick audit) only\n            full: If True, run Tier 3 (full audit) - includes all tiers\n            include_overlap: Include overlap analysis in documentation checks\n\n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        # Determine audit tier\n        if quick:\n            tier = 1\n            operation_name = \"audit --quick (Tier 1)\"\n        elif full:\n            tier = 3\n            operation_name = \"audit --full (Tier 3)\"\n        else:\n            tier = 2\n            operation_name = \"audit (Tier 2 - standard)\"\n\n        logger.info(f\"Starting {operation_name}...\")\n        logger.info(\"=\" * 50)\n\n        # Store audit tier for status file generation\n        self.current_audit_tier = tier\n\n        # Store overlap flag for tools that need it\n        self._include_overlap = include_overlap\n        if full and not include_overlap:\n            include_overlap = True\n            self._include_overlap = True\n\n        # Run tier-based tools\n        success = True\n        try:\n            # Tier 1: Quick audit tools (always run first)\n            logger.info(\"Running Tier 1 tools (quick audit)...\")\n            tier1_success = self._run_quick_audit_tools()\n            if not tier1_success:\n                success = False\n\n            # Tier 2: Standard audit tools (run if tier >= 2)\n            if tier >= 2:\n                logger.info(\"Running Tier 2 tools (standard audit)...\")\n                tier2_success = self._run_standard_audit_tools()\n                if not tier2_success:\n                    success = False\n\n            # Tier 3: Full audit tools (run if tier >= 3)\n            if tier >= 3:\n                logger.info(\"Running Tier 3 tools (full audit)...\")\n                tier3_success = self._run_full_audit_tools()\n                if not tier3_success:\n                    success = False\n\n        except Exception as e:\n            logger.error(f\"Error during audit execution: {e}\", exc_info=True)\n            success = False\n\n        # Save all tool results to central aggregation file\n        try:\n            self._save_audit_results_aggregated(tier)\n        except Exception as e:\n            logger.warning(f\"Failed to save aggregated audit results: {e}\")\n\n        # Reload all cache data to ensure we have the latest state before generating status\n        self._reload_all_cache_data()\n        \n        # Sync TODO.md with changelog BEFORE generating status reports\n        self._sync_todo_with_changelog()\n\n        # Generate all 4 output files ONCE at the end (not mid-audit)\n        try:\n            # Create AI-optimized status document\n            ai_status = self._generate_ai_status_document()\n            ai_status_file = create_output_file(\"development_tools/AI_STATUS.md\", ai_status)\n\n            # Create AI-optimized priorities document\n            ai_priorities = self._generate_ai_priorities_document()\n            ai_priorities_file = create_output_file(\"development_tools/AI_PRIORITIES.md\", ai_priorities)\n\n            # Create comprehensive consolidated report\n            consolidated_report = self._generate_consolidated_report()\n            consolidated_file = create_output_file(\"development_tools/consolidated_report.txt\", consolidated_report)\n\n            # Check and trim AI_CHANGELOG entries to prevent bloat\n            # Wrap in try-except to prevent errors from blocking file generation\n            try:\n                self._check_and_trim_changelog_entries()\n            except Exception as e:\n                logger.warning(f\"Changelog trim check failed (non-blocking): {e}\")\n\n            # Validate referenced paths exist\n            try:\n                self._validate_referenced_paths()\n            except Exception as e:\n                logger.warning(f\"Path validation failed (non-blocking): {e}\")\n\n            # Check for documentation duplicates and placeholders\n            try:\n                self._check_documentation_quality()\n            except Exception as e:\n                logger.warning(f\"Documentation quality check failed (non-blocking): {e}\")\n\n            # Check ASCII compliance\n            try:\n                self._check_ascii_compliance()\n            except Exception as e:\n                logger.warning(f\"ASCII compliance check failed (non-blocking): {e}\")\n\n            # Audit completed\n            logger.info(\"=\" * 50)\n            if success:\n                logger.info(f\"Completed {operation_name} successfully!\")\n                logger.info(f\"* AI Status: {ai_status_file}\")\n                logger.info(f\"* AI Priorities: {ai_priorities_file}\")\n                logger.info(f\"* Consolidated Report: {consolidated_file}\")\n                logger.info(f\"* JSON Data: development_tools/reports/analysis_detailed_results.json\")\n                logger.info(\"* Check development_tools/reports/archive/ for previous runs\")\n            else:\n                logger.warning(f\"Completed {operation_name} with some errors (see above)\")\n        except Exception as e:\n            import traceback\n            logger.error(f\"Error generating status files: {e}\")\n            logger.error(f\"Full traceback:\\n{traceback.format_exc()}\")\n            success = False\n\n        return success\n\n    # NOTE: This method manually merges results from tools that run during run_status().\n    # TODO: Consider refactoring to use standardized storage (save_tool_result) instead of manual merging.\n    # This would make it consistent with the audit tier system.\n    def _save_additional_tool_results(self):\n        \"\"\"Save results from additional tools to the cached file\n        \n        This method manually merges results from tools that run during run_status() but aren't\n        part of the audit tier system. Consider refactoring to use standardized storage.\n        \"\"\"\n        try:\n            import json\n            from datetime import datetime\n            results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n            \n            # Load existing results\n            if results_file.exists():\n                with open(results_file, 'r', encoding='utf-8') as f:\n                    cached_data = json.load(f)\n            else:\n                cached_data = {'results': {}}\n            \n            # Add legacy cleanup results if available\n            if hasattr(self, 'legacy_cleanup_summary') and self.legacy_cleanup_summary:\n                cached_data['results']['fix_legacy_references'] = {\n                    'success': True,\n                    'data': self.legacy_cleanup_summary,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            # Add validation results if available\n            if hasattr(self, 'validation_results') and self.validation_results:\n                cached_data['results']['analyze_ai_work'] = {\n                    'success': True,\n                    'data': self.validation_results,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            # Add system signals results if available\n            if hasattr(self, 'system_signals') and self.system_signals:\n                cached_data['results']['system_signals'] = {\n                    'success': True,\n                    'data': self.system_signals,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            # Add decision_support metrics if available\n            decision_metrics = self.results_cache.get('decision_support_metrics', {})\n            if decision_metrics:\n                cached_data['results']['decision_support'] = {\n                    'success': True,\n                    'data': {\n                        'decision_support_metrics': decision_metrics\n                    },\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            # Add aggregated doc sync summary if available\n            if hasattr(self, 'docs_sync_summary') and self.docs_sync_summary:\n                cached_data['results']['analyze_documentation_sync'] = {\n                    'success': True,\n                    'data': self.docs_sync_summary,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            # Add analyze_documentation results if available (includes overlap data)\n            if 'analyze_documentation' in self.results_cache:\n                analyze_docs_data = self.results_cache['analyze_documentation']\n                cached_data['results']['analyze_documentation'] = {\n                    'success': True,\n                    'data': analyze_docs_data,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            # Save updated results using create_output_file to ensure correct location and rotation\n            from ..shared.file_rotation import create_output_file\n            create_output_file(str(results_file), json.dumps(cached_data, indent=2))\n                \n        except Exception as e:\n            logger.warning(f\"Failed to save additional tool results: {e}\")\n\n    def _reload_all_cache_data(self):\n        \"\"\"Reload all cache data from disk to ensure we have the latest state before generating status files.\n        \n        Tries to load from standardized storage first, then falls back to central aggregation file.\n        \"\"\"\n        try:\n            # First, try loading from standardized storage (individual tool result files)\n            all_results = get_all_tool_results(project_root=self.project_root)\n            if all_results:\n                for tool_name, result_data in all_results.items():\n                    if isinstance(result_data, dict):\n                        tool_data = result_data.get('data', result_data)\n                        self.results_cache[tool_name] = tool_data\n                        # Special handling for analyze_documentation_sync to populate docs_sync_summary\n                        if tool_name == 'analyze_documentation_sync' and isinstance(tool_data, dict):\n                            self.docs_sync_summary = tool_data\n                        # Special handling for analyze_legacy_references to populate legacy_cleanup_summary\n                        if tool_name == 'analyze_legacy_references' and isinstance(tool_data, dict):\n                            self.legacy_cleanup_summary = tool_data\n            \n            # Also try loading from central aggregation file (backward compatibility fallback)\n            import json\n            results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n            \n            if results_file.exists():\n                with open(results_file, 'r', encoding='utf-8') as f:\n                    cached_data = json.load(f)\n                    \n                # Reload results_cache from cached data (only if not already loaded from standardized storage)\n                if 'results' in cached_data:\n                    for tool_name, tool_data in cached_data['results'].items():\n                        if tool_name not in self.results_cache and 'data' in tool_data:\n                            self.results_cache[tool_name] = tool_data['data']\n                    \n                    # Reload decision_support_metrics if present\n                    if 'decision_support' in cached_data['results']:\n                        ds_data = cached_data['results']['decision_support']\n                        if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                            self.results_cache['decision_support_metrics'] = ds_data['data']['decision_support_metrics']\n                \n                # Reload doc sync summary if present (only if not already loaded from standardized storage)\n                if not self.docs_sync_summary and 'analyze_documentation_sync' in cached_data.get('results', {}):\n                    doc_sync_data = cached_data['results']['analyze_documentation_sync']\n                    if 'data' in doc_sync_data:\n                        self.docs_sync_summary = doc_sync_data['data']\n                \n                # Reload legacy cleanup summary if present (only if not already loaded from standardized storage)\n                if not hasattr(self, 'legacy_cleanup_summary') or not self.legacy_cleanup_summary:\n                    if 'analyze_legacy_references' in cached_data.get('results', {}):\n                        legacy_data = cached_data['results']['analyze_legacy_references']\n                        if 'data' in legacy_data:\n                            self.legacy_cleanup_summary = legacy_data['data']\n                \n                # Reload coverage summary\n                coverage_summary = self._load_coverage_summary()\n                if coverage_summary:\n                    # Coverage summary is loaded on-demand, but we ensure it's fresh\n                    pass\n                \n                # Reload dev tools coverage\n                if not hasattr(self, 'dev_tools_coverage_results') or not self.dev_tools_coverage_results:\n                    self._load_dev_tools_coverage()\n                \n                # Reload config validation summary (loaded on-demand in status generation)\n                # Reload module dependency summary (should already be in results_cache)\n                if 'analyze_module_dependencies' in cached_data.get('results', {}):\n                    dep_data = cached_data['results']['analyze_module_dependencies']\n                    if 'data' in dep_data:\n                        self.module_dependency_summary = dep_data['data']\n                        \n        except Exception as e:\n            logger.debug(f\"Failed to reload cache data: {e}\")\n\n    def _run_quick_audit_tools(self) -> bool:\n        \"\"\"Run Tier 1 tools: Quick audit (core metrics only).\n        \n        Returns:\n            True if all tools succeeded, False otherwise\n        \"\"\"\n        successful = []\n        failed = []\n        \n        # Tier 1 tools: Core metrics only\n        tier1_tools = [\n            ('analyze_functions', self.run_analyze_functions),\n            ('analyze_documentation_sync', self.run_analyze_documentation_sync),\n            ('system_signals', self.run_system_signals),\n        ]\n        \n        # Handle quick_status separately to parse JSON output\n        try:\n            logger.info(\"  - Running quick_status...\")\n            quick_status_result = self.run_script('quick_status', 'json')\n            if quick_status_result.get('success'):\n                self.status_results = quick_status_result\n                # Parse JSON and store in status_summary\n                output = quick_status_result.get('output', '')\n                if output:\n                    try:\n                        import json\n                        parsed = json.loads(output)\n                        self.status_summary = parsed\n                        quick_status_result['data'] = parsed\n                        # Save to standardized storage\n                        try:\n                            save_tool_result('quick_status', 'reports', parsed, project_root=self.project_root)\n                        except Exception as e:\n                            logger.debug(f\"Failed to save quick_status result: {e}\")\n                        successful.append('quick_status')\n                    except json.JSONDecodeError:\n                        logger.warning(\"  - quick_status output could not be parsed as JSON\")\n                        failed.append('quick_status')\n                else:\n                    failed.append('quick_status')\n            else:\n                failed.append('quick_status')\n                if quick_status_result.get('error'):\n                    logger.error(f\"  - quick_status failed: {quick_status_result['error']}\")\n        except Exception as exc:\n            failed.append('quick_status')\n            logger.error(f\"  - quick_status failed: {exc}\")\n        \n        for tool_name, tool_func in tier1_tools:\n            try:\n                logger.info(f\"  - Running {tool_name}...\")\n                result = tool_func()\n                \n                # Handle both dict and bool return types\n                if isinstance(result, dict):\n                    success = result.get('success', False)\n                    if 'data' in result:\n                        self._extract_key_info(tool_name, result)\n                else:\n                    success = bool(result)\n                \n                if success:\n                    successful.append(tool_name)\n                    # Save result to standardized storage if it has data\n                    if isinstance(result, dict) and 'data' in result:\n                        try:\n                            # Determine domain for tool using standardized function\n                            domain = _get_domain_from_tool_name(tool_name, self.project_root)\n                            save_tool_result(tool_name, domain, result['data'], project_root=self.project_root)\n                        except Exception as e:\n                            logger.debug(f\"Failed to save {tool_name} result: {e}\")\n                else:\n                    failed.append(tool_name)\n                    error_msg = result.get('error', 'Unknown error') if isinstance(result, dict) else str(result)\n                    logger.warning(f\"  - {tool_name} completed with issues: {error_msg}\")\n            except Exception as exc:\n                failed.append(tool_name)\n                logger.error(f\"  - {tool_name} failed: {exc}\")\n        \n        if failed:\n            logger.warning(f\"Tier 1 completed with {len(failed)} failure(s): {', '.join(failed)}\")\n        else:\n            logger.info(f\"Tier 1 completed successfully ({len(successful)} tools)\")\n        \n        return len(failed) == 0\n\n    def _run_standard_audit_tools(self) -> bool:\n        \"\"\"Run Tier 2 tools: Standard audit (quality checks).\n        \n        Note: Tier 1 tools are already run before this method is called.\n        \n        Returns:\n            True if all tools succeeded, False otherwise\n        \"\"\"\n        successful = []\n        failed = []\n        \n        # Tier 2 tools: Quality checks\n        tier2_tools = [\n            ('analyze_documentation', lambda: self.run_analyze_documentation(include_overlap=getattr(self, '_include_overlap', False))),\n            ('analyze_error_handling', self.run_analyze_error_handling),\n            ('decision_support', self.run_decision_support),\n            ('analyze_config', lambda: self.run_script('analyze_config')),\n            ('analyze_ai_work', self.run_validate),\n            # Doc validators (check if docs need regeneration)\n            ('analyze_function_registry', self.run_analyze_function_registry),\n            ('analyze_module_dependencies', self.run_analyze_module_dependencies),\n        ]\n        \n        for tool_name, tool_func in tier2_tools:\n            try:\n                logger.info(f\"  - Running {tool_name}...\")\n                result = tool_func()\n                \n                # Handle both dict and bool return types\n                if isinstance(result, dict):\n                    success = result.get('success', False)\n                    if 'data' in result:\n                        self._extract_key_info(tool_name, result)\n                else:\n                    success = bool(result)\n                \n                if success:\n                    successful.append(tool_name)\n                    # Save result to standardized storage if it has data\n                    if isinstance(result, dict) and 'data' in result:\n                        try:\n                            # Determine domain for tool using standardized function\n                            domain = _get_domain_from_tool_name(tool_name, self.project_root)\n                            save_tool_result(tool_name, domain, result['data'], project_root=self.project_root)\n                        except Exception as e:\n                            logger.debug(f\"Failed to save {tool_name} result: {e}\")\n                else:\n                    failed.append(tool_name)\n                    error_msg = result.get('error', 'Unknown error') if isinstance(result, dict) else str(result)\n                    logger.warning(f\"  - {tool_name} completed with issues: {error_msg}\")\n            except Exception as exc:\n                failed.append(tool_name)\n                logger.error(f\"  - {tool_name} failed: {exc}\")\n        \n        if failed:\n            logger.warning(f\"Tier 2 completed with {len(failed)} failure(s): {', '.join(failed)}\")\n        else:\n            logger.info(f\"Tier 2 completed successfully ({len(successful)} tools)\")\n        \n        return len(failed) == 0\n\n    def _run_full_audit_tools(self) -> bool:\n        \"\"\"Run Tier 3 tools: Full audit (comprehensive analysis).\n        \n        Note: Tier 1 and Tier 2 tools are already run before this method is called.\n        \n        Returns:\n            True if all tools succeeded, False otherwise\n        \"\"\"\n        successful = []\n        failed = []\n        \n        # Tier 3 analyze tools\n        tier3_analyze_tools = [\n            ('generate_test_coverage', self.run_coverage_regeneration),\n            ('analyze_unused_imports', self.run_unused_imports_report),\n            ('analyze_legacy_references', self.run_analyze_legacy_references),\n        ]\n        \n        # Tier 3 report generators\n        tier3_report_tools = [\n            ('generate_legacy_reference_report', lambda: self.run_script('generate_legacy_reference_report')),\n            ('generate_test_coverage_reports', lambda: self.run_script('generate_test_coverage_reports')),\n            # Note: analyze_unused_imports generates UNUSED_IMPORTS_REPORT.md\n        ]\n        \n        # Run analyze tools\n        for tool_name, tool_func in tier3_analyze_tools:\n            try:\n                logger.info(f\"  - Running {tool_name}...\")\n                result = tool_func()\n                \n                # Handle both dict and bool return types\n                if isinstance(result, dict):\n                    success = result.get('success', False)\n                    if 'data' in result:\n                        self._extract_key_info(tool_name, result)\n                else:\n                    success = bool(result)\n                \n                if success:\n                    successful.append(tool_name)\n                    # Save result to standardized storage if it has data\n                    if isinstance(result, dict) and 'data' in result:\n                        try:\n                            # Determine domain for tool using standardized function\n                            domain = _get_domain_from_tool_name(tool_name, self.project_root)\n                            save_tool_result(tool_name, domain, result['data'], project_root=self.project_root)\n                        except Exception as e:\n                            logger.debug(f\"Failed to save {tool_name} result: {e}\")\n                else:\n                    failed.append(tool_name)\n                    error_msg = result.get('error', 'Unknown error') if isinstance(result, dict) else str(result)\n                    logger.warning(f\"  - {tool_name} completed with issues: {error_msg}\")\n            except Exception as exc:\n                failed.append(tool_name)\n                logger.error(f\"  - {tool_name} failed: {exc}\")\n        \n        # Run report generators\n        for tool_name, tool_func in tier3_report_tools:\n            try:\n                logger.info(f\"  - Running {tool_name} (report generation)...\")\n                # Prepare arguments for report generators\n                if tool_name == 'generate_legacy_reference_report':\n                    # Load legacy reference findings from standardized storage or results_cache\n                    try:\n                        # First try results_cache (data from current audit run)\n                        legacy_data = None\n                        if 'analyze_legacy_references' in self.results_cache:\n                            legacy_data = self.results_cache['analyze_legacy_references']\n                        else:\n                            # Try standardized storage\n                            from .output_storage import load_tool_result\n                            legacy_result = load_tool_result('analyze_legacy_references', 'legacy', project_root=self.project_root)\n                            if legacy_result:\n                                # load_tool_result already unwraps the 'data' key, so legacy_result IS the data\n                                legacy_data = legacy_result\n                        \n                        if legacy_data and isinstance(legacy_data, dict):\n                            # Extract findings from the data structure\n                            findings = legacy_data.get('findings', {})\n                            if findings:\n                                # Convert back to the format expected by generate_legacy_reference_report\n                                # Findings are stored as Dict[str, List[List]] (file_path, content, matches)\n                                # But generate_legacy_reference_report expects Dict[str, List[Tuple]]\n                                # We'll pass it as-is since JSON can handle lists\n                                import tempfile\n                                import json\n                                temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8')\n                                json.dump(findings, temp_file, indent=2)\n                                temp_file.close()\n                                result = self.run_script('generate_legacy_reference_report', '--findings-file', temp_file.name)\n                                # Clean up temp file\n                                try:\n                                    import os\n                                    os.unlink(temp_file.name)\n                                except Exception:\n                                    pass\n                                # Verify report was generated\n                                report_path = self.project_root / \"development_docs\" / \"LEGACY_REFERENCE_REPORT.md\"\n                                if report_path.exists():\n                                    logger.info(f\"  - Legacy reference report generated: {report_path}\")\n                                else:\n                                    logger.warning(f\"  - Legacy reference report not found at {report_path}\")\n                            else:\n                                logger.warning(f\"  - {tool_name}: No legacy reference findings found in data\")\n                                result = {'success': False, 'error': 'No legacy reference findings available'}\n                        else:\n                            logger.warning(f\"  - {tool_name}: No legacy reference data found in standardized storage or cache\")\n                            result = {'success': False, 'error': 'No legacy reference findings available'}\n                    except Exception as e:\n                        logger.warning(f\"  - {tool_name}: Failed to load findings: {e}\")\n                        result = {'success': False, 'error': str(e)}\n                elif tool_name == 'generate_test_coverage_reports':\n                    # Use coverage.json from project root\n                    coverage_json = self.project_root / 'coverage.json'\n                    if coverage_json.exists():\n                        result = self.run_script('generate_test_coverage_reports', '--input', str(coverage_json))\n                    else:\n                        logger.warning(f\"  - {tool_name}: coverage.json not found at {coverage_json}\")\n                        result = {'success': False, 'error': f'coverage.json not found at {coverage_json}'}\n                else:\n                    result = tool_func()\n                \n                if isinstance(result, dict):\n                    success = result.get('success', False)\n                    # Save result if available\n                    if success and 'data' in result:\n                        try:\n                            # Determine domain for tool using standardized function\n                            domain = _get_domain_from_tool_name(tool_name, self.project_root)\n                            save_tool_result(tool_name, domain, result['data'], project_root=self.project_root)\n                        except Exception as e:\n                            logger.debug(f\"Failed to save {tool_name} result: {e}\")\n                else:\n                    success = bool(result)\n                \n                if success:\n                    successful.append(tool_name)\n                else:\n                    failed.append(tool_name)\n                    error_msg = result.get('error', 'Unknown error') if isinstance(result, dict) else str(result)\n                    logger.warning(f\"  - {tool_name} completed with issues: {error_msg}\")\n            except Exception as exc:\n                failed.append(tool_name)\n                logger.error(f\"  - {tool_name} failed: {exc}\")\n        \n        # Run dev tools coverage (Tier 3 only)\n        try:\n            logger.info(\"  - Running development tools coverage analysis...\")\n            dev_tools_coverage = self.run_dev_tools_coverage()\n            if dev_tools_coverage.get('coverage_collected'):\n                overall = dev_tools_coverage.get('overall', {})\n                coverage_pct = overall.get('overall_coverage', 0)\n                logger.info(f\"  - Dev tools coverage: {coverage_pct:.1f}%\")\n                successful.append('dev_tools_coverage')\n            else:\n                logger.warning(\"  - Dev tools coverage data not collected\")\n                failed.append('dev_tools_coverage')\n        except Exception as exc:\n            logger.error(f\"  - Development tools coverage failed: {exc}\")\n            failed.append('dev_tools_coverage')\n        \n        if failed:\n            logger.warning(f\"Tier 3 completed with {len(failed)} failure(s): {', '.join(failed)}\")\n        else:\n            logger.info(f\"Tier 3 completed successfully ({len(successful)} tools)\")\n        \n        return len(failed) == 0\n\n    def _check_and_trim_changelog_entries(self) -> None:\n        \"\"\"Check and trim AI_CHANGELOG entries to prevent bloat.\"\"\"\n        try:\n            from ai_development_docs import changelog_manager  # type: ignore\n        except Exception:\n            changelog_manager = None\n        if changelog_manager and hasattr(changelog_manager, 'trim_change_log'):\n            try:\n                result = changelog_manager.trim_change_log()\n                if isinstance(result, dict):\n                    status = result.get('status')\n                    if status == 'ok':\n                        trimmed = result.get('trimmed_entries')\n                        archive_created = result.get('archive_created')\n                    if trimmed:\n                        logger.info(f\"   Trimmed {trimmed} old changelog entries\")\n                    if archive_created:\n                        logger.info(\"   Created archive: ai_development_docs/AI_CHANGELOG_ARCHIVE.md\")\n                else:\n                    logger.warning(f\"   Changelog trim reported an issue: {result.get('message')}\")\n            except Exception as exc:\n                logger.warning(f\"   Changelog check/trim failed: {exc}\")\n        else:\n            logger.info(\"   Changelog check: Tooling unavailable (skipping trim)\")\n\n    def _validate_referenced_paths(self) -> None:\n        \"\"\"Validate that all referenced paths in documentation exist.\"\"\"\n        try:\n            from ..docs.fix_version_sync import validate_referenced_paths  # type: ignore\n            result = validate_referenced_paths()\n            status = result.get('status') if isinstance(result, dict) else None\n            message = result.get('message') if isinstance(result, dict) else None\n            # Store path validation result for display in status\n            if isinstance(result, dict):\n                self.path_validation_result = result\n            if status == 'ok':\n                logger.info(f\"   Path validation: {message}\")\n            elif status == 'fail':\n                issues = result.get('issues_found', 'unknown') if isinstance(result, dict) else 'unknown'\n                logger.warning(f\"   Path validation failed: {message}\")\n                logger.warning(f\"   Found {issues} path issues - consider running documentation sync checker\")\n            else:\n                logger.warning(f\"   Path validation error: {message}\")\n        except Exception as exc:\n            logger.warning(f\"   Path validation failed: {exc}\")\n            self.path_validation_result = None\n\n    def _check_documentation_quality(self) -> None:\n        \"\"\"Check for documentation duplicates and placeholder content.\"\"\"\n        try:\n            data = self.results_cache.get('analyze_documentation')\n            if not isinstance(data, dict):\n                result = self.run_analyze_documentation()\n                data = result.get('data') if isinstance(result, dict) else None\n                if isinstance(data, dict):\n                    self.results_cache['analyze_documentation'] = data\n            if isinstance(data, dict):\n                duplicates = data.get('duplicates') or []\n                placeholders = data.get('placeholders') or []\n                if duplicates:\n                    logger.warning(f\"   Documentation quality: Found {len(duplicates)} verbatim duplicates\")\n                    logger.warning(\"   -> Remove duplicates between AI and human docs\")\n                else:\n                    logger.info(\"   Documentation quality: No verbatim duplicates found\")\n                if placeholders:\n                    logger.warning(f\"   Documentation quality: Found {len(placeholders)} files with placeholders\")\n                    logger.warning(\"   -> Replace placeholder content with actual content\")\n                else:\n                    logger.info(\"   Documentation quality: No placeholder content found\")\n            else:\n                logger.warning(\"   Documentation quality check unavailable: no analysis data\")\n        except Exception as exc:\n            logger.warning(f\"   Documentation quality check failed: {exc}\")\n\n    def _check_ascii_compliance(self) -> None:\n        \"\"\"Check for non-ASCII characters in documentation files.\"\"\"\n        try:\n            from ..docs.analyze_ascii_compliance import ASCIIComplianceAnalyzer  # type: ignore\n            analyzer = ASCIIComplianceAnalyzer()\n            results = analyzer.check_ascii_compliance()\n            total_issues = sum(len(issues) for issues in results.values())\n            if total_issues == 0:\n                logger.info(\"   ASCII compliance: All documentation files use ASCII-only characters\")\n            else:\n                logger.warning(f\"   ASCII compliance: Found {total_issues} non-ASCII characters in {len(results)} files\")\n                logger.warning(\"   -> Replace non-ASCII characters with ASCII equivalents\")\n        except Exception as exc:\n            logger.warning(f\"   ASCII compliance check failed: {exc}\")\n\n    def _sync_todo_with_changelog(self) -> None:\n        \"\"\"Sync TODO.md with AI_CHANGELOG.md to move completed entries.\"\"\"\n        try:\n            from ..docs.fix_version_sync import sync_todo_with_changelog  # type: ignore\n            result = sync_todo_with_changelog()\n            # Store result for status reports\n            self.todo_sync_result = result\n            status = result.get('status') if isinstance(result, dict) else None\n            if status == 'ok':\n                completed_entries = result.get('completed_entries', 0)\n                moved = result.get('moved_entries', 0)\n                if moved:\n                    logger.info(f\"   TODO sync: Moved {moved} completed entries from TODO.md\")\n                    print(f\"TODO sync: Moved {moved} completed entries from TODO.md\")\n                elif completed_entries > 0:\n                    logger.info(f\"   TODO sync: Found {completed_entries} completed entries in TODO.md that need review\")\n                    print(f\"TODO sync: Found {completed_entries} completed entries in TODO.md that need review\")\n                else:\n                    message = result.get('message')\n                    logger.info(f\"   TODO sync: {message}\")\n            else:\n                message = result.get('message') if isinstance(result, dict) else None\n                logger.warning(f\"   TODO sync failed: {message}\")\n        except Exception as exc:\n            logger.warning(f\"   TODO sync failed: {exc}\")\n            self.todo_sync_result = {'status': 'error', 'message': str(exc), 'completed_entries': 0}\n\n    def run_docs(self):\n\n        \"\"\"Update all documentation (OPTIONAL - not essential for audit)\"\"\"\n\n        logger.info(\"Starting documentation update...\")\n\n        logger.info(\"Updating documentation...\")\n\n        logger.info(\"=\" * 50)\n\n        success = True\n\n        # Generate function registry\n\n        try:\n\n            logger.info(\"  - Generating function registry...\")\n\n            result = self.run_script(\"generate_function_registry\")\n\n            if result['success']:\n\n                logger.info(\"  - Function registry generated successfully\")\n\n            else:\n\n                logger.error(f\"  - Function registry generation failed: {result['error']}\")\n\n                success = False\n\n        except Exception as exc:\n\n            logger.error(f\"  - Function registry generation failed: {exc}\")\n\n            success = False\n\n        # Generate module dependencies\n\n        try:\n\n            logger.info(\"  - Generating module dependencies...\")\n\n            result = self.run_script(\"generate_module_dependencies\")\n\n            if result['success']:\n\n                logger.info(\"  - Module dependencies generated successfully\")\n\n            else:\n\n                logger.error(f\"  - Module dependencies generation failed: {result['error']}\")\n\n                success = False\n\n        except Exception as exc:\n\n            logger.error(f\"  - Module dependencies generation failed: {exc}\")\n\n            success = False\n\n        # Generate directory trees\n\n        try:\n\n            logger.info(\"  - Generating directory trees...\")\n\n            self.generate_directory_trees()\n\n        except Exception as exc:\n\n            logger.error(f\"  - Directory tree generation failed: {exc}\")\n\n            success = False\n\n        # Run documentation sync check\n\n        try:\n\n            logger.info(\"  - Checking documentation sync...\")\n\n            if not self._run_doc_sync_check('--check'):\n\n                success = False\n\n        except Exception as exc:\n\n            logger.error(f\"  - Documentation sync check failed: {exc}\")\n\n            success = False\n\n        logger.info(\"=\" * 50)\n\n        if success:\n\n            logger.info(\"Completed documentation update successfully!\")\n\n        else:\n\n            logger.warning(\"Completed documentation update with issues.\")\n\n        return success\n\n    def run_validate(self):\n\n        \"\"\"Validate AI-generated work (simple command)\"\"\"\n\n        logger.info(\"Starting validation...\")\n\n        logger.info(\"Validating AI work...\")\n\n        logger.info(\"=\" * 50)\n\n        result = self.run_script('analyze_ai_work')\n\n        if result['success']:\n\n            # Store results for consolidated report\n            self.validation_results = result\n\n            logger.info(\"=\" * 50)\n\n            logger.info(\"Validation completed successfully!\")\n\n            return True\n\n        else:\n\n            logger.error(f\"Validation failed: {result['error']}\")\n\n            return False\n\n    def run_config(self):\n\n        \"\"\"Check configuration consistency (simple command)\"\"\"\n\n        logger.info(\"Starting configuration check...\")\n\n        logger.info(\"Checking configuration...\")\n\n        logger.info(\"=\" * 50)\n\n        result = self.run_script('analyze_config')\n\n        if result['success']:\n            # Print the report instead of just logging it\n            output = result.get('output', '')\n            if output:\n                print(output)\n            else:\n                # If no output, try to extract from JSON results\n                try:\n                    import json\n                    results_file = self.project_root / \"development_tools\" / \"config\" / \"analyze_config_results.json\"\n                    if results_file.exists():\n                        with open(results_file, 'r', encoding='utf-8') as f:\n                            data = json.load(f)\n                        validation_results = data.get('validation_results', {})\n                        summary = validation_results.get('summary', {})\n                        \n                        print(\"=\" * 80)\n                        print(\"CONFIGURATION VALIDATION REPORT\")\n                        print(\"=\" * 80)\n                        print(f\"SUMMARY:\")\n                        print(f\"   Tools using config: {summary.get('tools_using_config', 0)}/{summary.get('total_tools', 0)}\")\n                        print(f\"   Configuration valid: {'YES' if summary.get('config_valid', False) else 'NO'}\")\n                        print(f\"   Configuration complete: {'YES' if summary.get('config_complete', False) else 'NO'}\")\n                        print(f\"   Recommendations: {summary.get('total_recommendations', 0)}\")\n                        \n                        # Tool analysis\n                        tools_analysis = validation_results.get('tools_analysis', {})\n                        if tools_analysis:\n                            print(f\"TOOL ANALYSIS:\")\n                            for tool_name, analysis in tools_analysis.items():\n                                status = \"OK\" if analysis.get('imports_config', False) and not analysis.get('issues', []) else \"WARN\" if analysis.get('issues', []) else \"FAIL\"\n                                print(f\"   {status} {tool_name}\")\n                                if analysis.get('issues'):\n                                    for issue in analysis['issues']:\n                                        print(f\"      Issue: {issue}\")\n                        \n                        # Recommendations\n                        recommendations = validation_results.get('recommendations', [])\n                        if recommendations:\n                            print(f\"RECOMMENDATIONS:\")\n                            for i, rec in enumerate(recommendations, 1):\n                                print(f\"   {i}. {rec}\")\n                        \n                        print(f\"Configuration validation complete!\")\n                except Exception as e:\n                    logger.debug(f\"Could not extract config report from JSON: {e}\")\n                    logger.info(\"Configuration check completed!\")\n\n            return True\n        else:\n            logger.error(f\"Configuration check failed: {result['error']}\")\n            return False\n\n    # ===== ADVANCED COMMANDS (for AI collaborators) =====\n\n    def run_workflow(self, task_type: str, task_data: Optional[Dict] = None) -> bool:\n\n        \"\"\"Run workflow with audit-first protocol\"\"\"\n\n        logger.info(f\"Running workflow: {task_type}\")\n\n        logger.info(\"=\" * 50)\n\n        # Check trigger requirements\n\n        if not self.check_trigger_requirements(task_type):\n\n            return False\n\n        # Run audit first\n\n        audit_results = self.run_audit_first(task_type)\n\n        if not audit_results['success']:\n\n            logger.error(f\"Audit failed: {audit_results['error']}\")\n\n            return False\n\n        # Execute the task\n\n        task_success = self.execute_task(task_type, task_data)\n\n        # Validate the work\n\n        if task_success:\n\n            validation_results = self.validate_work(task_type, task_data or {})\n\n            self.show_validation_report(validation_results)\n\n        return task_success\n\n    # LEGACY COMPATIBILITY\n    # This method is kept for backward compatibility with any external callers.\n    # Removal plan: Search for \"run_quick_audit\" to find all callers, update them to use run_audit(quick=True) instead, then remove this method.\n    # Detection: Search codebase for \"run_quick_audit\" to find all references.\n    def run_quick_audit(self) -> bool:\n\n        \"\"\"Run quick audit (Tier 1) - legacy method for backward compatibility.\n        \n        This method is kept for backward compatibility but now uses the new tier structure.\n        New code should use run_audit(quick=True) instead.\n        \"\"\"\n        logger.warning(\"LEGACY: run_quick_audit() is deprecated. Use run_audit(quick=True) instead.\")\n        logger.info(\"Running quick audit (Tier 1)...\")\n        \n        # Use new tier-based structure\n        return self._run_quick_audit_tools()\n\n    def run_decision_support(self):\n\n        \"\"\"Get actionable insights for decision-making\"\"\"\n\n        logger.info(\"Getting actionable insights...\")\n\n        logger.info(\"=\" * 50)\n\n        result = self.run_script('decision_support')\n\n        if result['success']:\n            # Print the key findings instead of just logging\n            output = result.get('output', '')\n            if output:\n                print(output)\n            else:\n                # If no output, try to extract key findings from JSON\n                try:\n                    import json\n                    results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                    if results_file.exists():\n                        with open(results_file, 'r', encoding='utf-8') as f:\n                            data = json.load(f)\n                        if 'results' in data and 'decision_support' in data['results']:\n                            ds_data = data['results']['decision_support']\n                            if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                                metrics = ds_data['data']['decision_support_metrics']\n                                print(\"=== AI DECISION SUPPORT DASHBOARD ===\")\n                                if metrics.get('total_functions'):\n                                    print(f\"Total functions: {metrics['total_functions']}\")\n                                if metrics.get('critical_complexity'):\n                                    print(f\"[CRITICAL] Critical Complexity (>199 nodes): {metrics['critical_complexity']}\")\n                                if metrics.get('high_complexity'):\n                                    print(f\"[HIGH] High Complexity (100-199 nodes): {metrics['high_complexity']}\")\n                                if metrics.get('moderate_complexity'):\n                                    print(f\"[MODERATE] Moderate Complexity (50-99 nodes): {metrics['moderate_complexity']}\")\n                except Exception as e:\n                    logger.debug(f\"Could not extract decision support from JSON: {e}\")\n                    logger.info(\"Decision support completed!\")\n\n            # Return the result dict so metrics can be extracted\n            return result\n        else:\n            logger.error(f\"Decision support failed: {result['error']}\")\n            return result\n\n    def run_version_sync(self, scope: str = 'docs'):\n\n        \"\"\"Sync version numbers\"\"\"\n\n        logger.info(f\"Syncing versions for scope: {scope}\")\n\n        logger.info(\"=\" * 50)\n\n        result = self.run_script('fix_version_sync', 'sync', '--scope', scope)\n\n        if result['success']:\n\n            # Store results for consolidated report\n\n            self.fix_version_sync_results = result\n\n            logger.info(\"Version sync completed!\")\n\n            return True\n\n        else:\n\n            logger.error(f\"Version sync failed: {result['error']}\")\n\n            return False\n\n    def run_dev_tools_coverage(self) -> Dict:\n        \"\"\"Run coverage analysis specifically for development_tools directory.\"\"\"\n        logger.info(\"Starting development tools coverage analysis...\")\n        \n        try:\n            from ..tests.generate_test_coverage import CoverageMetricsRegenerator\n            \n            regenerator = CoverageMetricsRegenerator()\n            result = regenerator.run_dev_tools_coverage()\n            \n            # Store results for consolidated report\n            if not hasattr(self, 'dev_tools_coverage_results'):\n                self.dev_tools_coverage_results = {}\n            self.dev_tools_coverage_results = result\n            \n            logger.info(\"Completed development tools coverage analysis!\")\n            \n            # Reload dev tools coverage after regeneration\n            self._load_dev_tools_coverage()\n            \n            return result\n            \n        except Exception as exc:\n            logger.error(f\"Development tools coverage analysis failed: {exc}\")\n            return {'coverage_collected': False, 'error': str(exc)}\n\n    def run_status(self):\n\n        \"\"\"Get current system status - quick check that updates status files\"\"\"\n\n        logger.info(\"Starting status check...\")\n\n        logger.info(\"Getting system status...\")\n\n        logger.info(\"=\" * 50)\n\n        # Run quick status for basic system health\n        # quick_status.py expects 'json' as the first argument (sys.argv[1])\n        result = self.run_script('quick_status', 'json')\n\n        if result.get('success'):\n\n            self.status_results = result\n\n            parsed = None\n\n            output = result.get('output', '')\n\n            if output:\n\n                try:\n\n                    parsed = json.loads(output)\n\n                except json.JSONDecodeError:\n\n                    parsed = None\n\n            if parsed is not None:\n\n                result['data'] = parsed\n\n                self.status_summary = parsed\n\n                logger.info(\"Status check completed!\")\n\n            else:\n\n                logger.warning(\"Status check completed, but output could not be parsed as JSON.\")\n\n            # Run TODO sync check for status\n            logger.info(\"Checking TODO sync status...\")\n            self._sync_todo_with_changelog()\n\n            # Run system signals generator\n            logger.info(\"Generating system signals...\")\n            self.run_system_signals()\n            \n            # Save additional tool results to cached file (including system signals)\n            self._save_additional_tool_results()\n\n            # Generate all three status files with current data\n            logger.info(\"Generating status files...\")\n            \n            # AI Status\n            ai_status = self._generate_ai_status_document()\n            ai_status_file = create_output_file(\"development_tools/AI_STATUS.md\", ai_status)\n            logger.info(f\"AI Status: {ai_status_file}\")\n            \n            # AI Priorities\n            ai_priorities = self._generate_ai_priorities_document()\n            ai_priorities_file = create_output_file(\"development_tools/AI_PRIORITIES.md\", ai_priorities)\n            logger.info(f\"AI Priorities: {ai_priorities_file}\")\n            \n            # Consolidated Report\n            consolidated_report = self._generate_consolidated_report()\n            consolidated_file = create_output_file(\"development_tools/consolidated_report.txt\", consolidated_report)\n            logger.info(f\"Consolidated Report: {consolidated_file}\")\n\n            logger.info(\"Completed status check successfully!\")\n\n            return True\n\n        if result.get('output'):\n\n            logger.info(result['output'])\n\n        if result.get('error'):\n\n            logger.error(f\"Completed status check with errors: {result['error']}\")\n\n        return False\n\n    def run_documentation_sync(self):\n\n        \"\"\"Run documentation synchronization checks\"\"\"\n\n        logger.info(\"Starting documentation sync check...\")\n\n        logger.info(\"Running documentation synchronization checks...\")\n\n        if self._run_doc_sync_check('--check'):\n\n            logger.info(\"Completed documentation sync check successfully!\")\n\n            return True\n\n        logger.error(\"Completed documentation sync check with errors!\")\n\n        return False\n\n    def run_documentation_fix(self, fix_type: str = 'all', dry_run: bool = False) -> bool:\n\n        \"\"\"Run documentation fix operations.\n\n        Args:\n            fix_type: Type of fix to apply ('add-addresses', 'fix-ascii', 'number-headings', 'convert-links', 'all')\n            dry_run: If True, show what would be changed without making changes\n\n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n\n        logger.info(f\"Starting documentation fix (type: {fix_type}, dry_run: {dry_run})...\")\n\n        try:\n\n            # Use decomposed fixer classes (Batch 2 decomposition)\n            from development_tools.docs.fix_documentation_addresses import DocumentationAddressFixer\n            from development_tools.docs.fix_documentation_ascii import DocumentationASCIIFixer\n            from development_tools.docs.fix_documentation_headings import DocumentationHeadingFixer\n            from development_tools.docs.fix_documentation_links import DocumentationLinkFixer\n\n            results = {}\n\n            if fix_type in ('add-addresses', 'all'):\n\n                fixer = DocumentationAddressFixer(project_root=str(self.project_root))\n                result = fixer.fix_add_addresses(dry_run=dry_run)\n\n                results['add_addresses'] = result\n\n                print(f\"\\nAdd Addresses: Updated {result['updated']}, Skipped {result['skipped']}, Errors {result['errors']}\")\n\n            if fix_type in ('fix-ascii', 'all'):\n\n                fixer = DocumentationASCIIFixer(project_root=str(self.project_root))\n                result = fixer.fix_ascii(dry_run=dry_run)\n\n                results['fix_ascii'] = result\n\n                print(f\"\\nFix ASCII: Updated {result['files_updated']} files, Made {result['replacements_made']} replacements, Errors {result['errors']}\")\n\n            if fix_type in ('number-headings', 'all'):\n\n                fixer = DocumentationHeadingFixer(project_root=str(self.project_root))\n                result = fixer.fix_number_headings(dry_run=dry_run)\n\n                results['number_headings'] = result\n\n                print(f\"\\nNumber Headings: Updated {result['files_updated']} files, Fixed {result['issues_fixed']} issues, Errors {result['errors']}\")\n\n            if fix_type in ('convert-links', 'all'):\n\n                fixer = DocumentationLinkFixer(project_root=str(self.project_root))\n                result = fixer.fix_convert_links(dry_run=dry_run)\n\n                results['convert_links'] = result\n\n                print(f\"\\nConvert Links: Updated {result['files_updated']} files, Made {result['changes_made']} changes, Errors {result['errors']}\")\n\n            # Check for errors\n\n            total_errors = sum(r.get('errors', 0) for r in results.values())\n\n            if total_errors > 0:\n\n                logger.error(f\"Documentation fix completed with {total_errors} error(s)\")\n\n                return False\n\n            logger.info(\"Completed documentation fix successfully!\")\n\n            return True\n\n        except Exception as e:\n\n            logger.error(f\"Error running documentation fix: {e}\", exc_info=True)\n\n            return False\n\n    def run_coverage_regeneration(self):\n\n        \"\"\"Regenerate test coverage metrics\"\"\"\n\n        logger.info(\"Starting coverage regeneration...\")\n\n        logger.info(\"Regenerating test coverage metrics...\")\n\n        result = self.run_script('generate_test_coverage', '--update-plan', timeout=1800)  # 30 minute timeout for coverage\n\n        # Parse test results from output\n        output = result.get('output', '')\n        error_output = result.get('error', '')\n        test_results = self._parse_test_results_from_output(output)\n        \n        # Log error output if present for debugging\n        if error_output and logger:\n            logger.warning(f\"Coverage regeneration stderr: {error_output[:500]}\")\n        \n        # Check if coverage was collected successfully\n        # Only consider it collected if we see actual test output, not just existing files\n        coverage_collected = (\n            'TOTAL' in output or \n            'coverage:' in output.lower() or\n            ('passed' in output.lower() and 'failed' in output.lower()) or  # Test results present\n            ('[  ' in output and '%]' in output)  # Progress indicators from pytest\n        )\n        \n        # If no test output detected but coverage.json exists, warn that tests may not have run\n        if not coverage_collected and ((self.project_root / \"coverage.json\").exists() or (self.project_root / \"development_tools\" / \"tests\" / \"coverage.json\").exists()):\n            if logger:\n                logger.error(\"Coverage regeneration completed with no test output detected - tests likely did not run!\")\n                logger.error(f\"Script output length: {len(output)} chars, stderr length: {len(error_output)} chars\")\n                if error_output:\n                    logger.error(f\"Script stderr (first 1000 chars): {error_output[:1000]}\")\n                logger.error(\"This indicates the test suite did not execute. Check for import errors or pytest configuration issues.\")\n            # Don't consider it collected - this is a failure\n            coverage_collected = False\n\n        if result['success']:\n\n            # Store results for consolidated report\n\n            self.coverage_results = result\n\n            logger.info(\"Completed coverage regeneration successfully!\")\n            \n            logger.info(\"Coverage metrics regenerated and plan updated!\")\n            \n            # Run test marker analysis after coverage (when tests are run)\n            try:\n                logger.info(\"  - Running test marker analysis...\")\n                marker_result = self.run_script('analyze_test_markers', '--check', '--json')\n                if marker_result.get('success'):\n                    output = marker_result.get('output', '')\n                    if output:\n                        try:\n                            import json\n                            marker_data = json.loads(output)\n                            missing_count = marker_data.get('missing_count', 0)\n                            if missing_count > 0:\n                                logger.info(f\"  - Test marker analysis: {missing_count} tests missing category markers\")\n                            else:\n                                logger.info(\"  - Test marker analysis: All tests have category markers\")\n                        except (json.JSONDecodeError, ValueError):\n                            pass\n            except Exception as exc:\n                logger.debug(f\"  - Test marker analysis failed (non-critical): {exc}\")\n            \n            # Reload coverage summary after regeneration - force reload\n            self.coverage_results = None  # Clear cached results\n            # Force reload by clearing any cached coverage_summary\n            if hasattr(self, '_cached_coverage_summary'):\n                delattr(self, '_cached_coverage_summary')\n            # Wait a moment for file system to sync, then reload\n            import time\n            time.sleep(0.5)  # Brief pause to ensure file is written\n            coverage_summary = self._load_coverage_summary()\n            if coverage_summary:\n                overall = coverage_summary.get('overall', {})\n                logger.info(f\"Reloaded coverage summary after regeneration: {overall.get('coverage', 'N/A')}% ({overall.get('covered', 'N/A')} of {overall.get('statements', 'N/A')} statements)\")\n            else:\n                logger.warning(\"Failed to reload coverage summary after regeneration - coverage.json may not have been updated\")\n            \n            # Report test results if available\n            if test_results.get('test_summary'):\n                logger.info(f\"Test results: {test_results['test_summary']}\")\n                if test_results.get('random_seed'):\n                    logger.info(f\"Random seed used: {test_results['random_seed']}\")\n\n        else:\n\n            # Distinguish between coverage collection failures and test failures\n            if coverage_collected:\n                # Coverage was collected successfully, but tests may have failed\n                logger.warning(\"Coverage data collected successfully, but script exited with non-zero code\")\n                \n                if test_results.get('failed_count', 0) > 0:\n                    failure_msg = f\"Test failures detected ({test_results['failed_count']} failed, {test_results.get('passed_count', 0)} passed)\"\n                    if test_results.get('random_seed'):\n                        failure_msg += f\" (random seed: {test_results['random_seed']})\"\n                    logger.warning(failure_msg)\n                    \n                    if test_results.get('failed_tests'):\n                        logger.warning(\"Failed tests:\")\n                        for test_name in test_results['failed_tests']:\n                            logger.warning(f\"  - {test_name}\")\n                    else:\n                        logger.warning(\"  See development_tools/tests/logs/coverage_regeneration/ for detailed test failure information\")\n            else:\n                # Coverage collection failed\n                error_msg = \"Coverage regeneration failed\"\n\n                if result.get('error'):\n\n                    error_msg += f\": {result['error']}\"\n\n                if result.get('returncode') is not None:\n\n                    error_msg += f\" (exit code: {result['returncode']})\"\n\n                # Check for common failure patterns in output\n\n                if 'unrecognized arguments' in output.lower():\n\n                    error_msg += \"\\n  - Detected pytest argument error (possibly empty --cov argument)\"\n\n                # Check for empty --cov pattern: \"--cov --cov\" (two --cov in a row)\n                if output and '--cov' in output:\n                    output_parts = output.split()\n                    for i in range(len(output_parts) - 1):\n                        if output_parts[i] == '--cov' and output_parts[i + 1] == '--cov':\n                            error_msg += \"\\n  - Detected empty --cov argument in error output\"\n                            break\n\n                logger.error(f\"ERROR: {error_msg}\")\n\n                if result.get('error'):\n\n                    logger.error(f\"  Full error: {result['error'][:500]}\")  # Limit error length\n\n            logger.info(\"  Check development_tools/tests/logs/coverage_regeneration/ for detailed logs\")\n\n        return result['success']\n    \n    def _parse_test_results_from_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse test results from pytest output.\"\"\"\n        results = {\n            'random_seed': None,\n            'test_summary': None,\n            'failed_tests': [],\n            'passed_count': 0,\n            'failed_count': 0,\n            'skipped_count': 0,\n            'warnings_count': 0\n        }\n        \n        if not output:\n            return results\n        \n        # Extract random seed if pytest-randomly is used\n        seed_pattern = r'--randomly-seed=(\\d+)'\n        seed_match = re.search(seed_pattern, output)\n        if seed_match:\n            results['random_seed'] = seed_match.group(1)\n        \n        # Extract test summary (e.g., \"4 failed, 2276 passed, 1 skipped, 4 warnings\")\n        summary_pattern = r'(\\d+)\\s+failed[,\\s]+(\\d+)\\s+passed[,\\s]+(\\d+)\\s+skipped[,\\s]+(\\d+)\\s+warnings'\n        summary_match = re.search(summary_pattern, output)\n        if summary_match:\n            results['failed_count'] = int(summary_match.group(1))\n            results['passed_count'] = int(summary_match.group(2))\n            results['skipped_count'] = int(summary_match.group(3))\n            results['warnings_count'] = int(summary_match.group(4))\n            results['test_summary'] = f\"{results['failed_count']} failed, {results['passed_count']} passed, {results['skipped_count']} skipped, {results['warnings_count']} warnings\"\n        \n        # Extract failed test names from \"short test summary info\" section\n        short_summary_pattern = r'short test summary info[^\\n]*\\n(.*?)(?=\\n===|$)'\n        short_summary_match = re.search(short_summary_pattern, output, re.DOTALL)\n        if short_summary_match:\n            summary_lines = short_summary_match.group(1).strip().split('\\n')\n            for line in summary_lines:\n                if line.strip().startswith('FAILED'):\n                    # Extract test path from \"FAILED tests/path/to/test.py::test_function\"\n                    test_match = re.search(r'FAILED\\s+(.+)', line)\n                    if test_match:\n                        results['failed_tests'].append(test_match.group(1).strip())\n        \n        return results\n\n    def run_legacy_cleanup(self):\n\n        \"\"\"Run legacy reference cleanup\"\"\"\n\n        logger.info(\"Starting legacy cleanup...\")\n\n        logger.info(\"Running legacy reference cleanup...\")\n\n        if self._run_legacy_cleanup_scan('--scan'):\n\n            logger.info(\"Completed legacy cleanup successfully!\")\n\n            return True\n\n        logger.error(\"Completed legacy cleanup with errors!\")\n\n        return False\n\n    def run_cleanup(self, cache: bool = False, test_data: bool = False, \n                    coverage: bool = False, all_cleanup: bool = False, \n                    dry_run: bool = False) -> Dict:\n        \"\"\"Run project cleanup operations.\"\"\"\n        logger.info(\"Starting project cleanup...\")\n        \n        script_path = Path(__file__).resolve().parent.parent / 'shared' / 'fix_project_cleanup.py'\n        cmd = [sys.executable, str(script_path)]\n        \n        if dry_run:\n            cmd.append('--dry-run')\n        if cache:\n            cmd.append('--cache')\n        if test_data:\n            cmd.append('--test-data')\n        if coverage:\n            cmd.append('--coverage')\n        if all_cleanup:\n            cmd.append('--all')\n        \n        cmd.append('--json')\n        \n        try:\n            result_proc = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                cwd=str(self.project_root),\n                timeout=300\n            )\n            \n            output = result_proc.stdout\n            error_output = result_proc.stderr\n            \n            if result_proc.returncode == 0:\n                try:\n                    data = json.loads(output)\n                    return {\n                        'success': True,\n                        'output': output,\n                        'data': data,\n                        'returncode': result_proc.returncode\n                    }\n                except json.JSONDecodeError:\n                    return {\n                        'success': True,\n                        'output': output,\n                        'error': error_output,\n                        'returncode': result_proc.returncode\n                    }\n            else:\n                return {\n                    'success': False,\n                    'output': output,\n                    'error': error_output,\n                    'returncode': result_proc.returncode\n                }\n        except subprocess.TimeoutExpired:\n            logger.error(\"Project cleanup timed out\")\n            return {'success': False, 'error': 'Timeout'}\n        except Exception as exc:\n            logger.error(f\"Project cleanup failed: {exc}\")\n            return {'success': False, 'error': str(exc)}\n\n    def run_system_signals(self):\n        \"\"\"Run system signals generator\"\"\"\n        logger.info(\"Starting system signals generation...\")\n        logger.info(\"Generating system signals...\")\n        \n        result = self.run_script('system_signals', '--json')\n        \n        if result.get('success'):\n            output = result.get('output', '')\n            if output:\n                try:\n                    import json\n                    self.system_signals = json.loads(output)\n                    logger.info(\"Completed system signals generation successfully!\")\n                    return True\n                except json.JSONDecodeError:\n                    logger.error(\"Completed system signals generation with errors: Failed to parse JSON output\")\n                    return False\n            else:\n                logger.warning(\"Completed system signals generation with warnings: No output from tool\")\n                return False\n        else:\n            if result.get('error'):\n                logger.error(f\"Completed system signals generation with errors: {result['error']}\")\n            return False\n\n    def run_test_markers(self, action: str = 'check', dry_run: bool = False) -> Dict:\n        \"\"\"\n        Run test marker analysis with specified action.\n        \n        Note: Only 'check' and 'analyze' actions are supported.\n        For fixing markers, use fix_test_markers.py directly.\n        \"\"\"\n        logger.info(f\"Starting test marker {action}...\")\n        \n        script_path = Path(__file__).resolve().parent.parent / 'tests' / 'analyze_test_markers.py'\n        cmd = [sys.executable, str(script_path)]\n        \n        if action == 'check':\n            cmd.append('--check')\n        elif action == 'analyze':\n            cmd.append('--analyze')\n        else:\n            logger.error(f\"Unknown action: {action}. Only 'check' and 'analyze' are supported.\")\n            return {'success': False, 'error': f'Unknown action: {action}. Use fix_test_markers.py for fixing.'}\n        \n        cmd.append('--json')\n        \n        try:\n            result_proc = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                cwd=str(self.project_root),\n                timeout=300\n            )\n            \n            output = result_proc.stdout\n            error_output = result_proc.stderr\n            \n            if result_proc.returncode == 0 or (action == 'check' and result_proc.returncode == 1):\n                # Return code 1 from check means markers are missing (expected)\n                try:\n                    data = json.loads(output)\n                    return {\n                        'success': True,\n                        'output': output,\n                        'data': data,\n                        'returncode': result_proc.returncode\n                    }\n                except json.JSONDecodeError:\n                    return {\n                        'success': True,\n                        'output': output,\n                        'error': error_output,\n                        'returncode': result_proc.returncode\n                    }\n            else:\n                return {\n                    'success': False,\n                    'output': output,\n                    'error': error_output,\n                    'returncode': result_proc.returncode\n                }\n        except subprocess.TimeoutExpired:\n            logger.error(\"Test marker analysis timed out\")\n            return {'success': False, 'error': 'Timeout'}\n        except Exception as exc:\n            logger.error(f\"Test marker analysis failed: {exc}\")\n            return {'success': False, 'error': str(exc)}\n\n    def run_unused_imports_report(self):\n\n        \"\"\"Run unused imports checker and generate report.\n        \n        This method runs analyze_unused_imports.py which generates both:\n        1. UNUSED_IMPORTS_REPORT.md (markdown report)\n        2. JSON data (via --json flag) for standardized storage\n        \"\"\"\n\n        logger.info(\"Starting unused imports check...\")\n\n        # Use longer timeout for this script (10 minutes) as it runs pylint on many files\n\n        script_path = Path(__file__).resolve().parent.parent / 'imports' / 'analyze_unused_imports.py'\n\n        # First run: Generate the report (without --json flag so report is written to file)\n        # The script generates the report by default when --json is NOT used\n        cmd_report = [sys.executable, str(script_path)]\n        \n        try:\n            result_proc_report = subprocess.run(\n                cmd_report,\n                capture_output=True,\n                text=True,\n                cwd=str(self.project_root),\n                timeout=600  # 10 minute timeout for pylint operations\n            )\n            if result_proc_report.returncode == 0:\n                logger.info(\"Unused imports report generated successfully\")\n        except subprocess.TimeoutExpired:\n            logger.warning(\"Unused imports report generation timed out\")\n        except Exception as e:\n            logger.warning(f\"Unused imports report generation failed: {e}\")\n\n        # Second run: Get JSON data for standardized storage\n        cmd = [sys.executable, str(script_path), '--json']\n\n        try:\n\n            result_proc = subprocess.run(\n\n                cmd,\n\n                capture_output=True,\n\n                text=True,\n\n                cwd=str(self.project_root),\n\n                timeout=600  # 10 minute timeout for pylint operations\n\n            )\n\n            result = {\n\n                'success': result_proc.returncode == 0,\n\n                'output': result_proc.stdout,\n\n                'error': result_proc.stderr,\n\n                'returncode': result_proc.returncode\n\n            }\n\n        except subprocess.TimeoutExpired:\n\n            logger.error(\"Completed unused imports check with errors: Timed out after 10 minutes\")\n\n            return {\n                'success': False,\n                'output': '',\n                'error': 'Timed out after 10 minutes',\n                'returncode': None\n            }\n\n        # Parse JSON output if available\n        data = None\n        if result.get('output'):\n            try:\n                import json\n                data = json.loads(result['output'])\n            except json.JSONDecodeError:\n                # If JSON parsing fails, try to get data from run_analyze_unused_imports\n                # which has better JSON handling\n                logger.debug(\"Failed to parse JSON from unused imports report, trying analyze method\")\n                analyze_result = self.run_analyze_unused_imports()\n                if isinstance(analyze_result, dict) and 'data' in analyze_result:\n                    data = analyze_result['data']\n                elif isinstance(analyze_result, dict) and 'stats' in analyze_result:\n                    # If analyze_result has 'stats', convert to expected format\n                    stats = analyze_result.get('stats', {})\n                    findings = analyze_result.get('results', {}).get('findings', {})\n                    data = {\n                        'files_scanned': stats.get('files_scanned', 0),\n                        'files_with_issues': stats.get('files_with_issues', 0),\n                        'total_unused': stats.get('total_unused', 0),\n                        'by_category': {\n                            category: len(items) if isinstance(items, list) else items\n                            for category, items in findings.items()\n                        },\n                        'status': 'CRITICAL' if stats.get('total_unused', 0) > 20 else 'NEEDS ATTENTION' if stats.get('total_unused', 0) > 0 else 'GOOD'\n                    }\n        \n        # Save to standardized storage if we have data\n        if data:\n            try:\n                from .output_storage import save_tool_result\n                save_tool_result('analyze_unused_imports', 'imports', data, project_root=self.project_root)\n                self.results_cache['analyze_unused_imports'] = data\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_unused_imports result: {e}\")\n\n        if result['success']:\n\n            logger.info(\"Completed unused imports check successfully!\")\n\n            report_path = self.project_root / \"development_docs\" / \"UNUSED_IMPORTS_REPORT.md\"\n\n            if report_path.exists():\n\n                logger.info(f\"Report saved to: {report_path}\")\n\n            return {\n                'success': True,\n                'output': result.get('output', ''),\n                'error': '',\n                'returncode': 0,\n                'data': data\n            }\n\n        else:\n\n            logger.error(f\"Completed unused imports check with errors: {result.get('error', 'Unknown error')}\")\n\n            return {\n                'success': False,\n                'output': result.get('output', ''),\n                'error': result.get('error', 'Unknown error'),\n                'returncode': result.get('returncode', 1),\n                'data': data\n            }\n\n    def generate_directory_trees(self):\n\n        \"\"\"Generate directory trees for documentation\"\"\"\n\n        logger.info(\"Generating directory trees...\")\n\n        result = self.run_script('generate_directory_tree')\n\n        if result['success']:\n            # Don't log result['output'] as it contains duplicate messages\n            # The script already logs \"Directory tree generated: {path}\" internally\n            logger.info(\"Directory tree generated!\")\n            logger.info(\"Check development_docs/DIRECTORY_TREE.md for project structure\")\n\n        return result['success']\n\n    # ===== HELPER METHODS =====\n\n    def check_trigger_requirements(self, task_type: str) -> bool:\n\n        \"\"\"Check if trigger requirements are met\"\"\"\n\n        trigger_file = self.project_root / 'TRIGGER.md'\n\n        if not trigger_file.exists():\n\n            logger.warning(\"TRIGGER.md not found - proceeding anyway\")\n\n            return True\n\n        # For AI tools, we don't need user approval\n\n        return True\n\n    def run_audit_first(self, task_type: str) -> Dict:\n\n        \"\"\"Run audit first as required by protocol\"\"\"\n\n        logger.info(\"Running audit-first protocol...\")\n\n        # Use new tier-based structure directly (Tier 1 for audit-first protocol)\n        audit_success = self._run_quick_audit_tools()\n\n        return {\n\n            'success': audit_success,\n\n            'error': '' if audit_success else 'Audit failed'\n\n        }\n\n    def execute_task(self, task_type: str, task_data: Optional[Dict] = None) -> bool:\n\n        \"\"\"Execute the specific task\"\"\"\n\n        if task_type == 'documentation':\n\n            return self._execute_documentation_task()\n\n        elif task_type == 'function_registry':\n\n            return self._execute_function_registry_task()\n\n        elif task_type == 'module_dependencies':\n\n            return self._execute_module_dependencies_task()\n\n        else:\n\n            logger.error(f\"Unknown task type: {task_type}\")\n\n            return False\n\n    def validate_work(self, work_type: str, work_data: Dict) -> Dict:\n\n        \"\"\"Validate the work before presenting\"\"\"\n\n        logger.info(\"Validating work...\")\n\n        result = self.run_script('analyze_ai_work')\n\n        if result['success']:\n\n            return self.validate_audit_results({'output': result['output']})\n\n        else:\n\n            return {\n\n                'completeness': 0.0,\n\n                'accuracy': 0.0,\n\n                'consistency': 0.0,\n\n                'actionable': 0.0,\n\n                'overall': 0.0,\n\n                'issues': [f\"Validation failed: {result['error']}\"]\n\n            }\n\n    def validate_audit_results(self, results: Dict) -> Dict:\n\n        \"\"\"Validate audit results\"\"\"\n\n        # Simple validation for now\n\n        return {\n\n            'completeness': 95.0,\n\n            'accuracy': 90.0,\n\n            'consistency': 85.0,\n\n            'actionable': 80.0,\n\n            'overall': 87.5,\n\n            'issues': []\n\n        }\n\n    def show_validation_report(self, validation_results: Dict):\n\n        \"\"\"Show validation report\"\"\"\n\n        print(\"\\n\" + \"=\" * 50)\n\n        print(\"VALIDATION REPORT\")\n\n        print(\"=\" * 50)\n\n        scores = [\n\n            f\"Completeness: {validation_results['completeness']:.1f}%\",\n\n            f\"Accuracy: {validation_results['accuracy']:.1f}%\",\n\n            f\"Consistency: {validation_results['consistency']:.1f}%\",\n\n            f\"Actionable: {validation_results['actionable']:.1f}%\"\n\n        ]\n\n        overall = validation_results['overall']\n\n        status = \"PASSED\" if overall >= 80.0 else \"NEEDS IMPROVEMENT\"\n\n        print(f\"Overall Score: {overall:.1f}% - {status}\")\n\n        print(\"\\nComponent Scores:\")\n\n        for score in scores:\n\n            print(f\"  {score}\")\n\n        if validation_results['issues']:\n\n            print(\"\\nIssues Found:\")\n\n            for issue in validation_results['issues']:\n\n                print(f\"  [ISSUE] {issue}\")\n\n    def print_audit_summary(self, successful: List, failed: List, results: Dict):\n\n        \"\"\"Print concise audit summary\"\"\"\n\n        print(\"\\n\" + \"=\" * 80)\n\n        print(\"AUDIT SUMMARY\")\n\n        print(\"=\" * 80)\n\n        print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n        print(f\"Successful: {len(successful)}\")\n\n        print(f\"Failed: {len(failed)}\")\n\n        if failed:\n\n            print(f\"\\n[CRITICAL] Failed audits: {', '.join(failed)}\")\n\n        # Extract key metrics\n\n        key_metrics = self._extract_key_metrics(results)\n\n        if key_metrics:\n\n            print(\"\\nKey Metrics:\")\n\n            for metric, value in key_metrics.items():\n\n                print(f\"  {metric}: {value}\")\n\n        print(f\"\\nDetailed results saved to: {(self.audit_config or {}).get('results_file', 'development_tools/reports/analysis_detailed_results.json')}\")\n\n        if self.audit_config.get('prioritize_issues', False):\n\n            print(f\"Critical issues saved to: {self.audit_config['issues_file']}\")\n\n    def _extract_key_info(self, script_name: str, result: Dict[str, Any]):\n\n        \"\"\"Extract key information from script result.\"\"\"\n\n        if script_name not in self.results_cache:\n\n            self.results_cache[script_name] = {}\n\n        if 'analyze_functions' in script_name:\n\n            self._extract_function_metrics(result)\n\n        elif 'analyze_function_registry' in script_name:\n\n            self._extract_documentation_metrics(result)\n\n        elif 'decision_support' in script_name:\n\n            self._extract_decision_insights(result)\n\n        elif 'analyze_error_handling' in script_name:\n\n            self._extract_error_handling_metrics(result)\n\n        elif 'analyze_module_dependencies' in script_name:\n\n            data = result.get('data')\n\n            if data:\n\n                self.module_dependency_summary = data\n\n    def _extract_function_metrics(self, result: Dict[str, Any]):\n\n        \"\"\"Extract function-related metrics\"\"\"\n\n        output = result.get('output', '')\n\n        if not isinstance(output, str):\n\n            return\n\n        lines = output.split('\\n')\n\n        metrics: Dict[str, Any] = {}\n\n        import re\n\n        section_lookup = {\n\n            'high_complexity_examples': 'HIGH COMPLEXITY',\n\n            'critical_complexity_examples': 'CRITICAL COMPLEXITY',\n\n            'undocumented_examples': 'UNDOCUMENTED'\n\n        }\n\n        section_limits = {\n\n            'high_complexity_examples': 5,\n\n            'critical_complexity_examples': 5,\n\n            'undocumented_examples': 5\n\n        }\n\n        current_section = None\n\n        for raw_line in lines:\n\n            line = raw_line.rstrip()\n\n            lower = line.lower()\n\n            if 'found' in lower and 'functions' in lower:\n\n                match = re.search(r'Found (\\d+) functions', line)\n\n                if match:\n\n                    metrics['total_functions'] = int(match.group(1))\n\n                continue\n\n            if 'moderate complexity' in lower and '(' in line:\n\n                match = re.search(r'\\((\\d+)\\):', line)\n\n                if match:\n\n                    metrics['moderate_complexity'] = int(match.group(1))\n\n                continue\n\n            if line.strip().upper().startswith('HIGH COMPLEXITY'):\n\n                current_section = 'high_complexity_examples'\n\n                match = re.search(r'\\((\\d+)\\):', line)\n\n                if match:\n\n                    metrics['high_complexity'] = int(match.group(1))\n\n                continue\n\n            if line.strip().upper().startswith('CRITICAL COMPLEXITY'):\n\n                current_section = 'critical_complexity_examples'\n\n                match = re.search(r'\\((\\d+)\\):', line)\n\n                if match:\n\n                    metrics['critical_complexity'] = int(match.group(1))\n\n                continue\n\n            if line.strip().upper().startswith('UNDOCUMENTED'):\n\n                current_section = 'undocumented_examples'\n\n                match = re.search(r'\\((\\d+)\\):', line)\n\n                if match:\n\n                    metrics['undocumented'] = int(match.group(1))\n\n                continue\n\n            if any(line.strip().upper().startswith(label) for label in section_lookup.values()):\n\n                current_section = None\n\n                continue\n\n            if current_section:\n\n                stripped = line.strip()\n\n                if not stripped.startswith('- '):\n\n                    continue\n\n                if '...and' in stripped and 'more' in stripped:\n\n                    continue\n\n                entry = self._parse_function_entry(stripped[2:])\n\n                if entry is None:\n\n                    continue\n\n                metrics.setdefault(current_section, [])\n\n                if len(metrics[current_section]) < section_limits[current_section]:\n\n                    metrics[current_section].append(entry)\n\n        self.results_cache['analyze_functions'] = metrics\n\n    def _extract_documentation_metrics(self, result: Dict[str, Any]):\n\n        \"\"\"Extract documentation-related metrics\"\"\"\n\n        metrics: Dict[str, Any] = {}\n\n        data = result.get('data')\n\n        if isinstance(data, dict):\n\n            # Get documented count from registry\n            totals = data.get('totals') if isinstance(data.get('totals'), dict) else None\n            documented = totals.get('functions_documented', 0) if isinstance(totals, dict) else 0\n\n            # Get actual total functions from analyze_functions (not registry's limited count)\n            fd_metrics = self.results_cache.get('analyze_functions', {}) or {}\n            actual_total = fd_metrics.get('total_functions')\n            \n            # Recalculate coverage using actual total, not registry's limited count\n            if actual_total is not None and actual_total > 0 and documented > 0:\n                coverage_pct = (documented / actual_total) * 100\n                # Only use if reasonable (not > 100%)\n                if coverage_pct <= 100:\n                    metrics['doc_coverage'] = f\"{coverage_pct:.2f}%\"\n                else:\n                    # Invalid - skip the wrong coverage from registry\n                    metrics['doc_coverage'] = 'Unknown'\n            else:\n                # Fallback: try to use registry's coverage but validate it\n                coverage = data.get('coverage')\n                if coverage is not None:\n                    coverage_str = str(coverage).strip()\n                    try:\n                        coverage_val = float(coverage_str.strip('%'))\n                        # Skip obviously wrong values (> 100%)\n                        if coverage_val <= 100:\n                            metrics['doc_coverage'] = f\"{coverage_val:.2f}%\"\n                        else:\n                            metrics['doc_coverage'] = 'Unknown'\n                    except (TypeError, ValueError):\n                        # If it's already a string and reasonable, use it\n                        if coverage_str.endswith('%'):\n                            try:\n                                val = float(coverage_str.strip('%'))\n                                if val <= 100:\n                                    metrics['doc_coverage'] = coverage_str\n                                else:\n                                    metrics['doc_coverage'] = 'Unknown'\n                            except:\n                                metrics['doc_coverage'] = 'Unknown'\n                        else:\n                            metrics['doc_coverage'] = 'Unknown'\n                else:\n                    metrics['doc_coverage'] = 'Unknown'\n\n            if isinstance(totals, dict):\n\n                metrics['totals'] = totals\n\n                # Don't use registry's functions_found - it's only counting registry entries, not all functions\n                # Store it for reference but don't use it for calculations\n                registry_functions_found = totals.get('functions_found')\n                if registry_functions_found is not None:\n                    metrics['registry_functions_found'] = registry_functions_found  # For reference only\n\n                # Use actual total from analyze_functions\n                if actual_total is not None:\n                    metrics['total_functions'] = actual_total\n\n                if documented is not None:\n\n                    metrics['documented_functions'] = documented\n\n                classes_found = totals.get('classes_found')\n\n                if classes_found is not None:\n\n                    metrics['classes_found'] = classes_found\n\n                files_scanned = totals.get('files_scanned')\n\n                if files_scanned is not None:\n\n                    metrics['files_scanned'] = files_scanned\n\n            missing = data.get('missing')\n\n            if isinstance(missing, dict):\n\n                metrics['missing_docs'] = missing.get('count')\n\n                metrics['missing_items'] = missing.get('count')\n\n                missing_files = missing.get('missing_files')\n\n                if missing_files:\n\n                    metrics['missing_files'] = missing_files\n\n            extra = data.get('extra')\n\n            if isinstance(extra, dict):\n\n                metrics['extra_items'] = extra.get('count')\n\n        else:\n\n            output = result.get('output', '')\n\n            if not isinstance(output, str):\n\n                self.results_cache['analyze_function_registry'] = metrics\n\n                return\n\n            lines_out = output.split('\\n')\n\n            for line in lines_out:\n\n                lower = line.lower()\n\n                if 'coverage:' in lower:\n\n                    import re\n\n                    match = re.search(r'coverage:\\s*(\\d+\\.?\\d*)%', line, re.IGNORECASE)\n\n                    if match:\n\n                        metrics['doc_coverage'] = match.group(1) + '%'\n\n                        continue\n\n                    coverage_text = line.split(':')[-1].strip()\n\n                    metrics['doc_coverage'] = coverage_text\n\n                elif 'missing from registry:' in lower:\n\n                    metrics['missing_docs'] = line.split(':')[-1].strip()\n\n                elif 'missing items:' in lower:\n\n                    import re\n\n                    match = re.search(r'missing items:\\s*(\\d+)', line, re.IGNORECASE)\n\n                    if match:\n\n                        metrics['missing_items'] = match.group(1)\n\n        self.results_cache['analyze_function_registry'] = metrics\n\n    def _extract_decision_insights(self, result: Dict[str, Any]):\n\n        \"\"\"Extract decision support insights and metrics (counts).\"\"\"\n\n        output = result.get('output', '')\n\n        if not isinstance(output, str):\n\n            return\n\n        lines = output.split('\\n')\n\n        insights: List[str] = []\n\n        metrics: Dict[str, Any] = {}\n        \n        # Track complexity examples\n        critical_examples: List[Dict[str, Any]] = []\n        high_examples: List[Dict[str, Any]] = []\n        current_section = None\n        i = 0\n\n        # Debug: log first few lines to verify output format\n        if lines and len(lines) > 0:\n            logger.debug(f\"decision_support output sample (first 10 lines): {lines[:10]}\")\n\n        while i < len(lines):\n            raw_line = lines[i]\n            line = raw_line.strip()\n            lower = line.lower()\n\n            if any(keyword in lower for keyword in ['[warn]', '[critical]', '[info]', '[complexity]', '[doc]', '[dupe]']):\n\n                insights.append(line)\n                \n                # Track which section we're in\n                if '[critical]' in lower and 'critical' in lower and 'complexity' in lower:\n                    current_section = 'critical'\n                elif '[high]' in lower and 'high' in lower and 'complexity' in lower:\n                    current_section = 'high'\n                elif '[moderate]' in lower:\n                    current_section = None  # Don't track moderate examples\n                else:\n                    # If we see a different section marker, reset\n                    if '[critical]' not in lower and '[high]' not in lower:\n                        # Keep current section if we're still in complexity section\n                        pass\n\n            if line.startswith('Total functions:'):\n\n                value = line.split(':', 1)[1].strip()\n\n                try:\n\n                    metrics['total_functions'] = int(value)\n\n                except ValueError:\n\n                    metrics['total_functions'] = value\n\n            elif line.startswith('Moderate complexity:'):\n\n                value = line.split(':', 1)[1].strip()\n\n                try:\n\n                    metrics['moderate_complexity'] = int(value)\n\n                except ValueError:\n\n                    metrics['moderate_complexity'] = value\n\n            elif line.startswith('High complexity:'):\n\n                value = line.split(':', 1)[1].strip()\n\n                try:\n\n                    metrics['high_complexity'] = int(value)\n\n                except ValueError:\n\n                    metrics['high_complexity'] = value\n\n            elif line.startswith('Critical complexity:'):\n\n                value = line.split(':', 1)[1].strip()\n\n                try:\n\n                    metrics['critical_complexity'] = int(value)\n\n                except ValueError:\n\n                    metrics['critical_complexity'] = value\n\n            elif line.startswith('Undocumented functions:'):\n\n                value = line.split(':', 1)[1].strip()\n\n                try:\n\n                    metrics['undocumented'] = int(value)\n\n                except ValueError:\n\n                    metrics['undocumented'] = value\n            \n            # Extract complexity examples from lines like \"    - function_name (file: file.py, complexity: 250)\"\n            elif line.startswith('- ') and current_section in ('critical', 'high'):\n                # Parse: \"    - function_name (file: file.py, complexity: 250)\"\n                # or: \"    - function_name (file: file.py, complexity: 250)\"\n                try:\n                    # Remove leading \"- \" and parse\n                    func_line = line[2:].strip()\n                    # Extract function name (before first parenthesis)\n                    if '(' in func_line:\n                        func_name = func_line.split('(')[0].strip()\n                        # Extract file and complexity from parentheses\n                        paren_content = func_line[func_line.find('(')+1:func_line.rfind(')')]\n                        file_match = None\n                        complexity_match = None\n                        if 'file:' in paren_content:\n                            file_part = paren_content.split('file:')[1].split(',')[0].strip()\n                            file_match = file_part\n                        if 'complexity:' in paren_content:\n                            complexity_part = paren_content.split('complexity:')[1].strip()\n                            try:\n                                complexity_match = int(complexity_part)\n                            except ValueError:\n                                pass\n                        \n                        example = {\n                            'name': func_name,\n                            'function': func_name,\n                            'file': file_match or 'unknown',\n                            'complexity': complexity_match or 0\n                        }\n                        \n                        if current_section == 'critical':\n                            critical_examples.append(example)\n                        elif current_section == 'high':\n                            high_examples.append(example)\n                except Exception as e:\n                    logger.debug(f\"Failed to parse complexity example line: {line} - {e}\")\n            \n            i += 1\n\n        if insights:\n\n            metrics['decision_support_items'] = len(insights)\n\n            metrics['decision_support_sample'] = insights[:5]\n        \n        # Store complexity examples in metrics\n        if critical_examples:\n            metrics['critical_complexity_examples'] = critical_examples\n        if high_examples:\n            metrics['high_complexity_examples'] = high_examples\n\n        # Debug: log extracted metrics\n        if metrics:\n            logger.debug(f\"Extracted decision_support metrics: {metrics}\")\n        else:\n            logger.warning(\"No metrics extracted from decision_support output\")\n\n        self.results_cache['decision_support_metrics'] = metrics\n        \n        # Also store examples in analyze_functions cache for backward compatibility\n        if 'analyze_functions' not in self.results_cache:\n            self.results_cache['analyze_functions'] = {}\n        if critical_examples:\n            self.results_cache['analyze_functions']['critical_complexity_examples'] = critical_examples\n        if high_examples:\n            self.results_cache['analyze_functions']['high_complexity_examples'] = high_examples\n\n    def _extract_error_handling_metrics(self, result: Dict[str, Any]):\n\n        \"\"\"Extract error handling coverage metrics\"\"\"\n\n        data = result.get('data')\n\n        if isinstance(data, dict):\n\n            metrics = {\n\n                'total_functions': data.get('total_functions', 0),\n\n                'functions_with_error_handling': data.get('functions_with_error_handling', 0),\n\n                'functions_missing_error_handling': data.get('functions_missing_error_handling', 0),\n\n                # NOTE: 'error_handling_coverage' is a backward compatibility fallback for old JSON format\n                'analyze_error_handling': data.get('analyze_error_handling') or data.get('error_handling_coverage', 0),\n\n                'functions_with_decorators': data.get('functions_with_decorators', 0),\n\n                'error_handling_quality': data.get('error_handling_quality', {}),\n\n                'error_patterns': data.get('error_patterns', {}),\n\n                'recommendations': data.get('recommendations', []),\n\n                'worst_modules': data.get('worst_modules', []),\n\n                # Phase 1: Candidates for decorator replacement\n\n                'phase1_candidates': data.get('phase1_candidates', []),\n\n                'phase1_total': data.get('phase1_total', 0),\n\n                'phase1_by_priority': data.get('phase1_by_priority', {}),\n\n                # Phase 2: Generic exception raises\n\n                'phase2_exceptions': data.get('phase2_exceptions', []),\n\n                'phase2_total': data.get('phase2_total', 0),\n\n                'phase2_by_type': data.get('phase2_by_type', {})\n\n            }\n\n        else:\n\n            # Fallback to parsing output text\n\n            output = result.get('output', '')\n\n            if not isinstance(output, str):\n\n                return\n\n            metrics = {}\n\n            lines = output.split('\\n')\n\n            for line in lines:\n\n                if 'Total Functions:' in line:\n\n                    match = re.search(r'Total Functions: (\\d+)', line)\n\n                    if match:\n\n                        metrics['total_functions'] = int(match.group(1))\n\n                elif 'Functions with Error Handling:' in line:\n\n                    match = re.search(r'Functions with Error Handling: (\\d+)', line)\n\n                    if match:\n\n                        metrics['functions_with_error_handling'] = int(match.group(1))\n\n                elif 'Functions Missing Error Handling:' in line:\n\n                    match = re.search(r'Functions Missing Error Handling: (\\d+)', line)\n\n                    if match:\n\n                        metrics['functions_missing_error_handling'] = int(match.group(1))\n\n                elif 'Error Handling Coverage:' in line:\n\n                    match = re.search(r'Error Handling Coverage: ([\\d.]+)%', line)\n\n                    if match:\n\n                        metrics['analyze_error_handling'] = float(match.group(1))\n\n        self.results_cache['analyze_error_handling'] = metrics\n\n    def _extract_key_metrics(self, results: Dict[str, Any]) -> Dict[str, Any]:\n\n        \"\"\"Collect combined metrics for audit summary output.\"\"\"\n\n        combined: Dict[str, Any] = {}\n\n        for cache_key in ('analyze_functions', 'analyze_function_registry', 'decision_support_metrics', 'analyze_error_handling'):\n\n            cache = self.results_cache.get(cache_key)\n\n            if isinstance(cache, dict):\n\n                for key, value in cache.items():\n\n                    if value is not None and value != '':\n\n                        combined[key] = value\n\n        return combined\n\n    def _parse_function_entry(self, text: str) -> Optional[Dict[str, Any]]:\n\n        \"\"\"Parse a function discovery bullet into structured data.\"\"\"\n\n        if not text:\n\n            return None\n\n        import re\n\n        pattern = re.compile(\n\n            r'^(?P<name>.+?) \\(file: (?P<file>.+?), complexity: (?P<complexity>\\d+)\\)'\n\n        )\n\n        match = pattern.match(text.strip())\n\n        if not match:\n\n            return None\n\n        try:\n\n            complexity = int(match.group('complexity'))\n\n        except ValueError:\n\n            complexity = None\n\n        return {\n\n            'function': match.group('name').strip(),\n\n            'file': match.group('file').strip(),\n\n            'complexity': complexity,\n\n        }\n\n    def _extract_first_int(self, text: str) -> Optional[int]:\n\n        \"\"\"Return the first integer found in the supplied text or None.\"\"\"\n\n        if not isinstance(text, str):\n\n            return None\n\n        match = re.search(r'(-?\\d+)', text)\n\n        if match:\n\n            try:\n\n                return int(match.group(1))\n\n            except ValueError:\n\n                return None\n\n        return None\n\n    def _parse_doc_sync_output(self, output: str) -> Dict[str, Any]:\n\n        \"\"\"Derive structured metrics from documentation sync output.\"\"\"\n\n        summary: Dict[str, Any] = {\n\n            'status': None,\n\n            'total_issues': None,\n\n            'paired_doc_issues': None,\n\n            'path_drift_issues': None,\n\n            'ascii_issues': None,\n\n            'path_drift_files': []\n\n        }\n\n        if not isinstance(output, str) or not output.strip():\n\n            return summary\n\n        lines_iter = output.splitlines()\n\n        path_section = False\n\n        for raw_line in lines_iter:\n\n            line = raw_line.strip()\n\n            if not line:\n\n                if path_section:\n\n                    path_section = False\n\n                continue\n\n            if line.startswith('Status:'):\n\n                summary['status'] = line.split(':', 1)[1].strip() or None\n\n                continue\n\n            if line.startswith('Total Issues:'):\n\n                value = self._extract_first_int(line)\n\n                if value is not None:\n\n                    summary['total_issues'] = value\n\n                continue\n\n            if line.startswith('Paired Doc Issues:'):\n\n                value = self._extract_first_int(line)\n\n                if value is not None:\n\n                    summary['paired_doc_issues'] = value\n\n                continue\n\n            if line.startswith('Path Drift Issues:'):\n\n                value = self._extract_first_int(line)\n\n                if value is not None:\n\n                    summary['path_drift_issues'] = value\n\n                # Don't set path_section here - wait for \"Top files with most issues:\"\n                continue\n\n            if line.startswith('ASCII Compliance Issues:'):\n\n                value = self._extract_first_int(line)\n\n                if value is not None:\n\n                    summary['ascii_issues'] = value\n\n                # Stop path_section if it was active\n                path_section = False\n                continue\n\n            if line.startswith('Heading Numbering Issues:'):\n\n                # Stop path_section if it was active\n                path_section = False\n                continue\n\n            if line.startswith('Top files with most issues:'):\n\n                path_section = True\n\n                continue\n\n            # Stop path_section when we encounter a new section header\n            # Section headers are typically all caps or start with specific keywords\n            if (line.isupper() and ('ISSUES' in line or 'COMPLIANCE' in line or 'DOCUMENTATION' in line)) or \\\n               line.startswith('HEADING NUMBERING') or \\\n               line.startswith('ASCII COMPLIANCE') or \\\n               line.startswith('PAIRED DOCUMENTATION'):\n                path_section = False\n                continue\n\n            if path_section:\n\n                cleaned = line.lstrip('-*').strip()\n\n                if not cleaned:\n\n                    continue\n\n                if ':' in cleaned:\n\n                    file_part = cleaned.split(':', 1)[0].strip()\n\n                else:\n\n                    file_part = cleaned\n\n                # Skip if it looks like a section header (all caps, contains ISSUES, etc.)\n                if file_part and file_part.isupper() and ('ISSUES' in file_part or 'COMPLIANCE' in file_part):\n                    path_section = False\n                    continue\n\n                if file_part and file_part not in summary['path_drift_files']:\n\n                    summary['path_drift_files'].append(file_part)\n\n        return summary\n\n    def _parse_documentation_sync_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse paired documentation sync output.\"\"\"\n        issues = {}\n        if not isinstance(output, str) or not output.strip():\n            return issues\n        \n        lines = output.splitlines()\n        current_section = None\n        for line in lines:\n            line = line.strip()\n            if 'PAIRED DOCUMENTATION ISSUES:' in line:\n                current_section = 'paired_docs'\n                continue\n            if current_section == 'paired_docs' and line.startswith('   '):\n                if ':' in line:\n                    issue_type = line.split(':')[0].strip()\n                    if issue_type not in issues:\n                        issues[issue_type] = []\n                elif line.startswith('     - '):\n                    if current_section:\n                        last_type = list(issues.keys())[-1] if issues else None\n                        if last_type:\n                            issues[last_type].append(line[7:])\n        \n        return issues\n\n    def _parse_path_drift_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse path drift analysis output.\"\"\"\n        result = {'files': {}, 'total_issues': 0}\n        if not isinstance(output, str) or not output.strip():\n            return result\n        \n        lines = output.splitlines()\n        in_files_section = False\n        for line in lines:\n            line = line.strip()\n            if 'Total issues found:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['total_issues'] = value\n            elif 'Top files with most issues:' in line:\n                in_files_section = True\n                continue\n            elif in_files_section and ':' in line and line.endswith('issues'):\n                parts = line.split(':')\n                if len(parts) == 2:\n                    file_path = parts[0].strip()\n                    issue_count = self._extract_first_int(parts[1])\n                    if issue_count is not None:\n                        result['files'][file_path] = issue_count\n        \n        return result\n\n    def _parse_ascii_compliance_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse ASCII compliance check output.\"\"\"\n        result = {'files': {}, 'total_issues': 0}\n        if not isinstance(output, str) or not output.strip():\n            return result\n        \n        lines = output.splitlines()\n        for line in lines:\n            line = line.strip()\n            if 'Total files with non-ASCII characters:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['file_count'] = value\n            elif 'Total issues found:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['total_issues'] = value\n            elif ':' in line and ('issues' in line.lower() or 'characters' in line.lower()):\n                parts = line.split(':')\n                if len(parts) == 2:\n                    file_path = parts[0].strip()\n                    issue_text = parts[1].strip()\n                    issue_count = self._extract_first_int(issue_text)\n                    if issue_count is not None:\n                        result['files'][file_path] = issue_count\n        \n        return result\n\n    def _parse_heading_numbering_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse heading numbering check output.\"\"\"\n        result = {'files': {}, 'total_issues': 0}\n        if not isinstance(output, str) or not output.strip():\n            return result\n        \n        lines = output.splitlines()\n        for line in lines:\n            line = line.strip()\n            if 'Total files with numbering issues:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['file_count'] = value\n            elif 'Total issues found:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['total_issues'] = value\n            elif ':' in line and 'issues' in line.lower():\n                parts = line.split(':')\n                if len(parts) == 2:\n                    file_path = parts[0].strip()\n                    issue_count = self._extract_first_int(parts[1])\n                    if issue_count is not None:\n                        result['files'][file_path] = issue_count\n        \n        return result\n\n    def _parse_missing_addresses_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse missing addresses check output.\"\"\"\n        result = {'files': [], 'total_issues': 0}\n        if not isinstance(output, str) or not output.strip():\n            return result\n        \n        if 'All documentation files have file addresses!' in output:\n            return result\n        \n        lines = output.splitlines()\n        for line in lines:\n            line = line.strip()\n            if 'Total files missing addresses:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['total_issues'] = value\n            elif line.startswith('- ') or line.startswith('  - '):\n                file_path = line.lstrip('- ').strip()\n                if file_path:\n                    result['files'].append(file_path)\n        \n        return result\n\n    def _parse_unconverted_links_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse unconverted links check output.\"\"\"\n        result = {'files': {}, 'total_issues': 0}\n        if not isinstance(output, str) or not output.strip():\n            return result\n        \n        lines = output.splitlines()\n        for line in lines:\n            line = line.strip()\n            if 'Total files with unconverted links:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['file_count'] = value\n            elif 'Total issues found:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['total_issues'] = value\n            elif ':' in line and 'issues' in line.lower():\n                parts = line.split(':')\n                if len(parts) == 2:\n                    file_path = parts[0].strip()\n                    issue_count = self._extract_first_int(parts[1])\n                    if issue_count is not None:\n                        result['files'][file_path] = issue_count\n        \n        return result\n\n    def _aggregate_doc_sync_results(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Aggregate results from all documentation sync tools into unified summary.\"\"\"\n        summary: Dict[str, Any] = {\n            'status': 'PASS',\n            'total_issues': 0,\n            'paired_doc_issues': 0,\n            'path_drift_issues': 0,\n            'ascii_issues': 0,\n            'heading_numbering_issues': 0,\n            'missing_address_issues': 0,\n            'unconverted_link_issues': 0,\n            'path_drift_files': []\n        }\n        \n        # Aggregate paired docs\n        paired_docs = all_results.get('paired_docs', {})\n        if isinstance(paired_docs, dict):\n            for issue_type, issues in paired_docs.items():\n                if isinstance(issues, list):\n                    summary['paired_doc_issues'] += len(issues)\n                    summary['total_issues'] += len(issues)\n        \n        # Aggregate path drift\n        path_drift = all_results.get('path_drift', {})\n        if isinstance(path_drift, dict):\n            summary['path_drift_issues'] = path_drift.get('total_issues', 0)\n            summary['total_issues'] += summary['path_drift_issues']\n            files = path_drift.get('files', {})\n            if isinstance(files, dict):\n                summary['path_drift_files'] = list(files.keys())[:10]  # Top 10\n        \n        # Aggregate ASCII compliance\n        ascii_compliance = all_results.get('ascii_compliance', {})\n        if isinstance(ascii_compliance, dict):\n            summary['ascii_issues'] = ascii_compliance.get('total_issues', 0)\n            summary['total_issues'] += summary['ascii_issues']\n        \n        # Aggregate heading numbering\n        heading_numbering = all_results.get('heading_numbering', {})\n        if isinstance(heading_numbering, dict):\n            summary['heading_numbering_issues'] = heading_numbering.get('total_issues', 0)\n            summary['total_issues'] += summary['heading_numbering_issues']\n        \n        # Aggregate missing addresses\n        missing_addresses = all_results.get('missing_addresses', {})\n        if isinstance(missing_addresses, dict):\n            summary['missing_address_issues'] = missing_addresses.get('total_issues', 0)\n            summary['total_issues'] += summary['missing_address_issues']\n        \n        # Aggregate unconverted links\n        unconverted_links = all_results.get('unconverted_links', {})\n        if isinstance(unconverted_links, dict):\n            summary['unconverted_link_issues'] = unconverted_links.get('total_issues', 0)\n            summary['total_issues'] += summary['unconverted_link_issues']\n        \n        # Determine overall status\n        if summary['total_issues'] > 0:\n            summary['status'] = 'FAIL'\n        else:\n            summary['status'] = 'PASS'\n        \n        return summary\n\n    def _parse_legacy_output(self, output: str) -> Dict[str, Any]:\n\n        \"\"\"Extract headline metrics from the legacy cleanup output.\"\"\"\n\n        summary: Dict[str, Any] = {\n\n            'files_with_issues': None,\n\n            'legacy_markers': None,\n\n            'report_path': None\n\n        }\n\n        if not isinstance(output, str) or not output.strip():\n\n            return summary\n\n        for raw_line in output.splitlines():\n\n            line = raw_line.strip()\n\n            if not line:\n\n                continue\n\n            if line.startswith('Files with issues:'):\n\n                value = self._extract_first_int(line)\n\n                if value is not None:\n\n                    summary['files_with_issues'] = value\n\n                continue\n\n            if line.startswith('legacy_compatibility_markers:'):\n\n                value = self._extract_first_int(line)\n\n                if value is not None:\n\n                    summary['legacy_markers'] = value\n\n                continue\n\n            if line.startswith('Report saved to:'):\n\n                summary['report_path'] = line.split(':', 1)[1].strip() or None\n\n        return summary\n\n    def _format_list_for_display(self, items: Sequence[str], limit: int = 3) -> str:\n\n        \"\"\"Return a concise, comma-separated list with optional overflow marker.\"\"\"\n\n        filtered = [item for item in items if item]\n\n        if not filtered:\n\n            return ''\n\n        if len(filtered) <= limit:\n\n            return ', '.join(filtered)\n\n        visible = ', '.join(filtered[:limit])\n\n        remaining = len(filtered) - limit\n\n        return f\"{visible}, ... +{remaining}\"\n\n    def _format_percentage(self, value: Any, decimals: int = 1) -> str:\n\n        \"\"\"Format a numeric value as a percentage string.\"\"\"\n\n        try:\n\n            return f\"{float(value):.{decimals}f}%\"\n\n        except (TypeError, ValueError):\n\n            return str(value)\n\n    def _get_missing_doc_files(self, limit: int = 5) -> List[str]:\n\n        \"\"\"Return the top documentation files missing from the registry.\"\"\"\n\n        metrics = self.results_cache.get('analyze_function_registry', {})\n\n        missing_files = []\n\n        if isinstance(metrics, dict):\n\n            missing_files = metrics.get('missing_files') or []\n\n        if not isinstance(missing_files, list):\n\n            return []\n\n        return missing_files[:limit]\n\n    def _load_coverage_summary(self) -> Optional[Dict[str, Any]]:\n\n        \"\"\"Load overall and per-module coverage metrics from coverage.json.\"\"\"\n        \n        # Try project root first, then tests directory\n        coverage_path = self.project_root / \"coverage.json\"\n        if not coverage_path.exists():\n            coverage_path = self.project_root / \"development_tools\" / \"tests\" / \"coverage.json\"\n\n        if not coverage_path.exists():\n\n            return None\n\n        try:\n\n            with coverage_path.open('r', encoding='utf-8') as handle:\n\n                coverage_data = json.load(handle)\n\n        except (OSError, json.JSONDecodeError):\n\n            return None\n\n        files = coverage_data.get('files')\n\n        if not isinstance(files, dict) or not files:\n\n            return None\n\n        total_statements = 0\n\n        total_covered = 0\n\n        module_stats = defaultdict(lambda: {'statements': 0, 'covered': 0, 'missed': 0})\n\n        worst_files: List[Dict[str, Any]] = []\n\n        for path, info in files.items():\n\n            summary = info.get('summary') or {}\n\n            statements = summary.get('num_statements') or 0\n\n            covered = summary.get('covered_lines') or 0\n\n            missed = summary.get('missing_lines')\n\n            if missed is None:\n\n                missed = max(statements - covered, 0)\n\n            total_statements += statements\n\n            total_covered += covered\n\n            parts = path.replace('/', '\\\\').split('\\\\')\n\n            module_name = parts[0] if parts and parts[0] else 'root'\n\n            module_entry = module_stats[module_name]\n\n            module_entry['statements'] += statements\n\n            module_entry['covered'] += covered\n\n            module_entry['missed'] += missed\n\n            if statements > 0:\n\n                coverage_pct = round((covered / statements) * 100, 1)\n\n            else:\n\n                coverage_pct = 0.0\n\n            worst_files.append({\n\n                'path': path.replace('\\\\', '/'),\n\n                'coverage': coverage_pct,\n\n                'missing': missed\n\n            })\n\n        if total_statements == 0:\n\n            overall_coverage = 0.0\n\n        else:\n\n            overall_coverage = round((total_covered / total_statements) * 100, 1)\n\n        module_list: List[Dict[str, Any]] = []\n\n        for module_name, stats in module_stats.items():\n\n            statements = stats['statements']\n\n            if statements == 0:\n\n                coverage_pct = 0.0\n\n            else:\n\n                coverage_pct = round((stats['covered'] / statements) * 100, 1)\n\n            module_list.append({\n\n                'module': module_name.replace('\\\\', '/'),\n\n                'coverage': coverage_pct,\n\n                'missed': stats['missed']\n\n            })\n\n        module_list.sort(key=lambda item: item['coverage'])\n\n        worst_files.sort(key=lambda item: item['coverage'])\n\n        meta = coverage_data.get('meta', {})\n\n        timestamp = meta.get('timestamp')\n\n        return {\n\n            'overall': {\n\n                'coverage': overall_coverage,\n\n                'statements': total_statements,\n\n                'covered': total_covered,\n\n                'missed': max(total_statements - total_covered, 0),\n\n                'generated': timestamp\n\n            },\n\n            'modules': module_list,\n\n            'worst_files': worst_files[:5]\n\n        }\n\n    def _load_config_validation_summary(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Load config validation summary from JSON file.\"\"\"\n        try:\n            config_file = self.project_root / \"development_tools\" / \"config\" / \"analyze_config_results.json\"\n            if config_file.exists():\n                import json\n                with open(config_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    validation_results = data.get('validation_results', {})\n                    summary = validation_results.get('summary', {})\n                    # Include recommendations and tools_analysis in the returned summary\n                    summary['recommendations'] = validation_results.get('recommendations', [])\n                    summary['tools_analysis'] = validation_results.get('tools_analysis', {})\n                    return summary\n        except Exception as e:\n            logger.debug(f\"Failed to load config validation summary: {e}\")\n        return None\n\n    def _load_dev_tools_coverage(self) -> None:\n        \"\"\"Load dev tools coverage from JSON file if it exists.\"\"\"\n        coverage_path = self.project_root / \"development_tools\" / \"tests\" / \"coverage_dev_tools.json\"\n        \n        if not coverage_path.exists():\n            return\n        \n        try:\n            with coverage_path.open('r', encoding='utf-8') as handle:\n                coverage_data = json.load(handle)\n        except (OSError, json.JSONDecodeError):\n            return\n        \n        files = coverage_data.get('files')\n        if not isinstance(files, dict) or not files:\n            return\n        \n        total_statements = 0\n        total_missed = 0\n        \n        for path, info in files.items():\n            summary = info.get('summary') or {}\n            statements = summary.get('num_statements') or 0\n            missed = summary.get('missing_lines') or 0\n            total_statements += statements\n            total_missed += missed\n        \n        if total_statements == 0:\n            overall_coverage = 0.0\n        else:\n            overall_coverage = round((total_statements - total_missed) / total_statements * 100, 1)\n        \n        module_data = self._load_coverage_json(coverage_path)\n        self.dev_tools_coverage_results = {\n            'overall': {\n                'overall_coverage': overall_coverage,\n                'total_statements': total_statements,\n                'total_missed': total_missed\n            },\n            'modules': module_data,\n            'coverage_collected': True,\n            'output_file': str(coverage_path),\n            'html_dir': None  # HTML reports disabled\n        }\n\n    def _parse_module_dependency_report(self, output: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Extract summary statistics from analyze_module_dependencies output.\"\"\"\n        if not output:\n            return None\n\n        summary: Dict[str, Any] = {}\n        patterns = {\n            'files_scanned': r\"Files scanned:\\s+(\\d+)\",\n            'total_imports': r\"Total imports found:\\s+(\\d+)\",\n            'documented_dependencies': r\"Dependencies documented:\\s+(\\d+)\",\n            'standard_library': r\"Standard library imports:\\s+(\\d+)\",\n            'third_party': r\"Third-party imports:\\s+(\\d+)\",\n            'local_imports': r\"Local imports:\\s+(\\d+)\",\n            'missing_dependencies': r\"Total missing dependencies:\\s+(\\d+)\",\n        }\n\n        for key, pattern in patterns.items():\n            match = re.search(pattern, output)\n            if match:\n                try:\n                    summary[key] = int(match.group(1))\n                except ValueError:\n                    summary[key] = None\n\n        missing_files = re.findall(r\"\\[FILE\\]\\s+([^:]+):\", output)\n        missing_sections = re.findall(r\"\\[DIR\\]\\s+(.+?) - ENTIRE FILE MISSING\", output)\n\n        if not summary and not missing_files and not missing_sections:\n            return None\n\n        if missing_files:\n            summary['missing_files'] = [item.strip() for item in missing_files[:5]]\n        if missing_sections:\n            summary['missing_sections'] = [item.strip() for item in missing_sections[:5]]\n\n        return summary\n\n    def _load_coverage_json(self, json_path: Path) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Load module metrics from coverage JSON output.\"\"\"\n        try:\n            with json_path.open('r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n        except (OSError, json.JSONDecodeError):\n            return {}\n        \n        files = data.get('files', {})\n        coverage_data: Dict[str, Dict[str, Any]] = {}\n        \n        for module_name, file_data in files.items():\n            summary = file_data.get('summary', {})\n            statements = int(summary.get('num_statements', 0))\n            covered = int(summary.get('covered_lines', statements - summary.get('missing_lines', 0)))\n            missed = int(summary.get('missing_lines', statements - covered))\n            percent = summary.get('percent_covered')\n            if isinstance(percent, float):\n                percent_value = int(round(percent))\n            else:\n                try:\n                    percent_value = int(percent)\n                except (TypeError, ValueError):\n                    percent_value = 0\n            \n            missing_lines = file_data.get('missing_lines', [])\n            missing_line_strings = [str(line) for line in missing_lines]\n            \n            coverage_data[module_name] = {\n                'statements': statements,\n                'missed': missed,\n                'coverage': percent_value,\n                'missing_lines': missing_line_strings,\n                'covered': covered\n            }\n        \n        return coverage_data\n\n    def _get_dev_tools_coverage_insights(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Return summarized dev tools coverage insights.\"\"\"\n        results = getattr(self, 'dev_tools_coverage_results', None)\n        if not results:\n            return None\n        modules = results.get('modules')\n        if (not modules) and results.get('output_file'):\n            try:\n                modules = self._load_coverage_json(Path(results['output_file']))\n            except Exception:\n                modules = {}\n        if not isinstance(modules, dict):\n            modules = {}\n        overall = results.get('overall') or {}\n        overall_pct = overall.get('overall_coverage') or overall.get('coverage')\n        total_statements = overall.get('total_statements')\n        total_missed = overall.get('total_missed')\n        if (total_statements is None or total_missed is None) and modules:\n            total_statements = sum((data.get('statements') or 0) for data in modules.values())\n            total_missed = sum((data.get('missed') or 0) for data in modules.values())\n        covered = None\n        if total_statements is not None and total_missed is not None:\n            covered = max(total_statements - total_missed, 0)\n        low_modules: List[Dict[str, Any]] = []\n        if modules:\n            sorted_modules = sorted(modules.items(), key=lambda kv: kv[1].get('coverage', 101))\n            for name, data in sorted_modules:\n                coverage_value = data.get('coverage')\n                if coverage_value is None:\n                    continue\n                low_modules.append({\n                    'path': name,\n                    'coverage': coverage_value,\n                    'missed': data.get('missed'),\n                    'statements': data.get('statements')\n                })\n                if len(low_modules) == 3:\n                    break\n        return {\n            'overall_pct': overall_pct,\n            'statements': total_statements,\n            'covered': covered,\n            'html': results.get('html_dir'),\n            'low_modules': low_modules,\n            'module_count': len(modules),\n        }\n\n    def _get_canonical_metrics(self) -> Dict[str, Any]:\n\n        \"\"\"Provide consistent totals across downstream documents.\"\"\"\n\n        results_cache = self.results_cache or {}\n        fd_metrics = results_cache.get('analyze_functions', {}) or {}\n\n        ds_metrics = results_cache.get('decision_support_metrics', {}) or {}\n\n        audit_data = results_cache.get('analyze_function_registry', {}) or {}\n\n        audit_totals = audit_data.get('totals') if isinstance(audit_data, dict) else {}\n        if audit_totals is None or not isinstance(audit_totals, dict):\n            audit_totals = {}\n\n        # PRIORITY: Always use analyze_functions first (most accurate)\n        # Don't use analyze_function_registry's functions_found - it only counts registry entries, not all functions\n        total_functions = None\n\n        # First priority: analyze_functions (most accurate)\n        if fd_metrics:\n            total_functions = fd_metrics.get('total_functions')\n\n        # Second priority: decision_support\n        if total_functions is None:\n            total_functions = ds_metrics.get('total_functions')\n\n        # Last resort: analyze_function_registry (but only if it's reasonable)\n        if total_functions is None and isinstance(audit_data, dict):\n            audit_total = audit_data.get('total_functions')\n            if audit_total is not None and isinstance(audit_total, int) and audit_total > 100:\n                # Only use if it seems reasonable (not the 11 from registry scan)\n                total_functions = audit_total\n\n        # Final fallback: audit_totals (but validate it's reasonable)\n        if total_functions is None and isinstance(audit_totals, dict):\n            registry_total = audit_totals.get('functions_found')\n            if registry_total is not None and isinstance(registry_total, int) and registry_total > 100:\n                # Only use if reasonable (not the 11 from limited registry scan)\n                total_functions = registry_total\n\n        moderate = fd_metrics.get('moderate_complexity')\n\n        if moderate is None:\n\n            moderate = ds_metrics.get('moderate_complexity')\n\n        high = fd_metrics.get('high_complexity')\n\n        if high is None:\n\n            high = ds_metrics.get('high_complexity')\n\n        critical = fd_metrics.get('critical_complexity')\n\n        if critical is None:\n\n            critical = ds_metrics.get('critical_complexity')\n        \n        # If still missing, try loading from cache\n        if total_functions is None or moderate is None or high is None or critical is None:\n            try:\n                import json\n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    \n                    # Try analyze_functions first\n                    if 'results' in cached_data and 'analyze_functions' in cached_data['results']:\n                        func_data = cached_data['results']['analyze_functions']\n                        if 'data' in func_data:\n                            cached_metrics = func_data['data']\n                            if total_functions is None:\n                                total_functions = cached_metrics.get('total_functions')\n                            if moderate is None:\n                                moderate = cached_metrics.get('moderate_complexity')\n                            if high is None:\n                                high = cached_metrics.get('high_complexity')\n                            if critical is None:\n                                critical = cached_metrics.get('critical_complexity')\n                    \n                    # Fallback to decision_support\n                    if (total_functions is None or moderate is None or high is None or critical is None) and 'results' in cached_data:\n                        if 'decision_support' in cached_data['results']:\n                            ds_data = cached_data['results']['decision_support']\n                            if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                                cached_ds_metrics = ds_data['data']['decision_support_metrics']\n                                if total_functions is None:\n                                    total_functions = cached_ds_metrics.get('total_functions')\n                                if moderate is None:\n                                    moderate = cached_ds_metrics.get('moderate_complexity')\n                                if high is None:\n                                    high = cached_ds_metrics.get('high_complexity')\n                                if critical is None:\n                                    critical = cached_ds_metrics.get('critical_complexity')\n                    \n                    # Fallback to analyze_function_registry - parse high_complexity array\n                    if (high is None or critical is None) and 'results' in cached_data:\n                        if 'analyze_function_registry' in cached_data['results']:\n                            afr_data = cached_data['results']['analyze_function_registry']\n                            if 'data' in afr_data and 'analysis' in afr_data['data']:\n                                analysis = afr_data['data']['analysis']\n                                high_complexity_array = analysis.get('high_complexity', [])\n                                if isinstance(high_complexity_array, list):\n                                    # Count by thresholds: MODERATE=50, HIGH=100, CRITICAL=200\n                                    critical_count = sum(1 for f in high_complexity_array \n                                                        if isinstance(f, dict) and f.get('complexity', 0) >= 200)\n                                    high_count = sum(1 for f in high_complexity_array \n                                                   if isinstance(f, dict) and 100 <= f.get('complexity', 0) < 200)\n                                    if critical is None:\n                                        critical = critical_count\n                                    if high is None:\n                                        high = high_count\n                                    # For moderate, we'd need all functions, but we can estimate if we have total\n                                    if moderate is None and total_functions is not None and isinstance(total_functions, int):\n                                        # Estimate: total - high - critical (approximate, may include low complexity)\n                                        moderate = max(0, total_functions - high_count - critical_count)\n            except Exception as e:\n                logger.debug(f\"Failed to load metrics from cache in _get_canonical_metrics: {e}\")\n                pass\n\n        doc_coverage = audit_data.get('doc_coverage') if isinstance(audit_data, dict) else None\n\n        # Recalculate documentation coverage using actual total functions\n        # The registry's functions_found only counts functions in the registry, not all functions\n        audit_totals = audit_data.get('totals') if isinstance(audit_data, dict) else {}\n        if audit_totals is None or not isinstance(audit_totals, dict):\n            audit_totals = {}\n        documented = audit_totals.get('functions_documented', 0)\n        \n        # If documented is 0, try loading from cache file (structure might be different)\n        if documented == 0:\n            try:\n                import json\n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    \n                    # Try to get documented count from analyze_function_registry\n                    if 'results' in cached_data and 'analyze_function_registry' in cached_data['results']:\n                        afr_data = cached_data['results']['analyze_function_registry']\n                        if 'data' in afr_data:\n                            afr_data_dict = afr_data['data']\n                            if 'totals' in afr_data_dict:\n                                cached_totals = afr_data_dict['totals']\n                                if isinstance(cached_totals, dict):\n                                    documented = cached_totals.get('functions_documented', 0)\n            except Exception as e:\n                logger.debug(f\"Failed to load documented count from cache: {e}\")\n        \n        # ALWAYS recalculate documentation coverage using actual total functions\n        # Never trust the registry's coverage calculation - it uses wrong denominator\n        if total_functions is not None and isinstance(total_functions, (int, float)) and total_functions > 0:\n            if documented > 0:\n                coverage_pct = (documented / total_functions) * 100\n                # Only use if reasonable (0-100%)\n                if 0 <= coverage_pct <= 100:\n                    doc_coverage = f\"{coverage_pct:.2f}%\"\n                else:\n                    doc_coverage = 'Unknown'  # Invalid calculation\n            else:\n                doc_coverage = '0.00%'  # No documented functions\n        else:\n            # Can't calculate without total - mark as unknown\n            doc_coverage = 'Unknown'\n\n        # Validate any existing doc_coverage value and reject invalid ones\n        if isinstance(doc_coverage, str):\n            # Check for obviously wrong values\n            if '12690' in doc_coverage or 'Unknown' not in doc_coverage:\n                try:\n                    # Try to parse the percentage\n                    val_str = doc_coverage.strip('%').replace(',', '')\n                    if val_str.replace('.', '').isdigit():\n                        val = float(val_str)\n                        if val > 100:\n                            # Invalid - recalculate if we have the data\n                            if total_functions and documented:\n                                coverage_pct = (documented / total_functions) * 100\n                                if 0 <= coverage_pct <= 100:\n                                    doc_coverage = f\"{coverage_pct:.2f}%\"\n                                else:\n                                    doc_coverage = 'Unknown'\n                            else:\n                                doc_coverage = 'Unknown'\n                except (ValueError, TypeError):\n                    # Can't parse - might be valid string like \"Unknown\"\n                    if '12690' in doc_coverage:\n                        doc_coverage = 'Unknown'\n        \n        if doc_coverage is None or (isinstance(doc_coverage, str) and '12690' in doc_coverage):\n            doc_coverage = 'Unknown'\n        \n        # Return metrics dict with all values\n        return {\n            'total_functions': total_functions if total_functions is not None else 'Unknown',\n            'moderate': moderate if moderate is not None else 'Unknown',\n            'high': high if high is not None else 'Unknown',\n            'critical': critical if critical is not None else 'Unknown',\n            'doc_coverage': doc_coverage\n        }\n\n    def _extract_actionable_insights(self, output: str) -> str:\n\n        \"\"\"Extract and format actionable insights from raw output.\"\"\"\n\n        if not isinstance(output, str):\n\n            return 'No specific actionable insights found.'\n\n        lines = output.split('\\n')\n\n        insights = []\n\n        for line in lines:\n\n            if any(keyword in line.lower() for keyword in ['suggest', 'recommend', 'next step', 'action']):\n\n                insights.append(line.strip())\n\n        if insights:\n\n            return '\\n'.join(insights[:10])\n\n        return 'No specific actionable insights found.'\n\n    # LEGACY COMPATIBILITY\n    # This method was replaced by _save_audit_results_aggregated() which uses standardized storage.\n    # Removal plan: This method is not currently called. Remove after confirming no external callers exist.\n    # Detection: Search for \"_save_audit_results(\" to find any remaining callers.\n    def _save_audit_results(self, successful: List, failed: List, results: Dict):\n\n        \"\"\"Save detailed audit results (legacy method - kept for backward compatibility)\n        \n        DEPRECATED: Use _save_audit_results_aggregated() instead, which uses standardized storage.\n        \"\"\"\n        logger.warning(\"LEGACY: _save_audit_results() is deprecated. Use _save_audit_results_aggregated() instead.\")\n        \n        # Enhance results with extracted metrics from results_cache\n        enhanced_results = dict(results)\n        \n        # Add analyze_functions metrics if available\n        if 'analyze_functions' in self.results_cache:\n            func_metrics = self.results_cache['analyze_functions']\n            if 'analyze_functions' not in enhanced_results:\n                enhanced_results['analyze_functions'] = {'success': True, 'data': func_metrics}\n            elif 'data' not in enhanced_results.get('analyze_functions', {}):\n                enhanced_results['analyze_functions']['data'] = func_metrics\n        \n        # Add decision_support metrics if available\n        if 'decision_support_metrics' in self.results_cache:\n            ds_metrics = self.results_cache['decision_support_metrics']\n            # Check if decision_support exists and is a dict with proper structure\n            decision_support_result = enhanced_results.get('decision_support')\n            if decision_support_result is None or not isinstance(decision_support_result, dict):\n                # Replace boolean or None with proper dict structure\n                enhanced_results['decision_support'] = {\n                    'success': True,\n                    'data': {'decision_support_metrics': ds_metrics}\n                }\n            elif 'data' not in decision_support_result:\n                decision_support_result['data'] = {'decision_support_metrics': ds_metrics}\n            elif 'decision_support_metrics' not in decision_support_result.get('data', {}):\n                decision_support_result['data']['decision_support_metrics'] = ds_metrics\n\n        timestamp_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        timestamp_iso = datetime.now().isoformat()\n        audit_data = {\n            'generated_by': 'run_development_tools.py - AI Development Tools Runner',\n            'last_generated': timestamp_str,\n            'source': 'python development_tools/run_development_tools.py audit',\n            'note': 'This file is auto-generated. Do not edit manually.',\n            'timestamp': timestamp_iso,\n            'successful': successful,\n            'failed': failed,\n            'results': enhanced_results\n        }\n\n        # Normalise location relative to the project root\n        results_file_path = self.audit_config.get('results_file', 'development_tools/reports/analysis_detailed_results.json')\n        results_file = (self.project_root / results_file_path).resolve()\n\n        # Use file rotation for JSON files too\n\n        create_output_file(str(results_file), json.dumps(audit_data, indent=2))\n\n    def _save_audit_results_aggregated(self, tier: int):\n        \"\"\"Save aggregated audit results from all tool result files.\n        \n        Args:\n            tier: Audit tier (1=quick, 2=standard, 3=full)\n        \"\"\"\n        # Get all tool results from standardized storage\n        all_results = get_all_tool_results(project_root=self.project_root)\n        \n        # Convert to expected format\n        enhanced_results = {}\n        successful = []\n        failed = []\n        \n        for tool_name, result_data in all_results.items():\n            # Extract data from standardized format\n            if isinstance(result_data, dict):\n                tool_data = result_data.get('data', result_data)\n                enhanced_results[tool_name] = {\n                    'success': True,\n                    'data': tool_data,\n                    'timestamp': result_data.get('timestamp', datetime.now().isoformat())\n                }\n                successful.append(tool_name)\n            else:\n                enhanced_results[tool_name] = {\n                    'success': False,\n                    'data': {},\n                    'error': 'Invalid result format'\n                }\n                failed.append(tool_name)\n        \n        # Also include results from results_cache that might not be in files yet\n        for tool_name, data in self.results_cache.items():\n            if tool_name not in enhanced_results:\n                enhanced_results[tool_name] = {\n                    'success': True,\n                    'data': data,\n                    'timestamp': datetime.now().isoformat()\n                }\n                if tool_name not in successful:\n                    successful.append(tool_name)\n        \n        # Determine source command based on tier\n        if tier == 1:\n            source_cmd = 'python development_tools/run_development_tools.py audit --quick'\n        elif tier == 3:\n            source_cmd = 'python development_tools/run_development_tools.py audit --full'\n        else:\n            source_cmd = 'python development_tools/run_development_tools.py audit'\n        \n        timestamp_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        timestamp_iso = datetime.now().isoformat()\n        audit_data = {\n            'generated_by': 'run_development_tools.py - AI Development Tools Runner',\n            'last_generated': timestamp_str,\n            'source': source_cmd,\n            'audit_tier': tier,\n            'note': 'This file is auto-generated. Do not edit manually.',\n            'timestamp': timestamp_iso,\n            'successful': successful,\n            'failed': failed,\n            'results': enhanced_results\n        }\n\n        # Save to central aggregation file\n        results_file_path = self.audit_config.get('results_file', 'development_tools/reports/analysis_detailed_results.json')\n        results_file = (self.project_root / results_file_path).resolve()\n\n        # Ensure results_file is a Path object, not a string\n        if isinstance(results_file, str):\n            results_file = Path(results_file)\n        \n        # Import json if not already imported\n        import json\n        create_output_file(str(results_file), json.dumps(audit_data, indent=2))\n\n    def _generate_audit_report(self):\n\n        \"\"\"Generate comprehensive audit report\"\"\"\n\n        report_lines = []\n\n        report_lines.append(\"COMPREHENSIVE AUDIT REPORT\")\n\n        report_lines.append(\"=\" * 60)\n\n        report_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n        report_lines.append(\"\")\n\n        # Add detailed results for each component\n\n        for script_name, metrics in self.results_cache.items():\n\n            if metrics:\n\n                report_lines.append(f\"[{script_name.upper()}]\")\n\n                report_lines.append(\"-\" * 40)\n\n                if isinstance(metrics, dict):\n\n                    for key, value in metrics.items():\n\n                        report_lines.append(f\"  {key}: {value}\")\n\n                elif isinstance(metrics, list):\n\n                    for item in metrics:\n\n                        report_lines.append(f\"  {item}\")\n\n                report_lines.append(\"\")\n\n        # Add system information\n\n        report_lines.append(\"[SYSTEM INFO]\")\n\n        report_lines.append(\"-\" * 40)\n\n        report_lines.append(f\"Python version: {sys.version}\")\n\n        report_lines.append(f\"Working directory: {os.getcwd()}\")\n\n        report_lines.append(f\"Timestamp: {datetime.now().isoformat()}\")\n\n        return \"\\n\".join(report_lines)\n\n    def _generate_ai_status_document(self) -> str:\n\n        \"\"\"Generate AI-optimized status document.\"\"\"\n\n        lines: List[str] = []\n\n        lines.append(\"# AI Status - Current Codebase State\")\n\n        lines.append(\"\")\n\n        lines.append(\"> **File**: `development_tools/AI_STATUS.md`\")\n        lines.append(\"> **Generated**: This file is auto-generated. Do not edit manually.\")\n        lines.append(f\"> **Last Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        # Determine source command based on audit tier\n        if self.current_audit_tier == 1:\n            source_cmd = \"python development_tools/run_development_tools.py audit --quick\"\n            tier_name = \"Tier 1 (Quick Audit)\"\n        elif self.current_audit_tier == 3:\n            source_cmd = \"python development_tools/run_development_tools.py audit --full\"\n            tier_name = \"Tier 3 (Full Audit)\"\n        elif self.current_audit_tier == 2:\n            source_cmd = \"python development_tools/run_development_tools.py audit\"\n            tier_name = \"Tier 2 (Standard Audit)\"\n        else:\n            source_cmd = \"python development_tools/run_development_tools.py status\"\n            tier_name = \"Status Check (cached data)\"\n        lines.append(f\"> **Source**: `{source_cmd}`\")\n        if self.current_audit_tier:\n            lines.append(f\"> **Last Audit Tier**: {tier_name}\")\n        lines.append(\"> **Generated by**: run_development_tools.py - AI Development Tools Runner\")\n        lines.append(\"\")\n\n        def percent_text(value: Any, decimals: int = 1) -> str:\n\n            if value is None:\n\n                return \"Unknown\"\n\n            if isinstance(value, str):\n\n                return value if value.strip().endswith('%') else f\"{value}%\"\n\n            return self._format_percentage(value, decimals)\n\n        def to_int(value: Any) -> Optional[int]:\n            if isinstance(value, int):\n                return value\n            if isinstance(value, float):\n                return int(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return int(float(stripped))\n                except ValueError:\n                    return None\n            if isinstance(value, dict):\n                count = value.get('count')\n                return to_int(count)\n            return None\n\n        def to_float(value: Any) -> Optional[float]:\n            if isinstance(value, (int, float)):\n                return float(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return float(stripped)\n                except ValueError:\n                    return None\n            return None\n\n        metrics = self._get_canonical_metrics()\n        if not isinstance(metrics, dict):\n            metrics = {}\n\n        results_cache = self.results_cache or {}\n        doc_metrics = results_cache.get('analyze_function_registry', {}) or {}\n\n        error_metrics = results_cache.get('analyze_error_handling', {}) or {}\n\n        function_metrics = results_cache.get('analyze_functions', {}) or {}\n\n        analyze_docs = results_cache.get('analyze_documentation', {}) or {}\n        # analyze_documentation stores the payload directly, not wrapped in 'data'\n        if isinstance(analyze_docs, dict):\n            analyze_docs_data = analyze_docs\n        else:\n            analyze_docs_data = {}\n        \n        # Extract overlap analysis data\n        # Check if overlap analysis was run (indicated by presence of these keys, even if empty)\n        # If keys don't exist, overlap analysis wasn't run\n        # If keys exist but are None/empty, analysis was run but found nothing\n        overlap_analysis_ran = (\n            'section_overlaps' in analyze_docs_data or \n            'consolidation_recommendations' in analyze_docs_data\n        )\n        \n        section_overlaps = analyze_docs_data.get('section_overlaps', {}) if overlap_analysis_ran else {}\n        consolidation_recs = analyze_docs_data.get('consolidation_recommendations', []) if overlap_analysis_ran else []\n        \n        # Normalize to empty dict/list if None\n        if section_overlaps is None:\n            section_overlaps = {}\n        if consolidation_recs is None:\n            consolidation_recs = []\n        \n        doc_coverage = doc_metrics.get('doc_coverage', metrics.get('doc_coverage', 'Unknown'))\n\n        missing_docs = doc_metrics.get('missing_docs') or doc_metrics.get('missing_items')\n\n        missing_files = self._get_missing_doc_files(limit=4)\n\n        # NOTE: 'error_handling_coverage' is a backward compatibility fallback for old JSON format\n        error_coverage = error_metrics.get('analyze_error_handling') or error_metrics.get('error_handling_coverage')\n        \n        # Use the actual count from error analysis, not a recalculation\n        # The error analysis tool knows which functions actually need error handling\n        # Recalculating based on different totals can give incorrect results\n        missing_error_handlers = to_int(error_metrics.get('functions_missing_error_handling'))\n        \n        # Only recalculate coverage if totals differ, but keep the actual missing count\n        error_total = error_metrics.get('total_functions')\n        error_with_handling = error_metrics.get('functions_with_error_handling')\n        canonical_total = metrics.get('total_functions')\n        \n        if error_coverage is not None and canonical_total and error_total and error_with_handling:\n            if error_total != canonical_total:\n                recalc_coverage = (error_with_handling / canonical_total) * 100\n                if 0 <= recalc_coverage <= 100:\n                    error_coverage = recalc_coverage\n\n        worst_error_modules = error_metrics.get('worst_modules') or []\n\n        coverage_summary = self._load_coverage_summary() or {}\n        \n        # Load dev tools coverage if not already loaded\n        if not hasattr(self, 'dev_tools_coverage_results') or not self.dev_tools_coverage_results:\n            self._load_dev_tools_coverage()\n\n        doc_sync_summary = self.docs_sync_summary or {}\n        if not isinstance(doc_sync_summary, dict):\n            doc_sync_summary = {}\n\n        legacy_summary = self.legacy_cleanup_summary or {}\n\n        lines.append(\"## Snapshot\")\n\n        # Try to load cached audit results if not available in memory\n        total_functions = metrics.get('total_functions', 'Unknown') if metrics else 'Unknown'\n        moderate = metrics.get('moderate', 'Unknown') if metrics else 'Unknown'\n        high = metrics.get('high', 'Unknown') if metrics else 'Unknown'\n        critical = metrics.get('critical', 'Unknown') if metrics else 'Unknown'\n        \n        # If metrics are Unknown, try loading from cache\n        if total_functions == 'Unknown' or moderate == 'Unknown':\n            try:\n                import json\n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    \n                    # Try analyze_functions first\n                    if 'results' in cached_data and 'analyze_functions' in cached_data['results']:\n                        func_data = cached_data['results']['analyze_functions']\n                        if 'data' in func_data:\n                            cached_metrics = func_data['data']\n                            if total_functions == 'Unknown':\n                                total_functions = cached_metrics.get('total_functions', 'Unknown')\n                            if moderate == 'Unknown':\n                                moderate = cached_metrics.get('moderate_complexity', 'Unknown')\n                            if high == 'Unknown':\n                                high = cached_metrics.get('high_complexity', 'Unknown')\n                            if critical == 'Unknown':\n                                critical = cached_metrics.get('critical_complexity', 'Unknown')\n                    \n                    # Fallback to decision_support if still Unknown\n                    if (total_functions == 'Unknown' or moderate == 'Unknown') and 'results' in cached_data:\n                        # Check if decision_support was run and metrics extracted\n                        if 'decision_support' in cached_data['results']:\n                            ds_data = cached_data['results']['decision_support']\n                            if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                                ds_metrics = ds_data['data']['decision_support_metrics']\n                                if total_functions == 'Unknown':\n                                    total_functions = ds_metrics.get('total_functions', 'Unknown')\n                                if moderate == 'Unknown':\n                                    moderate = ds_metrics.get('moderate_complexity', 'Unknown')\n                                if high == 'Unknown':\n                                    high = ds_metrics.get('high_complexity', 'Unknown')\n                                if critical == 'Unknown':\n                                    critical = ds_metrics.get('critical_complexity', 'Unknown')\n            except Exception as e:\n                logger.debug(f\"Failed to load metrics from cache: {e}\")\n                pass\n        \n        if total_functions == 'Unknown':\n            lines.append(\"- **Total Functions**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n        else:\n            lines.append(f\"- **Total Functions**: {total_functions} (Moderate: {moderate}, High: {high}, Critical: {critical})\")\n\n        # Try to load documentation coverage from cached data\n        doc_coverage = metrics.get('doc_coverage', 'Unknown')\n        missing_docs = None\n        missing_files = []\n        if doc_coverage == 'Unknown' or doc_coverage is None:\n            try:\n                import json\n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    if 'results' in cached_data and 'analyze_function_registry' in cached_data['results']:\n                        func_reg_data = cached_data['results']['analyze_function_registry']\n                        if 'data' in func_reg_data:\n                            cached_metrics = func_reg_data['data']\n                            cached_doc_cov = cached_metrics.get('doc_coverage') or cached_metrics.get('coverage')\n                            if cached_doc_cov is not None:\n                                if isinstance(cached_doc_cov, (int, float)):\n                                    doc_coverage = f\"{float(cached_doc_cov):.2f}%\"\n                                else:\n                                    doc_coverage = str(cached_doc_cov)\n                            missing_docs = cached_metrics.get('missing') or cached_metrics.get('missing_docs') or cached_metrics.get('missing_items')\n                            missing_files = []\n                        else:\n                            # Parse from text output\n                            output = func_reg_data.get('output', '')\n                            if 'Documentation Coverage:' in output:\n                                import re\n                                match = re.search(r'Documentation Coverage:\\s*(\\d+\\.?\\d*)%', output)\n                                if match:\n                                    doc_coverage = match.group(1) + '%'\n            except Exception as e:\n                logger.debug(f\"Failed to load doc_coverage from cache in status: {e}\")\n                pass\n\n        doc_line = f\"- **Documentation Coverage**: {percent_text(doc_coverage, 2)}\"\n\n        if missing_docs:\n\n            doc_line += f\" ({missing_docs} items missing from registry)\"\n\n        lines.append(doc_line)\n\n        if missing_files:\n\n            lines.append(f\"- **Missing Documentation Files**: {self._format_list_for_display(missing_files, limit=4)}\")\n\n        # Try to load error handling coverage from cached data\n        error_coverage = 'Unknown'\n        missing_error_handlers = None\n        error_total = None\n        error_with_handling = None\n        if not error_coverage or error_coverage == 'Unknown':\n            try:\n                import json\n                results_file = Path(\"development_tools/reports/analysis_detailed_results.json\")\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    if 'results' in cached_data and 'analyze_error_handling' in cached_data['results']:\n                        error_data = cached_data['results']['analyze_error_handling']\n                        if 'data' in error_data:\n                            cached_metrics = error_data['data']\n                            # NOTE: 'error_handling_coverage' is a backward compatibility fallback for old JSON format\n                            error_coverage = cached_metrics.get('analyze_error_handling') or cached_metrics.get('error_handling_coverage', 'Unknown')\n                            missing_error_handlers = cached_metrics.get('functions_missing_error_handling')\n                            error_total = cached_metrics.get('total_functions')\n                            error_with_handling = cached_metrics.get('functions_with_error_handling')\n            except Exception:\n                pass\n        \n        # Use the actual count from error analysis, not a recalculation\n        missing_error_handlers = to_int(error_metrics.get('functions_missing_error_handling'))\n        \n        # Only recalculate coverage if totals differ, but keep the actual missing count\n        canonical_total = metrics.get('total_functions')\n        if error_coverage is not None and canonical_total and error_total and error_with_handling:\n            if error_total != canonical_total:\n                recalc_coverage = (error_with_handling / canonical_total) * 100\n                if 0 <= recalc_coverage <= 100:\n                    error_coverage = recalc_coverage\n\n        lines.append(\n\n            f\"- **Error Handling Coverage**: {percent_text(error_coverage, 1)}\"\n\n            + (f\" ({missing_error_handlers} functions without handlers)\" if missing_error_handlers is not None else \"\")\n\n        )\n\n        # Try to load doc sync data from cached data\n        if not doc_sync_summary:\n            try:\n                import json\n                results_file = Path(\"development_tools/reports/analysis_detailed_results.json\")\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    if 'results' in cached_data and 'analyze_documentation' in cached_data['results']:\n                        doc_sync_data = cached_data['results']['analyze_documentation']\n                        if 'data' in doc_sync_data:\n                            cached_metrics = doc_sync_data['data']\n                            # Create a doc_sync_summary from the cached data\n                            doc_sync_summary = {\n                                'status': 'GOOD' if not cached_metrics.get('artifacts') else 'NEEDS REVIEW',\n                                'total_issues': len(cached_metrics.get('artifacts', []))\n                            }\n            except Exception:\n                pass\n\n        if doc_sync_summary:\n\n            sync_status = doc_sync_summary.get('status', 'Unknown')\n\n            total_issues = doc_sync_summary.get('total_issues')\n\n            sync_line = f\"- **Doc Sync**: {sync_status}\"\n\n            if total_issues is not None:\n\n                sync_line += f\" ({total_issues} tracked issues)\"\n\n            lines.append(sync_line)\n\n        else:\n\n            lines.append(\"- **Doc Sync**: Not collected in this run (pending doc-sync refresh)\")\n\n        # Add test coverage to snapshot\n        if coverage_summary and isinstance(coverage_summary, dict):\n            overall = coverage_summary.get('overall') or {}\n            if overall.get('coverage') is not None:\n                lines.append(\n                    f\"- **Test Coverage**: {percent_text(overall.get('coverage'), 1)} \"\n                    f\"({overall.get('covered')} of {overall.get('statements')} statements)\"\n                )\n\n        lines.append(\"\")\n\n        lines.append(\"## Documentation Signals\")\n\n        # Use aggregated doc sync summary from current run first, then fall back to cache\n        doc_sync_summary_for_signals = None\n        if self.docs_sync_summary and isinstance(self.docs_sync_summary, dict):\n            # Use the aggregated summary from _run_doc_sync_check()\n            doc_sync_summary_for_signals = {\n                'status': self.docs_sync_summary.get('status', 'UNKNOWN'),\n                'path_drift_issues': self.docs_sync_summary.get('path_drift_issues', 0),\n                'paired_doc_issues': self.docs_sync_summary.get('paired_doc_issues', 0),\n                'ascii_issues': self.docs_sync_summary.get('ascii_issues', 0),\n                'heading_numbering_issues': self.docs_sync_summary.get('heading_numbering_issues', 0),\n                'missing_address_issues': self.docs_sync_summary.get('missing_address_issues', 0),\n                'unconverted_link_issues': self.docs_sync_summary.get('unconverted_link_issues', 0),\n                'path_drift_files': self.docs_sync_summary.get('path_drift_files', [])\n            }\n        \n        # Fall back to cache if not available in memory\n        if not doc_sync_summary_for_signals:\n            try:\n                # Try standardized storage first\n                from .output_storage import load_tool_result\n                doc_sync_result = load_tool_result('analyze_documentation_sync', 'docs', project_root=self.project_root)\n                if doc_sync_result:\n                    # load_tool_result already unwraps the 'data' key, so doc_sync_result IS the data\n                    cached_metrics = doc_sync_result if isinstance(doc_sync_result, dict) else {}\n                    doc_sync_summary_for_signals = {\n                        'status': cached_metrics.get('status', 'UNKNOWN'),\n                        'path_drift_issues': cached_metrics.get('path_drift_issues', 0),\n                        'paired_doc_issues': cached_metrics.get('paired_doc_issues', 0),\n                        'ascii_issues': cached_metrics.get('ascii_issues', 0),\n                        'path_drift_files': cached_metrics.get('path_drift_files', [])\n                    }\n                else:\n                    # Fall back to central aggregation file\n                    import json\n                    results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                    if results_file.exists():\n                        with open(results_file, 'r', encoding='utf-8') as f:\n                            cached_data = json.load(f)\n                        if 'results' in cached_data and 'analyze_documentation_sync' in cached_data['results']:\n                            doc_sync_data = cached_data['results']['analyze_documentation_sync']\n                            if 'data' in doc_sync_data:\n                                cached_metrics = doc_sync_data['data']\n                                # Create a doc_sync_summary from the cached data\n                                doc_sync_summary_for_signals = {\n                                    'status': cached_metrics.get('status', 'UNKNOWN'),\n                                    'path_drift_issues': cached_metrics.get('path_drift_issues', 0),\n                                    'paired_doc_issues': cached_metrics.get('paired_doc_issues', 0),\n                                    'ascii_issues': cached_metrics.get('ascii_issues', 0),\n                                    'path_drift_files': cached_metrics.get('path_drift_files', [])\n                                }\n            except Exception as e:\n                logger.debug(f\"Failed to load doc sync summary: {e}\")\n                pass\n\n        if doc_sync_summary_for_signals:\n\n            path_drift = doc_sync_summary_for_signals.get('path_drift_issues')\n\n            paired = doc_sync_summary_for_signals.get('paired_doc_issues')\n\n            ascii_issues = doc_sync_summary_for_signals.get('ascii_issues')\n\n            # Check for path validation issues first (separate from path_drift - checks if referenced paths exist)\n            # Path validation is more critical than path drift\n            path_val_issues = None\n            if hasattr(self, 'path_validation_result') and self.path_validation_result:\n                path_val_status = self.path_validation_result.get('status')\n                path_val_issues = self.path_validation_result.get('issues_found', 0)\n                if path_val_status == 'fail' and path_val_issues and path_val_issues > 0:\n                    lines.append(f\"- **Path Validation**: NEEDS ATTENTION ({path_val_issues} referenced paths don't exist)\")\n                elif path_val_status == 'ok' or (path_val_status == 'fail' and path_val_issues == 0):\n                    lines.append(f\"- **Path Validation**: CLEAN (all referenced paths exist)\")\n            \n            # Then check path drift (documentation path changes)\n            # Path drift should only show CLEAN if path_drift_issues is 0 AND path validation passed\n            if path_drift is not None:\n                # Path drift is independent of overall status - check path_drift_issues directly\n                # If path_drift_issues is 0, it means the tool ran and found 0 path drift issues\n                # (overall status can be FAIL due to other issues like paired_doc_issues)\n                # But also check path validation - if path validation found issues, show them instead\n                if path_drift == 0:\n                    # Path drift tool found 0 issues\n                    if path_val_issues is None:\n                        # Path validation didn't run, so trust path drift\n                        severity = \"CLEAN\"\n                        lines.append(f\"- **Path Drift**: {severity} ({path_drift} issues)\")\n                    elif path_val_issues == 0:\n                        # Both path drift and path validation found 0 issues\n                        severity = \"CLEAN\"\n                        lines.append(f\"- **Path Drift**: {severity} ({path_drift} issues)\")\n                    else:\n                        # Path drift found 0, but path validation found issues - show path validation issues\n                        severity = \"NEEDS ATTENTION\"\n                        lines.append(f\"- **Path Drift**: {severity} ({path_val_issues} path validation issues found)\")\n                else:\n                    # Path drift found issues\n                    severity = \"NEEDS ATTENTION\"\n                    lines.append(f\"- **Path Drift**: {severity} ({path_drift} issues)\")\n            elif path_val_issues is not None and path_val_issues > 0:\n                # Path drift didn't run, but path validation found issues\n                lines.append(f\"- **Path Drift**: NEEDS ATTENTION ({path_val_issues} path validation issues found)\")\n            else:\n                # If neither path_drift nor path_validation ran, show unknown\n                lines.append(\"- **Path Drift**: Unknown (run `audit` to check)\")\n\n            drift_files = doc_sync_summary_for_signals.get('path_drift_files') or []\n\n            if drift_files:\n\n                lines.append(f\"- **Drift Hotspots**: {self._format_list_for_display(drift_files, limit=4)}\")\n\n            if paired is not None:\n                status_label = \"SYNCHRONIZED\" if paired == 0 else \"NEEDS ATTENTION\"\n                lines.append(f\"- **Paired Docs**: {status_label} ({paired} issues)\")\n                # Add details about paired doc issues if available\n                if paired > 0 and doc_sync_summary_for_signals:\n                    paired_docs_data = doc_sync_summary_for_signals.get('paired_docs', {})\n                    if isinstance(paired_docs_data, dict):\n                        content_sync_issues = paired_docs_data.get('content_sync', [])\n                        if content_sync_issues:\n                            # Show first 2-3 issues\n                            for issue in content_sync_issues[:3]:\n                                lines.append(f\"  - {issue}\")\n                            if len(content_sync_issues) > 3:\n                                lines.append(f\"  - ...and {len(content_sync_issues) - 3} more issue(s)\")\n        \n        # Add ASCII Cleanup to Documentation Signals\n        if ascii_issues:\n            lines.append(f\"- **ASCII Cleanup**: {ascii_issues} files contain non-ASCII characters\")\n        \n        # Add Dependency Docs to Documentation Signals\n        dependency_summary = self.module_dependency_summary or self.results_cache.get('analyze_module_dependencies')\n        if dependency_summary:\n            missing_deps = dependency_summary.get('missing_dependencies')\n            if missing_deps:\n                lines.append(f\"- **Dependency Docs**: {missing_deps} undocumented references detected\")\n                missing_files = dependency_summary.get('missing_files') or dependency_summary.get('missing_sections') or []\n                if missing_files:\n                    lines.append(f\"  Top files: {self._format_list_for_display(missing_files, limit=3)}\")\n            else:\n                lines.append(\"- **Dependency Docs**: CLEAN (no undocumented dependencies)\")\n        \n        if not doc_sync_summary_for_signals:\n            lines.append(\"- Run `python development_tools/run_development_tools.py doc-sync` for drift details\")\n        \n        # Add config validation status\n        config_validation_summary = self._load_config_validation_summary()\n        if config_validation_summary:\n            config_valid = config_validation_summary.get('config_valid', False)\n            config_complete = config_validation_summary.get('config_complete', False)\n            total_recommendations = config_validation_summary.get('total_recommendations', 0)\n            if config_valid and config_complete and total_recommendations == 0:\n                lines.append(\"- **Config Validation**: CLEAN (no issues)\")\n            elif total_recommendations > 0:\n                lines.append(f\"- **Config Validation**: {total_recommendations} recommendations\")\n            else:\n                lines.append(\"- **Config Validation**: Needs attention\")\n        \n        # Add TODO sync status\n        todo_sync_result = getattr(self, 'todo_sync_result', None)\n        if todo_sync_result and isinstance(todo_sync_result, dict):\n            completed_entries = todo_sync_result.get('completed_entries', 0)\n            if completed_entries > 0:\n                lines.append(f\"- **TODO Sync**: {completed_entries} completed entries need review\")\n            else:\n                lines.append(\"- **TODO Sync**: CLEAN (no completed entries)\")\n        \n        # Add overlap analysis summary (always show, even if no overlaps found)\n        lines.append(\"\")\n        lines.append(\"## Documentation Overlap\")\n        overlap_count = len(section_overlaps) if section_overlaps else 0\n        consolidation_count = len(consolidation_recs) if consolidation_recs else 0\n        \n        if overlap_count > 0 or consolidation_count > 0:\n            if section_overlaps and overlap_count > 0:\n                lines.append(f\"- **Section Overlaps**: {overlap_count} sections duplicated across files\")\n                # Show first few overlaps\n                top_overlaps = sorted(section_overlaps.items(), key=lambda x: len(x[1]), reverse=True)[:3]\n                for section, files in top_overlaps:\n                    lines.append(f\"  - `{section}` appears in: {', '.join(files[:3])}{'...' if len(files) > 3 else ''}\")\n            # Consolidation opportunities moved to AI_PRIORITIES (not shown in AI_STATUS)\n        else:\n            if overlap_analysis_ran:\n                lines.append(\"- **Status**: No overlaps detected (analysis performed)\")\n                lines.append(\"  - Overlap analysis ran but found no section overlaps or consolidation opportunities\")\n            else:\n                lines.append(\"- **Status**: Overlap analysis not run (use `audit --full` or `--overlap` flag)\")\n                lines.append(\"  - Standard audits skip overlap analysis by default; run `audit --full` or use `--overlap` flag to include it\")\n\n        doc_artifacts = analyze_docs.get('artifacts') if isinstance(analyze_docs, dict) else None\n\n        if doc_artifacts:\n\n            primary_artifact = doc_artifacts[0]\n\n            file_name = primary_artifact.get('file')\n\n            line_no = primary_artifact.get('line')\n\n            pattern = primary_artifact.get('pattern')\n\n            lines.append(\n\n                f\"- **Content Cleanup**: {file_name} line {line_no} flagged for {pattern.replace('_', ' ')}\"\n\n            )\n\n            if len(doc_artifacts) > 1:\n\n                lines.append(f\"- Additional documentation artifacts: {len(doc_artifacts) - 1} more findings\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Error Handling\")\n\n        if error_metrics:\n\n            if missing_error_handlers:\n\n                lines.append(f\"- **Missing Error Handling**: {missing_error_handlers} functions lack protections\")\n\n            decorated = error_metrics.get('functions_with_decorators')\n\n            if decorated is not None:\n\n                lines.append(f\"- **@handle_errors Usage**: {decorated} functions already use the decorator\")\n\n            # Phase 1: Candidates for decorator replacement\n\n            phase1_total = error_metrics.get('phase1_total', 0)\n\n            if phase1_total > 0:\n\n                phase1_by_priority = error_metrics.get('phase1_by_priority', {})\n\n                priority_counts = []\n\n                if phase1_by_priority.get('high', 0) > 0:\n\n                    priority_counts.append(f\"{phase1_by_priority['high']} high\")\n\n                if phase1_by_priority.get('medium', 0) > 0:\n\n                    priority_counts.append(f\"{phase1_by_priority['medium']} medium\")\n\n                if phase1_by_priority.get('low', 0) > 0:\n\n                    priority_counts.append(f\"{phase1_by_priority['low']} low\")\n\n                priority_text = ', '.join(priority_counts) if priority_counts else '0'\n\n                lines.append(f\"- **Phase 1 Candidates**: {phase1_total} functions need decorator replacement ({priority_text} priority)\")\n\n            # Phase 2: Generic exception raises\n\n            phase2_total = error_metrics.get('phase2_total', 0)\n\n            if phase2_total > 0:\n\n                phase2_by_type = error_metrics.get('phase2_by_type', {})\n\n                type_counts = [f\"{count} {exc_type}\" for exc_type, count in sorted(phase2_by_type.items(), key=lambda x: x[1], reverse=True)[:3]]\n\n                type_text = ', '.join(type_counts) if type_counts else '0'\n\n                if len(phase2_by_type) > 3:\n\n                    type_text += f\", ... +{len(phase2_by_type) - 3} more\"\n\n                lines.append(f\"- **Phase 2 Exceptions**: {phase2_total} generic exception raises need categorization ({type_text})\")\n\n            if worst_error_modules:\n\n                module_descriptions = []\n\n                # Filter out 100% modules (missing 0) - they don't need attention\n                # Convert coverage to float for comparison (handles both string and numeric values)\n                modules_needing_attention = []\n                for m in worst_error_modules[:5]:\n                    missing = m.get('missing', 0)\n                    coverage_val = m.get('coverage', 100)\n                    if isinstance(coverage_val, str):\n                        coverage_val = to_float(coverage_val) or 100\n                    elif not isinstance(coverage_val, (int, float)):\n                        coverage_val = 100\n                    if missing > 0 and coverage_val < 100:\n                        modules_needing_attention.append(m)\n                \n                for module in modules_needing_attention[:3]:\n                    module_name = module.get('module', 'Unknown')\n                    coverage_value = module.get('coverage')\n                    coverage_text = percent_text(coverage_value, 1)\n                    missing = module.get('missing')\n                    total = module.get('total')\n\n                    detail = f\"{module_name} ({coverage_text}\"\n\n                    if missing is not None and total is not None:\n                        detail += f\", missing {missing}/{total}\"\n\n                    detail += \")\"\n\n                    module_descriptions.append(detail)\n                \n                if module_descriptions:\n                    lines.append(f\"- **Modules to Prioritize**: {', '.join(module_descriptions)}\")\n\n        else:\n            # Try to load cached error handling data\n            try:\n                import json\n                results_file = Path(\"development_tools/reports/analysis_detailed_results.json\")\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    if 'results' in cached_data and 'analyze_error_handling' in cached_data['results']:\n                        error_data = cached_data['results']['analyze_error_handling']\n                        if 'data' in error_data:\n                            error_metrics = error_data['data']\n                        else:\n                            # Try reading from the file if 'data' key is missing\n                            # LEGACY COMPATIBILITY: Reading from old file location for backward compatibility\n                            # New standardized storage location: error_handling/jsons/analyze_error_handling_results.json\n                            try:\n                                json_file = self.project_root / 'development_tools' / 'error_handling' / 'error_handling_details.json'\n                                if json_file.exists():\n                                    with open(json_file, 'r', encoding='utf-8') as f:\n                                        file_data = json.load(f)\n                                    # Handle new structure with metadata wrapper\n                                    if 'error_handling_results' in file_data:\n                                        error_metrics = file_data['error_handling_results']\n                                    else:\n                                        # Fallback to old structure (direct results)\n                                        error_metrics = file_data\n                                else:\n                                    # Try new standardized storage location\n                                    from .output_storage import load_tool_result\n                                    loaded_data = load_tool_result('analyze_error_handling', 'error_handling', project_root=self.project_root)\n                                    if loaded_data:\n                                        error_metrics = loaded_data\n                                    else:\n                                        error_metrics = None\n                            except (OSError, json.JSONDecodeError):\n                                error_metrics = None\n                        \n                        if error_metrics:\n                            # NOTE: 'error_handling_coverage' is a backward compatibility fallback for old JSON format\n                            coverage = error_metrics.get('analyze_error_handling') or error_metrics.get('error_handling_coverage', 'Unknown')\n                            if coverage != 'Unknown':\n                                lines.append(f\"- **Error Handling Coverage**: {coverage:.1f}%\")\n                                lines.append(f\"- **Functions with Error Handling**: {error_metrics.get('functions_with_error_handling', 'Unknown')}\")\n                                lines.append(f\"- **Functions Missing Error Handling**: {error_metrics.get('functions_missing_error_handling', 'Unknown')}\")\n                                \n                                # Add Phase 1 and Phase 2 if available\n                                phase1_total = error_metrics.get('phase1_total', 0)\n                                if phase1_total > 0:\n                                    phase1_by_priority = error_metrics.get('phase1_by_priority', {})\n                                    priority_counts = []\n                                    if phase1_by_priority.get('high', 0) > 0:\n                                        priority_counts.append(f\"{phase1_by_priority['high']} high\")\n                                    if phase1_by_priority.get('medium', 0) > 0:\n                                        priority_counts.append(f\"{phase1_by_priority['medium']} medium\")\n                                    if phase1_by_priority.get('low', 0) > 0:\n                                        priority_counts.append(f\"{phase1_by_priority['low']} low\")\n                                    priority_text = ', '.join(priority_counts) if priority_counts else '0'\n                                    lines.append(f\"- **Phase 1 Candidates**: {phase1_total} functions need decorator replacement ({priority_text} priority)\")\n                                \n                                phase2_total = error_metrics.get('phase2_total', 0)\n                                if phase2_total > 0:\n                                    phase2_by_type = error_metrics.get('phase2_by_type', {})\n                                    type_counts = [f\"{count} {exc_type}\" for exc_type, count in sorted(phase2_by_type.items(), key=lambda x: x[1], reverse=True)[:3]]\n                                    type_text = ', '.join(type_counts) if type_counts else '0'\n                                    if len(phase2_by_type) > 3:\n                                        type_text += f\", ... +{len(phase2_by_type) - 3} more\"\n                                    lines.append(f\"- **Phase 2 Exceptions**: {phase2_total} generic exception raises need categorization ({type_text})\")\n                            else:\n                                lines.append(\"- **Error Handling**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n                        else:\n                            lines.append(\"- **Error Handling**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n                    else:\n                        lines.append(\"- **Error Handling**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n                else:\n                    lines.append(\"- **Error Handling**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n            except Exception:\n                lines.append(\"- **Error Handling**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Test Coverage\")\n\n        dev_tools_insights = self._get_dev_tools_coverage_insights()\n\n        if coverage_summary and isinstance(coverage_summary, dict):\n\n            overall = coverage_summary.get('overall') or {}\n\n            lines.append(\n\n                f\"- **Overall Coverage**: {percent_text(overall.get('coverage'), 1)} \"\n\n                f\"({overall.get('covered')} of {overall.get('statements')} statements)\"\n\n            )\n\n            generated = overall.get('generated')\n\n            if generated:\n                pass  # Generated timestamp available\n\n            module_gaps = (coverage_summary.get('modules') or [])[:3]\n\n            if module_gaps:\n\n                descriptions = [\n\n                    f\"{m['module']} ({percent_text(m.get('coverage'), 1)}, missing {m.get('missed')} lines)\"\n\n                    for m in module_gaps\n\n                ]\n\n                lines.append(f\"    - **Lowest Coverage Domains**: {', '.join(descriptions)}\")\n\n            worst_files = (coverage_summary or {}).get('worst_files') or []\n\n            if worst_files:\n\n                descriptions = [\n\n                    f\"{item['path']} ({percent_text(item.get('coverage'), 1)})\"\n\n                    for item in worst_files[:3]\n\n                ]\n\n                lines.append(f\"    - **Files Needing Tests**: {', '.join(descriptions)}\")\n            \n            if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n                dev_pct = dev_tools_insights['overall_pct']\n                dev_statements = dev_tools_insights.get('statements')\n                dev_covered = dev_tools_insights.get('covered')\n                summary_line = f\"- **Development Tools Coverage**: {percent_text(dev_pct, 1)}\"\n                if dev_statements is not None and dev_covered is not None:\n                    summary_line += f\" ({dev_covered} of {dev_statements} statements)\"\n                lines.append(summary_line)\n                low_modules = dev_tools_insights.get('low_modules') or []\n                if low_modules:\n                    dev_descriptions = [\n                        f\"{Path(item['path']).name} ({percent_text(item.get('coverage'), 1)}, missing {item.get('missed')} lines)\"\n                        for item in low_modules[:5]\n                    ]\n                    lines.append(f\"    - **Modules with Lowest Coverage**: {', '.join(dev_descriptions)}\")\n\n        else:\n\n            lines.append(\"- Coverage data unavailable; run `audit --full` to regenerate metrics\")\n\n        lines.append(\"\")\n\n        # Add unused imports status\n        # Try results_cache first (from current audit), then standardized storage\n        unused_imports_data = self.results_cache.get('analyze_unused_imports', {}) or {}\n        if not unused_imports_data or not isinstance(unused_imports_data, dict):\n            try:\n                from .output_storage import load_tool_result\n                unused_result = load_tool_result('analyze_unused_imports', 'imports', project_root=self.project_root)\n                if unused_result:\n                    # load_tool_result already unwraps the 'data' key, so unused_result IS the data\n                    unused_imports_data = unused_result if isinstance(unused_result, dict) else {}\n            except Exception as e:\n                logger.debug(f\"Failed to load unused imports for AI_STATUS: {e}\")\n                unused_imports_data = {}\n        \n        if unused_imports_data and isinstance(unused_imports_data, dict):\n            total_unused = unused_imports_data.get('total_unused', 0)\n            files_with_issues = unused_imports_data.get('files_with_issues', 0)\n            if total_unused > 0 or files_with_issues > 0:\n                lines.append(\"## Unused Imports\")\n                lines.append(f\"- **Total Unused**: {total_unused} imports across {files_with_issues} files\")\n                # Add category breakdown\n                by_category = unused_imports_data.get('by_category') or {}\n                if by_category:\n                    obvious = by_category.get('obvious_unused', 0)\n                    type_only = by_category.get('type_hints_only', 0)\n                    if obvious > 0:\n                        lines.append(f\"    - **Obvious Removals**: {obvious} imports\")\n                    if type_only > 0:\n                        lines.append(f\"    - **Type-Only Imports**: {type_only} imports\")\n            else:\n                lines.append(\"## Unused Imports\")\n                lines.append(\"- **Status**: CLEAN (no unused imports detected)\")\n        else:\n            lines.append(\"## Unused Imports\")\n            lines.append(\"- **Status**: Data unavailable (run `audit --full` for latest scan)\")\n        \n        lines.append(\"\")\n\n        lines.append(\"## Legacy References\")\n\n        if legacy_summary:\n\n            legacy_issues = legacy_summary.get('files_with_issues')\n\n            if legacy_issues == 0:\n\n                lines.append(\"- **Legacy References**: CLEAN (0 files flagged)\")\n\n            elif legacy_issues is not None:\n\n                lines.append(f\"- **Legacy References**: {legacy_issues} files still reference legacy patterns\")\n\n            report_path = legacy_summary.get('report_path')\n\n            if report_path:\n\n                lines.append(f\"- **Detailed Report**: {report_path}\")\n\n        else:\n            # Try to load from standardized storage first, then fall back to central aggregation\n            legacy_found = False\n            try:\n                from .output_storage import load_tool_result\n                legacy_result = load_tool_result('analyze_legacy_references', 'legacy', project_root=self.project_root)\n                if legacy_result and 'data' in legacy_result:\n                    legacy_data = legacy_result['data']\n                    # Extract files_with_issues from the legacy data structure\n                    if isinstance(legacy_data, dict):\n                        # Try different possible keys for legacy issues count\n                        legacy_issues = (legacy_data.get('files_with_issues') or \n                                       legacy_data.get('total_files') or\n                                       len(legacy_data.get('findings', {}).get('legacy_compatibility_markers', [])))\n                        if legacy_issues is not None and legacy_issues != 'Unknown':\n                            if legacy_issues == 0:\n                                lines.append(\"- **Legacy References**: CLEAN (0 files flagged)\")\n                            else:\n                                lines.append(f\"- **Legacy References**: {legacy_issues} files still reference legacy patterns\")\n                            report_path = legacy_data.get('report_path') or 'development_docs/LEGACY_REFERENCE_REPORT.md'\n                            if report_path:\n                                # Ensure report_path is a Path object before calling .exists()\n                                report_path_obj = Path(report_path) if isinstance(report_path, str) else report_path\n                                if isinstance(report_path_obj, Path) and report_path_obj.exists():\n                                    lines.append(f\"- **Detailed Report**: {report_path}\")\n                            legacy_found = True\n                else:\n                    # LEGACY COMPATIBILITY\n                    # Fall back to central aggregation file (analysis_detailed_results.json) if standardized storage not available.\n                    # New standardized storage location: development_tools/legacy/jsons/analyze_legacy_references_results.json\n                    # Removal plan: After standardized storage is fully adopted, remove this fallback. All tools should use standardized storage.\n                    # Detection: Search for \"analysis_detailed_results.json\" and \"analyze_legacy_references\" to find all fallback references.\n                    logger.debug(\"LEGACY: Falling back to central aggregation file for legacy references (prefer standardized storage)\")\n                    import json\n                    results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                    if results_file.exists():\n                        with open(results_file, 'r', encoding='utf-8') as f:\n                            cached_data = json.load(f)\n                        # Check for analyze_legacy_references (not fix_legacy_references)\n                        if 'results' in cached_data and 'analyze_legacy_references' in cached_data['results']:\n                            legacy_data = cached_data['results']['analyze_legacy_references']\n                            if 'data' in legacy_data:\n                                cached_legacy = legacy_data['data']\n                                if isinstance(cached_legacy, dict):\n                                    legacy_issues = (cached_legacy.get('files_with_issues') or \n                                                   cached_legacy.get('total_files') or\n                                                   len(cached_legacy.get('findings', {}).get('legacy_compatibility_markers', [])))\n                                    if legacy_issues is not None and legacy_issues != 'Unknown':\n                                        if legacy_issues == 0:\n                                            lines.append(\"- **Legacy References**: CLEAN (0 files flagged)\")\n                                        else:\n                                            lines.append(f\"- **Legacy References**: {legacy_issues} files still reference legacy patterns\")\n                                        report_path = cached_legacy.get('report_path') or 'development_docs/LEGACY_REFERENCE_REPORT.md'\n                                        if report_path and Path(report_path).exists():\n                                            lines.append(f\"- **Detailed Report**: {report_path}\")\n                                        legacy_found = True\n            except Exception as e:\n                logger.debug(f\"Failed to load legacy data: {e}\")\n            \n            # If we still don't have legacy data, show unavailable message\n            if not legacy_found:\n                # Try one more time to load from results_cache (in case it was just run)\n                if 'analyze_legacy_references' in self.results_cache:\n                    legacy_cache = self.results_cache['analyze_legacy_references']\n                    if isinstance(legacy_cache, dict):\n                        legacy_issues = legacy_cache.get('files_with_issues', 0)\n                        if legacy_issues == 0:\n                            lines.append(\"- **Legacy References**: CLEAN (0 files flagged)\")\n                        else:\n                            lines.append(f\"- **Legacy References**: {legacy_issues} files still reference legacy patterns\")\n                        report_path = legacy_cache.get('report_path') or 'development_docs/LEGACY_REFERENCE_REPORT.md'\n                        if report_path and Path(report_path).exists():\n                            lines.append(f\"- **Detailed Report**: {report_path}\")\n                        legacy_found = True\n                \n                if not legacy_found:\n                    lines.append(\"- Legacy reference data unavailable (run `audit --full` for latest scan)\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Validation Status\")\n\n        validation_output = ''\n        if hasattr(self, 'validation_results') and self.validation_results:\n            validation_output = self.validation_results.get('output', '')\n        \n        if not validation_output:\n            # Try to load from standardized storage first, then fall back to central aggregation\n            try:\n                from .output_storage import load_tool_result\n                validation_result = load_tool_result('analyze_ai_work', 'ai_work', project_root=self.project_root)\n                if validation_result and 'data' in validation_result:\n                    data = validation_result['data']\n                    validation_output = data.get('output', '') or ''\n                else:\n                    # Fall back to central aggregation file\n                    import json\n                    results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                    if results_file.exists():\n                        with open(results_file, 'r', encoding='utf-8') as f:\n                            cached_data = json.load(f)\n                        if 'results' in cached_data and 'analyze_ai_work' in cached_data['results']:\n                            validation_result = cached_data['results']['analyze_ai_work']\n                            if 'data' in validation_result:\n                                data = validation_result['data']\n                                validation_output = data.get('output', '') or ''\n            except Exception as e:\n                logger.debug(f\"Failed to load validation from cache: {e}\")\n        \n        if validation_output:\n            # Parse text output for status\n            if 'POOR' in validation_output:\n                lines.append(\"- **AI Work Validation**: POOR - documentation or tests missing\")\n            elif 'GOOD' in validation_output:\n                lines.append(\"- **AI Work Validation**: GOOD - keep current standards\")\n            elif 'NEEDS ATTENTION' in validation_output or 'FAIR' in validation_output:\n                lines.append(\"- **AI Work Validation**: NEEDS ATTENTION - see consolidated report for details\")\n            else:\n                lines.append(\"- **AI Work Validation**: Status available (see consolidated report)\")\n        else:\n            lines.append(\"- Validation results unavailable (run `audit` for latest validation)\")\n\n        lines.append(\"\")\n\n        lines.append(\"## System Signals\")\n\n        if hasattr(self, 'system_signals') and self.system_signals:\n\n            system_health = self.system_signals.get('system_health', {})\n\n            overall_status = system_health.get('overall_status')\n\n            if overall_status:\n\n                lines.append(f\"- **System Health**: {overall_status}\")\n\n            missing_core = [\n\n                name for name, state in (system_health.get('core_files') or {}).items()\n\n                if state != 'OK'\n\n            ]\n\n            if missing_core:\n\n                lines.append(f\"- **Core File Issues**: {self._format_list_for_display(missing_core, limit=3)}\")\n\n            recent_activity = self.system_signals.get('recent_activity', {})\n\n            last_audit = recent_activity.get('last_audit')\n\n            if last_audit:\n\n                lines.append(f\"- **Last Audit**: {last_audit}\")\n\n            recent_changes = recent_activity.get('recent_changes') or []\n\n            if recent_changes:\n\n                lines.append(f\"- **Recent Changes**: {self._format_list_for_display(recent_changes, limit=3)}\")\n\n            # Add critical alerts if any\n            critical_alerts = self.system_signals.get('critical_alerts', [])\n            if critical_alerts:\n                lines.append(f\"- **Critical Alerts**: {len(critical_alerts)} active alert(s)\")\n                # Show first few alerts\n                for alert in critical_alerts[:3]:\n                    alert_text = alert if isinstance(alert, str) else alert.get('message', str(alert))\n                    lines.append(f\"  - {alert_text}\")\n\n        else:\n            # Try to load cached system signals if not available in memory\n            signals_loaded = False\n            system_signals = None\n            try:\n                import json\n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    if 'results' in cached_data and 'system_signals' in cached_data['results']:\n                        signals_data = cached_data['results']['system_signals']\n                        # Handle both 'data' wrapper and direct signals\n                        if 'data' in signals_data:\n                            system_signals = signals_data['data']\n                        else:\n                            system_signals = signals_data\n                        \n                        if system_signals:\n                            signals_loaded = True\n                            system_health = system_signals.get('system_health', {})\n                            overall_status = system_health.get('overall_status')\n                            if overall_status:\n                                lines.append(f\"- **System Health**: {overall_status}\")\n                            \n                            missing_core = [\n                                name for name, state in (system_health.get('core_files') or {}).items()\n                                if state != 'OK'\n                            ]\n                            if missing_core:\n                                lines.append(f\"- **Core File Issues**: {self._format_list_for_display(missing_core, limit=3)}\")\n                            \n                            recent_activity = system_signals.get('recent_activity', {})\n                            last_audit = recent_activity.get('last_audit')\n                            if last_audit:\n                                lines.append(f\"- **Last Audit**: {last_audit}\")\n                            \n                            recent_changes = recent_activity.get('recent_changes') or []\n                            if recent_changes:\n                                lines.append(f\"- **Recent Changes**: {self._format_list_for_display(recent_changes, limit=3)}\")\n                            \n                            critical_alerts = system_signals.get('critical_alerts', [])\n                            if critical_alerts:\n                                lines.append(f\"- **Critical Alerts**: {len(critical_alerts)} active alerts\")\n                            \n                            signals_loaded = True\n            except Exception as e:\n                logger.debug(f\"Failed to load system signals from cache: {e}\")\n            \n            if not signals_loaded:\n                lines.append(\"- System signals data unavailable (run `system-signals` command)\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Quick Commands\")\n\n        lines.append(\"- `python development_tools/run_development_tools.py status` - Refresh this snapshot\")\n\n        lines.append(\"- `python development_tools/run_development_tools.py audit --full` - Regenerate all metrics\")\n\n        lines.append(\"- `python development_tools/run_development_tools.py doc-sync` - Update documentation pairing data\")\n\n        lines.append(\"\")\n\n        return \"\\n\".join(lines)\n\n    def _generate_ai_priorities_document(self) -> str:\n        \"\"\"Generate AI-optimized priorities document with immediate next steps.\"\"\"\n        lines: List[str] = []\n        lines.append(\"# AI Priorities - Immediate Next Steps\")\n        lines.append(\"\")\n        lines.append(\"> **File**: `development_tools/AI_PRIORITIES.md`\")\n        lines.append(\"> **Generated**: This file is auto-generated. Do not edit manually.\")\n        lines.append(f\"> **Last Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        # Determine source command based on audit tier\n        if self.current_audit_tier == 1:\n            source_cmd = \"python development_tools/run_development_tools.py audit --quick\"\n            tier_name = \"Tier 1 (Quick Audit)\"\n        elif self.current_audit_tier == 3:\n            source_cmd = \"python development_tools/run_development_tools.py audit --full\"\n            tier_name = \"Tier 3 (Full Audit)\"\n        elif self.current_audit_tier == 2:\n            source_cmd = \"python development_tools/run_development_tools.py audit\"\n            tier_name = \"Tier 2 (Standard Audit)\"\n        else:\n            source_cmd = \"python development_tools/run_development_tools.py status\"\n            tier_name = \"Status Check (cached data)\"\n        lines.append(f\"> **Source**: `{source_cmd}`\")\n        if self.current_audit_tier:\n            lines.append(f\"> **Last Audit Tier**: {tier_name}\")\n        lines.append(\"> **Generated by**: run_development_tools.py - AI Development Tools Runner\")\n        lines.append(\"\")\n\n        def percent_text(value: Any, decimals: int = 1) -> str:\n            if value is None:\n                return \"Unknown\"\n            if isinstance(value, str):\n                trimmed = value.strip()\n                return trimmed if trimmed.endswith('%') else f\"{trimmed}%\"\n            return self._format_percentage(value, decimals)\n\n        def to_int(value: Any) -> Optional[int]:\n            if isinstance(value, int):\n                return value\n            if isinstance(value, float):\n                return int(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return int(float(stripped))\n                except ValueError:\n                    return None\n            if isinstance(value, dict):\n                count = value.get('count')\n                return to_int(count)\n            return None\n\n        def to_float(value: Any) -> Optional[float]:\n            if isinstance(value, (int, float)):\n                return float(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return float(stripped)\n                except ValueError:\n                    return None\n            return None\n\n        metrics = self._get_canonical_metrics()\n        doc_metrics = self.results_cache.get('analyze_function_registry', {}) or {}\n        error_metrics = self.results_cache.get('analyze_error_handling', {}) or {}\n        function_metrics = self.results_cache.get('analyze_functions', {}) or {}\n        doc_sync_summary = self.docs_sync_summary or {}\n        legacy_summary = self.legacy_cleanup_summary or {}\n        coverage_summary = self._load_coverage_summary()\n        \n        # Load dev tools coverage if not already loaded\n        if not hasattr(self, 'dev_tools_coverage_results') or not self.dev_tools_coverage_results:\n            self._load_dev_tools_coverage()\n        \n        analyze_docs_result = self.results_cache.get('analyze_documentation', {}) or {}\n        # analyze_documentation stores the payload directly, not wrapped in 'data'\n        if isinstance(analyze_docs_result, dict):\n            analyze_data = analyze_docs_result\n        else:\n            analyze_data = {}\n        \n        # Extract overlap analysis data\n        section_overlaps = analyze_data.get('section_overlaps', {})\n        consolidation_recs = analyze_data.get('consolidation_recommendations', [])\n\n        # Get doc coverage from canonical metrics first (most accurate)\n        doc_coverage_value = metrics.get('doc_coverage')\n        if doc_coverage_value is None or doc_coverage_value == 'Unknown':\n            # Fallback to doc_metrics\n            doc_coverage_value = doc_metrics.get('doc_coverage')\n\n        missing_docs_count = to_int(doc_metrics.get('missing_docs') or doc_metrics.get('missing_items'))\n        missing_doc_files = doc_metrics.get('missing_files') or self._get_missing_doc_files(limit=5)\n        \n        # Calculate missing documentation count from total functions and documented functions\n        total_funcs = metrics.get('total_functions')\n        documented_funcs = doc_metrics.get('totals', {}).get('functions_documented') if isinstance(doc_metrics.get('totals'), dict) else None\n        if total_funcs and documented_funcs is not None:\n            missing_docs_calculated = total_funcs - documented_funcs\n            if missing_docs_count is None or missing_docs_count == 0:\n                missing_docs_count = missing_docs_calculated\n\n        # NOTE: 'error_handling_coverage' is a backward compatibility fallback for old JSON format\n        error_coverage = error_metrics.get('analyze_error_handling') or error_metrics.get('error_handling_coverage')\n        \n        # Recalculate error handling coverage using canonical function count for consistency\n        error_total = error_metrics.get('total_functions')\n        error_with_handling = error_metrics.get('functions_with_error_handling')\n        canonical_total = metrics.get('total_functions')\n        \n        # Use the actual count from error analysis, not a recalculation\n        # The error analysis tool knows which functions actually need error handling\n        # Recalculating based on different totals can give incorrect results\n        missing_error_handlers = to_int(error_metrics.get('functions_missing_error_handling'))\n        \n        # Only recalculate coverage if totals differ, but keep the actual missing count\n        if error_coverage is not None and canonical_total and error_total and error_with_handling:\n            if error_total != canonical_total:\n                recalc_coverage = (error_with_handling / canonical_total) * 100\n                if 0 <= recalc_coverage <= 100:\n                    error_coverage = recalc_coverage\n        \n        worst_error_modules = error_metrics.get('worst_modules') or []\n\n        path_drift_count = to_int(doc_sync_summary.get('path_drift_issues')) if doc_sync_summary else None\n        path_drift_files = doc_sync_summary.get('path_drift_files') if doc_sync_summary else []\n        # Filter out section headers that were incorrectly parsed as file names\n        path_drift_files = [f for f in path_drift_files if not (f.isupper() and ('ISSUES' in f or 'COMPLIANCE' in f or 'DOCUMENTATION' in f or 'NUMBERING' in f))]\n        paired_doc_issues = to_int(doc_sync_summary.get('paired_doc_issues')) if doc_sync_summary else None\n        ascii_issues = to_int(doc_sync_summary.get('ascii_issues')) if doc_sync_summary else None\n\n        legacy_files = to_int(legacy_summary.get('files_with_issues')) if legacy_summary else None\n        legacy_markers = to_int(legacy_summary.get('legacy_markers')) if legacy_summary else None\n        legacy_report = legacy_summary.get('report_path') if legacy_summary else None\n\n        low_coverage_modules: List[Dict[str, Any]] = []\n        coverage_overall = None\n        worst_coverage_files: List[Dict[str, Any]] = []\n        if coverage_summary:\n            coverage_overall = (coverage_summary or {}).get('overall')\n            module_entries = (coverage_summary or {}).get('modules') or []\n            for module in module_entries:\n                coverage_value = to_float(module.get('coverage'))\n                # Convert coverage to float for comparison (handles both string and numeric values)\n                coverage_float = to_float(coverage_value) if coverage_value is not None else None\n                if coverage_float is not None and coverage_float < 80:\n                    low_coverage_modules.append(module)\n            low_coverage_modules = low_coverage_modules[:3]\n            worst_coverage_files = (coverage_summary or {}).get('worst_files') or []\n        \n        # Get dev tools coverage if available\n        dev_tools_coverage_overall = None\n        if hasattr(self, 'dev_tools_coverage_results') and self.dev_tools_coverage_results:\n            dev_tools_coverage_overall = self.dev_tools_coverage_results.get('overall', {})\n        dev_tools_insights = self._get_dev_tools_coverage_insights()\n\n        analyze_artifacts = analyze_data.get('artifacts') or []\n        analyze_duplicates = analyze_data.get('duplicates') or []\n        analyze_placeholders = analyze_data.get('placeholders') or []\n\n        # Get examples from function_metrics, ensuring we have the latest data\n        critical_examples = function_metrics.get('critical_complexity_examples') or []\n        high_examples = function_metrics.get('high_complexity_examples') or []\n        \n        # Also check decision_support_metrics for examples (extracted from text output)\n        decision_metrics = self.results_cache.get('decision_support_metrics', {})\n        if decision_metrics:\n            if not critical_examples and 'critical_complexity_examples' in decision_metrics:\n                critical_examples = decision_metrics.get('critical_complexity_examples', [])\n                function_metrics['critical_complexity_examples'] = critical_examples\n            if not high_examples and 'high_complexity_examples' in decision_metrics:\n                high_examples = decision_metrics.get('high_complexity_examples', [])\n                function_metrics['high_complexity_examples'] = high_examples\n        \n        # If examples are still missing, try to reload from standardized storage\n        if not critical_examples and not high_examples:\n            try:\n                from .output_storage import load_tool_result\n                func_result = load_tool_result('analyze_functions', 'functions', project_root=self.project_root)\n                if func_result and isinstance(func_result, dict):\n                    # load_tool_result unwraps 'data', so func_result IS the data\n                    if 'critical_complexity_examples' in func_result:\n                        critical_examples = func_result.get('critical_complexity_examples', [])\n                        function_metrics['critical_complexity_examples'] = critical_examples\n                    if 'high_complexity_examples' in func_result:\n                        high_examples = func_result.get('high_complexity_examples', [])\n                        function_metrics['high_complexity_examples'] = high_examples\n            except Exception:\n                pass  # If loading fails, continue with what we have\n        \n        # Get complexity metrics from canonical metrics or decision_support\n        moderate_complex = to_int(metrics.get('moderate'))\n        high_complex = to_int(metrics.get('high'))\n        critical_complex = to_int(metrics.get('critical'))\n        \n        # If not in metrics, try loading from decision_support_metrics\n        if moderate_complex is None or high_complex is None or critical_complex is None:\n            decision_metrics = self.results_cache.get('decision_support_metrics', {})\n            if decision_metrics:\n                if moderate_complex is None:\n                    moderate_complex = to_int(decision_metrics.get('moderate_complexity'))\n                if high_complex is None:\n                    high_complex = to_int(decision_metrics.get('high_complexity'))\n                if critical_complex is None:\n                    critical_complex = to_int(decision_metrics.get('critical_complexity'))\n\n        priority_items: List[Dict[str, Any]] = []\n\n        def add_priority(order: int, title: str, reason: str, bullets: List[str]) -> None:\n            if not reason:\n                return\n            priority_items.append({\n                'order': order,\n                'title': title,\n                'reason': reason,\n                'bullets': [bullet for bullet in bullets if bullet]\n            })\n\n        if path_drift_count and path_drift_count > 0:\n            drift_details: List[str] = []\n            if path_drift_files:\n                drift_details.append(\n                    f\"Top offenders: {self._format_list_for_display(path_drift_files, limit=3)}\"\n                )\n            if paired_doc_issues:\n                drift_details.append(\n                    f\"{paired_doc_issues} paired documentation sets affected alongside drift.\"\n                )\n            drift_details.append(\n                \"Run `python development_tools/run_development_tools.py doc-sync --fix` after adjustments.\"\n            )\n            add_priority(\n                order=1,\n                title=\"Stabilize documentation drift\",\n                reason=f\"{path_drift_count} documentation paths are out of sync.\",\n                bullets=drift_details\n            )\n\n        if missing_docs_count and missing_docs_count > 0:\n            doc_bullets: List[str] = []\n            # Show first few functions or modules that need documentation\n            if missing_doc_files:\n                doc_bullets.append(\n                    f\"Start with: {self._format_list_for_display(list(missing_doc_files)[:3], limit=3)}\"\n                )\n            # Try to get examples of undocumented functions\n            undocumented_examples = function_metrics.get('undocumented_examples') or []\n            if undocumented_examples and isinstance(undocumented_examples, list):\n                example_names = [ex.get('name', ex) if isinstance(ex, dict) else str(ex) for ex in undocumented_examples[:3]]\n                if example_names:\n                    doc_bullets.append(\n                        f\"Example functions needing docstrings: {self._format_list_for_display(example_names, limit=3)}\"\n                    )\n            doc_bullets.append(\n                \"Add docstrings to functions missing them. Regenerate registry entries via `python development_tools/run_development_tools.py docs`.\"\n            )\n            # Calculate total and documented for better context\n            total_funcs = metrics.get('total_functions')\n            documented_funcs = doc_metrics.get('totals', {}).get('functions_documented') if isinstance(doc_metrics.get('totals'), dict) else None\n            reason_text = f\"{missing_docs_count} functions are missing documentation\"\n            if total_funcs and documented_funcs is not None:\n                reason_text += f\" ({total_funcs} total, {documented_funcs} documented)\"\n            reason_text += \".\"\n            add_priority(\n                order=2,\n                title=\"Add documentation to missing functions\",\n                reason=reason_text,\n                bullets=doc_bullets\n            )\n\n        # Phase 1: Decorator replacement candidates\n        phase1_total = to_int(error_metrics.get('phase1_total', 0))\n        phase1_by_priority = error_metrics.get('phase1_by_priority', {}) or {}\n        phase1_high = to_int(phase1_by_priority.get('high', 0))\n        \n        if phase1_total and phase1_total > 0:\n            phase1_bullets: List[str] = []\n            if phase1_high and phase1_high > 0:\n                phase1_bullets.append(\n                    f\"Start with {phase1_high} high-priority candidates (entry points and critical operations).\"\n                )\n            phase1_medium = to_int(phase1_by_priority.get('medium', 0))\n            if phase1_medium and phase1_medium > 0:\n                phase1_bullets.append(\n                    f\"Then process {phase1_medium} medium-priority functions.\"\n                )\n            phase1_bullets.append(\n                \"Apply `@handle_errors` decorator to replace basic try-except blocks.\"\n            )\n            add_priority(\n                order=3,\n                title=\"Phase 1: Replace basic try-except with decorators\",\n                reason=f\"{phase1_total} functions have try-except blocks that should use `@handle_errors` decorator.\",\n                bullets=phase1_bullets\n            )\n        \n        # Phase 2: Generic exception categorization\n        phase2_total = to_int(error_metrics.get('phase2_total', 0))\n        phase2_by_type = error_metrics.get('phase2_by_type', {}) or {}\n        \n        if phase2_total and phase2_total > 0:\n            phase2_bullets: List[str] = []\n            top_exceptions = sorted(phase2_by_type.items(), key=lambda x: x[1], reverse=True)[:3]\n            if top_exceptions:\n                exc_details = [f\"{count} {exc_type}\" for exc_type, count in top_exceptions]\n                phase2_bullets.append(\n                    f\"Most common: {self._format_list_for_display(exc_details, limit=3)}\"\n                )\n            phase2_bullets.append(\n                \"Replace generic exceptions (ValueError, Exception, KeyError, TypeError) with specific project error classes.\"\n            )\n            phase2_bullets.append(\n                \"See `ai_development_docs/AI_ERROR_HANDLING_GUIDE.md` for categorization rules.\"\n            )\n            add_priority(\n                order=4,\n                title=\"Phase 2: Categorize generic exceptions\",\n                reason=f\"{phase2_total} generic exception raises need categorization into project-specific error classes.\",\n                bullets=phase2_bullets\n            )\n        \n        # General error handling priority (if no Phase 1/2 but still missing handlers)\n        if missing_error_handlers and missing_error_handlers > 0 and not (phase1_total or phase2_total):\n            module_samples: List[str] = []\n            for module in worst_error_modules[:3]:\n                module_name = module.get('module', 'Unknown')\n                coverage_value = percent_text(module.get('coverage'), 1)\n                missing = module.get('missing')\n                total = module.get('total')\n                detail = f\"{module_name} ({coverage_value}\"\n                if missing is not None and total is not None:\n                    detail += f\", missing {missing}/{total}\"\n                detail += \")\"\n                module_samples.append(detail)\n            error_bullets = []\n            if module_samples:\n                error_bullets.append(\n                    f\"Focus modules: {self._format_list_for_display(module_samples, limit=3)}\"\n                )\n            error_bullets.append(\n                \"Convert remaining basic handlers to `@handle_errors` and add guard tests.\"\n            )\n            add_priority(\n                order=3,\n                title=\"Harden critical error handling\",\n                reason=f\"{missing_error_handlers} functions still lack structured error recovery.\",\n                bullets=error_bullets\n            )\n\n        # Adjust order numbers based on whether Phase 1/2 priorities exist\n        coverage_order = 4\n        dev_coverage_order = coverage_order + 1\n        dependency_order = dev_coverage_order + 1\n        legacy_order = dependency_order + 1\n        if phase1_total or phase2_total:\n            coverage_order = 5\n            dev_coverage_order = coverage_order + 1\n            dependency_order = dev_coverage_order + 1\n            legacy_order = dependency_order + 1\n        \n        if low_coverage_modules:\n            coverage_highlights = [\n                f\"{module.get('module', 'module')} ({percent_text(module.get('coverage'), 1)}, {module.get('missed')} lines missing)\"\n                for module in low_coverage_modules\n            ]\n            coverage_bullets = [\n                f\"Target domains: {self._format_list_for_display(coverage_highlights, limit=3)}\",\n                \"Add scenario tests before the next full audit to lift domain coverage above 80%.\"\n            ]\n            add_priority(\n                order=coverage_order,\n                title=\"Raise coverage for low-performing domains\",\n                reason=f\"{len(low_coverage_modules)} key domains remain below the 80% target.\",\n                bullets=coverage_bullets\n            )\n\n        if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n            dev_pct = dev_tools_insights['overall_pct']\n            low_dev_modules = dev_tools_insights.get('low_modules') or []\n            if dev_pct < 60 or low_dev_modules:\n                dev_bullets: List[str] = []\n                if low_dev_modules:\n                    highlights = [\n                        f\"{Path(item['path']).name} ({percent_text(item.get('coverage'), 1)})\"\n                        for item in low_dev_modules\n                    ]\n                    dev_bullets.append(f\"Focus on: {self._format_list_for_display(highlights, limit=3)}\")\n                if dev_tools_insights.get('html'):\n                    dev_bullets.append(f\"Review HTML report at {dev_tools_insights['html']}\")\n                dev_bullets.append(\"Strengthen tests in `tests/development_tools/` for fragile helpers.\")\n                add_priority(\n                    order=dev_coverage_order,\n                    title=\"Raise development tools coverage\",\n                    reason=f\"Development tools coverage is {percent_text(dev_pct, 1)} (target 60%+).\",\n                    bullets=dev_bullets\n                )\n\n        dependency_summary = self.module_dependency_summary or self.results_cache.get('analyze_module_dependencies')\n\n        if dependency_summary and dependency_summary.get('missing_dependencies'):\n\n            missing = dependency_summary['missing_dependencies']\n\n            dep_bullets = []\n\n            files = dependency_summary.get('missing_files') or dependency_summary.get('missing_sections') or []\n\n            if files:\n\n                dep_bullets.append(f\"Affected files: {self._format_list_for_display(files, limit=3)}\")\n\n            dep_bullets.append(\"Regenerate dependencies via `python development_tools/run_development_tools.py docs` and rerun the audit.\")\n\n            add_priority(\n\n                order=dependency_order,\n\n                title=\"Refresh dependency documentation\",\n\n                reason=f\"{missing} module dependencies are undocumented.\",\n\n                bullets=dep_bullets\n\n            )\n\n        if legacy_files and legacy_files > 0:\n            legacy_bullets: List[str] = []\n            if legacy_markers:\n                legacy_bullets.append(f\"{legacy_markers} legacy markers still surface during scans.\")\n            if legacy_report:\n                legacy_bullets.append(f\"Review {legacy_report} for exact locations.\")\n            legacy_bullets.append(\n                \"Use `python development_tools/run_development_tools.py legacy --apply` to replace deprecated helpers.\"\n            )\n            add_priority(\n                order=legacy_order,\n                title=\"Retire remaining legacy references\",\n                reason=f\"{legacy_files} files still depend on legacy compatibility markers.\",\n                bullets=legacy_bullets\n            )\n        \n        # Add complexity refactoring priority if there are critical or high complexity functions\n        complexity_order = legacy_order + 1\n        if critical_complex and critical_complex > 0:\n            complexity_bullets: List[str] = []\n            if critical_examples:\n                # Get top 3 functions with highest complexity\n                # Sort by complexity if available, otherwise use first 3\n                sorted_examples = sorted(\n                    critical_examples[:10],  # Check first 10 for sorting\n                    key=lambda x: x.get('complexity', 0) if isinstance(x, dict) else 0,\n                    reverse=True\n                )[:3]\n                example_names = []\n                for ex in sorted_examples:\n                    if isinstance(ex, dict):\n                        func_name = ex.get('name', ex.get('function', 'unknown'))\n                        file_name = ex.get('file', '')\n                        if file_name:\n                            example_names.append(f\"{func_name} ({file_name})\")\n                        else:\n                            example_names.append(func_name)\n                    else:\n                        example_names.append(str(ex))\n                if example_names:\n                    complexity_bullets.append(\n                        f\"Highest complexity: {self._format_list_for_display(example_names, limit=3)}\"\n                    )\n            if high_complex and high_complex > 0 and critical_complex <= 10:  # Only show if critical count is low\n                complexity_bullets.append(\n                    f\"Then address {high_complex} high-complexity functions (100-199 nodes).\"\n                )\n            add_priority(\n                order=complexity_order,\n                title=\"Refactor high-complexity functions\",\n                reason=f\"{critical_complex} critical-complexity functions (>199 nodes) need immediate attention.\",\n                bullets=complexity_bullets\n            )\n        elif high_complex and high_complex > 0:\n            complexity_bullets: List[str] = []\n            if high_examples:\n                # Get top 3 functions with highest complexity\n                sorted_examples = sorted(\n                    high_examples[:10],\n                    key=lambda x: x.get('complexity', 0) if isinstance(x, dict) else 0,\n                    reverse=True\n                )[:3]\n                example_names = []\n                for ex in sorted_examples:\n                    if isinstance(ex, dict):\n                        func_name = ex.get('name', ex.get('function', 'unknown'))\n                        file_name = ex.get('file', '')\n                        if file_name:\n                            example_names.append(f\"{func_name} ({file_name})\")\n                        else:\n                            example_names.append(func_name)\n                    else:\n                        example_names.append(str(ex))\n                if example_names:\n                    complexity_bullets.append(\n                        f\"Highest complexity: {self._format_list_for_display(example_names, limit=3)}\"\n                    )\n            add_priority(\n                order=complexity_order,\n                title=\"Refactor high-complexity functions\",\n                reason=f\"{high_complex} high-complexity functions (100-199 nodes) should be simplified.\",\n                bullets=complexity_bullets\n            )\n\n        # Add TODO sync as priority if there are completed entries\n        todo_sync_result = getattr(self, 'todo_sync_result', None)\n        if todo_sync_result and isinstance(todo_sync_result, dict):\n            completed_entries = todo_sync_result.get('completed_entries', 0)\n            if completed_entries > 0:\n                todo_order = max([item['order'] for item in priority_items] + [0]) + 1\n                add_priority(\n                    order=todo_order,\n                    title=\"Review completed TODO entries\",\n                    reason=f\"{completed_entries} completed entry/entries in TODO.md need review for changelog.\",\n                    bullets=[\n                        \"Review completed entries in TODO.md\",\n                        \"If already documented in changelogs, remove from TODO.md\",\n                        \"If not documented, move to CHANGELOG_DETAIL.md and AI_CHANGELOG.md first, then remove from TODO.md\"\n                    ]\n                )\n        \n        lines.append(\"## Immediate Focus (Ranked)\")\n        if priority_items:\n            for idx, item in enumerate(sorted(priority_items, key=lambda entry: entry['order']), start=1):\n                lines.append(f\"{idx}. **{item['title']}**  -  {item['reason']}\")\n                for bullet in item['bullets']:\n                    lines.append(f\"   - {bullet}\")\n        else:\n            lines.append(\"All signals are green. Re-run `python development_tools/run_development_tools.py status` to monitor.\")\n        lines.append(\"\")\n\n        quick_wins: List[str] = []\n        if ascii_issues:\n            quick_wins.append(f\"Normalize {ascii_issues} file(s) with non-ASCII characters via doc-fix.\")\n        if paired_doc_issues and not (path_drift_count and path_drift_count > 0):\n            # Add actionable details about paired doc issues\n            if doc_sync_summary:\n                paired_docs_data = doc_sync_summary.get('paired_docs', {})\n                if isinstance(paired_docs_data, dict):\n                    content_sync_issues = paired_docs_data.get('content_sync', [])\n                    if content_sync_issues:\n                        # Show total count and first actionable issue\n                        quick_wins.append(f\"Resolve {paired_doc_issues} paired doc sync issue(s). Start with: {content_sync_issues[0]}\")\n                        if len(content_sync_issues) > 1:\n                            quick_wins.append(f\"  - Plus {len(content_sync_issues) - 1} more issue(s) to address\")\n                    else:\n                        quick_wins.append(f\"Resolve {paired_doc_issues} unpaired documentation sets flagged by doc-sync.\")\n                else:\n                    quick_wins.append(f\"Resolve {paired_doc_issues} unpaired documentation sets flagged by doc-sync.\")\n            else:\n                quick_wins.append(f\"Resolve {paired_doc_issues} unpaired documentation sets flagged by doc-sync.\")\n        if analyze_artifacts:\n            artifact = analyze_artifacts[0]\n            location = artifact.get('file', 'unknown')\n            line_number = artifact.get('line')\n            if line_number:\n                location = f\"{location}:{line_number}\"\n            pattern = artifact.get('pattern', 'lint issue')\n            quick_wins.append(f\"Clean `{pattern}` marker in {location}.\")\n        if analyze_duplicates:\n            quick_wins.append(f\"Merge {len(analyze_duplicates)} duplicate documentation block(s).\")\n        if analyze_placeholders:\n            quick_wins.append(f\"Replace {len(analyze_placeholders)} placeholder section(s) flagged by docs scan.\")\n        \n        # Add unused imports to quick wins if available\n        unused_imports_data = self.results_cache.get('analyze_unused_imports', {}) or {}\n        if not unused_imports_data or not isinstance(unused_imports_data, dict):\n            # Try to load from standardized storage\n            try:\n                from .output_storage import load_tool_result\n                unused_result = load_tool_result('analyze_unused_imports', 'imports', project_root=self.project_root)\n                if unused_result:\n                    # load_tool_result already unwraps the 'data' key, so unused_result IS the data\n                    unused_imports_data = unused_result if isinstance(unused_result, dict) else {}\n            except Exception as e:\n                logger.debug(f\"Failed to load unused imports for AI_PRIORITIES: {e}\")\n                unused_imports_data = {}\n        \n        if unused_imports_data and isinstance(unused_imports_data, dict):\n            total_unused = unused_imports_data.get('total_unused', 0)\n            files_with_issues = unused_imports_data.get('files_with_issues', 0)\n            if total_unused > 0 and files_with_issues > 0:\n                by_category = unused_imports_data.get('by_category') or {}\n                obvious = by_category.get('obvious_unused', 0)\n                if obvious > 0:\n                    quick_wins.append(f\"Remove {obvious} obvious unused import(s) across {files_with_issues} file(s).\")\n\n        lines.append(\"## Quick Wins\")\n        if quick_wins:\n            for win in quick_wins:\n                lines.append(f\"- {win}\")\n        else:\n            lines.append(\"- No immediate quick wins identified. Re-run doc-sync after tackling focus items.\")\n        \n        # Add overlap analysis information only if there are issues to prioritize\n        consolidation_count = len(consolidation_recs) if consolidation_recs else 0\n        overlap_count = len(section_overlaps) if section_overlaps else 0\n        \n        # Add consolidation opportunities as priority items\n        if consolidation_recs and consolidation_count > 0:\n            consolidation_order = 11  # After config but before TODO\n            consolidation_bullets: List[str] = []\n            for rec in consolidation_recs[:3]:  # Show top 3\n                category = rec.get('category', 'Unknown')\n                files = rec.get('files', [])\n                suggestion = rec.get('suggestion', '')\n                if files and len(files) > 1:\n                    consolidation_bullets.append(f\"{category}: {len(files)} files - {suggestion}\")\n                    consolidation_bullets.append(f\"  Files: {', '.join(files[:3])}{'...' if len(files) > 3 else ''}\")\n            add_priority(\n                order=consolidation_order,\n                title=\"Consolidate documentation files\",\n                reason=f\"{consolidation_count} file groups could be consolidated to reduce redundancy.\",\n                bullets=consolidation_bullets\n            )\n        elif overlap_count > 0:\n            # If only overlaps (no consolidation recs), add as lower priority\n            overlap_order = 12\n            overlap_bullets = [f\"{overlap_count} section overlaps detected - review for consolidation opportunities\"]\n            add_priority(\n                order=overlap_order,\n                title=\"Review documentation overlaps\",\n                reason=f\"{overlap_count} section overlaps detected across documentation files.\",\n                bullets=overlap_bullets\n            )\n        \n        lines.append(\"\")\n\n        watch_list: List[str] = []\n        # Only add doc coverage to watch list if below threshold (90%)\n        # Convert coverage to float for comparison (handles both string and numeric values)\n        doc_coverage_float = to_float(doc_coverage_value) if doc_coverage_value is not None else None\n        if doc_coverage_float is not None and doc_coverage_float < 90:\n            watch_list.append(f\"Documentation coverage sits at {percent_text(doc_coverage_value, 2)} (target 90%).\")\n        if coverage_overall:\n            coverage_pct = coverage_overall.get('coverage', 0)\n            target = 80  # Standard target for test coverage\n            watch_list.append(\n                f\"Overall test coverage is {percent_text(coverage_pct, 1)} (target {target}%) \"\n                f\"({coverage_overall.get('covered')} / {coverage_overall.get('statements')} statements).\"\n            )\n        if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n            dev_pct = dev_tools_insights['overall_pct']\n            detail = f\"Development tools coverage is {percent_text(dev_pct, 1)} (target 60%+).\"\n            low_modules = dev_tools_insights.get('low_modules') or []\n            if low_modules:\n                focus = [\n                    f\"{Path(item['path']).name} ({percent_text(item.get('coverage'), 1)})\"\n                    for item in low_modules\n                ]\n                detail += f\" Focus on {self._format_list_for_display(focus, limit=2)}.\"\n            watch_list.append(detail)\n        dependency_summary = self.module_dependency_summary or self.results_cache.get('analyze_module_dependencies')\n        # Removed dependency docs and TODO sync from watchlist per user feedback\n        # (these belong in priorities, not watchlist)\n        \n        # Add high complexity monitoring to watchlist if there are high complexity functions\n        if high_examples:\n            high_focus = [\n                f\"{entry['function']} ({entry['file']})\"\n                for entry in high_examples[:3]  # Show top 3\n            ]\n            if high_focus:\n                watch_list.append(f\"Monitor high complexity functions: {self._format_list_for_display(high_focus, limit=3)}.\")\n        \n        if legacy_markers and (not legacy_files or legacy_files == 0):\n            watch_list.append(f\"{legacy_markers} legacy markers remain; schedule periodic cleanup post-sprint.\")\n        \n        # Add config validation recommendations to priorities if significant\n        config_validation_summary = self._load_config_validation_summary()\n        if config_validation_summary:\n            total_recommendations = config_validation_summary.get('total_recommendations', 0)\n            recommendations = config_validation_summary.get('recommendations', [])\n            if total_recommendations > 0 and recommendations:\n                # Add as a priority item (lower priority than error handling but higher than TODO)\n                config_order = 10  # Lower priority than error handling (3-4) but before TODO\n                config_bullets: List[str] = []\n                # Show top 2-3 recommendations\n                for rec in recommendations[:3]:\n                    rec_text = rec if isinstance(rec, str) else rec.get('message', str(rec))\n                    config_bullets.append(rec_text)\n                if len(recommendations) > 3:\n                    config_bullets.append(f\"...and {len(recommendations) - 3} more recommendation(s)\")\n                add_priority(\n                    order=config_order,\n                    title=\"Update tools to use centralized config\",\n                    reason=f\"{total_recommendations} config validation recommendation(s) pending review.\",\n                    bullets=config_bullets\n                )\n                # Don't add to watch list since it's already in priorities\n        \n        # Add AI work validation to watch list (lightweight structural validation only)\n        validation_output = ''\n        if hasattr(self, 'validation_results') and self.validation_results:\n            validation_output = self.validation_results.get('output', '')\n        \n        if not validation_output:\n            try:\n                import json\n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    if 'results' in cached_data and 'analyze_ai_work' in cached_data['results']:\n                        validation_result = cached_data['results']['analyze_ai_work']\n                        if 'data' in validation_result:\n                            data = validation_result['data']\n                            validation_output = data.get('output', '') or ''\n            except Exception:\n                pass\n        \n        if validation_output and ('POOR' in validation_output or 'NEEDS ATTENTION' in validation_output or 'FAIR' in validation_output):\n            watch_list.append(\"AI Work Validation: Structural validation issues detected (see consolidated report)\")\n\n        lines.append(\"## Watch List\")\n        if watch_list:\n            for item in watch_list:\n                lines.append(f\"- {item}\")\n        else:\n            lines.append(\"- No outstanding watch items. Continue regular audits to maintain signal quality.\")\n        lines.append(\"\")\n\n        lines.append(\"## Follow-up Commands\")\n        lines.append(\"- `python development_tools/run_development_tools.py doc-sync`  -  refresh drift, pairing, and ASCII metrics.\")\n        lines.append(\"- `python development_tools/run_development_tools.py legacy --apply`  -  update legacy references in-place.\")\n        lines.append(\"- `python development_tools/run_development_tools.py audit --full`  -  rebuild coverage and hygiene data after fixes.\")\n        lines.append(\"- `python development_tools/run_development_tools.py status`  -  confirm the latest health snapshot.\")\n\n        return '\\n'.join(lines)\n    def _generate_consolidated_report(self) -> str:\n\n        \"\"\"Generate comprehensive consolidated report combining all tool outputs.\"\"\"\n\n        lines: List[str] = []\n\n        lines.append(\"# Comprehensive AI Development Tools Report\")\n        lines.append(\"\")\n        lines.append(\"> **Generated**: This file is auto-generated. Do not edit manually.\")\n        lines.append(f\"> **Last Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        # Determine source command based on audit tier\n        if self.current_audit_tier == 1:\n            source_cmd = \"python development_tools/run_development_tools.py audit --quick\"\n            tier_name = \"Tier 1 (Quick Audit)\"\n        elif self.current_audit_tier == 3:\n            source_cmd = \"python development_tools/run_development_tools.py audit --full\"\n            tier_name = \"Tier 3 (Full Audit)\"\n        elif self.current_audit_tier == 2:\n            source_cmd = \"python development_tools/run_development_tools.py audit\"\n            tier_name = \"Tier 2 (Standard Audit)\"\n        else:\n            source_cmd = \"python development_tools/run_development_tools.py status\"\n            tier_name = \"Status Check (cached data)\"\n        lines.append(f\"> **Source**: `{source_cmd}`\")\n        if self.current_audit_tier:\n            lines.append(f\"> **Last Audit Tier**: {tier_name}\")\n        lines.append(\"> **Generated by**: run_development_tools.py - AI Development Tools Runner\")\n        lines.append(\"\")\n\n        def percent_text(value: Any, decimals: int = 1) -> str:\n\n            if value is None:\n\n                return \"Unknown\"\n\n            if isinstance(value, str):\n\n                return value if value.strip().endswith('%') else f\"{value}%\"\n\n            return self._format_percentage(value, decimals)\n\n        def to_int(value: Any) -> Optional[int]:\n            if isinstance(value, int):\n                return value\n            if isinstance(value, float):\n                return int(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return int(float(stripped))\n                except ValueError:\n                    return None\n            if isinstance(value, dict):\n                count = value.get('count')\n                return to_int(count)\n            return None\n\n        def to_float(value: Any) -> Optional[float]:\n            if isinstance(value, (int, float)):\n                return float(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return float(stripped)\n                except ValueError:\n                    return None\n            return None\n\n        metrics = self._get_canonical_metrics()\n\n        doc_metrics = self.results_cache.get('analyze_function_registry', {}) or {}\n\n        doc_coverage = doc_metrics.get('doc_coverage', metrics.get('doc_coverage'))\n\n        missing_docs = doc_metrics.get('missing_docs') or doc_metrics.get('missing_items')\n\n        doc_totals = doc_metrics.get('totals') or {}\n\n        documented_functions = doc_totals.get('functions_documented')\n\n        doc_sync_summary = self.docs_sync_summary or {}\n\n        # Initialize unused_imports_data early so it can be used later\n        unused_imports_data = self.results_cache.get('analyze_unused_imports', {}) or {}\n        if not unused_imports_data or not isinstance(unused_imports_data, dict):\n            try:\n                from .output_storage import load_tool_result\n                unused_result = load_tool_result('analyze_unused_imports', 'imports', project_root=self.project_root)\n                if unused_result:\n                    unused_imports_data = unused_result if isinstance(unused_result, dict) else {}\n            except Exception:\n                unused_imports_data = {}\n\n        analyze_docs = self.results_cache.get('analyze_documentation', {}) or {}\n        # analyze_documentation stores the payload directly, not wrapped in 'data'\n        if isinstance(analyze_docs, dict):\n            analyze_docs_data = analyze_docs\n        else:\n            analyze_docs_data = {}\n\n        doc_artifacts = analyze_docs_data.get('artifacts') if isinstance(analyze_docs_data, dict) else None\n        \n        # Extract overlap analysis data\n        # Check if overlap analysis was run (indicated by presence of these keys, even if empty)\n        overlap_analysis_ran = (\n            'section_overlaps' in analyze_docs_data or \n            'consolidation_recommendations' in analyze_docs_data\n        )\n        \n        section_overlaps = analyze_docs_data.get('section_overlaps', {}) if overlap_analysis_ran else {}\n        consolidation_recs = analyze_docs_data.get('consolidation_recommendations', []) if overlap_analysis_ran else []\n        file_purposes = analyze_docs_data.get('file_purposes', {})\n        \n        # Normalize to empty dict/list if None\n        if section_overlaps is None:\n            section_overlaps = {}\n        if consolidation_recs is None:\n            consolidation_recs = []\n\n        error_metrics = self.results_cache.get('analyze_error_handling', {}) or {}\n\n        missing_error_handlers = error_metrics.get('functions_missing_error_handling')\n\n        error_recommendations = error_metrics.get('recommendations') or []\n\n        worst_error_modules = error_metrics.get('worst_modules') or []\n\n        coverage_summary = self._load_coverage_summary()\n        \n        # Load dev tools coverage if not already loaded\n        if not hasattr(self, 'dev_tools_coverage_results') or not self.dev_tools_coverage_results:\n            self._load_dev_tools_coverage()\n\n        # Try to load legacy_summary from instance variable, then results_cache, then standardized storage\n        legacy_summary = self.legacy_cleanup_summary or {}\n        if not legacy_summary and 'analyze_legacy_references' in self.results_cache:\n            cached_data = self.results_cache['analyze_legacy_references']\n            if isinstance(cached_data, dict):\n                legacy_summary = {\n                    'files_with_issues': cached_data.get('files_with_issues', 0),\n                    'legacy_markers': cached_data.get('legacy_markers', 0),\n                    'report_path': cached_data.get('report_path', 'development_docs/LEGACY_REFERENCE_REPORT.md')\n                }\n\n        # Try to load unused_imports_data from results_cache with correct key\n        unused_imports_data = self.results_cache.get('analyze_unused_imports', {}) or {}\n\n        # Load function_metrics from results_cache, ensuring examples are included\n        function_metrics = self.results_cache.get('analyze_functions', {}) or {}\n        # If examples are missing, try to load from standardized storage\n        if not function_metrics.get('critical_complexity_examples') and not function_metrics.get('high_complexity_examples'):\n            try:\n                from .output_storage import load_tool_result\n                func_result = load_tool_result('analyze_functions', 'functions', project_root=self.project_root)\n                if func_result and isinstance(func_result, dict):\n                    # load_tool_result already unwraps the 'data' key, so func_result IS the data\n                    if 'critical_complexity_examples' in func_result:\n                        function_metrics['critical_complexity_examples'] = func_result.get('critical_complexity_examples', [])\n                    if 'high_complexity_examples' in func_result:\n                        function_metrics['high_complexity_examples'] = func_result.get('high_complexity_examples', [])\n            except Exception:\n                pass  # If loading fails, continue with what we have\n\n        decision_metrics = self.results_cache.get('decision_support_metrics', {}) or {}\n\n        validation_output = \"\"\n\n        if hasattr(self, 'validation_results') and self.validation_results:\n\n            validation_output = self.validation_results.get('output', '') or \"\"\n        else:\n            # Try to load from cache\n            try:\n                import json\n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    if 'results' in cached_data and 'analyze_ai_work' in cached_data['results']:\n                        validation_data = cached_data['results']['analyze_ai_work']\n                        # Handle nested data structure: results.analyze_ai_work.data.output\n                        if 'data' in validation_data:\n                            data = validation_data['data']\n                            # Check if data is a dict with 'output' key, or if it's the result dict itself\n                            if isinstance(data, dict):\n                                validation_output = data.get('output', '') or ''\n                                # If output is empty, check if there's a nested structure\n                                if not validation_output and 'data' in data:\n                                    validation_output = data['data'].get('output', '') or ''\n                        else:\n                            validation_output = validation_data.get('output', '') or \"\"\n            except Exception:\n                pass\n\n        results_file = self.audit_config.get('results_file', 'development_tools/reports/analysis_detailed_results.json')\n\n        issues_file_str = self.audit_config.get('issues_file', 'development_tools/critical_issues.txt')\n        # Convert to Path object for .exists() check\n        issues_file = self.project_root / issues_file_str if isinstance(issues_file_str, str) else Path(issues_file_str)\n\n        # Recalculate doc_coverage if Unknown (same logic as Documentation Findings section)\n        # Do this BEFORE Executive Summary so it's available for display\n        if doc_coverage == 'Unknown' or doc_coverage is None:\n            results_cache = self.results_cache or {}\n            audit_data = results_cache.get('analyze_function_registry', {}) or {}\n            audit_totals = audit_data.get('totals') if isinstance(audit_data, dict) else {}\n            if audit_totals is None or not isinstance(audit_totals, dict):\n                audit_totals = {}\n            documented = audit_totals.get('functions_documented', 0)\n            total_funcs = metrics.get('total_functions')\n            if total_funcs and isinstance(total_funcs, (int, float)) and total_funcs > 0 and documented > 0:\n                coverage_pct = (documented / total_funcs) * 100\n                if 0 <= coverage_pct <= 100:\n                    doc_coverage = f\"{coverage_pct:.2f}%\"\n\n        lines.append(\"## Executive Summary\")\n\n        lines.append(f\"- Documentation coverage {percent_text(doc_coverage, 2)} with {missing_docs or 0} registry gaps\")\n\n        # NOTE: 'error_handling_coverage' is a backward compatibility fallback for old JSON format\n        error_cov = error_metrics.get('analyze_error_handling') or error_metrics.get('error_handling_coverage')\n        \n        # Recalculate error handling coverage using canonical function count for consistency\n        # Use the actual count from error analysis, not a recalculation\n        missing_error_handlers = to_int(error_metrics.get('functions_missing_error_handling', 0))\n        \n        # Only recalculate coverage if totals differ, but keep the actual missing count\n        error_total = error_metrics.get('total_functions')\n        error_with_handling = error_metrics.get('functions_with_error_handling')\n        canonical_total = metrics.get('total_functions')\n        \n        if error_cov is not None and canonical_total and error_total and error_with_handling:\n            if error_total != canonical_total:\n                # Recalculate coverage using canonical total\n                recalc_coverage = (error_with_handling / canonical_total) * 100\n                if 0 <= recalc_coverage <= 100:\n                    error_cov = recalc_coverage\n\n        if error_cov is not None:\n\n            lines.append(f\"- Error handling coverage {percent_text(error_cov, 1)}; {missing_error_handlers or 0} functions need protection\")\n\n        dev_tools_insights = self._get_dev_tools_coverage_insights()\n\n        if coverage_summary:\n\n            overall = coverage_summary.get('overall') or {}\n            overall_cov = overall.get('coverage')\n\n            lines.append(f\"- Overall test coverage {percent_text(overall_cov, 1)} across {overall.get('statements', 0)} statements\")\n            \n            if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n                dev_pct = dev_tools_insights['overall_pct']\n                summary_line = f\"- Development tools coverage {percent_text(dev_pct, 1)}\"\n                if dev_tools_insights.get('covered') is not None and dev_tools_insights.get('statements') is not None:\n                    summary_line += f\" ({dev_tools_insights['covered']} of {dev_tools_insights['statements']} statements)\"\n                lines.append(summary_line)\n\n        elif hasattr(self, 'coverage_results') and self.coverage_results:\n\n            lines.append(\"- Coverage regeneration flagged issues; inspect coverage.json for details\")\n\n        path_drift = doc_sync_summary.get('path_drift_issues') if doc_sync_summary else None\n\n        if path_drift is not None:\n\n            lines.append(f\"- Documentation path drift: {path_drift} files need sync\")\n\n        legacy_issues = legacy_summary.get('files_with_issues')\n\n        if legacy_issues is not None:\n\n            lines.append(f\"- Legacy references outstanding in {legacy_issues} files\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Audit Metrics\")\n\n        # Get complexity metrics, trying multiple sources\n        total_funcs = metrics.get('total_functions', 'Unknown')\n        moderate = metrics.get('moderate', 'Unknown')\n        high = metrics.get('high', 'Unknown')\n        critical = metrics.get('critical', 'Unknown')\n        \n        # If still Unknown, try loading from analyze_functions or decision_support cache\n        if moderate == 'Unknown' or high == 'Unknown':\n            try:\n                import json\n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    # Try analyze_functions first\n                    if 'results' in cached_data and 'analyze_functions' in cached_data['results']:\n                        func_data = cached_data['results']['analyze_functions']\n                        if 'data' in func_data:\n                            cached_metrics = func_data['data']\n                            if moderate == 'Unknown':\n                                moderate = cached_metrics.get('moderate_complexity', 'Unknown')\n                            if high == 'Unknown':\n                                high = cached_metrics.get('high_complexity', 'Unknown')\n                            if critical == 'Unknown':\n                                critical = cached_metrics.get('critical_complexity', 'Unknown')\n                            if total_funcs == 'Unknown':\n                                total_funcs = cached_metrics.get('total_functions', 'Unknown')\n                    # Fallback to decision_support\n                    if (moderate == 'Unknown' or high == 'Unknown') and 'results' in cached_data and 'decision_support' in cached_data['results']:\n                        ds_data = cached_data['results']['decision_support']\n                        if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                            ds_metrics = ds_data['data']['decision_support_metrics']\n                            if moderate == 'Unknown':\n                                moderate = ds_metrics.get('moderate_complexity', 'Unknown')\n                            if high == 'Unknown':\n                                high = ds_metrics.get('high_complexity', 'Unknown')\n                            if critical == 'Unknown':\n                                critical = ds_metrics.get('critical_complexity', 'Unknown')\n                            if total_funcs == 'Unknown':\n                                total_funcs = ds_metrics.get('total_functions', 'Unknown')\n            except Exception as e:\n                logger.debug(f\"Failed to load complexity from cache in consolidated report: {e}\")\n                pass\n\n        lines.append(f\"- **Total Functions**: {total_funcs}\")\n\n        lines.append(f\"- **Complexity Distribution**: Moderate {moderate}, High {high}, Critical {critical}\")\n\n        if documented_functions is not None:\n\n            lines.append(f\"- **Documented Functions**: {documented_functions}\")\n\n        if decision_metrics:\n\n            actions = decision_metrics.get('decision_support_items')\n\n            if actions:\n\n                lines.append(f\"- **Decision Support Signals Captured**: {actions}\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Documentation Findings\")\n\n        # Use canonical metrics for doc_coverage (same as AI_STATUS.md)\n        doc_coverage = metrics.get('doc_coverage', 'Unknown')\n        if doc_coverage == 'Unknown' or doc_coverage is None:\n            # Recalculate if we have the data\n            audit_data = results_cache.get('analyze_function_registry', {}) or {}\n            audit_totals = audit_data.get('totals') if isinstance(audit_data, dict) else {}\n            if audit_totals is None or not isinstance(audit_totals, dict):\n                audit_totals = {}\n            documented = audit_totals.get('functions_documented', 0)\n            total_funcs = metrics.get('total_functions')\n            if total_funcs and isinstance(total_funcs, (int, float)) and total_funcs > 0 and documented > 0:\n                coverage_pct = (documented / total_funcs) * 100\n                if 0 <= coverage_pct <= 100:\n                    doc_coverage = f\"{coverage_pct:.2f}%\"\n        \n        lines.append(f\"- **Coverage**: {percent_text(doc_coverage, 2)}\")\n\n        if missing_docs:\n\n            lines.append(f\"- **Missing Registry Entries**: {missing_docs}\")\n\n        missing_files = self._get_missing_doc_files(limit=8)\n\n        if missing_files:\n\n            lines.append(f\"- **Docs to Update**: {self._format_list_for_display(missing_files, limit=8)}\")\n        \n        # Add overlap analysis section (always show, even if no overlaps found)\n        lines.append(\"\")\n        lines.append(\"## Documentation Overlap Analysis\")\n        overlap_count = len(section_overlaps) if section_overlaps else 0\n        consolidation_count = len(consolidation_recs) if consolidation_recs else 0\n        \n        if overlap_count > 0 or consolidation_count > 0:\n            if section_overlaps:\n                lines.append(f\"- **Section Overlaps**: {overlap_count} sections appear in multiple files\")\n                # Show top 5 overlaps\n                top_overlaps = sorted(section_overlaps.items(), key=lambda x: len(x[1]), reverse=True)[:5]\n                for section_name, files in top_overlaps:\n                    if len(files) > 1:\n                        lines.append(f\"  - Section '{section_name}' appears in {len(files)} files: {', '.join(files[:3])}{'...' if len(files) > 3 else ''}\")\n            if consolidation_recs:\n                lines.append(f\"- **Consolidation Opportunities**: {consolidation_count} file groups identified for potential consolidation\")\n                for rec in consolidation_recs[:3]:  # Show top 3 recommendations\n                    category = rec.get('category', 'Unknown')\n                    files = rec.get('files', [])\n                    suggestion = rec.get('suggestion', '')\n                    if files:\n                        lines.append(f\"  - {category}: {len(files)} files ({', '.join(files[:2])}{'...' if len(files) > 2 else ''}) - {suggestion}\")\n        else:\n            if overlap_analysis_ran:\n                lines.append(\"- **Status**: No section overlaps or consolidation opportunities detected\")\n                lines.append(\"- Overlap analysis was performed during this audit\")\n            else:\n                lines.append(\"- **Status**: Overlap analysis not run (use `audit --full` or `--overlap` flag)\")\n                lines.append(\"  - Standard audits skip overlap analysis by default; run `audit --full` or use `--overlap` flag to include it\")\n\n        if doc_sync_summary:\n\n            path_drift = doc_sync_summary.get('path_drift_issues')\n\n            paired = doc_sync_summary.get('paired_doc_issues')\n\n            ascii_issues = doc_sync_summary.get('ascii_issues')\n\n            total_issues = doc_sync_summary.get('total_issues')\n\n            lines.append(f\"- **Doc Sync Status**: {doc_sync_summary.get('status', 'Unknown')} ({total_issues or 0} issues)\")\n\n            if path_drift:\n\n                lines.append(f\"  - Path drift in {path_drift} files\")\n\n            if paired:\n                lines.append(f\"  - {paired} paired documents out of sync\")\n                # Add details about paired doc issues\n                paired_docs_data = doc_sync_summary.get('paired_docs', {})\n                if isinstance(paired_docs_data, dict):\n                    content_sync_issues = paired_docs_data.get('content_sync', [])\n                    if content_sync_issues:\n                        for issue in content_sync_issues[:3]:\n                            lines.append(f\"    - {issue}\")\n                        if len(content_sync_issues) > 3:\n                            lines.append(f\"    - ...and {len(content_sync_issues) - 3} more issue(s)\")\n\n            if ascii_issues:\n                lines.append(f\"  - {ascii_issues} files contain non-ASCII characters\")\n\n            drift_files = doc_sync_summary.get('path_drift_files') or []\n            if path_drift and drift_files:\n                lines.append(f\"  - Path drift hotspots: {self._format_list_for_display(drift_files, limit=5)}\")\n\n        else:\n\n            lines.append(\"- Run doc-sync to capture current documentation drift data\")\n        \n        # Add config validation status\n        config_validation_summary = self._load_config_validation_summary()\n        if config_validation_summary:\n            lines.append(\"\")\n            lines.append(\"## Configuration Validation\")\n            config_valid = config_validation_summary.get('config_valid', False)\n            config_complete = config_validation_summary.get('config_complete', False)\n            total_recommendations = config_validation_summary.get('total_recommendations', 0)\n            tools_using_config = config_validation_summary.get('tools_using_config', 0)\n            total_tools = config_validation_summary.get('total_tools', 0)\n            recommendations = config_validation_summary.get('recommendations', [])\n            tools_analysis = config_validation_summary.get('tools_analysis', {})\n            \n            lines.append(f\"- **Config Valid**: {'Yes' if config_valid else 'No'}\")\n            lines.append(f\"- **Config Complete**: {'Yes' if config_complete else 'No'}\")\n            if total_tools > 0:\n                lines.append(f\"- **Tools Using Config**: {tools_using_config}/{total_tools}\")\n            if total_recommendations > 0:\n                lines.append(f\"- **Total Recommendations**: {total_recommendations}\")\n                # Show first few recommendations\n                if recommendations:\n                    lines.append(\"\")\n                    lines.append(\"**Top Recommendations:**\")\n                    for i, rec in enumerate(recommendations[:5], 1):\n                        rec_text = rec if isinstance(rec, str) else rec.get('message', str(rec))\n                        lines.append(f\"  {i}. {rec_text}\")\n                    if len(recommendations) > 5:\n                        lines.append(f\"  ... and {len(recommendations) - 5} more recommendations\")\n                # List tools with issues\n                tools_with_issues = [\n                    tool for tool, data in tools_analysis.items() \n                    if data.get('issues') and len(data.get('issues', [])) > 0\n                ]\n                if tools_with_issues:\n                    lines.append(f\"- **Tools Needing Updates**: {', '.join(tools_with_issues[:3])}{'...' if len(tools_with_issues) > 3 else ''}\")\n        \n        # Add TODO sync status\n        todo_sync_result = getattr(self, 'todo_sync_result', None)\n        if todo_sync_result and isinstance(todo_sync_result, dict):\n            lines.append(\"\")\n            lines.append(\"## TODO Sync Status\")\n            completed_entries = todo_sync_result.get('completed_entries', 0)\n            if completed_entries > 0:\n                lines.append(f\"- **Status**: {completed_entries} completed entry/entries in TODO.md need review\")\n                lines.append(\"- **Action**: Review completed entries - if already documented in changelogs, remove from TODO.md; otherwise move to CHANGELOG_DETAIL.md and AI_CHANGELOG.md first\")\n            else:\n                lines.append(\"- **Status**: CLEAN (no completed entries found)\")\n        \n        # Dependency Docs belongs in Documentation section, not TODO Sync Status\n        # (Dependency Docs is already shown in the Documentation Signals section above)\n\n        if doc_artifacts:\n\n            artifact = doc_artifacts[0]\n\n            lines.append(f\"- **Content Fix**: {artifact.get('file')} line {artifact.get('line')} flagged for {artifact.get('pattern')}\")\n\n            if len(doc_artifacts) > 1:\n\n                lines.append(f\"  - Additional documentation findings: {len(doc_artifacts) - 1} more items\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Error Handling Analysis\")\n\n        if error_metrics:\n            # Use recalculated error_cov from Executive Summary section (already calculated above)\n            # If not recalculated, get from error_metrics\n            if 'error_cov' not in locals() or error_cov is None:\n                # NOTE: 'error_handling_coverage' is a backward compatibility fallback for old JSON format\n                error_cov = error_metrics.get('analyze_error_handling') or error_metrics.get('error_handling_coverage')\n                # Recalculate if needed\n                # Use the actual count from error analysis, not a recalculation\n                missing_error_handlers = to_int(error_metrics.get('functions_missing_error_handling', 0))\n                \n                # Only recalculate coverage if totals differ, but keep the actual missing count\n                error_total = error_metrics.get('total_functions')\n                error_with_handling = error_metrics.get('functions_with_error_handling')\n                canonical_total = metrics.get('total_functions')\n                if error_cov is not None and canonical_total and error_total and error_with_handling:\n                    if error_total != canonical_total:\n                        recalc_coverage = (error_with_handling / canonical_total) * 100\n                        if 0 <= recalc_coverage <= 100:\n                            error_cov = recalc_coverage\n\n            lines.append(f\"- **Coverage**: {percent_text(error_cov, 1)}\")\n\n            lines.append(f\"- **Functions Missing Protection**: {missing_error_handlers or 0}\")\n\n            quality = error_metrics.get('error_handling_quality') or {}\n\n            basic = quality.get('basic')\n\n            none = quality.get('none')\n\n            if basic:\n\n                lines.append(f\"- **Upgrade Targets**: {basic} functions rely on basic try-except blocks\")\n\n            if none:\n\n                lines.append(f\"- **Critical Items**: {none} functions have no error handling\")\n\n            # Phase 1: Decorator replacement candidates\n\n            phase1_total = error_metrics.get('phase1_total', 0)\n\n            if phase1_total > 0:\n\n                phase1_by_priority = error_metrics.get('phase1_by_priority', {})\n\n                priority_breakdown = []\n\n                if phase1_by_priority.get('high', 0) > 0:\n\n                    priority_breakdown.append(f\"{phase1_by_priority['high']} high\")\n\n                if phase1_by_priority.get('medium', 0) > 0:\n\n                    priority_breakdown.append(f\"{phase1_by_priority['medium']} medium\")\n\n                if phase1_by_priority.get('low', 0) > 0:\n\n                    priority_breakdown.append(f\"{phase1_by_priority['low']} low\")\n\n                priority_text = ', '.join(priority_breakdown) if priority_breakdown else '0'\n\n                lines.append(f\"- **Phase 1 Candidates**: {phase1_total} functions need `@handle_errors` decorator ({priority_text} priority)\")\n\n            # Phase 2: Generic exception categorization\n\n            phase2_total = error_metrics.get('phase2_total', 0)\n\n            if phase2_total > 0:\n\n                phase2_by_type = error_metrics.get('phase2_by_type', {})\n\n                type_breakdown = [f\"{count} {exc_type}\" for exc_type, count in sorted(phase2_by_type.items(), key=lambda x: x[1], reverse=True)[:5]]\n\n                type_text = ', '.join(type_breakdown) if type_breakdown else '0'\n\n                if len(phase2_by_type) > 5:\n\n                    type_text += f\", ... +{len(phase2_by_type) - 5} more\"\n\n                lines.append(f\"- **Phase 2 Exceptions**: {phase2_total} generic exception raises need categorization ({type_text})\")\n\n            if error_recommendations:\n\n                lines.append(f\"- **Top Recommendation**: {error_recommendations[0]}\")\n\n            if worst_error_modules:\n\n                module_summaries = []\n\n                # Filter out 100% modules (missing 0) - they don't need attention\n                modules_needing_attention = [\n                    m for m in worst_error_modules[:5] \n                    if m.get('missing', 0) > 0 and m.get('coverage', 100) < 100\n                ]\n                \n                for module in modules_needing_attention:\n                    module_name = module.get('module', 'Unknown')\n                    coverage_pct = percent_text(module.get('coverage'), 1)\n                    missing = module.get('missing')\n                    total = module.get('total')\n\n                    detail = f\"{module_name} ({coverage_pct}\"\n\n                    if missing is not None and total is not None:\n                        detail += f\", missing {missing}/{total}\"\n\n                    detail += \")\"\n\n                    module_summaries.append(detail)\n                \n                if module_summaries:\n                    lines.append(f\"- **Modules Requiring Attention**: {', '.join(module_summaries)}\")\n\n        else:\n\n            lines.append(\"- **Error Handling**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Testing & Coverage\")\n        \n        if coverage_summary and isinstance(coverage_summary, dict):\n            overall = coverage_summary.get('overall') or {}\n            lines.append(f\"- **Overall Coverage**: {percent_text(overall.get('coverage'), 1)} ({overall.get('covered')} of {overall.get('statements')} statements)\")\n\n            # Convert coverage to float for comparison (handles both string and numeric values)\n            module_gaps = []\n            for m in (coverage_summary.get('modules') or []):\n                coverage_val = m.get('coverage', 100)\n                if isinstance(coverage_val, str):\n                    coverage_val = to_float(coverage_val) or 100\n                elif not isinstance(coverage_val, (int, float)):\n                    coverage_val = 100\n                if coverage_val < 90:\n                    module_gaps.append(m)\n\n            if module_gaps:\n                module_descriptions = [\n                    f\"{m['module']} ({percent_text(m.get('coverage'), 1)}, missing {m.get('missed')} lines)\"\n                    for m in module_gaps[:5]\n                ]\n                lines.append(f\"    - **Modules with Lowest Coverage**: {', '.join(module_descriptions)}\")\n\n            worst_files = (coverage_summary or {}).get('worst_files') or []\n\n            if worst_files:\n                file_descriptions = [\n                    f\"{item['path']} ({percent_text(item.get('coverage'), 1)})\"\n                    for item in worst_files[:5]\n                ]\n                lines.append(f\"    - **Files with Lowest Coverage**: {', '.join(file_descriptions)}\")\n\n            if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n                dev_pct = dev_tools_insights['overall_pct']\n                dev_line = f\"- **Development Tools Coverage**: {percent_text(dev_pct, 1)}\"\n                if dev_tools_insights.get('covered') is not None and dev_tools_insights.get('statements') is not None:\n                    dev_line += f\" ({dev_tools_insights['covered']} of {dev_tools_insights['statements']} statements)\"\n                lines.append(dev_line)\n                low_modules = dev_tools_insights.get('low_modules') or []\n                if low_modules:\n                    dev_descriptions = [\n                        f\"{Path(item['path']).name} ({percent_text(item.get('coverage'), 1)}, missing {item.get('missed')} lines)\"\n                        for item in low_modules[:5]\n                    ]\n                    lines.append(f\"    - **Modules with Lowest Coverage**: {', '.join(dev_descriptions)}\")\n                if dev_tools_insights.get('html'):\n                    lines.append(f\"- **Dev Tools Report**: {dev_tools_insights['html']}\")\n\n            generated = overall.get('generated')\n\n            if generated:\n                pass  # Generated timestamp available\n\n        elif hasattr(self, 'coverage_results') and self.coverage_results:\n\n            lines.append(\"- Coverage regeneration completed with issues; inspect coverage.json for gap details\")\n\n        else:\n\n            lines.append(\"- Run `audit --full` to regenerate coverage metrics\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Complexity & Refactoring\")\n        \n        # Try to load complexity from decision_support if analyze_functions doesn't have it\n        if function_metrics.get('high_complexity') == 'Unknown' or function_metrics.get('high_complexity') is None:\n            if decision_metrics:\n                function_metrics['high_complexity'] = decision_metrics.get('high_complexity', 'Unknown')\n                function_metrics['critical_complexity'] = decision_metrics.get('critical_complexity', 'Unknown')\n                function_metrics['moderate_complexity'] = decision_metrics.get('moderate_complexity', 'Unknown')\n        \n        # If still unknown, try loading from cache\n        if function_metrics.get('high_complexity') == 'Unknown' or function_metrics.get('high_complexity') is None:\n            try:\n                import json\n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    # Try analyze_functions first\n                    if 'results' in cached_data and 'analyze_functions' in cached_data['results']:\n                        func_data = cached_data['results']['analyze_functions']\n                        if 'data' in func_data:\n                            cached_metrics = func_data['data']\n                            function_metrics['high_complexity'] = cached_metrics.get('high_complexity', 'Unknown')\n                            function_metrics['critical_complexity'] = cached_metrics.get('critical_complexity', 'Unknown')\n                            function_metrics['moderate_complexity'] = cached_metrics.get('moderate_complexity', 'Unknown')\n                            # Also load examples if available\n                            if 'critical_complexity_examples' in cached_metrics:\n                                function_metrics['critical_complexity_examples'] = cached_metrics.get('critical_complexity_examples', [])\n                            if 'high_complexity_examples' in cached_metrics:\n                                function_metrics['high_complexity_examples'] = cached_metrics.get('high_complexity_examples', [])\n                    # Fallback to decision_support\n                    if function_metrics.get('high_complexity') == 'Unknown' and 'results' in cached_data and 'decision_support' in cached_data['results']:\n                        ds_data = cached_data['results']['decision_support']\n                        if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                            ds_metrics = ds_data['data']['decision_support_metrics']\n                            function_metrics['high_complexity'] = ds_metrics.get('high_complexity', 'Unknown')\n                            function_metrics['critical_complexity'] = ds_metrics.get('critical_complexity', 'Unknown')\n                            function_metrics['moderate_complexity'] = ds_metrics.get('moderate_complexity', 'Unknown')\n                            # Also try to load examples from decision_support if available\n                            if 'critical_complexity_examples' in ds_metrics:\n                                function_metrics['critical_complexity_examples'] = ds_metrics.get('critical_complexity_examples', [])\n                            if 'high_complexity_examples' in ds_metrics:\n                                function_metrics['high_complexity_examples'] = ds_metrics.get('high_complexity_examples', [])\n            except Exception:\n                pass\n\n        lines.append(f\"- **High Complexity Functions**: {function_metrics.get('high_complexity', 'Unknown')}\")\n\n        lines.append(f\"- **Critical Complexity Functions**: {function_metrics.get('critical_complexity', 'Unknown')}\")\n\n        critical_examples = function_metrics.get('critical_complexity_examples') or []\n\n        if critical_examples:\n\n            critical_items = [\n\n                f\"{item['function']} ({item['file']})\"\n\n                for item in critical_examples[:5]\n\n            ]\n\n            lines.append(f\"- **Critical Examples**: {', '.join(critical_items)}\")\n\n        high_examples = function_metrics.get('high_complexity_examples') or []\n\n        if high_examples:\n\n            high_items = [\n\n                f\"{item['function']} ({item['file']})\"\n\n                for item in high_examples[:5]\n\n            ]\n\n            lines.append(f\"- **High Complexity Examples**: {', '.join(high_items)}\")\n\n        undocumented_examples = function_metrics.get('undocumented_examples') or []\n\n        if undocumented_examples:\n\n            undocumented_items = [\n\n                f\"{item['function']} ({item['file']})\"\n\n                for item in undocumented_examples[:5]\n\n            ]\n\n            lines.append(f\"- **Undocumented Functions**: {', '.join(undocumented_items)}\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Legacy & Code Hygiene\")\n\n        # Try to load legacy data from standardized storage if not in legacy_summary\n        if not legacy_summary:\n            try:\n                from .output_storage import load_tool_result\n                legacy_result = load_tool_result('analyze_legacy_references', 'legacy', project_root=self.project_root)\n                if legacy_result:\n                    # load_tool_result already unwraps the 'data' key, so legacy_result IS the data\n                    legacy_data = legacy_result\n                    if isinstance(legacy_data, dict):\n                        # Calculate files_with_issues from findings structure\n                        findings = legacy_data.get('findings', {})\n                        if findings:\n                            # Count total files across all pattern types\n                            total_files = sum(len(file_list) for file_list in findings.values())\n                            # Count total markers (matches) across all files\n                            total_markers = 0\n                            for pattern_type, file_list in findings.items():\n                                for file_entry in file_list:\n                                    # file_entry is [file_path, content, matches]\n                                    if len(file_entry) >= 3:\n                                        matches = file_entry[2]\n                                        if isinstance(matches, list):\n                                            total_markers += len(matches)\n                            \n                            legacy_issues = legacy_data.get('files_with_issues') or total_files\n                            legacy_markers = legacy_data.get('legacy_markers') or total_markers\n                        else:\n                            # Fallback to direct values if findings not present\n                            legacy_issues = legacy_data.get('files_with_issues') or 0\n                            legacy_markers = legacy_data.get('legacy_markers') or 0\n                        \n                        report_path = legacy_data.get('report_path') or 'development_docs/LEGACY_REFERENCE_REPORT.md'\n                        if legacy_issues is not None:\n                            legacy_summary = {\n                                'files_with_issues': legacy_issues,\n                                'legacy_markers': legacy_markers,\n                                'report_path': report_path\n                            }\n            except Exception as e:\n                logger.debug(f\"Failed to load legacy data for consolidated report: {e}\")\n                # LEGACY COMPATIBILITY: Fallback to results_cache or central aggregation file\n                if 'analyze_legacy_references' in self.results_cache:\n                    cached_data = self.results_cache['analyze_legacy_references']\n                    if isinstance(cached_data, dict):\n                        legacy_summary = {\n                            'files_with_issues': cached_data.get('files_with_issues', 0),\n                            'legacy_markers': cached_data.get('legacy_markers', 0),\n                            'report_path': cached_data.get('report_path', 'development_docs/LEGACY_REFERENCE_REPORT.md')\n                        }\n                else:\n                    # Try central aggregation file as last resort\n                    try:\n                        import json\n                        results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                        if results_file.exists():\n                            with open(results_file, 'r', encoding='utf-8') as f:\n                                cached_data = json.load(f)\n                            if 'results' in cached_data and 'analyze_legacy_references' in cached_data['results']:\n                                legacy_data = cached_data['results']['analyze_legacy_references']\n                                if 'data' in legacy_data:\n                                    cached_legacy = legacy_data['data']\n                                    if isinstance(cached_legacy, dict):\n                                        legacy_summary = {\n                                            'files_with_issues': cached_legacy.get('files_with_issues', 0),\n                                            'legacy_markers': cached_legacy.get('legacy_markers', 0),\n                                            'report_path': cached_legacy.get('report_path', 'development_docs/LEGACY_REFERENCE_REPORT.md')\n                                        }\n                    except Exception:\n                        pass  # If all fallbacks fail, legacy_summary remains empty\n\n        if legacy_summary:\n            legacy_issues = legacy_summary.get('files_with_issues')\n            if legacy_issues is not None:\n                lines.append(f\"- **Files with Legacy Markers**: {legacy_issues}\")\n            else:\n                lines.append(\"- **Files with Legacy Markers**: Unknown\")\n\n            legacy_markers = legacy_summary.get('legacy_markers')\n            if legacy_markers is not None:\n                lines.append(f\"- **Markers Found**: {legacy_markers}\")\n\n            report_path = legacy_summary.get('report_path')\n            if report_path:\n                lines.append(f\"- **Detailed Report**: {report_path}\")\n        else:\n            lines.append(\"- **Legacy References**: Data unavailable (run `audit --full` for latest scan)\")\n\n        # Try to load unused imports from standardized storage if not in cache\n        if not unused_imports_data:\n            try:\n                from .output_storage import load_tool_result\n                unused_result = load_tool_result('analyze_unused_imports', 'imports', project_root=self.project_root)\n                if unused_result:\n                    # load_tool_result already unwraps the 'data' key, so unused_result IS the data\n                    unused_imports_data = unused_result\n            except Exception as e:\n                logger.debug(f\"Failed to load unused imports data for consolidated report: {e}\")\n                # LEGACY COMPATIBILITY: Fallback to results_cache or central aggregation file\n                if 'analyze_unused_imports' in self.results_cache:\n                    unused_imports_data = self.results_cache['analyze_unused_imports']\n                else:\n                    # Try central aggregation file as last resort\n                    try:\n                        import json\n                        results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                        if results_file.exists():\n                            with open(results_file, 'r', encoding='utf-8') as f:\n                                cached_data = json.load(f)\n                            if 'results' in cached_data and 'analyze_unused_imports' in cached_data['results']:\n                                unused_data = cached_data['results']['analyze_unused_imports']\n                                if 'data' in unused_data:\n                                    unused_imports_data = unused_data['data']\n                    except Exception:\n                        pass  # If all fallbacks fail, unused_imports_data remains empty\n\n        if unused_imports_data:\n            total_unused = unused_imports_data.get('total_unused', 0)\n            files_with_issues = unused_imports_data.get('files_with_issues', 0)\n            if total_unused > 0 or files_with_issues > 0:\n                lines.append(f\"- **Unused Imports**: {total_unused} across {files_with_issues} files\")\n                by_category = unused_imports_data.get('by_category') or {}\n                obvious = by_category.get('obvious_unused')\n                if obvious:\n                    lines.append(f\"  - Obvious removals: {obvious}\")\n                type_only = by_category.get('type_hints_only')\n                if type_only:\n                    lines.append(f\"  - Type-only imports: {type_only}\")\n                report_path = self.project_root / 'development_docs' / 'UNUSED_IMPORTS_REPORT.md'\n                # Ensure report_path is a Path object before calling .exists()\n                if isinstance(report_path, Path) and report_path.exists():\n                    lines.append(f\"- **Detailed Report**: {report_path}\")\n            else:\n                lines.append(\"- **Unused Imports**: CLEAN (no unused imports detected)\")\n        else:\n            lines.append(\"- **Unused Imports**: Data unavailable (run `audit --full` for latest scan)\")\n\n        ascii_issues = doc_sync_summary.get('ascii_issues') if doc_sync_summary else None\n\n        if ascii_issues:\n\n            lines.append(f\"- **ASCII Cleanup**: {ascii_issues} files need normalization\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Validation & Follow-ups\")\n\n        if validation_output:\n            # Parse text output for status\n            if 'POOR' in validation_output:\n                lines.append(\"- **AI Work Validation**: POOR - documentation or tests missing\")\n            elif 'GOOD' in validation_output:\n                lines.append(\"- **AI Work Validation**: GOOD - keep current standards\")\n            elif 'NEEDS ATTENTION' in validation_output or 'FAIR' in validation_output:\n                lines.append(\"- **AI Work Validation**: NEEDS ATTENTION - structural validation issues detected\")\n            else:\n                lines.append(\"- **AI Work Validation**: Status available (see validation output)\")\n        else:\n            lines.append(\"- Validation results unavailable for this run\")\n\n        lines.append(\"- **Suggested Commands**:\")\n\n        lines.append(\"  - `python development_tools/run_development_tools.py doc-sync`\")\n\n        lines.append(\"  - `python development_tools/run_development_tools.py audit --full`\")\n\n        lines.append(\"  - `python development_tools/run_development_tools.py legacy`\")\n\n        lines.append(\"  - `python development_tools/run_development_tools.py status`\")\n\n        lines.append(\"\")\n\n        lines.append(\"## Reference Files\")\n\n        # Critical issues summary (if exists)\n        if issues_file.exists():\n            lines.append(f\"- Critical issues summary: {issues_file}\")\n\n        lines.append(\"- Latest AI status: development_tools/AI_STATUS.md\")\n        lines.append(\"- Current AI priorities: development_tools/AI_PRIORITIES.md\")\n        lines.append(\"- Detailed JSON results: development_tools/reports/analysis_detailed_results.json\")\n        \n        # Legacy reference report\n        legacy_report = self.project_root / 'development_docs' / 'LEGACY_REFERENCE_REPORT.md'\n        if legacy_report.exists():\n            # Use relative path from project root\n            rel_path = legacy_report.relative_to(self.project_root)\n            lines.append(f\"- Legacy reference report: {rel_path.as_posix()}\")\n        \n        # Test coverage report\n        coverage_report = self.project_root / 'development_docs' / 'TEST_COVERAGE_REPORT.md'\n        if not coverage_report.exists():\n            # LEGACY COMPATIBILITY\n            # Fallback to old filename TEST_COVERAGE_EXPANSION_PLAN.md during transition period.\n            # New standardized filename: TEST_COVERAGE_REPORT.md\n            # Removal plan: After one release cycle, remove this fallback. All tools should generate TEST_COVERAGE_REPORT.md.\n            # Detection: Search for \"TEST_COVERAGE_EXPANSION_PLAN.md\" to find all references.\n            logger.debug(\"LEGACY: Falling back to TEST_COVERAGE_EXPANSION_PLAN.md (new name: TEST_COVERAGE_REPORT.md)\")\n            coverage_report = self.project_root / 'development_docs' / 'TEST_COVERAGE_EXPANSION_PLAN.md'\n        if coverage_report.exists():\n            # Use relative path from project root\n            rel_path = coverage_report.relative_to(self.project_root)\n            lines.append(f\"- Test coverage report: {rel_path.as_posix()}\")\n        \n        # Unused imports report\n        unused_report = self.project_root / 'development_docs' / 'UNUSED_IMPORTS_REPORT.md'\n        if unused_report.exists():\n            # Use relative path from project root\n            rel_path = unused_report.relative_to(self.project_root)\n            lines.append(f\"- Unused imports detail: {rel_path.as_posix()}\")\n        \n        # Historical audit data archive\n        archive_dir = self.project_root / 'development_tools' / 'reports' / 'archive'\n        if archive_dir.exists():\n            lines.append(f\"- Historical audit data: development_tools/reports/archive\")\n\n        lines.append(\"\")\n\n        return \"\\n\".join(lines)\n\n    def _identify_critical_issues(self) -> List[str]:\n\n        \"\"\"Identify critical issues from audit results\"\"\"\n\n        issues = []\n\n        # Check function documentation coverage\n\n        if 'analyze_functions' in self.results_cache:\n\n            metrics = self.results_cache['analyze_functions']\n\n            if 'coverage' in metrics:\n\n                try:\n\n                    coverage = float(metrics['coverage'].replace('%', ''))\n\n                    if coverage < 90:\n\n                        issues.append(f\"Low documentation coverage: {coverage}%\")\n\n                except:\n\n                    pass\n\n        # Check for failed audits\n\n        if hasattr(self, '_last_failed_audits'):\n\n            for audit in self._last_failed_audits:\n\n                issues.append(f\"Failed audit: {audit}\")\n\n        return issues\n\n    def _generate_action_items(self) -> List[str]:\n\n        \"\"\"Generate actionable items from audit results\"\"\"\n\n        actions = []\n\n        # Documentation improvements (only if needed)\n\n        if 'analyze_functions' in self.results_cache:\n\n            metrics = self.results_cache['analyze_functions']\n\n            if 'coverage' in metrics:\n\n                try:\n\n                    coverage = float(metrics['coverage'].replace('%', ''))\n\n                    if coverage < 95:\n\n                        actions.append(f\"Improve documentation coverage (currently {coverage}%)\")\n\n                except:\n\n                    pass\n\n        # Complexity management (only if significant)\n\n        if 'decision_support' in self.results_cache:\n\n            insights = self.results_cache['decision_support']\n\n            if isinstance(insights, list) and insights:\n\n                # Look for complexity warnings\n\n                complexity_warnings = [insight for insight in insights if 'complexity' in insight.lower()]\n\n                if complexity_warnings:\n\n                    actions.append(\"Refactor high complexity functions for maintainability\")\n\n        # Core development tasks\n\n        actions.append(\"Review TODO.md for next development priorities\")\n\n        actions.append(\"Run comprehensive testing before major changes\")\n\n        # Maintenance tasks (updated changelog reference)\n\n        actions.append(\"Update AI_CHANGELOG.md and CHANGELOG_DETAIL.md with recent changes\")\n\n        return actions\n\n    def _get_system_status(self) -> str:\n\n        \"\"\"Get current system status\"\"\"\n\n        status_lines = []\n\n        status_lines.append(\"SYSTEM STATUS\")\n\n        status_lines.append(\"=\" * 30)\n\n        # Check key files (configurable for portability)\n\n        for file_path in self.key_files:\n\n            if Path(file_path).exists():\n\n                status_lines.append(f\"[OK] {file_path}\")\n\n            else:\n\n                status_lines.append(f\"[MISSING] {file_path}\")\n\n        # Check recent audit results\n\n        results_file_name = (self.audit_config or {}).get('results_file', 'development_tools/reports/analysis_detailed_results.json')\n\n        for prefix in ('ai_tools/', 'development_tools/'):\n\n            if results_file_name.startswith(prefix):\n\n                results_file_name = results_file_name[len(prefix):]\n\n                break\n\n        results_file = self.project_root / results_file_name\n\n        if results_file.exists():\n\n            try:\n\n                with open(results_file, 'r') as f:\n\n                    data = json.load(f)\n\n                    timestamp = data.get('timestamp', 'Unknown')\n\n                    status_lines.append(f\"[AUDIT] Last audit: {timestamp}\")\n\n            except:\n\n                status_lines.append(\"[AUDIT] Last audit: Unknown\")\n\n        else:\n\n            status_lines.append(\"[AUDIT] No recent audit found\")\n\n        return '\\n'.join(status_lines)\n\n    def _execute_documentation_task(self) -> bool:\n\n        \"\"\"Execute documentation update task\"\"\"\n\n        logger.info(\"Updating documentation...\")\n\n        result = self.run_script('generate_documentation')\n\n        return result['success']\n\n    def _execute_function_registry_task(self) -> bool:\n\n        \"\"\"Execute function registry task\"\"\"\n\n        logger.info(\"Updating function registry...\")\n\n        result = self.run_script('generate_function_registry')\n\n        return result['success']\n\n    def _execute_module_dependencies_task(self) -> bool:\n\n        \"\"\"Execute module dependencies task\"\"\"\n\n        logger.info(\"Updating module dependencies...\")\n\n        result = self.run_script('generate_module_dependencies')\n\n        return result['success']\n\n    def _run_doc_sync_check(self, *args) -> bool:\n\n        \"\"\"Run all documentation sync checks and aggregate results.\"\"\"\n\n        all_results = {}\n\n        # Run paired documentation sync\n        logger.info(\"Running paired documentation synchronization checks...\")\n        result = self.run_script('analyze_documentation_sync', *args)\n        # For analysis tools, success means they ran and produced output (even if issues found)\n        if result.get('output') or result.get('success'):\n            all_results['paired_docs'] = self._parse_documentation_sync_output(result.get('output', ''))\n        else:\n            logger.warning(f\"analyze_documentation_sync failed: {result.get('error', 'Unknown error')}\")\n\n        # Run path drift analysis\n        logger.info(\"Running path drift analysis...\")\n        result = self.run_script('analyze_path_drift')\n        if result.get('output') or result.get('success'):\n            all_results['path_drift'] = self._parse_path_drift_output(result.get('output', ''))\n        else:\n            logger.warning(f\"analyze_path_drift failed: {result.get('error', 'Unknown error')}\")\n\n        # Run ASCII compliance check\n        logger.info(\"Running ASCII compliance check...\")\n        result = self.run_script('analyze_ascii_compliance')\n        if result.get('output') or result.get('success'):\n            all_results['ascii_compliance'] = self._parse_ascii_compliance_output(result.get('output', ''))\n        else:\n            logger.warning(f\"analyze_ascii_compliance failed: {result.get('error', 'Unknown error')}\")\n\n        # Run heading numbering check\n        logger.info(\"Running heading numbering check...\")\n        result = self.run_script('analyze_heading_numbering')\n        if result.get('output') or result.get('success'):\n            all_results['heading_numbering'] = self._parse_heading_numbering_output(result.get('output', ''))\n        else:\n            logger.warning(f\"analyze_heading_numbering failed: {result.get('error', 'Unknown error')}\")\n\n        # Run missing addresses check\n        logger.info(\"Running missing addresses check...\")\n        result = self.run_script('analyze_missing_addresses')\n        if result.get('output') or result.get('success'):\n            all_results['missing_addresses'] = self._parse_missing_addresses_output(result.get('output', ''))\n        else:\n            logger.warning(f\"analyze_missing_addresses failed: {result.get('error', 'Unknown error')}\")\n\n        # Run unconverted links check\n        logger.info(\"Running unconverted links check...\")\n        result = self.run_script('analyze_unconverted_links')\n        if result.get('output') or result.get('success'):\n            all_results['unconverted_links'] = self._parse_unconverted_links_output(result.get('output', ''))\n        else:\n            logger.warning(f\"analyze_unconverted_links failed: {result.get('error', 'Unknown error')}\")\n\n        # Aggregate all results\n        summary = self._aggregate_doc_sync_results(all_results)\n\n        self.docs_sync_results = {'success': True, 'summary': summary, 'all_results': all_results}\n        self.docs_sync_summary = summary\n\n        logger.info(f\"Documentation sync summary: {summary.get('status', 'UNKNOWN')} - {summary.get('total_issues', 0)} total issues\")\n\n        return True\n\n    def _run_legacy_cleanup_scan(self, *args) -> bool:\n\n        \"\"\"Run legacy cleanup and store structured results.\"\"\"\n\n        result = self.run_script('fix_legacy_references', *args)\n\n        if result.get('success'):\n\n            summary = self._parse_legacy_output(result.get('output', ''))\n\n            result['summary'] = summary\n\n            self.legacy_cleanup_results = result\n\n            self.legacy_cleanup_summary = summary\n\n            return True\n\n        if result.get('output'):\n\n            logger.info(result['output'])\n\n        if result.get('error'):\n\n            logger.error(result['error'])\n\n        return False\n\n    def show_help(self):\n\n        \"\"\"Show comprehensive help and the available command list.\"\"\"\n\n        print(\"AI Development Tools Runner\")\n        print(\"=\" * 50)\n        print(f\"Comprehensive AI collaboration tools for the {self.project_name} project\")\n        print()\n        print(\"USAGE:\")\n        print(\"  python development_tools/run_development_tools.py <command> [options]\")\n        print()\n        print(\"AVAILABLE COMMANDS:\")\n        print()\n\n        for section, metadata in COMMAND_TIERS.items():\n            print(f\"  {section}:\")\n            description = metadata.get(\"description\")\n            if description:\n                print(f\"    {description}\")\n            for cmd_name in metadata.get(\"commands\", []):\n                if cmd_name in COMMAND_REGISTRY:\n                    cmd = COMMAND_REGISTRY[cmd_name]\n                    print(f\"    {cmd.name:<16} {cmd.help}\")\n            if metadata.get(\"tier\") == \"experimental\":\n                print(\"    WARNING: Experimental commands may change or fail; run only with approval.\")\n            print()\n\n        print(\"EXAMPLES:\")\n        print(\"  python development_tools/run_development_tools.py status\")\n        print(\"  python development_tools/run_development_tools.py audit --full\")\n        print(\"  python development_tools/run_development_tools.py docs\")\n        print(\"  python development_tools/run_development_tools.py unused-imports\")\n        print()\n        print(\"For detailed command options:\")\n        print(\"  python development_tools/run_development_tools.py <command> --help\")\n\n@dataclass(frozen=True)\n\nclass CommandRegistration:\n\n    name: str\n\n    handler: Callable[[\"AIToolsService\", Sequence[str]], int]\n\n    help: str\n\n    description: str = ''\n\ndef _print_command_help(parser: argparse.ArgumentParser) -> None:\n\n    parser.print_help()\n\n    print()\n\ndef _audit_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    parser = argparse.ArgumentParser(prog='audit', add_help=False)\n\n    parser.add_argument('--full', action='store_true', help='Run comprehensive audit (Tier 3 - includes coverage and dependencies).')\n    \n    parser.add_argument('--quick', action='store_true', help='Run quick audit (Tier 1 - core metrics only).')\n    \n    # LEGACY COMPATIBILITY\n    # --fast flag is deprecated in favor of --quick for consistency with tier naming.\n    # Removal plan: Remove --fast flag after one release cycle. Update any scripts/docs using --fast.\n    # Detection: Search for \"--fast\" in scripts, documentation, and usage examples.\n    parser.add_argument('--fast', action='store_true', help='[DEPRECATED] Use --quick instead. Force fast audit (skip coverage).')\n\n    parser.add_argument('--include-tests', action='store_true', help='Include test files in analysis.')\n\n    parser.add_argument('--include-dev-tools', action='store_true', help='Include development_tools in analysis.')\n\n    parser.add_argument('--include-all', action='store_true', help='Include tests and dev tools (equivalent to --include-tests --include-dev-tools).')\n\n    parser.add_argument('--overlap', action='store_true', help='Include overlap analysis in documentation analysis (section overlaps and consolidation recommendations).')\n\n    if any(arg in ('-h', '--help') for arg in argv):\n\n        _print_command_help(parser)\n\n        return 0\n\n    ns = parser.parse_args(list(argv))\n\n    # Determine audit tier based on flags\n    # --quick takes precedence, then --full, then default (standard)\n    # --fast is deprecated but still supported for backward compatibility\n    quick_mode = ns.quick\n    full_mode = ns.full\n    \n    # LEGACY COMPATIBILITY: Handle deprecated --fast flag\n    if ns.fast and not ns.quick and not ns.full:\n        quick_mode = True\n        logger.warning(\"LEGACY: --fast flag is deprecated. Use --quick instead.\")\n\n    # Set exclusion configuration\n\n    service.set_exclusion_config(\n\n        include_tests=ns.include_tests or ns.include_all,\n\n        include_dev_tools=ns.include_dev_tools or ns.include_all\n\n    )\n\n    success = service.run_audit(quick=quick_mode, full=full_mode, include_overlap=ns.overlap)\n\n    return 0 if success else 1\n\ndef _docs_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'docs' command does not accept additional arguments.\")\n\n            return 2\n\n        print(\"Usage: docs\")\n\n        return 0\n\n    success = service.run_docs()\n\n    return 0 if success else 1\n\ndef _validate_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'validate' command does not accept additional arguments.\")\n\n            return 2\n\n        print(\"Usage: validate\")\n\n        return 0\n\n    success = service.run_validate()\n\n    return 0 if success else 1\n\ndef _config_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'config' command does not accept additional arguments.\")\n\n            return 2\n\n        print(\"Usage: config\")\n\n        return 0\n\n    success = service.run_config()\n\n    return 0 if success else 1\n\ndef _workflow_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    parser = argparse.ArgumentParser(prog='workflow', add_help=False)\n\n    parser.add_argument('task_type', help='Workflow task to execute')\n\n    if any(arg in ('-h', '--help') for arg in argv):\n\n        _print_command_help(parser)\n\n        return 0\n\n    if not argv:\n\n        print(\"Usage: workflow <task_type>\")\n\n        return 2\n\n    ns = parser.parse_args(list(argv))\n\n    success = service.run_workflow(ns.task_type)\n\n    return 0 if success else 1\n\n# LEGACY COMPATIBILITY\n# quick-audit command is deprecated in favor of 'audit --quick' for consistency.\n# Removal plan: Remove quick-audit command and _quick_audit_command handler after one release cycle.\n# Detection: Search for \"quick-audit\" in scripts, documentation, and usage examples.\ndef _quick_audit_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'quick-audit' command does not accept additional arguments.\")\n            print(\"Note: 'quick-audit' is deprecated. Use 'audit --quick' instead.\")\n\n            return 2\n\n        print(\"Usage: quick-audit\")\n        print(\"Note: This command is deprecated. Use 'audit --quick' instead.\")\n\n        return 0\n\n    # LEGACY: quick-audit command - redirect to audit --quick\n    logger.warning(\"LEGACY: 'quick-audit' command is deprecated. Use 'audit --quick' instead.\")\n    success = service.run_audit(quick=True)\n\n    return 0 if success else 1\n\ndef _decision_support_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'decision-support' command does not accept additional arguments.\")\n\n            return 2\n\n        print(\"Usage: decision-support\")\n\n        return 0\n\n    result = service.run_decision_support()\n\n    return 0 if (isinstance(result, dict) and result.get('success', False)) or result else 1\n\ndef _fix_version_sync_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    parser = argparse.ArgumentParser(prog='version-sync', add_help=False)\n\n    parser.add_argument('scope', nargs='?', default='docs', help='Scope to sync (docs, core, ai_docs, all).')\n\n    if any(arg in ('-h', '--help') for arg in argv):\n\n        _print_command_help(parser)\n\n        return 0\n\n    ns = parser.parse_args(list(argv))\n\n    success = service.run_version_sync(ns.scope)\n\n    return 0 if success else 1\n\ndef _status_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'status' command does not accept additional arguments.\")\n\n            return 2\n\n        print(\"Usage: status\")\n\n        return 0\n\n    success = service.run_status()\n\n    return 0 if success else 1\n\ndef _system_signals_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n    \"\"\"Handle system-signals command\"\"\"\n    if argv:\n        if any(arg not in ('-h', '--help') for arg in argv):\n            print(\"The 'system-signals' command does not accept additional arguments.\")\n            return 2\n        print(\"Usage: system-signals\")\n        return 0\n\n    success = service.run_system_signals()\n    return 0 if success else 1\n\ndef _doc_sync_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'doc-sync' command does not accept additional arguments.\")\n\n            return 2\n\n        print(\"Usage: doc-sync\")\n\n        return 0\n\n    success = service.run_documentation_sync()\n\n    return 0 if success else 1\n\ndef _doc_fix_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    \"\"\"Handle doc-fix command with options for different fix types.\"\"\"\n\n    parser = argparse.ArgumentParser(description='Fix documentation issues')\n\n    parser.add_argument('--add-addresses', action='store_true', help='Add file addresses to documentation files')\n\n    parser.add_argument('--fix-ascii', action='store_true', help='Fix non-ASCII characters in documentation')\n\n    parser.add_argument('--number-headings', action='store_true', help='Number H2 and H3 headings in documentation')\n\n    parser.add_argument('--convert-links', action='store_true', help='Convert file paths to markdown links')\n\n    parser.add_argument('--all', action='store_true', help='Apply all fix operations')\n\n    parser.add_argument('--dry-run', action='store_true', help='Show what would be changed without making changes')\n\n    try:\n\n        args = parser.parse_args(argv)\n\n    except SystemExit:\n\n        return 2\n\n    # Determine fix type\n\n    fix_types = []\n\n    if args.add_addresses:\n\n        fix_types.append('add-addresses')\n\n    if args.fix_ascii:\n\n        fix_types.append('fix-ascii')\n\n    if args.number_headings:\n\n        fix_types.append('number-headings')\n\n    if args.convert_links:\n\n        fix_types.append('convert-links')\n\n    if args.all:\n\n        fix_type = 'all'\n\n    elif len(fix_types) == 1:\n\n        fix_type = fix_types[0]\n\n    elif len(fix_types) > 1:\n\n        print(\"Error: Can only specify one fix type at a time (or use --all)\")\n\n        return 2\n\n    else:\n\n        # Default to all if nothing specified\n\n        fix_type = 'all'\n\n    success = service.run_documentation_fix(fix_type=fix_type, dry_run=args.dry_run)\n\n    return 0 if success else 1\n\ndef _coverage_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'coverage' command does not accept additional arguments.\")\n\n            return 2\n\n        print(\"Usage: coverage\")\n\n        return 0\n\n    success = service.run_coverage_regeneration()\n\n    return 0 if success else 1\n\ndef _legacy_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'legacy' command does not accept additional arguments.\")\n\n            return 2\n\n        print(\"Usage: legacy\")\n\n        return 0\n\n    success = service.run_legacy_cleanup()\n\n    return 0 if success else 1\n\ndef _unused_imports_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n    \"\"\"Handle unused-imports command.\"\"\"\n    if argv:\n        if '-h' in argv or '--help' in argv:\n            print(\"Usage: unused-imports\")\n            return 0\n        \n        if any(arg not in ('-h', '--help') for arg in argv):\n            print(\"The 'unused-imports' command does not accept additional arguments.\")\n            print(\"Usage: unused-imports\")\n            return 2\n\n    success = service.run_unused_imports_report()\n\n    return 0 if success else 1\n\ndef _cleanup_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n    \"\"\"Handle cleanup command.\"\"\"\n    import argparse\n    \n    parser = argparse.ArgumentParser(prog='cleanup', add_help=False)\n    parser.add_argument('--cache', action='store_true', help='Clean cache directories (__pycache__, .pytest_cache)')\n    parser.add_argument('--test-data', action='store_true', help='Clean test data directories')\n    parser.add_argument('--coverage', action='store_true', help='Clean coverage files and logs')\n    parser.add_argument('--all', action='store_true', help='Clean all categories (default if no specific category specified)')\n    parser.add_argument('--dry-run', action='store_true', help='Show what would be removed without actually removing')\n    \n    if '-h' in argv or '--help' in argv:\n        print(\"Usage: cleanup [--cache] [--test-data] [--coverage] [--all] [--dry-run]\")\n        print(\"  --cache      Clean cache directories (__pycache__, .pytest_cache)\")\n        print(\"  --test-data  Clean test data directories\")\n        print(\"  --coverage   Clean coverage files and logs\")\n        print(\"  --all        Clean all categories (default if no specific category specified)\")\n        print(\"  --dry-run    Show what would be removed without making changes\")\n        return 0\n    \n    try:\n        args, unknown = parser.parse_known_args(argv)\n        if unknown:\n            print(f\"Unknown arguments: {unknown}\")\n            print(\"Usage: cleanup [--cache] [--test-data] [--coverage] [--all] [--dry-run]\")\n            return 2\n    except SystemExit:\n        return 2\n    \n    result = service.run_cleanup(\n        cache=args.cache,\n        test_data=args.test_data,\n        coverage=args.coverage,\n        all_cleanup=args.all,\n        dry_run=args.dry_run\n    )\n    \n    if result.get('success'):\n        data = result.get('data', {})\n        total_removed = data.get('total_removed', 0)\n        total_failed = data.get('total_failed', 0)\n        if args.dry_run:\n            logger.info(f\"DRY RUN: Would remove {total_removed} items\")\n        else:\n            logger.info(f\"Cleanup completed: {total_removed} items removed, {total_failed} failed\")\n        return 0 if total_failed == 0 else 1\n    else:\n        logger.error(f\"Cleanup failed: {result.get('error', 'Unknown error')}\")\n        return 1\n\ndef _trees_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    if argv:\n\n        if any(arg not in ('-h', '--help') for arg in argv):\n\n            print(\"The 'trees' command does not accept additional arguments.\")\n\n            return 2\n\n        print(\"Usage: trees\")\n\n        return 0\n\n    success = service.generate_directory_trees()\n\n    return 0 if success else 1\n\ndef _show_help_command(service: \"AIToolsService\", argv: Sequence[str]) -> int:\n\n    service.show_help()\n\n    return 0\n\nCOMMAND_REGISTRY = OrderedDict([\n\n    ('audit', CommandRegistration('audit', _audit_command, 'Run audit (Tier 2 - standard). Use --quick for Tier 1, --full for Tier 3.')),\n\n    ('docs', CommandRegistration('docs', _docs_command, 'Regenerate documentation artifacts.')),\n\n    ('validate', CommandRegistration('validate', _validate_command, 'Validate AI-generated work.')),\n\n    ('config', CommandRegistration('config', _config_command, 'Check configuration consistency.')),\n\n    ('workflow', CommandRegistration('workflow', _workflow_command, 'Execute an audit-first workflow task.')),\n\n    ('quick-audit', CommandRegistration('quick-audit', _quick_audit_command, '[DEPRECATED] Use \"audit --quick\" instead. Run quick audit (Tier 1).')),\n\n    ('decision-support', CommandRegistration('decision-support', _decision_support_command, 'Generate decision support insights.')),\n\n    ('version-sync', CommandRegistration('version-sync', _fix_version_sync_command, 'Synchronize version metadata.')),\n\n    ('status', CommandRegistration('status', _status_command, 'Print quick system status.')),\n\n    ('system-signals', CommandRegistration('system-signals', _system_signals_command, 'Generate system health and status signals.')),\n\n    ('doc-sync', CommandRegistration('doc-sync', _doc_sync_command, 'Check documentation synchronisation.')),\n\n    ('doc-fix', CommandRegistration('doc-fix', _doc_fix_command, 'Fix documentation issues (addresses, ASCII, headings, links).')),\n\n    ('coverage', CommandRegistration('coverage', _coverage_command, 'Regenerate coverage metrics.')),\n\n    ('legacy', CommandRegistration('legacy', _legacy_command, 'Scan for legacy references.')),\n\n    ('unused-imports', CommandRegistration('unused-imports', _unused_imports_command, 'Detect unused imports in codebase.')),\n\n    ('cleanup', CommandRegistration('cleanup', _cleanup_command, 'Clean up project cache and temporary files.')),\n\n    ('trees', CommandRegistration('trees', _trees_command, 'Generate directory tree reports.')),\n\n    ('help', CommandRegistration('help', _show_help_command, 'Show detailed help information.')),\n\n])\n\ndef list_commands() -> Sequence[CommandRegistration]:\n\n    return tuple(COMMAND_REGISTRY.values())\n\n",
              [
                {
                  "pattern": "# LEGACY COMPATIBILITY:",
                  "match": "# LEGACY COMPATIBILITY:",
                  "line": 546,
                  "line_content": "# LEGACY COMPATIBILITY: Reading from old file location for backward compatibility",
                  "start": 16381,
                  "end": 16404
                },
                {
                  "pattern": "# LEGACY COMPATIBILITY:",
                  "match": "# LEGACY COMPATIBILITY:",
                  "line": 5382,
                  "line_content": "# LEGACY COMPATIBILITY: Reading from old file location for backward compatibility",
                  "start": 211815,
                  "end": 211838
                },
                {
                  "pattern": "# LEGACY COMPATIBILITY:",
                  "match": "# LEGACY COMPATIBILITY:",
                  "line": 7372,
                  "line_content": "# LEGACY COMPATIBILITY: Fallback to results_cache or central aggregation file",
                  "start": 317969,
                  "end": 317992
                },
                {
                  "pattern": "# LEGACY COMPATIBILITY:",
                  "match": "# LEGACY COMPATIBILITY:",
                  "line": 7429,
                  "line_content": "# LEGACY COMPATIBILITY: Fallback to results_cache or central aggregation file",
                  "start": 321491,
                  "end": 321514
                },
                {
                  "pattern": "# LEGACY COMPATIBILITY:",
                  "match": "# LEGACY COMPATIBILITY:",
                  "line": 7912,
                  "line_content": "# LEGACY COMPATIBILITY: Handle deprecated --fast flag",
                  "start": 340044,
                  "end": 340067
                }
              ]
            ],
            [
              "tests\\fixtures\\development_tools_demo\\legacy_code.py",
              "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
              [
                {
                  "pattern": "# LEGACY COMPATIBILITY:",
                  "match": "# LEGACY COMPATIBILITY:",
                  "line": 7,
                  "line_content": "# LEGACY COMPATIBILITY: This function is kept for backward compatibility",
                  "start": 144,
                  "end": 167
                },
                {
                  "pattern": "# LEGACY COMPATIBILITY:",
                  "match": "# LEGACY COMPATIBILITY:",
                  "line": 13,
                  "line_content": "# LEGACY COMPATIBILITY: Old import pattern",
                  "start": 302,
                  "end": 325
                }
              ]
            ]
          ],
          "old_bot_directory": [
            [
              "tests\\development_tools\\test_legacy_reference_cleanup.py",
              "\"\"\"\nTests for fix_legacy_references.py.\n\nTests scanning, verification, and safe cleanup operations.\n\"\"\"\n\nimport pytest\nimport sys\nfrom pathlib import Path\n\n# Import helper from conftest\nfrom tests.development_tools.conftest import load_development_tools_module\n\n# Load the modules using the helper\ncleanup_module = load_development_tools_module(\"fix_legacy_references\")\nanalyzer_module = load_development_tools_module(\"analyze_legacy_references\")\nreport_module = load_development_tools_module(\"generate_legacy_reference_report\")\n\nLegacyReferenceFixer = cleanup_module.LegacyReferenceFixer\nLegacyReferenceAnalyzer = analyzer_module.LegacyReferenceAnalyzer\nLegacyReferenceReportGenerator = report_module.LegacyReferenceReportGenerator\n\n\nclass TestLegacyScanning:\n    \"\"\"Test legacy reference scanning.\"\"\"\n    \n    @pytest.mark.unit\n    def test_scan_for_legacy_references_finds_markers(self, demo_project_root):\n        \"\"\"Test that legacy markers are found.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        findings = analyzer.scan_for_legacy_references()\n        \n        # Should find legacy compatibility markers in legacy_code.py\n        if 'legacy_compatibility_markers' in findings:\n            legacy_files = [file_path for file_path, _, _ in findings['legacy_compatibility_markers']]\n            assert any('legacy_code.py' in str(f) for f in legacy_files)\n    \n    @pytest.mark.unit\n    def test_scan_for_legacy_references_respects_preserve_files(self, demo_project_root):\n        \"\"\"Test that preserved files are skipped.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Add a preserve pattern that matches our demo project\n        original_preserve = analyzer.preserve_files\n        analyzer.preserve_files = analyzer.preserve_files | {'README.md'}\n        \n        findings = analyzer.scan_for_legacy_references()\n        \n        # README.md should not appear in findings\n        for pattern_type, files in findings.items():\n            for file_path, _, _ in files:\n                assert 'README.md' not in str(file_path)\n        \n        analyzer.preserve_files = original_preserve\n    \n    @pytest.mark.unit\n    def test_should_skip_file_exclusions(self, demo_project_root):\n        \"\"\"Test that excluded files are skipped.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Test with a file that should be excluded\n        excluded_file = demo_project_root / \"__pycache__\" / \"test.pyc\"\n        excluded_file.parent.mkdir(exist_ok=True)\n        excluded_file.write_bytes(b\"test\")\n        \n        should_skip = analyzer.should_skip_file(excluded_file)\n        assert should_skip is True\n        \n        # Clean up - remove file and directory if empty\n        excluded_file.unlink()\n        try:\n            excluded_file.parent.rmdir()\n        except OSError:\n            # Directory not empty, that's okay - just remove our test file\n            pass\n\n\nclass TestReferenceFinding:\n    \"\"\"Test finding specific legacy references.\"\"\"\n    \n    @pytest.mark.unit\n    def test_find_all_references_specific_item(self, demo_project_root):\n        \"\"\"Test that specific legacy items are found.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Verify the legacy file exists\n        legacy_file = demo_project_root / \"legacy_code.py\"\n        assert legacy_file.exists(), \"legacy_code.py should exist in demo project\"\n        \n        # Verify the item exists in the file\n        content = legacy_file.read_text(encoding='utf-8')\n        assert 'LegacyChannelWrapper' in content, \"LegacyChannelWrapper should exist in legacy_code.py\"\n        \n        # Verify the file is not being skipped\n        assert not analyzer.should_skip_file(legacy_file), \"legacy_code.py should not be skipped\"\n        \n        # Find references to LegacyChannelWrapper\n        # Note: find_all_references searches for the item as a pattern in files\n        references = analyzer.find_all_references('LegacyChannelWrapper')\n        \n        # The function should find the class definition\n        # It searches for patterns like 'class LegacyChannelWrapper', 'LegacyChannelWrapper(', etc.\n        assert len(references) > 0, f\"find_all_references should find LegacyChannelWrapper. Found: {references}\"\n        assert any('legacy_code.py' in file_path for file_path in references.keys()), \\\n            f\"legacy_code.py should be in references. Found: {list(references.keys())}\"\n\n\nclass TestRemovalReadiness:\n    \"\"\"Test removal readiness verification.\"\"\"\n    \n    @pytest.mark.unit\n    def test_verify_removal_readiness_ready(self, demo_project_root):\n        \"\"\"Test that items with no active code references are ready.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Test with a non-existent item (should be ready)\n        verification = analyzer.verify_removal_readiness('NonExistentItem12345')\n        \n        # Should be ready (no references found)\n        assert verification['ready_for_removal'] is True\n        assert len(verification['categorized']['active_code']) == 0\n    \n    @pytest.mark.unit\n    def test_verify_removal_readiness_not_ready(self, demo_project_root):\n        \"\"\"Test that items with active code references are not ready.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Test with LegacyChannelWrapper which exists in legacy_code.py\n        verification = analyzer.verify_removal_readiness('LegacyChannelWrapper')\n        \n        # Should not be ready (has references)\n        # Note: May be ready if only in legacy_code.py which might be considered legacy itself\n        assert 'ready_for_removal' in verification\n        assert 'references' in verification\n        assert 'recommendations' in verification\n\n\nclass TestCleanupOperations:\n    \"\"\"Test cleanup operations (safe, using copies).\"\"\"\n    \n    @pytest.mark.unit\n    def test_cleanup_legacy_references_dry_run(self, temp_project_copy):\n        \"\"\"Test that dry-run reports planned changes without modifying files.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(temp_project_copy))\n        fixer = LegacyReferenceFixer(str(temp_project_copy))\n        \n        # Verify legacy_code.py exists in the copied project\n        legacy_file = temp_project_copy / \"legacy_code.py\"\n        assert legacy_file.exists(), f\"legacy_code.py should exist in {temp_project_copy}\"\n        \n        # Verify the file is not being skipped\n        assert not analyzer.should_skip_file(legacy_file), f\"legacy_code.py should not be skipped\"\n        \n        # Scan for legacy references\n        findings = analyzer.scan_for_legacy_references()\n        \n        # Check if we have any findings\n        has_findings = any(len(files) > 0 for files in findings.values())\n        assert has_findings, f\"No legacy references found. Findings: {dict(findings)}\"\n        \n        # Run cleanup in dry-run mode\n        cleanup_results = fixer.cleanup_legacy_references(findings, dry_run=True)\n        \n        # Should report what would be changed (structure may vary)\n        # The results should be a dict with some indication of what would change\n        assert isinstance(cleanup_results, dict), f\"Expected dict, got {type(cleanup_results)}\"\n        # May have keys like 'files_would_update', 'changes', 'files_updated', etc.\n        assert len(cleanup_results) > 0, \"Cleanup results should not be empty\"\n        \n        # Verify files were NOT actually modified\n        legacy_file = temp_project_copy / \"legacy_code.py\"\n        if legacy_file.exists():\n            original_content = legacy_file.read_text(encoding='utf-8')\n            # Should still contain legacy markers\n            assert 'LEGACY COMPATIBILITY' in original_content\n    \n    @pytest.mark.unit\n    def test_cleanup_legacy_references_actual_cleanup(self, temp_project_copy):\n        \"\"\"Test that actual cleanup modifies files correctly.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(temp_project_copy))\n        fixer = LegacyReferenceFixer(str(temp_project_copy))\n        \n        # Verify legacy_code.py exists in the copied project\n        legacy_file = temp_project_copy / \"legacy_code.py\"\n        assert legacy_file.exists(), f\"legacy_code.py should exist in {temp_project_copy}\"\n        \n        # Verify the file is not being skipped\n        assert not analyzer.should_skip_file(legacy_file), f\"legacy_code.py should not be skipped\"\n        \n        # Scan for legacy references\n        findings = analyzer.scan_for_legacy_references()\n        \n        # Check if we have any findings (findings is dict of pattern_type -> list of (file_path, content, matches))\n        has_findings = any(len(files) > 0 for files in findings.values())\n        assert has_findings, f\"No legacy references found. Findings: {dict(findings)}\"\n        \n        # Run cleanup in actual mode (not dry-run)\n        cleanup_results = fixer.cleanup_legacy_references(findings, dry_run=False)\n        \n        # Should return a dict with results structure\n        assert isinstance(cleanup_results, dict)\n        # The results dict should have expected keys (may be empty lists if no changes made)\n        assert 'files_updated' in cleanup_results or 'changes' in cleanup_results or 'errors' in cleanup_results or 'files_would_update' in cleanup_results\n        \n        # Note: Actual cleanup may modify files, but we're using a copy so it's safe\n\n\nclass TestReportGeneration:\n    \"\"\"Test report generation.\"\"\"\n    \n    @pytest.mark.unit\n    def test_generate_cleanup_report_structure(self, demo_project_root):\n        \"\"\"Test that report has expected structure.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        report_gen = LegacyReferenceReportGenerator(str(demo_project_root))\n        \n        findings = analyzer.scan_for_legacy_references()\n        report = report_gen.generate_cleanup_report(findings)\n        \n        # Should have expected sections\n        assert '# Legacy Reference Cleanup Report' in report\n        assert '## Summary' in report or 'Summary' in report\n\n\nclass TestReplacementMappings:\n    \"\"\"Test replacement mappings.\"\"\"\n    \n    @pytest.mark.unit\n    def test_get_replacement_mappings(self, demo_project_root):\n        \"\"\"Test that replacement mappings work correctly.\"\"\"\n        fixer = LegacyReferenceFixer(str(demo_project_root))\n        \n        # Test various replacements\n        test_cases = [\n            ('bot/', 'communication/'),\n            ('from bot.', 'from communication.'),\n            ('import bot.', 'import communication.'),\n        ]\n        \n        for original, expected_start in test_cases:\n            replacement = fixer.get_replacement(original)\n            # Should start with expected replacement\n            assert replacement.startswith(expected_start) or original not in fixer.replacement_mappings\n\n",
              [
                {
                  "pattern": "bot/",
                  "match": "bot/",
                  "line": 236,
                  "line_content": "('bot/', 'communication/'),",
                  "start": 10510,
                  "end": 10514
                },
                {
                  "pattern": "from\\s+bot\\.",
                  "match": "from bot.",
                  "line": 237,
                  "line_content": "('from bot.', 'from communication.'),",
                  "start": 10550,
                  "end": 10559
                },
                {
                  "pattern": "import\\s+bot\\.",
                  "match": "import bot.",
                  "line": 238,
                  "line_content": "('import bot.', 'import communication.'),",
                  "start": 10600,
                  "end": 10611
                }
              ]
            ],
            [
              "tests\\development_tools\\test_path_drift_detection.py",
              "#!/usr/bin/env python3\n\"\"\"\nTest path drift detection functionality.\n\nThis test verifies that path drift detection correctly identifies\ndocumentation references to files that don't exist.\n\"\"\"\n\nimport pytest\nimport tempfile\nimport shutil\nfrom pathlib import Path\nimport sys\n\n# Add project root to path\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nfrom development_tools.docs.analyze_path_drift import PathDriftAnalyzer\n\n\nclass TestPathDriftDetection:\n    \"\"\"Test path drift detection.\"\"\"\n    \n    def test_path_drift_detects_missing_file(self, tmp_path):\n        \"\"\"Test that path drift detection finds references to non-existent files.\"\"\"\n        # Create a temporary project structure\n        project_dir = tmp_path / \"test_project\"\n        project_dir.mkdir()\n        \n        # Create a documentation file with a reference to a non-existent file\n        docs_dir = project_dir / \"docs\"\n        docs_dir.mkdir()\n        \n        doc_file = docs_dir / \"README.md\"\n        doc_file.write_text(\"\"\"\n# Test Documentation\n\nThis file references a non-existent file: `core/nonexistent_module.py`\n\nAlso references: `tests/missing_test.py`\n\"\"\")\n        \n        # Create analyzer\n        analyzer = PathDriftAnalyzer(project_root=str(project_dir))\n        \n        # Run path drift check\n        results = analyzer.check_path_drift()\n        \n        # Verify that the documentation file is flagged\n        doc_file_str = str(doc_file.relative_to(project_dir))\n        assert doc_file_str in results, f\"Expected {doc_file_str} to be in drift results\"\n        \n        # Verify that the issues are detected\n        issues = results[doc_file_str]\n        assert len(issues) > 0, \"Expected at least one path drift issue\"\n        \n        # Check that the non-existent files are mentioned\n        issue_text = ' '.join(issues)\n        assert 'nonexistent_module.py' in issue_text or 'missing_test.py' in issue_text, \\\n            f\"Expected non-existent file to be mentioned in issues: {issues}\"\n    \n    def test_path_drift_ignores_existing_files(self, tmp_path):\n        \"\"\"Test that path drift detection doesn't flag existing files.\"\"\"\n        # Create a temporary project structure\n        project_dir = tmp_path / \"test_project\"\n        project_dir.mkdir()\n        \n        # Create actual files\n        core_dir = project_dir / \"core\"\n        core_dir.mkdir()\n        existing_file = core_dir / \"existing_module.py\"\n        existing_file.write_text(\"# Existing module\")\n        \n        # Create documentation that references the existing file\n        docs_dir = project_dir / \"docs\"\n        docs_dir.mkdir()\n        \n        doc_file = docs_dir / \"README.md\"\n        doc_file.write_text(f\"\"\"\n# Test Documentation\n\nThis file references an existing file: `core/existing_module.py`\n\"\"\")\n        \n        # Create analyzer\n        analyzer = PathDriftAnalyzer(project_root=str(project_dir))\n        \n        # Run path drift check\n        results = analyzer.check_path_drift()\n        \n        # Verify that the documentation file is NOT flagged (or flagged for other reasons only)\n        doc_file_str = str(doc_file.relative_to(project_dir))\n        if doc_file_str in results:\n            # If it's flagged, make sure it's not for the existing file\n            issues = results[doc_file_str]\n            issue_text = ' '.join(issues)\n            assert 'existing_module.py' not in issue_text, \\\n                f\"Existing file should not be flagged: {issues}\"\n    \n    def test_path_drift_with_legacy_documentation(self, tmp_path):\n        \"\"\"Test that path drift detection skips legacy documentation files.\"\"\"\n        # Create a temporary project structure\n        project_dir = tmp_path / \"test_project\"\n        project_dir.mkdir()\n        \n        # Create legacy documentation file\n        dev_docs_dir = project_dir / \"development_docs\"\n        dev_docs_dir.mkdir(parents=True)\n        \n        legacy_file = dev_docs_dir / \"LEGACY_REFERENCE_REPORT.md\"\n        legacy_file.write_text(\"\"\"\n# Legacy Reference Report\n\nThis file intentionally references old paths:\n- `bot/old_module.py` (legacy path)\n- `old_directory/file.py` (historical reference)\n\"\"\")\n        \n        # Create analyzer\n        analyzer = PathDriftAnalyzer(project_root=str(project_dir))\n        \n        # Run path drift check\n        results = analyzer.check_path_drift()\n        \n        # Verify that legacy documentation is skipped\n        legacy_file_str = str(legacy_file.relative_to(project_dir))\n        # Legacy files should be skipped, so they shouldn't appear in results\n        # (or if they do, it should be for non-legacy reasons)\n        if legacy_file_str in results:\n            # If it appears, verify it's not for the legacy paths\n            issues = results[legacy_file_str]\n            issue_text = ' '.join(issues)\n            # Legacy paths should be filtered out\n            assert 'old_module.py' not in issue_text or 'old_directory' not in issue_text, \\\n                f\"Legacy paths should be filtered: {issues}\"\n\n",
              [
                {
                  "pattern": "bot/",
                  "match": "bot/",
                  "line": 117,
                  "line_content": "- `bot/old_module.py` (legacy path)",
                  "start": 4149,
                  "end": 4153
                }
              ]
            ],
            [
              "tests\\fixtures\\development_tools_demo\\legacy_code.py",
              "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
              [
                {
                  "pattern": "bot/",
                  "match": "bot/",
                  "line": 21,
                  "line_content": "old_path = \"bot/communication/old_file.py\"",
                  "start": 620,
                  "end": 624
                },
                {
                  "pattern": "from\\s+bot\\.",
                  "match": "from bot.",
                  "line": 15,
                  "line_content": "# from bot.communication import old_module  # noqa: F401",
                  "start": 437,
                  "end": 446
                }
              ]
            ]
          ],
          "deprecated_functions": [
            [
              "tests\\development_tools\\test_legacy_reference_cleanup.py",
              "\"\"\"\nTests for fix_legacy_references.py.\n\nTests scanning, verification, and safe cleanup operations.\n\"\"\"\n\nimport pytest\nimport sys\nfrom pathlib import Path\n\n# Import helper from conftest\nfrom tests.development_tools.conftest import load_development_tools_module\n\n# Load the modules using the helper\ncleanup_module = load_development_tools_module(\"fix_legacy_references\")\nanalyzer_module = load_development_tools_module(\"analyze_legacy_references\")\nreport_module = load_development_tools_module(\"generate_legacy_reference_report\")\n\nLegacyReferenceFixer = cleanup_module.LegacyReferenceFixer\nLegacyReferenceAnalyzer = analyzer_module.LegacyReferenceAnalyzer\nLegacyReferenceReportGenerator = report_module.LegacyReferenceReportGenerator\n\n\nclass TestLegacyScanning:\n    \"\"\"Test legacy reference scanning.\"\"\"\n    \n    @pytest.mark.unit\n    def test_scan_for_legacy_references_finds_markers(self, demo_project_root):\n        \"\"\"Test that legacy markers are found.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        findings = analyzer.scan_for_legacy_references()\n        \n        # Should find legacy compatibility markers in legacy_code.py\n        if 'legacy_compatibility_markers' in findings:\n            legacy_files = [file_path for file_path, _, _ in findings['legacy_compatibility_markers']]\n            assert any('legacy_code.py' in str(f) for f in legacy_files)\n    \n    @pytest.mark.unit\n    def test_scan_for_legacy_references_respects_preserve_files(self, demo_project_root):\n        \"\"\"Test that preserved files are skipped.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Add a preserve pattern that matches our demo project\n        original_preserve = analyzer.preserve_files\n        analyzer.preserve_files = analyzer.preserve_files | {'README.md'}\n        \n        findings = analyzer.scan_for_legacy_references()\n        \n        # README.md should not appear in findings\n        for pattern_type, files in findings.items():\n            for file_path, _, _ in files:\n                assert 'README.md' not in str(file_path)\n        \n        analyzer.preserve_files = original_preserve\n    \n    @pytest.mark.unit\n    def test_should_skip_file_exclusions(self, demo_project_root):\n        \"\"\"Test that excluded files are skipped.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Test with a file that should be excluded\n        excluded_file = demo_project_root / \"__pycache__\" / \"test.pyc\"\n        excluded_file.parent.mkdir(exist_ok=True)\n        excluded_file.write_bytes(b\"test\")\n        \n        should_skip = analyzer.should_skip_file(excluded_file)\n        assert should_skip is True\n        \n        # Clean up - remove file and directory if empty\n        excluded_file.unlink()\n        try:\n            excluded_file.parent.rmdir()\n        except OSError:\n            # Directory not empty, that's okay - just remove our test file\n            pass\n\n\nclass TestReferenceFinding:\n    \"\"\"Test finding specific legacy references.\"\"\"\n    \n    @pytest.mark.unit\n    def test_find_all_references_specific_item(self, demo_project_root):\n        \"\"\"Test that specific legacy items are found.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Verify the legacy file exists\n        legacy_file = demo_project_root / \"legacy_code.py\"\n        assert legacy_file.exists(), \"legacy_code.py should exist in demo project\"\n        \n        # Verify the item exists in the file\n        content = legacy_file.read_text(encoding='utf-8')\n        assert 'LegacyChannelWrapper' in content, \"LegacyChannelWrapper should exist in legacy_code.py\"\n        \n        # Verify the file is not being skipped\n        assert not analyzer.should_skip_file(legacy_file), \"legacy_code.py should not be skipped\"\n        \n        # Find references to LegacyChannelWrapper\n        # Note: find_all_references searches for the item as a pattern in files\n        references = analyzer.find_all_references('LegacyChannelWrapper')\n        \n        # The function should find the class definition\n        # It searches for patterns like 'class LegacyChannelWrapper', 'LegacyChannelWrapper(', etc.\n        assert len(references) > 0, f\"find_all_references should find LegacyChannelWrapper. Found: {references}\"\n        assert any('legacy_code.py' in file_path for file_path in references.keys()), \\\n            f\"legacy_code.py should be in references. Found: {list(references.keys())}\"\n\n\nclass TestRemovalReadiness:\n    \"\"\"Test removal readiness verification.\"\"\"\n    \n    @pytest.mark.unit\n    def test_verify_removal_readiness_ready(self, demo_project_root):\n        \"\"\"Test that items with no active code references are ready.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Test with a non-existent item (should be ready)\n        verification = analyzer.verify_removal_readiness('NonExistentItem12345')\n        \n        # Should be ready (no references found)\n        assert verification['ready_for_removal'] is True\n        assert len(verification['categorized']['active_code']) == 0\n    \n    @pytest.mark.unit\n    def test_verify_removal_readiness_not_ready(self, demo_project_root):\n        \"\"\"Test that items with active code references are not ready.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        \n        # Test with LegacyChannelWrapper which exists in legacy_code.py\n        verification = analyzer.verify_removal_readiness('LegacyChannelWrapper')\n        \n        # Should not be ready (has references)\n        # Note: May be ready if only in legacy_code.py which might be considered legacy itself\n        assert 'ready_for_removal' in verification\n        assert 'references' in verification\n        assert 'recommendations' in verification\n\n\nclass TestCleanupOperations:\n    \"\"\"Test cleanup operations (safe, using copies).\"\"\"\n    \n    @pytest.mark.unit\n    def test_cleanup_legacy_references_dry_run(self, temp_project_copy):\n        \"\"\"Test that dry-run reports planned changes without modifying files.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(temp_project_copy))\n        fixer = LegacyReferenceFixer(str(temp_project_copy))\n        \n        # Verify legacy_code.py exists in the copied project\n        legacy_file = temp_project_copy / \"legacy_code.py\"\n        assert legacy_file.exists(), f\"legacy_code.py should exist in {temp_project_copy}\"\n        \n        # Verify the file is not being skipped\n        assert not analyzer.should_skip_file(legacy_file), f\"legacy_code.py should not be skipped\"\n        \n        # Scan for legacy references\n        findings = analyzer.scan_for_legacy_references()\n        \n        # Check if we have any findings\n        has_findings = any(len(files) > 0 for files in findings.values())\n        assert has_findings, f\"No legacy references found. Findings: {dict(findings)}\"\n        \n        # Run cleanup in dry-run mode\n        cleanup_results = fixer.cleanup_legacy_references(findings, dry_run=True)\n        \n        # Should report what would be changed (structure may vary)\n        # The results should be a dict with some indication of what would change\n        assert isinstance(cleanup_results, dict), f\"Expected dict, got {type(cleanup_results)}\"\n        # May have keys like 'files_would_update', 'changes', 'files_updated', etc.\n        assert len(cleanup_results) > 0, \"Cleanup results should not be empty\"\n        \n        # Verify files were NOT actually modified\n        legacy_file = temp_project_copy / \"legacy_code.py\"\n        if legacy_file.exists():\n            original_content = legacy_file.read_text(encoding='utf-8')\n            # Should still contain legacy markers\n            assert 'LEGACY COMPATIBILITY' in original_content\n    \n    @pytest.mark.unit\n    def test_cleanup_legacy_references_actual_cleanup(self, temp_project_copy):\n        \"\"\"Test that actual cleanup modifies files correctly.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(temp_project_copy))\n        fixer = LegacyReferenceFixer(str(temp_project_copy))\n        \n        # Verify legacy_code.py exists in the copied project\n        legacy_file = temp_project_copy / \"legacy_code.py\"\n        assert legacy_file.exists(), f\"legacy_code.py should exist in {temp_project_copy}\"\n        \n        # Verify the file is not being skipped\n        assert not analyzer.should_skip_file(legacy_file), f\"legacy_code.py should not be skipped\"\n        \n        # Scan for legacy references\n        findings = analyzer.scan_for_legacy_references()\n        \n        # Check if we have any findings (findings is dict of pattern_type -> list of (file_path, content, matches))\n        has_findings = any(len(files) > 0 for files in findings.values())\n        assert has_findings, f\"No legacy references found. Findings: {dict(findings)}\"\n        \n        # Run cleanup in actual mode (not dry-run)\n        cleanup_results = fixer.cleanup_legacy_references(findings, dry_run=False)\n        \n        # Should return a dict with results structure\n        assert isinstance(cleanup_results, dict)\n        # The results dict should have expected keys (may be empty lists if no changes made)\n        assert 'files_updated' in cleanup_results or 'changes' in cleanup_results or 'errors' in cleanup_results or 'files_would_update' in cleanup_results\n        \n        # Note: Actual cleanup may modify files, but we're using a copy so it's safe\n\n\nclass TestReportGeneration:\n    \"\"\"Test report generation.\"\"\"\n    \n    @pytest.mark.unit\n    def test_generate_cleanup_report_structure(self, demo_project_root):\n        \"\"\"Test that report has expected structure.\"\"\"\n        analyzer = LegacyReferenceAnalyzer(str(demo_project_root))\n        report_gen = LegacyReferenceReportGenerator(str(demo_project_root))\n        \n        findings = analyzer.scan_for_legacy_references()\n        report = report_gen.generate_cleanup_report(findings)\n        \n        # Should have expected sections\n        assert '# Legacy Reference Cleanup Report' in report\n        assert '## Summary' in report or 'Summary' in report\n\n\nclass TestReplacementMappings:\n    \"\"\"Test replacement mappings.\"\"\"\n    \n    @pytest.mark.unit\n    def test_get_replacement_mappings(self, demo_project_root):\n        \"\"\"Test that replacement mappings work correctly.\"\"\"\n        fixer = LegacyReferenceFixer(str(demo_project_root))\n        \n        # Test various replacements\n        test_cases = [\n            ('bot/', 'communication/'),\n            ('from bot.', 'from communication.'),\n            ('import bot.', 'import communication.'),\n        ]\n        \n        for original, expected_start in test_cases:\n            replacement = fixer.get_replacement(original)\n            # Should start with expected replacement\n            assert replacement.startswith(expected_start) or original not in fixer.replacement_mappings\n\n",
              [
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 93,
                  "line_content": "assert 'LegacyChannelWrapper' in content, \"LegacyChannelWrapper should exist in legacy_code.py\"",
                  "start": 3609,
                  "end": 3629
                },
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 93,
                  "line_content": "assert 'LegacyChannelWrapper' in content, \"LegacyChannelWrapper should exist in legacy_code.py\"",
                  "start": 3644,
                  "end": 3664
                },
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 98,
                  "line_content": "# Find references to LegacyChannelWrapper",
                  "start": 3889,
                  "end": 3909
                },
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 100,
                  "line_content": "references = analyzer.find_all_references('LegacyChannelWrapper')",
                  "start": 4041,
                  "end": 4061
                },
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 103,
                  "line_content": "# It searches for patterns like 'class LegacyChannelWrapper', 'LegacyChannelWrapper(', etc.",
                  "start": 4176,
                  "end": 4196
                },
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 103,
                  "line_content": "# It searches for patterns like 'class LegacyChannelWrapper', 'LegacyChannelWrapper(', etc.",
                  "start": 4200,
                  "end": 4220
                },
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 104,
                  "line_content": "assert len(references) > 0, f\"find_all_references should find LegacyChannelWrapper. Found: {references}\"",
                  "start": 4299,
                  "end": 4319
                },
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 129,
                  "line_content": "# Test with LegacyChannelWrapper which exists in legacy_code.py",
                  "start": 5431,
                  "end": 5451
                },
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 130,
                  "line_content": "verification = analyzer.verify_removal_readiness('LegacyChannelWrapper')",
                  "start": 5541,
                  "end": 5561
                }
              ]
            ],
            [
              "tests\\fixtures\\development_tools_demo\\legacy_code.py",
              "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
              [
                {
                  "pattern": "LegacyChannelWrapper",
                  "match": "LegacyChannelWrapper",
                  "line": 25,
                  "line_content": "class LegacyChannelWrapper:",
                  "start": 679,
                  "end": 699
                },
                {
                  "pattern": "_create_legacy_channel_access\\(",
                  "match": "_create_legacy_channel_access(",
                  "line": 30,
                  "line_content": "def _create_legacy_channel_access():",
                  "start": 748,
                  "end": 778
                }
              ]
            ]
          ],
          "old_import_paths": [
            [
              "tests\\fixtures\\development_tools_demo\\legacy_code.py",
              "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
              [
                {
                  "pattern": "from\\s+bot\\.communication",
                  "match": "from bot.communication",
                  "line": 15,
                  "line_content": "# from bot.communication import old_module  # noqa: F401",
                  "start": 437,
                  "end": 459
                }
              ]
            ]
          ],
          "historical_references": [
            [
              "tests\\fixtures\\development_tools_demo\\legacy_code.py",
              "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
              [
                {
                  "pattern": "bot/communication",
                  "match": "bot/communication",
                  "line": 21,
                  "line_content": "old_path = \"bot/communication/old_file.py\"",
                  "start": 620,
                  "end": 637
                }
              ]
            ]
          ]
        },
        "files_with_issues": 10,
        "legacy_markers": 28,
        "report_path": "development_docs/LEGACY_REFERENCE_REPORT.md"
      },
      "timestamp": "2025-12-04T07:36:44.358401"
    },
    "quick_status": {
      "success": true,
      "data": {
        "timestamp": "2025-12-04T07:33:37.739883",
        "system_health": {
          "core_files": {
            "run_mhm.py": "OK",
            "core/service.py": "OK",
            "core/config.py": "OK"
          },
          "key_directories": {
            "core": "OK",
            "communication": "OK",
            "ui": "OK",
            "user": "OK",
            "tasks": "OK",
            "tests": "OK",
            "ai": "OK"
          },
          "overall_status": "OK"
        },
        "documentation_status": {
          "coverage": "Unknown",
          "recent_audit": "2025-12-04T07:31:55.501252",
          "key_files": {
            "README.md": "OK",
            "ai_development_docs/AI_CHANGELOG.md": "OK",
            "development_docs/CHANGELOG_DETAIL.md": "OK",
            "TODO.md": "OK",
            "development_docs/FUNCTION_REGISTRY_DETAIL.md": "OK",
            "development_docs/MODULE_DEPENDENCIES_DETAIL.md": "OK"
          }
        },
        "critical_issues": [],
        "action_items": [],
        "recent_activity": {
          "last_audit": "2025-12-04T07:31:55.501252",
          "recent_changes": [
            "coverage.json",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_CHANGELOG.md",
            "TODO.md",
            "DOCUMENTATION_GUIDE.md"
          ]
        }
      },
      "timestamp": "2025-12-04T07:33:37.866106"
    }
  }
}