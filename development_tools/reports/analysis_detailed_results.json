{
  "generated_by": "run_development_tools.py - AI Development Tools Runner",
  "last_generated": "2026-01-18 01:55:43",
  "source": "python development_tools/run_development_tools.py audit --full",
  "audit_tier": 3,
  "note": "This file is auto-generated. Do not edit manually.",
  "timestamp": "2026-01-18T01:55:43.498939",
  "successful": [
    "analyze_functions",
    "analyze_function_patterns",
    "analyze_function_registry",
    "analyze_package_exports",
    "analyze_ascii_compliance",
    "analyze_documentation",
    "analyze_documentation_sync",
    "analyze_heading_numbering",
    "analyze_missing_addresses",
    "analyze_path_drift",
    "analyze_unconverted_links",
    "analyze_error_handling",
    "analyze_test_coverage",
    "analyze_test_markers",
    "generate_dev_tools_coverage",
    "analyze_dependency_patterns",
    "analyze_module_dependencies",
    "analyze_module_imports",
    "analyze_unused_imports",
    "analyze_legacy_references",
    "analyze_config",
    "analyze_ai_work",
    "analyze_system_signals",
    "decision_support",
    "quick_status",
    "system_signals",
    "decision_support_metrics"
  ],
  "failed": [],
  "results": {
    "analyze_functions": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 442,
          "files_affected": 0
        },
        "details": {
          "total_functions": 1505,
          "moderate_complexity": 155,
          "high_complexity": 146,
          "critical_complexity": 141,
          "undocumented": 0,
          "undocumented_examples": [],
          "handlers": 815,
          "tests": 59,
          "utilities": 0,
          "critical_complexity_examples": [
            {
              "name": "_detect_mode",
              "function": "_detect_mode",
              "file": "chatbot.py",
              "complexity": 355
            },
            {
              "name": "generate_response",
              "function": "generate_response",
              "file": "chatbot.py",
              "complexity": 781
            },
            {
              "name": "generate_personalized_message",
              "function": "generate_personalized_message",
              "file": "chatbot.py",
              "complexity": 206
            },
            {
              "name": "generate_contextual_response",
              "function": "generate_contextual_response",
              "file": "chatbot.py",
              "complexity": 657
            },
            {
              "name": "_clean_system_prompt_leaks",
              "function": "_clean_system_prompt_leaks",
              "file": "chatbot.py",
              "complexity": 309
            },
            {
              "name": "__init__",
              "function": "__init__",
              "file": "chatbot.py",
              "complexity": 122
            },
            {
              "name": "_make_cache_key_inputs",
              "function": "_make_cache_key_inputs",
              "file": "chatbot.py",
              "complexity": 180
            },
            {
              "name": "_call_lm_studio_api",
              "function": "_call_lm_studio_api",
              "file": "chatbot.py",
              "complexity": 184
            },
            {
              "name": "_detect_resource_constraints",
              "function": "_detect_resource_constraints",
              "file": "chatbot.py",
              "complexity": 101
            },
            {
              "name": "_smart_truncate_response",
              "function": "_smart_truncate_response",
              "file": "chatbot.py",
              "complexity": 141
            },
            {
              "name": "__init__",
              "function": "__init__",
              "file": "cache_manager.py",
              "complexity": 75
            },
            {
              "name": "_generate_key",
              "function": "_generate_key",
              "file": "cache_manager.py",
              "complexity": 57
            },
            {
              "name": "_cleanup_lru",
              "function": "_cleanup_lru",
              "file": "cache_manager.py",
              "complexity": 90
            },
            {
              "name": "_validate_email",
              "function": "_validate_email",
              "file": "schemas.py",
              "complexity": 0
            },
            {
              "name": "_validate_timezone",
              "function": "_validate_timezone",
              "file": "schemas.py",
              "complexity": 0
            },
            {
              "name": "on_save",
              "function": "on_save",
              "file": "ui_app_qt.py",
              "complexity": 0
            },
            {
              "name": "on_personalization_save",
              "function": "on_personalization_save",
              "file": "account_creator_dialog.py",
              "complexity": 0
            },
            {
              "name": "set_contact_info",
              "function": "set_contact_info",
              "file": "channel_selection_widget.py",
              "complexity": 0
            }
          ]
        }
      },
      "timestamp": "2026-01-18T01:46:33"
    },
    "analyze_function_patterns": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 0,
          "files_affected": 0
        },
        "details": {
          "handlers": [
            {
              "file": "communication/command_handlers/account_handler.py",
              "class": "AccountManagementHandler",
              "methods": 9,
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/analytics_handler.py",
              "class": "AnalyticsHandler",
              "methods": 18,
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/base_handler.py",
              "class": "InteractionHandler",
              "methods": 7,
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/checkin_handler.py",
              "class": "CheckinHandler",
              "methods": 7,
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "class": "HelpHandler",
              "methods": 9,
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/notebook_handler.py",
              "class": "NotebookHandler",
              "methods": 28,
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/profile_handler.py",
              "class": "ProfileHandler",
              "methods": 8,
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/schedule_handler.py",
              "class": "ScheduleManagementHandler",
              "methods": 10,
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/task_handler.py",
              "class": "TaskManagementHandler",
              "methods": 28,
              "has_doc": true
            }
          ],
          "managers": [],
          "factories": [],
          "context_managers": [
            {
              "file": "core/error_handling.py",
              "class": "SafeFileContext",
              "methods": 3,
              "has_doc": true
            }
          ],
          "widgets": [
            {
              "file": "ui/widgets/category_selection_widget.py",
              "class": "CategorySelectionWidget",
              "methods": 3,
              "has_doc": false
            },
            {
              "file": "ui/widgets/channel_selection_widget.py",
              "class": "ChannelSelectionWidget",
              "methods": 8,
              "has_doc": false
            },
            {
              "file": "ui/widgets/checkin_settings_widget.py",
              "class": "CheckinSettingsWidget",
              "methods": 28,
              "has_doc": true
            },
            {
              "file": "ui/widgets/period_row_widget.py",
              "class": "PeriodRowWidget",
              "methods": 23,
              "has_doc": true
            },
            {
              "file": "ui/widgets/tag_widget.py",
              "class": "TagWidget",
              "methods": 15,
              "has_doc": true
            },
            {
              "file": "ui/widgets/task_settings_widget.py",
              "class": "TaskSettingsWidget",
              "methods": 18,
              "has_doc": false
            },
            {
              "file": "ui/widgets/user_profile_settings_widget.py",
              "class": "UserProfileSettingsWidget",
              "methods": 14,
              "has_doc": true
            }
          ],
          "dialogs": [
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "class": "AccountCreatorDialog",
              "methods": 48,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/admin_panel.py",
              "class": "AdminPanelDialog",
              "methods": 4,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "class": "CategoryManagementDialog",
              "methods": 6,
              "has_doc": false
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "class": "ChannelManagementDialog",
              "methods": 5,
              "has_doc": false
            },
            {
              "file": "ui/dialogs/checkin_management_dialog.py",
              "class": "CheckinManagementDialog",
              "methods": 6,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "class": "MessageEditDialog",
              "methods": 5,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "class": "MessageEditorDialog",
              "methods": 11,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/process_watcher_dialog.py",
              "class": "ProcessWatcherDialog",
              "methods": 15,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/schedule_editor_dialog.py",
              "class": "ScheduleEditorDialog",
              "methods": 18,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_completion_dialog.py",
              "class": "TaskCompletionDialog",
              "methods": 8,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_crud_dialog.py",
              "class": "TaskCrudDialog",
              "methods": 14,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_edit_dialog.py",
              "class": "TaskEditDialog",
              "methods": 23,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_management_dialog.py",
              "class": "TaskManagementDialog",
              "methods": 4,
              "has_doc": false
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "class": "UserAnalyticsDialog",
              "methods": 15,
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_profile_dialog.py",
              "class": "UserProfileDialog",
              "methods": 18,
              "has_doc": true
            }
          ],
          "validators": [],
          "decorators": [
            {
              "file": "core/error_handling.py",
              "function": "handle_errors",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "handle_error",
              "has_doc": true
            }
          ],
          "schemas": [],
          "entry_points": [
            {
              "file": "ai/cache_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ai/cache_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ai/chatbot.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ai/chatbot.py",
              "function": "generate_response",
              "has_doc": true
            },
            {
              "file": "ai/context_builder.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ai/conversation_history.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ai/lm_studio_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ai/prompt_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_monitor.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/core/retry_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/command_parser.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "function": "handle_message",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/message_router.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/base/base_channel.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/base/command_registry.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/base/command_registry.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/base/rich_formatter.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/api_client.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/bot.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/checkin_view.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/event_handler.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/task_reminder_view.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/webhook_server.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/welcome_handler.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/email/bot.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/backup_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/checkin_analytics.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/checkin_dynamic_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/config.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/file_auditor.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/headless_service.py",
              "function": "main",
              "has_doc": true
            },
            {
              "file": "core/headless_service.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/logger.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/logger.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/logger.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/logger.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/logger.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/logger.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/message_analytics.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "main",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/service_utilities.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/generate_ui_files.py",
              "function": "main",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "main",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/admin_panel.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/checkin_management_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/process_watcher_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/schedule_editor_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_completion_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_crud_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_edit_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_management_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_profile_dialog.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/widgets/category_selection_widget.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/widgets/channel_selection_widget.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/widgets/checkin_settings_widget.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/widgets/period_row_widget.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/widgets/tag_widget.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/widgets/task_settings_widget.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "ui/widgets/user_profile_settings_widget.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "user/context_manager.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "user/user_preferences.py",
              "function": "__init__",
              "has_doc": true
            },
            {
              "file": "run_headless_service.py",
              "function": "main",
              "has_doc": true
            },
            {
              "file": "run_mhm.py",
              "function": "main",
              "has_doc": true
            }
          ],
          "data_access": [
            {
              "file": "ai/chatbot.py",
              "function": "reload_system_prompt",
              "has_doc": true
            },
            {
              "file": "ai/lm_studio_manager.py",
              "function": "load_model_automatically",
              "has_doc": true
            },
            {
              "file": "ai/prompt_manager.py",
              "function": "_load_custom_prompt",
              "has_doc": true
            },
            {
              "file": "ai/prompt_manager.py",
              "function": "reload_custom_prompt",
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/account_handler.py",
              "function": "_get_user_id_by_username",
              "has_doc": true
            },
            {
              "file": "communication/core/welcome_manager.py",
              "function": "_load_welcome_tracking",
              "has_doc": true
            },
            {
              "file": "communication/core/welcome_manager.py",
              "function": "_save_welcome_tracking",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "function": "_load_user_states",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "function": "_save_user_states",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "function": "get_user_suggestions",
              "has_doc": true
            },
            {
              "file": "core/checkin_dynamic_manager.py",
              "function": "_load_data",
              "has_doc": true
            },
            {
              "file": "core/checkin_dynamic_manager.py",
              "function": "save_custom_question",
              "has_doc": true
            },
            {
              "file": "core/config.py",
              "function": "get_user_data_dir",
              "has_doc": true
            },
            {
              "file": "core/config.py",
              "function": "get_user_file_path",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "_get_user_friendly_message",
              "has_doc": true
            },
            {
              "file": "core/file_operations.py",
              "function": "load_json_data",
              "has_doc": true
            },
            {
              "file": "core/file_operations.py",
              "function": "save_json_data",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "load_user_messages",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "load_default_messages",
              "has_doc": true
            },
            {
              "file": "core/response_tracking.py",
              "function": "get_user_info_for_tracking",
              "has_doc": true
            },
            {
              "file": "core/schedule_management.py",
              "function": "get_user_info_for_schedule_management",
              "has_doc": true
            },
            {
              "file": "core/service_utilities.py",
              "function": "load_and_localize_datetime",
              "has_doc": true
            },
            {
              "file": "core/tags.py",
              "function": "_load_default_tags_from_resources",
              "has_doc": true
            },
            {
              "file": "core/tags.py",
              "function": "load_user_tags",
              "has_doc": true
            },
            {
              "file": "core/tags.py",
              "function": "save_user_tags",
              "has_doc": true
            },
            {
              "file": "core/tags.py",
              "function": "get_user_tags",
              "has_doc": true
            },
            {
              "file": "core/ui_management.py",
              "function": "load_period_widgets_for_category",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_data__load_account",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__save_account",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_data__load_preferences",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__save_preferences",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_data__load_context",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__save_context",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_data__load_schedules",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__save_schedules",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_data__load_tags",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__save_tags",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "load_and_ensure_ids",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "get_user_data",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__validate_input",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__create_backup",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__validate_data",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__preserve_preference_settings",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__normalize_data",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__merge_single_type",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__save_single_type",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__update_index",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__check_cross_file_invariants",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__merge_all_types",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_save_user_data__write_all_types",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "save_user_data",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "save_user_data_transaction",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "get_user_categories",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "get_user_data_with_metadata",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_load_presets_json",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_id_by_identifier__by_internal_username",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_id_by_identifier__by_email",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_id_by_identifier__by_phone",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_id_by_identifier__by_chat_id",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "_get_user_id_by_identifier__by_discord_user_id",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "get_user_id_by_identifier",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "get_user_data_summary",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "get_user_info_for_data_manager",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "get_user_summary",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "get_user_analytics_summary",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "get_user_message_files",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "get_user_data_summary",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__initialize_summary",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__process_core_files",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__add_file_info",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__add_special_file_details",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__add_schedule_details",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__add_sent_messages_details",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__process_message_files",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__ensure_message_files",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__process_enabled_message_files",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__process_orphaned_message_files",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__add_message_file_info",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__add_missing_message_file_info",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__process_log_files",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "_get_user_data_summary__add_log_file_info",
              "has_doc": true
            },
            {
              "file": "tasks/task_management.py",
              "function": "load_active_tasks",
              "has_doc": true
            },
            {
              "file": "tasks/task_management.py",
              "function": "save_active_tasks",
              "has_doc": true
            },
            {
              "file": "tasks/task_management.py",
              "function": "load_completed_tasks",
              "has_doc": true
            },
            {
              "file": "tasks/task_management.py",
              "function": "save_completed_tasks",
              "has_doc": true
            },
            {
              "file": "tasks/task_management.py",
              "function": "get_user_task_stats",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "load_ui",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "load_theme",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "load_user_categories",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "function": "load_widgets",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "function": "load_category_widget",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "function": "load_message_service_widget",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "function": "load_task_management_widget",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "function": "load_checkin_settings_widget",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "function": "load_user_category_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "function": "save_category_settings",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "function": "_on_save_clicked",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "function": "save_channel_settings",
              "has_doc": false
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "function": "load_user_channel_data",
              "has_doc": false
            },
            {
              "file": "ui/dialogs/checkin_management_dialog.py",
              "function": "load_user_checkin_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/checkin_management_dialog.py",
              "function": "save_checkin_settings",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "load_message_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "save_message",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "load_messages",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/schedule_editor_dialog.py",
              "function": "load_existing_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/schedule_editor_dialog.py",
              "function": "save_schedule",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_crud_dialog.py",
              "function": "load_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_edit_dialog.py",
              "function": "load_task_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_edit_dialog.py",
              "function": "load_recurring_task_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_edit_dialog.py",
              "function": "save_task",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/task_management_dialog.py",
              "function": "save_task_settings",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "function": "load_analytics_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "function": "load_overview_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "function": "load_mood_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "function": "load_habits_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "function": "load_sleep_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "function": "load_quantitative_data",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/user_profile_dialog.py",
              "function": "save_personalization",
              "has_doc": true
            },
            {
              "file": "ui/widgets/checkin_settings_widget.py",
              "function": "load_existing_data",
              "has_doc": true
            },
            {
              "file": "ui/widgets/period_row_widget.py",
              "function": "load_period_data",
              "has_doc": true
            },
            {
              "file": "ui/widgets/period_row_widget.py",
              "function": "load_days",
              "has_doc": true
            },
            {
              "file": "ui/widgets/tag_widget.py",
              "function": "load_tags",
              "has_doc": true
            },
            {
              "file": "ui/widgets/task_settings_widget.py",
              "function": "load_existing_data",
              "has_doc": false
            },
            {
              "file": "ui/widgets/task_settings_widget.py",
              "function": "load_recurring_task_settings",
              "has_doc": true
            },
            {
              "file": "ui/widgets/task_settings_widget.py",
              "function": "save_recurring_task_settings",
              "has_doc": true
            },
            {
              "file": "ui/widgets/user_profile_settings_widget.py",
              "function": "load_existing_data",
              "has_doc": true
            },
            {
              "file": "user/context_manager.py",
              "function": "_get_user_profile",
              "has_doc": true
            },
            {
              "file": "user/context_manager.py",
              "function": "_get_user_preferences",
              "has_doc": true
            },
            {
              "file": "user/user_context.py",
              "function": "load_user_data",
              "has_doc": true
            },
            {
              "file": "user/user_context.py",
              "function": "save_user_data",
              "has_doc": true
            },
            {
              "file": "user/user_context.py",
              "function": "get_user_id",
              "has_doc": true
            },
            {
              "file": "user/user_preferences.py",
              "function": "load_preferences",
              "has_doc": true
            },
            {
              "file": "user/user_preferences.py",
              "function": "save_preferences",
              "has_doc": true
            }
          ],
          "communication": [
            {
              "file": "ai/chatbot.py",
              "function": "_get_fallback_personalized_message",
              "has_doc": true
            },
            {
              "file": "ai/chatbot.py",
              "function": "generate_personalized_message",
              "has_doc": true
            },
            {
              "file": "ai/conversation_history.py",
              "function": "add_message",
              "has_doc": true
            },
            {
              "file": "ai/conversation_history.py",
              "function": "get_recent_messages",
              "has_doc": true
            },
            {
              "file": "ai/conversation_history.py",
              "function": "get_session_messages",
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/account_handler.py",
              "function": "_send_confirmation_code",
              "has_doc": true
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "function": "_handle_messages",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "send_message_sync__run_async_sync",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "send_message_sync__queue_failed_message",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "_send_email_response",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "send_message_sync",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "handle_message_sending",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "_should_send_checkin_prompt",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "_send_checkin_prompt",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "_send_ai_generated_message",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "_send_predefined_message",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "_create_task_reminder_message",
              "has_doc": true
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "function": "_select_weighted_message",
              "has_doc": true
            },
            {
              "file": "communication/core/retry_manager.py",
              "function": "queue_failed_message",
              "has_doc": true
            },
            {
              "file": "communication/core/welcome_manager.py",
              "function": "get_welcome_message",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "function": "handle_inbound_message",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "function": "start_messages_flow",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "function": "_generate_completion_message",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "function": "handle_user_message",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "function": "get_mapped_message",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/message_router.py",
              "function": "get_message_router",
              "has_doc": true
            },
            {
              "file": "communication/message_processing/message_router.py",
              "function": "route_message",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/base/message_formatter.py",
              "function": "get_message_formatter",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/base/message_formatter.py",
              "function": "format_message",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/base/message_formatter.py",
              "function": "format_message",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/base/message_formatter.py",
              "function": "format_message",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/bot.py",
              "function": "can_send_messages",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/event_handler.py",
              "function": "add_message_handler",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/event_handler.py",
              "function": "add_disconnect_handler",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/webhook_server.py",
              "function": "log_message",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/welcome_handler.py",
              "function": "get_welcome_message",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/discord/welcome_handler.py",
              "function": "get_welcome_message_view",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/email/bot.py",
              "function": "send_message__send_email_sync",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/email/bot.py",
              "function": "_receive_emails_sync",
              "has_doc": true
            },
            {
              "file": "communication/communication_channels/email/bot.py",
              "function": "_receive_emails_sync__extract_body",
              "has_doc": true
            },
            {
              "file": "core/auto_cleanup.py",
              "function": "cleanup_old_message_archives",
              "has_doc": true
            },
            {
              "file": "core/auto_cleanup.py",
              "function": "archive_old_messages_for_all_users",
              "has_doc": true
            },
            {
              "file": "core/file_operations.py",
              "function": "_create_user_files__sent_messages_file",
              "has_doc": true
            },
            {
              "file": "core/file_operations.py",
              "function": "_create_user_files__message_files",
              "has_doc": true
            },
            {
              "file": "core/headless_service.py",
              "function": "send_test_message",
              "has_doc": true
            },
            {
              "file": "core/headless_service.py",
              "function": "reschedule_messages",
              "has_doc": true
            },
            {
              "file": "core/message_analytics.py",
              "function": "get_message_frequency",
              "has_doc": true
            },
            {
              "file": "core/message_analytics.py",
              "function": "get_message_summary",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "get_message_categories",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "add_message",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "edit_message",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "update_message",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "delete_message",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "get_recent_messages",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "store_sent_message",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "archive_old_messages",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "create_message_file_from_defaults",
              "has_doc": true
            },
            {
              "file": "core/message_management.py",
              "function": "ensure_user_message_files",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "reset_and_reschedule_daily_messages",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "schedule_daily_message_job",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "schedule_message_for_period",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "schedule_message_at_random_time",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "handle_sending_scheduled_message",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "_remove_user_message_job",
              "has_doc": true
            },
            {
              "file": "core/schemas.py",
              "function": "validate_messages_file_dict",
              "has_doc": false
            },
            {
              "file": "core/service.py",
              "function": "_check_and_fix_logging__verify_test_message_present",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_check_test_message_requests__get_base_directory",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_check_test_message_requests__discover_request_files",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_check_test_message_requests__parse_request_file",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_check_test_message_requests__validate_request_data",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_check_test_message_requests__process_valid_request",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_check_test_message_requests__get_message_content",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_check_test_message_requests__write_response",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_check_test_message_requests__cleanup_request_file",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_check_test_message_requests__handle_processing_error",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "check_test_message_requests",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_cleanup_test_message_requests__get_base_directory",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_cleanup_test_message_requests__is_test_message_request_file",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "_cleanup_test_message_requests__remove_request_file",
              "has_doc": true
            },
            {
              "file": "core/service.py",
              "function": "cleanup_test_message_requests",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "update_message_references",
              "has_doc": true
            },
            {
              "file": "core/user_data_manager.py",
              "function": "update_message_references",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "connect_signals",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "edit_user_messages",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "open_message_editor",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "_send_test_message__validate_user_selection",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "_send_test_message__validate_service_running",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "_send_test_message__get_selected_category",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "send_test_message",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "send_actual_test_message",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "send_checkin_prompt",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "send_task_reminder",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "function": "on_enable_messages_toggled",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "open_message_editor_dialog",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "show_no_messages_state",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "update_message_count",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "add_new_message",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "edit_selected_message",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "edit_message_by_row",
              "has_doc": true
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "function": "delete_message_by_row",
              "has_doc": true
            },
            {
              "file": "ui/widgets/checkin_settings_widget.py",
              "function": "connect_question_checkboxes",
              "has_doc": true
            }
          ],
          "error_handlers": [
            {
              "file": "core/error_handling.py",
              "function": "handle_errors",
              "has_doc": true
            },
            {
              "file": "core/error_handling.py",
              "function": "handle_error",
              "has_doc": true
            }
          ],
          "schedulers": [
            {
              "file": "communication/command_handlers/schedule_handler.py",
              "function": "_handle_add_schedule_period",
              "has_doc": true
            },
            {
              "file": "core/file_operations.py",
              "function": "_create_user_files__schedules_file",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "run_full_scheduler_standalone",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "run_user_scheduler_standalone",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "run_category_scheduler_standalone",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "run_daily_scheduler",
              "has_doc": true
            },
            {
              "file": "core/scheduler.py",
              "function": "run_full_daily_scheduler",
              "has_doc": true
            },
            {
              "file": "core/schedule_management.py",
              "function": "add_schedule_period",
              "has_doc": false
            },
            {
              "file": "core/service_utilities.py",
              "function": "create_reschedule_request",
              "has_doc": true
            },
            {
              "file": "core/user_data_handlers.py",
              "function": "create_default_schedule_periods",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "run_full_scheduler",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "run_user_scheduler",
              "has_doc": true
            },
            {
              "file": "ui/ui_app_qt.py",
              "function": "run_category_scheduler",
              "has_doc": true
            }
          ]
        }
      },
      "timestamp": "2026-01-18T01:46:28"
    },
    "analyze_function_registry": {
      "success": true,
      "data": {
        "details": {
          "analysis": {
            "duplicate_count": 70,
            "duplicate_sample": [
              {
                "files": [
                  "communication/__init__.py",
                  "communication/core/__init__.py",
                  "core/__init__.py"
                ],
                "name": "__getattr__"
              },
              {
                "files": [
                  "ai/cache_manager.py",
                  "ai/chatbot.py",
                  "ai/context_builder.py",
                  "ai/conversation_history.py",
                  "ai/lm_studio_manager.py",
                  "ai/prompt_manager.py",
                  "communication/communication_channels/base/base_channel.py",
                  "communication/communication_channels/base/command_registry.py",
                  "communication/communication_channels/base/rich_formatter.py",
                  "communication/communication_channels/discord/account_flow_handler.py",
                  "communication/communication_channels/discord/api_client.py",
                  "communication/communication_channels/discord/bot.py",
                  "communication/communication_channels/discord/checkin_view.py",
                  "communication/communication_channels/discord/event_handler.py",
                  "communication/communication_channels/discord/task_reminder_view.py",
                  "communication/communication_channels/discord/webhook_server.py",
                  "communication/communication_channels/discord/welcome_handler.py",
                  "communication/communication_channels/email/bot.py",
                  "communication/core/channel_monitor.py",
                  "communication/core/channel_orchestrator.py",
                  "communication/core/retry_manager.py",
                  "communication/message_processing/command_parser.py",
                  "communication/message_processing/conversation_flow_manager.py",
                  "communication/message_processing/interaction_manager.py",
                  "communication/message_processing/message_router.py",
                  "core/backup_manager.py",
                  "core/checkin_analytics.py",
                  "core/checkin_dynamic_manager.py",
                  "core/config.py",
                  "core/error_handling.py",
                  "core/file_auditor.py",
                  "core/headless_service.py",
                  "core/logger.py",
                  "core/message_analytics.py",
                  "core/scheduler.py",
                  "core/service.py",
                  "core/service_utilities.py",
                  "core/user_data_manager.py",
                  "ui/dialogs/account_creator_dialog.py",
                  "ui/dialogs/admin_panel.py",
                  "ui/dialogs/category_management_dialog.py",
                  "ui/dialogs/channel_management_dialog.py",
                  "ui/dialogs/checkin_management_dialog.py",
                  "ui/dialogs/message_editor_dialog.py",
                  "ui/dialogs/process_watcher_dialog.py",
                  "ui/dialogs/schedule_editor_dialog.py",
                  "ui/dialogs/task_completion_dialog.py",
                  "ui/dialogs/task_crud_dialog.py",
                  "ui/dialogs/task_edit_dialog.py",
                  "ui/dialogs/task_management_dialog.py",
                  "ui/dialogs/user_analytics_dialog.py",
                  "ui/dialogs/user_profile_dialog.py",
                  "ui/ui_app_qt.py",
                  "ui/widgets/category_selection_widget.py",
                  "ui/widgets/channel_selection_widget.py",
                  "ui/widgets/checkin_settings_widget.py",
                  "ui/widgets/dynamic_list_container.py",
                  "ui/widgets/dynamic_list_field.py",
                  "ui/widgets/period_row_widget.py",
                  "ui/widgets/tag_widget.py",
                  "ui/widgets/task_settings_widget.py",
                  "ui/widgets/user_profile_settings_widget.py",
                  "user/context_manager.py",
                  "user/user_preferences.py"
                ],
                "name": "__init__"
              },
              {
                "files": [
                  "ai/chatbot.py",
                  "communication/core/channel_orchestrator.py",
                  "user/user_context.py"
                ],
                "name": "__new__"
              },
              {
                "files": [
                  "ai/context_builder.py",
                  "ai/conversation_history.py",
                  "communication/communication_channels/base/base_channel.py",
                  "communication/communication_channels/base/command_registry.py",
                  "communication/communication_channels/discord/event_handler.py",
                  "ui/widgets/dynamic_list_container.py"
                ],
                "name": "__post_init__"
              },
              {
                "files": [
                  "communication/command_handlers/analytics_handler.py",
                  "communication/command_handlers/task_handler.py"
                ],
                "name": "_handle_task_stats"
              }
            ],
            "duplicates": {
              "__getattr__": [
                "communication/__init__.py",
                "communication/core/__init__.py",
                "core/__init__.py"
              ],
              "__init__": [
                "ai/cache_manager.py",
                "ai/chatbot.py",
                "ai/context_builder.py",
                "ai/conversation_history.py",
                "ai/lm_studio_manager.py",
                "ai/prompt_manager.py",
                "communication/communication_channels/base/base_channel.py",
                "communication/communication_channels/base/command_registry.py",
                "communication/communication_channels/base/rich_formatter.py",
                "communication/communication_channels/discord/account_flow_handler.py",
                "communication/communication_channels/discord/api_client.py",
                "communication/communication_channels/discord/bot.py",
                "communication/communication_channels/discord/checkin_view.py",
                "communication/communication_channels/discord/event_handler.py",
                "communication/communication_channels/discord/task_reminder_view.py",
                "communication/communication_channels/discord/webhook_server.py",
                "communication/communication_channels/discord/welcome_handler.py",
                "communication/communication_channels/email/bot.py",
                "communication/core/channel_monitor.py",
                "communication/core/channel_orchestrator.py",
                "communication/core/retry_manager.py",
                "communication/message_processing/command_parser.py",
                "communication/message_processing/conversation_flow_manager.py",
                "communication/message_processing/interaction_manager.py",
                "communication/message_processing/message_router.py",
                "core/backup_manager.py",
                "core/checkin_analytics.py",
                "core/checkin_dynamic_manager.py",
                "core/config.py",
                "core/error_handling.py",
                "core/file_auditor.py",
                "core/headless_service.py",
                "core/logger.py",
                "core/message_analytics.py",
                "core/scheduler.py",
                "core/service.py",
                "core/service_utilities.py",
                "core/user_data_manager.py",
                "ui/dialogs/account_creator_dialog.py",
                "ui/dialogs/admin_panel.py",
                "ui/dialogs/category_management_dialog.py",
                "ui/dialogs/channel_management_dialog.py",
                "ui/dialogs/checkin_management_dialog.py",
                "ui/dialogs/message_editor_dialog.py",
                "ui/dialogs/process_watcher_dialog.py",
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/dialogs/task_completion_dialog.py",
                "ui/dialogs/task_crud_dialog.py",
                "ui/dialogs/task_edit_dialog.py",
                "ui/dialogs/task_management_dialog.py",
                "ui/dialogs/user_analytics_dialog.py",
                "ui/dialogs/user_profile_dialog.py",
                "ui/ui_app_qt.py",
                "ui/widgets/category_selection_widget.py",
                "ui/widgets/channel_selection_widget.py",
                "ui/widgets/checkin_settings_widget.py",
                "ui/widgets/dynamic_list_container.py",
                "ui/widgets/dynamic_list_field.py",
                "ui/widgets/period_row_widget.py",
                "ui/widgets/tag_widget.py",
                "ui/widgets/task_settings_widget.py",
                "ui/widgets/user_profile_settings_widget.py",
                "user/context_manager.py",
                "user/user_preferences.py"
              ],
              "__new__": [
                "ai/chatbot.py",
                "communication/core/channel_orchestrator.py",
                "user/user_context.py"
              ],
              "__post_init__": [
                "ai/context_builder.py",
                "ai/conversation_history.py",
                "communication/communication_channels/base/base_channel.py",
                "communication/communication_channels/base/command_registry.py",
                "communication/communication_channels/discord/event_handler.py",
                "ui/widgets/dynamic_list_container.py"
              ],
              "_handle_task_stats": [
                "communication/command_handlers/analytics_handler.py",
                "communication/command_handlers/task_handler.py"
              ],
              "_is_valid_intent": [
                "communication/message_processing/command_parser.py",
                "communication/message_processing/interaction_manager.py"
              ],
              "accept": [
                "ui/dialogs/account_creator_dialog.py",
                "ui/dialogs/schedule_editor_dialog.py"
              ],
              "add_message": [
                "ai/conversation_history.py",
                "core/message_management.py"
              ],
              "add_new_period": [
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/widgets/checkin_settings_widget.py",
                "ui/widgets/task_settings_widget.py"
              ],
              "can_handle": [
                "communication/command_handlers/account_handler.py",
                "communication/command_handlers/analytics_handler.py",
                "communication/command_handlers/base_handler.py",
                "communication/command_handlers/checkin_handler.py",
                "communication/command_handlers/interaction_handlers.py",
                "communication/command_handlers/notebook_handler.py",
                "communication/command_handlers/profile_handler.py",
                "communication/command_handlers/schedule_handler.py",
                "communication/command_handlers/task_handler.py",
                "core/error_handling.py"
              ],
              "cancel": [
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/dialogs/user_profile_dialog.py"
              ],
              "center_dialog": [
                "ui/dialogs/account_creator_dialog.py",
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/dialogs/user_profile_dialog.py"
              ],
              "channel_type": [
                "communication/communication_channels/base/base_channel.py",
                "communication/communication_channels/discord/bot.py",
                "communication/communication_channels/email/bot.py"
              ],
              "cleanup_task_reminders": [
                "core/scheduler.py",
                "tasks/task_management.py"
              ],
              "clear_welcomed_status": [
                "communication/communication_channels/discord/welcome_handler.py",
                "communication/core/welcome_manager.py"
              ],
              "closeEvent": [
                "ui/dialogs/process_watcher_dialog.py",
                "ui/ui_app_qt.py"
              ],
              "close_dialog": [
                "ui/dialogs/account_creator_dialog.py",
                "ui/dialogs/schedule_editor_dialog.py"
              ],
              "create_new_user": [
                "core/user_data_handlers.py",
                "ui/ui_app_qt.py"
              ],
              "critical": [
                "core/file_auditor.py",
                "core/logger.py"
              ],
              "debug": [
                "core/file_auditor.py",
                "core/logger.py"
              ],
              "error": [
                "core/file_auditor.py",
                "core/logger.py"
              ],
              "find_lowest_available_period_number": [
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/widgets/checkin_settings_widget.py",
                "ui/widgets/task_settings_widget.py"
              ],
              "get_available_data_types": [
                "core/checkin_analytics.py",
                "core/user_data_handlers.py"
              ],
              "get_available_tags": [
                "ui/widgets/tag_widget.py",
                "ui/widgets/task_settings_widget.py"
              ],
              "get_checkin_settings": [
                "ui/dialogs/checkin_management_dialog.py",
                "ui/widgets/checkin_settings_widget.py"
              ],
              "get_command_definitions": [
                "communication/message_processing/interaction_manager.py",
                "communication/message_processing/message_router.py"
              ],
              "get_examples": [
                "communication/command_handlers/account_handler.py",
                "communication/command_handlers/analytics_handler.py",
                "communication/command_handlers/base_handler.py",
                "communication/command_handlers/checkin_handler.py",
                "communication/command_handlers/interaction_handlers.py",
                "communication/command_handlers/notebook_handler.py",
                "communication/command_handlers/profile_handler.py",
                "communication/command_handlers/schedule_handler.py",
                "communication/command_handlers/task_handler.py"
              ],
              "get_help": [
                "communication/command_handlers/account_handler.py",
                "communication/command_handlers/analytics_handler.py",
                "communication/command_handlers/base_handler.py",
                "communication/command_handlers/checkin_handler.py",
                "communication/command_handlers/interaction_handlers.py",
                "communication/command_handlers/notebook_handler.py",
                "communication/command_handlers/profile_handler.py",
                "communication/command_handlers/schedule_handler.py",
                "communication/command_handlers/task_handler.py"
              ],
              "get_recent_messages": [
                "ai/conversation_history.py",
                "core/message_management.py"
              ],
              "get_registered_channels": [
                "communication/core/channel_orchestrator.py",
                "communication/core/factory.py"
              ],
              "get_selected_categories": [
                "ui/dialogs/category_management_dialog.py",
                "ui/widgets/category_selection_widget.py"
              ],
              "get_selected_channel": [
                "ui/dialogs/channel_management_dialog.py",
                "ui/widgets/channel_selection_widget.py"
              ],
              "get_slash_command_map": [
                "communication/message_processing/interaction_manager.py",
                "communication/message_processing/message_router.py"
              ],
              "get_statistics": [
                "ai/conversation_history.py",
                "ui/dialogs/task_management_dialog.py",
                "ui/widgets/task_settings_widget.py"
              ],
              "get_timestamp_for_sorting": [
                "core/message_management.py",
                "core/response_tracking.py"
              ],
              "get_welcome_message": [
                "communication/communication_channels/discord/welcome_handler.py",
                "communication/core/welcome_manager.py"
              ],
              "handle": [
                "communication/command_handlers/account_handler.py",
                "communication/command_handlers/analytics_handler.py",
                "communication/command_handlers/base_handler.py",
                "communication/command_handlers/checkin_handler.py",
                "communication/command_handlers/interaction_handlers.py",
                "communication/command_handlers/notebook_handler.py",
                "communication/command_handlers/profile_handler.py",
                "communication/command_handlers/schedule_handler.py",
                "communication/command_handlers/task_handler.py"
              ],
              "handle_task_reminder": [
                "communication/core/channel_orchestrator.py",
                "core/scheduler.py"
              ],
              "has_been_welcomed": [
                "communication/communication_channels/discord/welcome_handler.py",
                "communication/core/welcome_manager.py"
              ],
              "info": [
                "core/file_auditor.py",
                "core/logger.py"
              ],
              "is_ready": [
                "ai/lm_studio_manager.py",
                "communication/communication_channels/base/base_channel.py"
              ],
              "is_schedule_period_active": [
                "core/schedule_management.py",
                "user/user_preferences.py"
              ],
              "is_service_running": [
                "core/service_utilities.py",
                "ui/ui_app_qt.py"
              ],
              "keyPressEvent": [
                "ui/dialogs/account_creator_dialog.py",
                "ui/dialogs/user_profile_dialog.py"
              ],
              "load_existing_data": [
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/widgets/checkin_settings_widget.py",
                "ui/widgets/task_settings_widget.py",
                "ui/widgets/user_profile_settings_widget.py"
              ],
              "main": [
                "core/headless_service.py",
                "core/service.py",
                "run_headless_service.py",
                "run_mhm.py",
                "run_tests.py",
                "ui/generate_ui_files.py",
                "ui/ui_app_qt.py"
              ],
              "mark_as_welcomed": [
                "communication/communication_channels/discord/welcome_handler.py",
                "communication/core/welcome_manager.py"
              ],
              "open_personalization_dialog": [
                "ui/dialogs/account_creator_dialog.py",
                "ui/dialogs/user_profile_dialog.py"
              ],
              "open_schedule_editor": [
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/ui_app_qt.py"
              ],
              "populate_timezones": [
                "ui/widgets/channel_selection_widget.py",
                "ui/widgets/user_profile_settings_widget.py"
              ],
              "refresh_tags": [
                "ui/widgets/tag_widget.py",
                "ui/widgets/task_settings_widget.py"
              ],
              "remove_period_row": [
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/widgets/checkin_settings_widget.py",
                "ui/widgets/task_settings_widget.py"
              ],
              "save_user_data": [
                "core/user_data_handlers.py",
                "user/user_context.py"
              ],
              "send_test_message": [
                "core/headless_service.py",
                "ui/ui_app_qt.py"
              ],
              "set_checkin_settings": [
                "ui/dialogs/checkin_management_dialog.py",
                "ui/widgets/checkin_settings_widget.py"
              ],
              "set_schedule_period_active": [
                "core/schedule_management.py",
                "user/user_preferences.py"
              ],
              "set_selected_categories": [
                "ui/dialogs/category_management_dialog.py",
                "ui/widgets/category_selection_widget.py"
              ],
              "set_selected_channel": [
                "ui/dialogs/channel_management_dialog.py",
                "ui/widgets/channel_selection_widget.py"
              ],
              "setup_connections": [
                "ui/dialogs/account_creator_dialog.py",
                "ui/dialogs/message_editor_dialog.py",
                "ui/dialogs/task_completion_dialog.py",
                "ui/dialogs/task_crud_dialog.py",
                "ui/dialogs/task_edit_dialog.py",
                "ui/dialogs/user_analytics_dialog.py",
                "ui/widgets/checkin_settings_widget.py",
                "ui/widgets/tag_widget.py",
                "ui/widgets/task_settings_widget.py"
              ],
              "setup_functionality": [
                "ui/dialogs/schedule_editor_dialog.py",
                "ui/widgets/period_row_widget.py"
              ],
              "setup_initial_state": [
                "ui/dialogs/message_editor_dialog.py",
                "ui/dialogs/user_analytics_dialog.py"
              ],
              "setup_ui": [
                "ui/dialogs/admin_panel.py",
                "ui/dialogs/message_editor_dialog.py",
                "ui/dialogs/process_watcher_dialog.py",
                "ui/dialogs/task_completion_dialog.py",
                "ui/dialogs/task_crud_dialog.py",
                "ui/dialogs/task_edit_dialog.py",
                "ui/dialogs/user_profile_dialog.py",
                "ui/widgets/tag_widget.py"
              ],
              "showEvent": [
                "ui/widgets/checkin_settings_widget.py",
                "ui/widgets/task_settings_widget.py"
              ],
              "signal_handler": [
                "core/service.py",
                "run_tests.py"
              ],
              "sort_key": [
                "core/schedule_management.py",
                "ui/dialogs/schedule_editor_dialog.py"
              ],
              "start": [
                "communication/communication_channels/discord/webhook_server.py",
                "core/file_auditor.py",
                "core/service.py"
              ],
              "stop": [
                "communication/communication_channels/discord/webhook_server.py",
                "core/file_auditor.py"
              ],
              "undo_last_tag_delete": [
                "ui/widgets/tag_widget.py",
                "ui/widgets/task_settings_widget.py"
              ],
              "validate_configuration": [
                "core/service.py",
                "ui/ui_app_qt.py"
              ],
              "warning": [
                "core/file_auditor.py",
                "core/logger.py"
              ]
            },
            "high_complexity": [
              {
                "complexity": 2625,
                "file": "communication/communication_channels/discord/bot.py",
                "has_docstring": true,
                "name": "initialize__register_events"
              },
              {
                "complexity": 2543,
                "file": "communication/message_processing/interaction_manager.py",
                "has_docstring": true,
                "name": "handle_message"
              },
              {
                "complexity": 2298,
                "file": "communication/message_processing/command_parser.py",
                "has_docstring": true,
                "name": "_extract_entities_rule_based"
              },
              {
                "complexity": 2208,
                "file": "ai/chatbot.py",
                "has_docstring": true,
                "name": "_create_comprehensive_context_prompt"
              },
              {
                "complexity": 2197,
                "file": "run_tests.py",
                "has_docstring": true,
                "name": "run_command"
              },
              {
                "complexity": 2099,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "get_user_data"
              },
              {
                "complexity": 1904,
                "file": "run_tests.py",
                "has_docstring": true,
                "name": "print_combined_summary"
              },
              {
                "complexity": 1565,
                "file": "ui/widgets/checkin_settings_widget.py",
                "has_docstring": true,
                "name": "_show_question_dialog"
              },
              {
                "complexity": 1477,
                "file": "run_tests.py",
                "has_docstring": true,
                "name": "main"
              },
              {
                "complexity": 1107,
                "file": "ai/chatbot.py",
                "has_docstring": true,
                "name": "_get_contextual_fallback"
              },
              {
                "complexity": 915,
                "file": "ui/widgets/checkin_settings_widget.py",
                "has_docstring": true,
                "name": "set_question_checkboxes"
              },
              {
                "complexity": 894,
                "file": "communication/core/channel_orchestrator.py",
                "has_docstring": true,
                "name": "_send_predefined_message"
              },
              {
                "complexity": 888,
                "file": "communication/command_handlers/profile_handler.py",
                "has_docstring": true,
                "name": "_format_profile_text"
              },
              {
                "complexity": 849,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_handle_task_reminder_followup"
              },
              {
                "complexity": 844,
                "file": "core/checkin_analytics.py",
                "has_docstring": true,
                "name": "get_quantitative_summaries"
              },
              {
                "complexity": 834,
                "file": "communication/command_handlers/profile_handler.py",
                "has_docstring": true,
                "name": "_handle_update_profile"
              },
              {
                "complexity": 813,
                "file": "communication/command_handlers/task_handler.py",
                "has_docstring": true,
                "name": "_handle_create_task"
              },
              {
                "complexity": 785,
                "file": "core/logger.py",
                "has_docstring": true,
                "name": "doRollover"
              },
              {
                "complexity": 781,
                "file": "ai/chatbot.py",
                "has_docstring": true,
                "name": "generate_response"
              },
              {
                "complexity": 744,
                "file": "core/service.py",
                "has_docstring": true,
                "name": "run_service_loop"
              },
              {
                "complexity": 735,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "refresh_user_list"
              },
              {
                "complexity": 715,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_parse_reminder_periods_from_text"
              },
              {
                "complexity": 709,
                "file": "core/logger.py",
                "has_docstring": true,
                "name": "__init__"
              },
              {
                "complexity": 708,
                "file": "communication/communication_channels/discord/webhook_handler.py",
                "has_docstring": true,
                "name": "handle_application_authorized"
              },
              {
                "complexity": 698,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_select_checkin_questions_with_weighting"
              },
              {
                "complexity": 696,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "validate_configuration"
              },
              {
                "complexity": 689,
                "file": "ui/widgets/user_profile_settings_widget.py",
                "has_docstring": true,
                "name": "__init__"
              },
              {
                "complexity": 686,
                "file": "ui/dialogs/account_creator_dialog.py",
                "has_docstring": true,
                "name": "validate_input"
              },
              {
                "complexity": 677,
                "file": "ui/widgets/user_profile_settings_widget.py",
                "has_docstring": true,
                "name": "load_existing_data"
              },
              {
                "complexity": 659,
                "file": "ui/dialogs/checkin_management_dialog.py",
                "has_docstring": true,
                "name": "save_checkin_settings"
              },
              {
                "complexity": 657,
                "file": "ai/chatbot.py",
                "has_docstring": true,
                "name": "generate_contextual_response"
              },
              {
                "complexity": 657,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "save_user_data"
              },
              {
                "complexity": 651,
                "file": "run_headless_service.py",
                "has_docstring": true,
                "name": "main"
              },
              {
                "complexity": 640,
                "file": "core/user_data_validation.py",
                "has_docstring": true,
                "name": "validate_user_update"
              },
              {
                "complexity": 618,
                "file": "ui/dialogs/task_edit_dialog.py",
                "has_docstring": true,
                "name": "render_reminder_period_row"
              },
              {
                "complexity": 615,
                "file": "communication/command_handlers/schedule_handler.py",
                "has_docstring": true,
                "name": "_handle_show_schedule"
              },
              {
                "complexity": 608,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_handle_note_body_flow"
              },
              {
                "complexity": 605,
                "file": "communication/command_handlers/schedule_handler.py",
                "has_docstring": true,
                "name": "_handle_edit_schedule_period"
              },
              {
                "complexity": 601,
                "file": "communication/message_processing/interaction_manager.py",
                "has_docstring": true,
                "name": "get_user_suggestions"
              },
              {
                "complexity": 591,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "stop_service"
              },
              {
                "complexity": 584,
                "file": "communication/message_processing/command_parser.py",
                "has_docstring": true,
                "name": "_rule_based_parse"
              },
              {
                "complexity": 583,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_handle_list_items_flow"
              },
              {
                "complexity": 580,
                "file": "communication/communication_channels/email/bot.py",
                "has_docstring": true,
                "name": "_receive_emails_sync"
              },
              {
                "complexity": 575,
                "file": "communication/command_handlers/task_handler.py",
                "has_docstring": true,
                "name": "_handle_create_task__parse_relative_date"
              },
              {
                "complexity": 571,
                "file": "communication/message_processing/command_parser.py",
                "has_docstring": true,
                "name": "__init__"
              },
              {
                "complexity": 563,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_handle_task_due_date_flow"
              },
              {
                "complexity": 563,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "system_health_check"
              },
              {
                "complexity": 562,
                "file": "communication/message_processing/command_parser.py",
                "has_docstring": true,
                "name": "parse"
              },
              {
                "complexity": 560,
                "file": "core/schedule_management.py",
                "has_docstring": true,
                "name": "get_schedule_time_periods"
              },
              {
                "complexity": 559,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "_save_user_data__merge_single_type"
              },
              {
                "complexity": 559,
                "file": "run_tests.py",
                "has_docstring": true,
                "name": "save_partial_results"
              },
              {
                "complexity": 557,
                "file": "communication/communication_channels/discord/webhook_server.py",
                "has_docstring": true,
                "name": "do_POST"
              },
              {
                "complexity": 544,
                "file": "tasks/task_management.py",
                "has_docstring": true,
                "name": "create_task"
              },
              {
                "complexity": 538,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_handle_checkin"
              },
              {
                "complexity": 537,
                "file": "ai/context_builder.py",
                "has_docstring": true,
                "name": "create_context_prompt"
              },
              {
                "complexity": 533,
                "file": "communication/command_handlers/profile_handler.py",
                "has_docstring": true,
                "name": "_handle_show_profile"
              },
              {
                "complexity": 523,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "create_new_user"
              },
              {
                "complexity": 522,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_parse_date_time_from_text"
              },
              {
                "complexity": 516,
                "file": "core/checkin_analytics.py",
                "has_docstring": true,
                "name": "get_sleep_analysis"
              },
              {
                "complexity": 515,
                "file": "ai/chatbot.py",
                "has_docstring": true,
                "name": "_extract_command_from_response"
              },
              {
                "complexity": 510,
                "file": "communication/command_handlers/account_handler.py",
                "has_docstring": true,
                "name": "_handle_link_account"
              },
              {
                "complexity": 510,
                "file": "core/user_data_manager.py",
                "has_docstring": true,
                "name": "update_user_index"
              },
              {
                "complexity": 499,
                "file": "core/logger.py",
                "has_docstring": true,
                "name": "get_component_logger"
              },
              {
                "complexity": 499,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "schedule_all_users_immediately"
              },
              {
                "complexity": 499,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "update_user_preferences"
              },
              {
                "complexity": 498,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_generate_context_aware_reminder_suggestions"
              },
              {
                "complexity": 495,
                "file": "communication/command_handlers/analytics_handler.py",
                "has_docstring": true,
                "name": "_handle_checkin_analysis"
              },
              {
                "complexity": 494,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "_check_discord_status"
              },
              {
                "complexity": 493,
                "file": "core/headless_service.py",
                "has_docstring": true,
                "name": "start_headless_service"
              },
              {
                "complexity": 491,
                "file": "core/user_data_manager.py",
                "has_docstring": true,
                "name": "rebuild_full_index"
              },
              {
                "complexity": 477,
                "file": "communication/core/channel_orchestrator.py",
                "has_docstring": true,
                "name": "handle_task_reminder"
              },
              {
                "complexity": 476,
                "file": "core/service.py",
                "has_docstring": true,
                "name": "start"
              },
              {
                "complexity": 475,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "connect_signals"
              },
              {
                "complexity": 474,
                "file": "communication/command_handlers/notebook_handler.py",
                "has_docstring": true,
                "name": "handle"
              },
              {
                "complexity": 463,
                "file": "core/checkin_dynamic_manager.py",
                "has_docstring": true,
                "name": "_parse_numerical_response"
              },
              {
                "complexity": 455,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "send_task_reminder"
              },
              {
                "complexity": 452,
                "file": "communication/communication_channels/discord/bot.py",
                "has_docstring": true,
                "name": "initialize__register_commands"
              },
              {
                "complexity": 448,
                "file": "communication/core/channel_orchestrator.py",
                "has_docstring": true,
                "name": "_email_polling_loop"
              },
              {
                "complexity": 448,
                "file": "core/user_data_validation.py",
                "has_docstring": true,
                "name": "validate_schedule_periods"
              },
              {
                "complexity": 443,
                "file": "core/message_management.py",
                "has_docstring": true,
                "name": "get_recent_messages"
              },
              {
                "complexity": 435,
                "file": "communication/command_handlers/analytics_handler.py",
                "has_docstring": true,
                "name": "_handle_task_analytics"
              },
              {
                "complexity": 435,
                "file": "core/checkin_dynamic_manager.py",
                "has_docstring": true,
                "name": "validate_answer"
              },
              {
                "complexity": 433,
                "file": "ui/dialogs/channel_management_dialog.py",
                "has_docstring": false,
                "name": "save_channel_settings"
              },
              {
                "complexity": 432,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "_save_user_data__validate_data"
              },
              {
                "complexity": 431,
                "file": "ui/dialogs/user_analytics_dialog.py",
                "has_docstring": true,
                "name": "load_mood_data"
              },
              {
                "complexity": 430,
                "file": "core/user_data_manager.py",
                "has_docstring": true,
                "name": "update_message_references"
              },
              {
                "complexity": 426,
                "file": "communication/message_processing/command_parser.py",
                "has_docstring": true,
                "name": "_ai_enhanced_parse"
              },
              {
                "complexity": 424,
                "file": "communication/communication_channels/discord/bot.py",
                "has_docstring": true,
                "name": "_start_ngrok_tunnel"
              },
              {
                "complexity": 424,
                "file": "tasks/task_management.py",
                "has_docstring": true,
                "name": "update_task"
              },
              {
                "complexity": 423,
                "file": "ui/dialogs/task_edit_dialog.py",
                "has_docstring": true,
                "name": "load_task_data"
              },
              {
                "complexity": 419,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "schedule_message_at_random_time"
              },
              {
                "complexity": 413,
                "file": "communication/core/channel_orchestrator.py",
                "has_docstring": true,
                "name": "send_message_sync"
              },
              {
                "complexity": 413,
                "file": "ui/dialogs/user_analytics_dialog.py",
                "has_docstring": true,
                "name": "load_overview_data"
              },
              {
                "complexity": 409,
                "file": "ui/dialogs/process_watcher_dialog.py",
                "has_docstring": true,
                "name": "show_process_details"
              },
              {
                "complexity": 407,
                "file": "communication/communication_channels/discord/task_reminder_view.py",
                "has_docstring": true,
                "name": "get_task_reminder_view"
              },
              {
                "complexity": 407,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "clear_all_accumulated_jobs"
              },
              {
                "complexity": 403,
                "file": "communication/communication_channels/discord/bot.py",
                "has_docstring": true,
                "name": "_create_discord_embed"
              },
              {
                "complexity": 401,
                "file": "core/checkin_analytics.py",
                "has_docstring": true,
                "name": "get_mood_trends"
              },
              {
                "complexity": 401,
                "file": "core/checkin_analytics.py",
                "has_docstring": true,
                "name": "get_energy_trends"
              },
              {
                "complexity": 401,
                "file": "run_tests.py",
                "has_docstring": true,
                "name": "interrupt_handler"
              },
              {
                "complexity": 400,
                "file": "ui/dialogs/category_management_dialog.py",
                "has_docstring": true,
                "name": "save_category_settings"
              },
              {
                "complexity": 399,
                "file": "ui/widgets/checkin_settings_widget.py",
                "has_docstring": true,
                "name": "_delete_custom_question"
              },
              {
                "complexity": 397,
                "file": "core/error_handling.py",
                "has_docstring": true,
                "name": "handle_error"
              },
              {
                "complexity": 396,
                "file": "communication/command_handlers/schedule_handler.py",
                "has_docstring": true,
                "name": "_handle_add_schedule_period"
              },
              {
                "complexity": 393,
                "file": "core/file_operations.py",
                "has_docstring": true,
                "name": "load_json_data"
              },
              {
                "complexity": 393,
                "file": "core/user_data_manager.py",
                "has_docstring": true,
                "name": "backup_user_data"
              },
              {
                "complexity": 392,
                "file": "communication/command_handlers/notebook_handler.py",
                "has_docstring": true,
                "name": "_handle_create_note"
              },
              {
                "complexity": 392,
                "file": "tasks/task_management.py",
                "has_docstring": true,
                "name": "_create_next_recurring_task_instance"
              },
              {
                "complexity": 389,
                "file": "core/user_data_manager.py",
                "has_docstring": true,
                "name": "export_user_data"
              },
              {
                "complexity": 388,
                "file": "ui/dialogs/channel_management_dialog.py",
                "has_docstring": true,
                "name": "__init__"
              },
              {
                "complexity": 387,
                "file": "core/message_analytics.py",
                "has_docstring": true,
                "name": "get_message_frequency"
              },
              {
                "complexity": 386,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "send_checkin_prompt"
              },
              {
                "complexity": 385,
                "file": "core/message_management.py",
                "has_docstring": true,
                "name": "archive_old_messages"
              },
              {
                "complexity": 384,
                "file": "tasks/task_management.py",
                "has_docstring": true,
                "name": "complete_task"
              },
              {
                "complexity": 383,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "get_random_time_within_period"
              },
              {
                "complexity": 383,
                "file": "ui/dialogs/process_watcher_dialog.py",
                "has_docstring": true,
                "name": "update_mhm_processes"
              },
              {
                "complexity": 382,
                "file": "ui/widgets/dynamic_list_field.py",
                "has_docstring": true,
                "name": "__init__"
              },
              {
                "complexity": 380,
                "file": "core/user_data_validation.py",
                "has_docstring": true,
                "name": "validate_new_user_data"
              },
              {
                "complexity": 379,
                "file": "communication/message_processing/interaction_manager.py",
                "has_docstring": false,
                "name": "__init__"
              },
              {
                "complexity": 378,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "handle_inbound_message"
              },
              {
                "complexity": 376,
                "file": "ui/dialogs/task_edit_dialog.py",
                "has_docstring": true,
                "name": "save_task"
              },
              {
                "complexity": 375,
                "file": "ui/dialogs/process_watcher_dialog.py",
                "has_docstring": true,
                "name": "update_all_processes"
              },
              {
                "complexity": 375,
                "file": "ui/dialogs/user_analytics_dialog.py",
                "has_docstring": true,
                "name": "load_sleep_data"
              },
              {
                "complexity": 373,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "cleanup_task_reminders"
              },
              {
                "complexity": 372,
                "file": "communication/communication_channels/discord/bot.py",
                "has_docstring": true,
                "name": "_check_dns_resolution"
              },
              {
                "complexity": 371,
                "file": "core/backup_manager.py",
                "has_docstring": true,
                "name": "create_backup"
              },
              {
                "complexity": 371,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "_check_email_status"
              },
              {
                "complexity": 371,
                "file": "ui/dialogs/checkin_management_dialog.py",
                "has_docstring": true,
                "name": "__init__"
              },
              {
                "complexity": 369,
                "file": "ui/widgets/checkin_settings_widget.py",
                "has_docstring": true,
                "name": "_validate_question_counts"
              },
              {
                "complexity": 368,
                "file": "communication/core/channel_orchestrator.py",
                "has_docstring": true,
                "name": "_initialize_channel_with_retry_sync"
              },
              {
                "complexity": 367,
                "file": "core/logger.py",
                "has_docstring": true,
                "name": "_get_log_paths_for_environment"
              },
              {
                "complexity": 366,
                "file": "ai/context_builder.py",
                "has_docstring": true,
                "name": "analyze_context"
              },
              {
                "complexity": 366,
                "file": "communication/command_handlers/task_handler.py",
                "has_docstring": true,
                "name": "_find_task_by_identifier"
              },
              {
                "complexity": 364,
                "file": "communication/command_handlers/analytics_handler.py",
                "has_docstring": true,
                "name": "_handle_task_stats"
              },
              {
                "complexity": 361,
                "file": "communication/__init__.py",
                "has_docstring": true,
                "name": "__getattr__"
              },
              {
                "complexity": 361,
                "file": "core/auto_cleanup.py",
                "has_docstring": true,
                "name": "cleanup_old_backup_files"
              },
              {
                "complexity": 361,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "schedule_message_for_period"
              },
              {
                "complexity": 361,
                "file": "ui/dialogs/task_management_dialog.py",
                "has_docstring": true,
                "name": "save_task_settings"
              },
              {
                "complexity": 361,
                "file": "ui/widgets/checkin_settings_widget.py",
                "has_docstring": true,
                "name": "undo_last_question_delete"
              },
              {
                "complexity": 360,
                "file": "communication/command_handlers/task_handler.py",
                "has_docstring": true,
                "name": "_handle_complete_task"
              },
              {
                "complexity": 360,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "_save_user_data__check_cross_file_invariants"
              },
              {
                "complexity": 360,
                "file": "ui/dialogs/account_creator_dialog.py",
                "has_docstring": true,
                "name": "setup_connections"
              },
              {
                "complexity": 359,
                "file": "communication/communication_channels/discord/checkin_view.py",
                "has_docstring": true,
                "name": "get_checkin_view"
              },
              {
                "complexity": 359,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "_get_user_data__load_account"
              },
              {
                "complexity": 359,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "view_all_users_summary"
              },
              {
                "complexity": 358,
                "file": "communication/communication_channels/discord/bot.py",
                "has_docstring": true,
                "name": "_stop_ngrok_tunnel"
              },
              {
                "complexity": 355,
                "file": "ai/chatbot.py",
                "has_docstring": true,
                "name": "_detect_mode"
              },
              {
                "complexity": 353,
                "file": "core/file_operations.py",
                "has_docstring": true,
                "name": "save_json_data"
              },
              {
                "complexity": 352,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "run_daily_scheduler"
              },
              {
                "complexity": 351,
                "file": "core/tags.py",
                "has_docstring": true,
                "name": "load_user_tags"
              },
              {
                "complexity": 345,
                "file": "core/config.py",
                "has_docstring": true,
                "name": "validate_logging_configuration"
              },
              {
                "complexity": 345,
                "file": "core/file_operations.py",
                "has_docstring": true,
                "name": "determine_file_path"
              },
              {
                "complexity": 345,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "schedule_all_task_reminders"
              },
              {
                "complexity": 342,
                "file": "core/message_management.py",
                "has_docstring": true,
                "name": "ensure_user_message_files"
              },
              {
                "complexity": 339,
                "file": "core/backup_manager.py",
                "has_docstring": true,
                "name": "_cleanup_old_backups"
              },
              {
                "complexity": 339,
                "file": "core/message_management.py",
                "has_docstring": true,
                "name": "create_message_file_from_defaults"
              },
              {
                "complexity": 338,
                "file": "communication/command_handlers/task_handler.py",
                "has_docstring": true,
                "name": "_handle_task_stats"
              },
              {
                "complexity": 337,
                "file": "ui/dialogs/account_creator_dialog.py",
                "has_docstring": true,
                "name": "_validate_and_accept__update_user_index"
              },
              {
                "complexity": 336,
                "file": "ui/dialogs/user_analytics_dialog.py",
                "has_docstring": true,
                "name": "load_habits_data"
              },
              {
                "complexity": 334,
                "file": "communication/command_handlers/task_handler.py",
                "has_docstring": true,
                "name": "_handle_delete_task"
              },
              {
                "complexity": 334,
                "file": "ui/dialogs/message_editor_dialog.py",
                "has_docstring": true,
                "name": "populate_table"
              },
              {
                "complexity": 332,
                "file": "ui/dialogs/task_edit_dialog.py",
                "has_docstring": true,
                "name": "collect_reminder_periods"
              },
              {
                "complexity": 330,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "schedule_daily_message_job"
              },
              {
                "complexity": 329,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "_get_user_data__load_preferences"
              },
              {
                "complexity": 329,
                "file": "run_tests.py",
                "has_docstring": true,
                "name": "parse_junit_xml"
              },
              {
                "complexity": 328,
                "file": "communication/communication_channels/email/bot.py",
                "has_docstring": true,
                "name": "_receive_emails_sync__extract_body"
              },
              {
                "complexity": 328,
                "file": "core/user_data_manager.py",
                "has_docstring": true,
                "name": "get_user_summary"
              },
              {
                "complexity": 326,
                "file": "core/user_data_validation.py",
                "has_docstring": true,
                "name": "validate_personalization_data"
              },
              {
                "complexity": 325,
                "file": "core/user_data_manager.py",
                "has_docstring": true,
                "name": "get_user_info_for_data_manager"
              },
              {
                "complexity": 324,
                "file": "ui/widgets/tag_widget.py",
                "has_docstring": true,
                "name": "edit_tag"
              },
              {
                "complexity": 322,
                "file": "communication/message_processing/interaction_manager.py",
                "has_docstring": true,
                "name": "_enhance_response_with_ai"
              },
              {
                "complexity": 318,
                "file": "core/error_handling.py",
                "has_docstring": true,
                "name": "handle_errors"
              },
              {
                "complexity": 317,
                "file": "core/logger.py",
                "has_docstring": true,
                "name": "get_log_file_info"
              },
              {
                "complexity": 314,
                "file": "core/service_utilities.py",
                "has_docstring": true,
                "name": "get_service_processes"
              },
              {
                "complexity": 314,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "_get_user_data__load_schedules"
              },
              {
                "complexity": 314,
                "file": "ui/dialogs/task_crud_dialog.py",
                "has_docstring": true,
                "name": "refresh_active_tasks"
              },
              {
                "complexity": 313,
                "file": "core/logger.py",
                "has_docstring": true,
                "name": "compress_old_logs"
              },
              {
                "complexity": 313,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "cleanup_orphaned_task_reminders"
              },
              {
                "complexity": 313,
                "file": "ui/widgets/dynamic_list_container.py",
                "has_docstring": true,
                "name": "__init__"
              },
              {
                "complexity": 312,
                "file": "communication/communication_channels/discord/bot.py",
                "has_docstring": true,
                "name": "_check_network_connectivity"
              },
              {
                "complexity": 311,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "_get_user_data__load_context"
              },
              {
                "complexity": 310,
                "file": "ui/dialogs/user_profile_dialog.py",
                "has_docstring": true,
                "name": "add_loved_one_widget"
              },
              {
                "complexity": 309,
                "file": "ai/chatbot.py",
                "has_docstring": true,
                "name": "_clean_system_prompt_leaks"
              },
              {
                "complexity": 308,
                "file": "communication/command_handlers/account_handler.py",
                "has_docstring": true,
                "name": "_handle_create_account"
              },
              {
                "complexity": 308,
                "file": "core/user_data_manager.py",
                "has_docstring": true,
                "name": "remove_from_index"
              },
              {
                "complexity": 307,
                "file": "communication/command_handlers/task_handler.py",
                "has_docstring": true,
                "name": "_handle_update_task"
              },
              {
                "complexity": 307,
                "file": "ui/dialogs/user_profile_dialog.py",
                "has_docstring": true,
                "name": "create_custom_field_list"
              },
              {
                "complexity": 307,
                "file": "ui/widgets/period_row_widget.py",
                "has_docstring": true,
                "name": "__init__"
              },
              {
                "complexity": 304,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "schedule_checkin_at_exact_time"
              },
              {
                "complexity": 304,
                "file": "core/user_data_manager.py",
                "has_docstring": true,
                "name": "delete_user_completely"
              },
              {
                "complexity": 303,
                "file": "core/user_data_handlers.py",
                "has_docstring": true,
                "name": "ensure_category_has_default_schedule"
              },
              {
                "complexity": 302,
                "file": "communication/command_handlers/analytics_handler.py",
                "has_docstring": true,
                "name": "_handle_quant_summary"
              },
              {
                "complexity": 302,
                "file": "communication/command_handlers/interaction_handlers.py",
                "has_docstring": true,
                "name": "get_interaction_handler"
              },
              {
                "complexity": 302,
                "file": "communication/command_handlers/interaction_handlers.py",
                "has_docstring": true,
                "name": "get_all_handlers"
              },
              {
                "complexity": 301,
                "file": "communication/message_processing/conversation_flow_manager.py",
                "has_docstring": true,
                "name": "_handle_command_during_checkin"
              },
              {
                "complexity": 301,
                "file": "ui/ui_app_qt.py",
                "has_docstring": true,
                "name": "view_cache_status"
              },
              {
                "complexity": 298,
                "file": "ui/dialogs/task_management_dialog.py",
                "has_docstring": true,
                "name": "__init__"
              },
              {
                "complexity": 298,
                "file": "ui/widgets/tag_widget.py",
                "has_docstring": true,
                "name": "delete_tag"
              },
              {
                "complexity": 297,
                "file": "core/scheduler.py",
                "has_docstring": true,
                "name": "schedule_task_reminder_at_time"
              },
              {
                "complexity": 295,
                "file": "communication/core/retry_manager.py",
                "has_docstring": true,
                "name": "_process_retry_queue"
              }
            ],
            "high_complexity_total": 1100,
            "undocumented_handlers": [
              {
                "file": "communication/communication_channels/email/bot.py",
                "name": "_get_email_config"
              },
              {
                "file": "communication/core/channel_orchestrator.py",
                "name": "create_view"
              },
              {
                "file": "communication/core/channel_orchestrator.py",
                "name": "create_view"
              },
              {
                "file": "core/schemas.py",
                "name": "_validate_email"
              },
              {
                "file": "core/schemas.py",
                "name": "_validate_timezone"
              },
              {
                "file": "core/schemas.py",
                "name": "validate_account_dict"
              },
              {
                "file": "core/schemas.py",
                "name": "validate_messages_file_dict"
              },
              {
                "file": "core/schemas.py",
                "name": "validate_preferences_dict"
              },
              {
                "file": "core/schemas.py",
                "name": "validate_schedules_dict"
              },
              {
                "file": "core/user_data_handlers.py",
                "name": "_ensure_default_loaders_once"
              },
              {
                "file": "core/user_data_validation.py",
                "name": "validate_schedule_periods__validate_time_format"
              },
              {
                "file": "run_tests.py",
                "name": "signal_handler"
              },
              {
                "file": "ui/dialogs/account_creator_dialog.py",
                "name": "on_personalization_save"
              },
              {
                "file": "ui/dialogs/category_management_dialog.py",
                "name": "get_selected_categories"
              },
              {
                "file": "ui/dialogs/category_management_dialog.py",
                "name": "set_selected_categories"
              },
              {
                "file": "ui/dialogs/channel_management_dialog.py",
                "name": "get_selected_channel"
              },
              {
                "file": "ui/dialogs/channel_management_dialog.py",
                "name": "load_user_channel_data"
              },
              {
                "file": "ui/dialogs/channel_management_dialog.py",
                "name": "save_channel_settings"
              },
              {
                "file": "ui/dialogs/channel_management_dialog.py",
                "name": "set_selected_channel"
              },
              {
                "file": "ui/dialogs/checkin_management_dialog.py",
                "name": "on_enable_checkins_toggled"
              },
              {
                "file": "ui/dialogs/task_management_dialog.py",
                "name": "get_statistics"
              },
              {
                "file": "ui/dialogs/task_management_dialog.py",
                "name": "on_enable_task_management_toggled"
              },
              {
                "file": "ui/ui_app_qt.py",
                "name": "on_save"
              },
              {
                "file": "ui/widgets/category_selection_widget.py",
                "name": "get_selected_categories"
              },
              {
                "file": "ui/widgets/category_selection_widget.py",
                "name": "set_selected_categories"
              },
              {
                "file": "ui/widgets/channel_selection_widget.py",
                "name": "get_selected_channel"
              },
              {
                "file": "ui/widgets/channel_selection_widget.py",
                "name": "set_contact_info"
              },
              {
                "file": "ui/widgets/channel_selection_widget.py",
                "name": "set_selected_channel"
              },
              {
                "file": "ui/widgets/dynamic_list_container.py",
                "name": "_on_preset_toggled"
              },
              {
                "file": "ui/widgets/dynamic_list_container.py",
                "name": "_on_row_deleted"
              },
              {
                "file": "ui/widgets/dynamic_list_container.py",
                "name": "get_values"
              },
              {
                "file": "ui/widgets/dynamic_list_container.py",
                "name": "set_values"
              },
              {
                "file": "ui/widgets/dynamic_list_field.py",
                "name": "_on_delete"
              },
              {
                "file": "ui/widgets/dynamic_list_field.py",
                "name": "get_text"
              },
              {
                "file": "ui/widgets/dynamic_list_field.py",
                "name": "is_checked"
              },
              {
                "file": "ui/widgets/dynamic_list_field.py",
                "name": "set_checked"
              },
              {
                "file": "ui/widgets/dynamic_list_field.py",
                "name": "set_text"
              },
              {
                "file": "ui/widgets/task_settings_widget.py",
                "name": "load_existing_data"
              }
            ],
            "undocumented_handlers_total": 38,
            "undocumented_other": [
              {
                "file": "communication/message_processing/conversation_flow_manager.py",
                "name": "start_profile_flow"
              },
              {
                "file": "communication/message_processing/conversation_flow_manager.py",
                "name": "start_schedule_flow"
              },
              {
                "file": "communication/message_processing/interaction_manager.py",
                "name": "__init__"
              },
              {
                "file": "communication/message_processing/interaction_manager.py",
                "name": "_augment_suggestions"
              },
              {
                "file": "core/error_handling.py",
                "name": "decorator"
              },
              {
                "file": "core/error_handling.py",
                "name": "wrapper"
              },
              {
                "file": "core/file_auditor.py",
                "name": "__init__"
              },
              {
                "file": "core/file_auditor.py",
                "name": "_classify_path"
              },
              {
                "file": "core/file_auditor.py",
                "name": "_split_env_list"
              },
              {
                "file": "core/file_auditor.py",
                "name": "start_auditor"
              },
              {
                "file": "core/file_auditor.py",
                "name": "stop_auditor"
              },
              {
                "file": "core/schedule_management.py",
                "name": "add_schedule_period"
              },
              {
                "file": "core/schedule_management.py",
                "name": "edit_schedule_period"
              },
              {
                "file": "core/scheduler.py",
                "name": "scheduler_loop"
              },
              {
                "file": "core/schemas.py",
                "name": "_coerce_bool"
              },
              {
                "file": "core/schemas.py",
                "name": "_normalize_contact"
              },
              {
                "file": "core/schemas.py",
                "name": "_valid_days"
              },
              {
                "file": "core/schemas.py",
                "name": "_valid_time"
              },
              {
                "file": "core/schemas.py",
                "name": "to_dict"
              },
              {
                "file": "core/user_data_validation.py",
                "name": "is_valid_email"
              },
              {
                "file": "core/user_data_validation.py",
                "name": "is_valid_phone"
              },
              {
                "file": "ui/ui_app_qt.py",
                "name": "cleanup_old_requests"
              },
              {
                "file": "ui/widgets/checkin_settings_widget.py",
                "name": "on_template_selected"
              },
              {
                "file": "ui/widgets/dynamic_list_container.py",
                "name": "_add_blank_row"
              },
              {
                "file": "ui/widgets/dynamic_list_container.py",
                "name": "_deduplicate_values"
              },
              {
                "file": "ui/widgets/dynamic_list_container.py",
                "name": "_ensure_single_blank_row"
              },
              {
                "file": "ui/widgets/dynamic_list_container.py",
                "name": "_first_blank_index"
              },
              {
                "file": "ui/widgets/dynamic_list_container.py",
                "name": "_on_row_edited"
              },
              {
                "file": "ui/widgets/dynamic_list_field.py",
                "name": "is_blank"
              }
            ],
            "undocumented_other_total": 29
          },
          "coverage": 93.98,
          "errors": [],
          "extra": {
            "count": 0,
            "files": {}
          },
          "missing": {
            "count": 20,
            "files": {
              "run_tests.py": [
                "check_critical_resources",
                "check_resource_warnings",
                "cleanup_orphaned_pytest_processes",
                "detect_stuck_process",
                "extract_failures_from_junit_xml",
                "extract_pytest_session_info",
                "extract_results_from_output",
                "interrupt_handler",
                "kill_process_tree_windows",
                "main",
                "monitor_resources",
                "parse_junit_xml",
                "print_combined_summary",
                "print_test_mode_info",
                "read_output",
                "run_command",
                "run_static_logging_check",
                "save_partial_results",
                "setup_test_logger",
                "signal_handler"
              ]
            },
            "missing_files": [
              "run_tests.py"
            ]
          },
          "registry_sections": {
            "run_tests.py": {
              "classes": [],
              "functions": [
                {
                  "args": [
                    "pid"
                  ],
                  "docstring": "Kill a process and all its children on Windows.\n\n    Returns:\n        bool: True if taskkill succeeded, False otherwise",
                  "name": "kill_process_tree_windows"
                },
                {
                  "args": [],
                  "docstring": "Find and kill any orphaned pytest worker processes on Windows.\n\n    Returns:\n        int: Number of orphaned processes found and killed",
                  "name": "cleanup_orphaned_pytest_processes"
                },
                {
                  "args": [
                    "signum",
                    "frame"
                  ],
                  "docstring": "Handle interrupt signals (Ctrl+C) gracefully.",
                  "name": "interrupt_handler"
                },
                {
                  "args": [],
                  "docstring": "Monitor system resource usage and return metrics.",
                  "name": "monitor_resources"
                },
                {
                  "args": [
                    "resources"
                  ],
                  "docstring": "Check if resources exceed warning thresholds.",
                  "name": "check_resource_warnings"
                },
                {
                  "args": [
                    "resources"
                  ],
                  "docstring": "Check if resources exceed critical thresholds requiring termination.",
                  "name": "check_critical_resources"
                },
                {
                  "args": [
                    "output_text"
                  ],
                  "docstring": "Extract test results from pytest output text when JUnit XML is unavailable.",
                  "name": "extract_results_from_output"
                },
                {
                  "args": [
                    "output_text"
                  ],
                  "docstring": "Extract pytest session information from output text.",
                  "name": "extract_pytest_session_info"
                },
                {
                  "args": [
                    "junit_xml_path",
                    "interrupted",
                    "output_text",
                    "test_context"
                  ],
                  "docstring": "Save partial test results from JUnit XML, falling back to output text parsing.",
                  "name": "save_partial_results"
                },
                {
                  "args": [
                    "last_output_time",
                    "current_time",
                    "threshold"
                  ],
                  "docstring": "Detect if process appears stuck (no output for extended period).",
                  "name": "detect_stuck_process"
                },
                {
                  "args": [
                    "xml_path"
                  ],
                  "docstring": "Extract detailed failure information from JUnit XML.\n\n    Returns a list of dicts with 'test', 'message', and 'type' keys.",
                  "name": "extract_failures_from_junit_xml"
                },
                {
                  "args": [
                    "xml_path"
                  ],
                  "docstring": "Parse JUnit XML report to extract test statistics.\n\n    Returns a dictionary with: passed, failed, skipped, warnings, errors, total",
                  "name": "parse_junit_xml"
                },
                {
                  "args": [
                    "cmd",
                    "description",
                    "progress_interval",
                    "capture_output",
                    "test_context"
                  ],
                  "docstring": "Run a command and return results with periodic progress logs.\n\n    Args:\n        cmd: Command to run\n        description: Description for progress messages\n        progress_interval: Seconds between progress updates\n        capture_output: If True, capture results via JUnit XML (always True in practice)\n        test_context: Optional dict with test run context (mode, phase, config, etc.)\n\n    Returns:\n        dict with 'success', 'output', 'results', 'duration', 'warnings', 'failures' keys",
                  "name": "run_command"
                },
                {
                  "args": [],
                  "docstring": "Set up logger for test duration logging.\n\n    Creates a logger for test run duration logging and ensures the tests/logs\n    directory exists. Returns a configured logger instance.\n\n    Returns:\n        logging.Logger: Configured logger instance for test runs",
                  "name": "setup_test_logger"
                },
                {
                  "args": [
                    "parallel_results",
                    "no_parallel_results",
                    "description"
                  ],
                  "docstring": "Print a combined summary of test results from both parallel and serial runs.\n\n    Args:\n        parallel_results: Results dict from parallel test run (or None if not run)\n        no_parallel_results: Results dict from serial test run (or None if not run)\n        description: Test mode description",
                  "name": "print_combined_summary"
                },
                {
                  "args": [],
                  "docstring": "Print helpful information about test modes.",
                  "name": "print_test_mode_info"
                },
                {
                  "args": [],
                  "docstring": "Run the static logging enforcement script before executing tests.",
                  "name": "run_static_logging_check"
                },
                {
                  "args": [],
                  "docstring": "Main entry point for MHM test runner.\n\n    Parses command-line arguments and executes pytest with appropriate configuration\n    based on the selected test mode (all, fast, unit, integration, behavior, ui, slow).\n\n    Returns:\n        int: Exit code (0 for success, 1 for failure)",
                  "name": "main"
                },
                {
                  "args": [
                    "signum",
                    "frame"
                  ],
                  "docstring": "",
                  "name": "signal_handler"
                },
                {
                  "args": [
                    "pipe",
                    "queue_obj"
                  ],
                  "docstring": "Read from pipe and put lines in queue, also write to terminal.",
                  "name": "read_output"
                }
              ]
            }
          },
          "totals": {
            "classes_found": 154,
            "files_scanned": 110,
            "functions_documented": 1436,
            "functions_found": 1528
          }
        },
        "summary": {
          "files_affected": 0,
          "total_issues": 20
        }
      },
      "timestamp": "2026-01-18T01:46:36"
    },
    "analyze_package_exports": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 173,
          "files_affected": 2
        },
        "details": {
          "total_missing_exports": 153,
          "total_unnecessary_exports": 20,
          "packages_with_missing": 2,
          "packages": {
            "core": {
              "package": "core",
              "current_exports_count": 178,
              "should_export_count": 285,
              "already_exported": [
                "AIError",
                "AccountModel",
                "BackupDirectoryRotatingFileHandler",
                "BackupManager",
                "CONTEXT_CACHE_TTL",
                "CategoryScheduleModel",
                "ChannelModel",
                "CheckinAnalytics",
                "CommunicationError",
                "ComponentLogger",
                "ConfigValidationError",
                "ConfigurationError",
                "ConfigurationRecovery",
                "DISCORD_APPLICATION_ID",
                "DISCORD_BOT_TOKEN",
                "DataError",
                "DynamicCheckinManager",
                "EMAIL_IMAP_SERVER",
                "EMAIL_SMTP_PASSWORD",
                "EMAIL_SMTP_SERVER",
                "EMAIL_SMTP_USERNAME",
                "ErrorHandler",
                "ErrorRecoveryStrategy",
                "ExcludeLoggerNamesFilter",
                "FeaturesModel",
                "FileAuditor",
                "FileOperationError",
                "HeadlessServiceManager",
                "InitializationError",
                "InvalidTimeFormatError",
                "LM_STUDIO_API_KEY",
                "LM_STUDIO_BASE_URL",
                "LM_STUDIO_MODEL",
                "MHMError",
                "MHMService",
                "MessageModel",
                "MessagesFileModel",
                "PeriodModel",
                "PreferencesModel",
                "PytestContextLogFormatter",
                "RecoveryError",
                "SCHEDULER_INTERVAL",
                "SchedulerError",
                "SchedulerManager",
                "SchedulesModel",
                "Throttler",
                "UserDataManager",
                "UserInterfaceError",
                "ValidationError",
                "add_schedule_period",
                "archive_old_messages_for_all_users",
                "auto_cleanup_if_needed",
                "backup_manager",
                "backup_user_data",
                "build_user_index",
                "cleanup_data_directory",
                "cleanup_tests_data_directory",
                "clear_user_caches",
                "collect_period_data_from_widgets",
                "create_default_schedule_periods",
                "create_reschedule_request",
                "create_user_files",
                "delete_user_completely",
                "determine_file_path",
                "dynamic_checkin_manager",
                "ensure_all_categories_have_schedules",
                "ensure_category_has_default_schedule",
                "ensure_user_directory",
                "export_user_data",
                "get_active_schedules",
                "get_all_user_ids",
                "get_all_user_summaries",
                "get_available_channels",
                "get_available_data_types",
                "get_backups_dir",
                "get_channel_class_mapping",
                "get_cleanup_status",
                "get_component_logger",
                "get_current_active_schedules",
                "get_data_type_info",
                "get_last_cleanup_timestamp",
                "get_logger",
                "get_message_categories",
                "get_recent_checkins",
                "get_recent_messages",
                "get_recent_responses",
                "get_service_processes",
                "get_user_analytics_summary",
                "get_user_categories",
                "get_user_data",
                "get_user_data_dir",
                "get_user_data_summary",
                "get_user_file_path",
                "get_user_id_by_identifier",
                "get_user_info_for_data_manager",
                "get_user_summary",
                "get_verbose_mode",
                "handle_ai_error",
                "handle_errors",
                "handle_network_error",
                "is_headless_service_running",
                "is_schedule_active",
                "is_service_running",
                "is_ui_service_running",
                "is_user_checkins_enabled",
                "is_valid_email",
                "load_and_localize_datetime",
                "load_json_data",
                "load_period_widgets_for_category",
                "load_user_messages",
                "migrate_legacy_schedules_structure",
                "perform_cleanup",
                "print_configuration_report",
                "rebuild_user_index",
                "record_created",
                "register_data_loader",
                "register_default_loaders",
                "save_json_data",
                "save_user_data",
                "save_user_data_transaction",
                "set_console_log_level",
                "set_verbose_mode",
                "setup_logging",
                "setup_third_party_error_logging",
                "should_run_cleanup",
                "start_auditor",
                "stop_auditor",
                "store_chat_interaction",
                "store_sent_message",
                "suppress_noisy_logging",
                "toggle_verbose_logging",
                "update_channel_preferences",
                "update_cleanup_timestamp",
                "update_message_references",
                "update_user_account",
                "update_user_context",
                "update_user_index",
                "update_user_preferences",
                "update_user_schedules",
                "validate_account_dict",
                "validate_ai_configuration",
                "validate_all_configuration",
                "validate_and_raise_if_invalid",
                "validate_communication_channels",
                "validate_core_paths",
                "validate_discord_config",
                "validate_email_config",
                "validate_environment_variables",
                "validate_file_organization_settings",
                "validate_logging_configuration",
                "validate_messages_file_dict",
                "validate_minimum_config",
                "validate_preferences_dict",
                "validate_schedule_periods",
                "validate_scheduler_configuration",
                "validate_schedules_dict",
                "verify_file_access",
                "wait_for_network"
              ],
              "missing_exports": [
                "BotInitializationError",
                "ChannelFactory",
                "ChannelMonitor",
                "CommunicationManager",
                "DATE_ONLY",
                "DISCORD_AUTO_NGROK",
                "DISCORD_PUBLIC_KEY",
                "DISCORD_WEBHOOK_PORT",
                "FileNotFoundRecovery",
                "HeartbeatWarningFilter",
                "JSONDecodeRecovery",
                "MessageAnalytics",
                "MessageSendError",
                "NetworkRecovery",
                "QueuedMessage",
                "RetryManager",
                "TIMESTAMP_FULL",
                "TIMESTAMP_MINUTE",
                "TIMEZONE_OPTIONS",
                "TIME_ONLY_MINUTE",
                "add_message",
                "add_period_widget_to_layout",
                "add_user_tag",
                "apply_test_context_formatter_to_all_loggers",
                "archive_old_messages",
                "calculate_cache_size",
                "cleanup_old_archives",
                "cleanup_old_backup_files",
                "cleanup_old_logs",
                "cleanup_old_message_archives",
                "cleanup_old_request_files",
                "clear_all_accumulated_jobs_standalone",
                "clear_log_file_locks",
                "clear_period_widgets_from_layout",
                "clear_schedule_periods_cache",
                "compress_old_logs",
                "create_automatic_backup",
                "create_message_file_from_defaults",
                "create_new_user",
                "current_base_dir",
                "delete_message",
                "delete_schedule_period",
                "disable_module_logging",
                "edit_message",
                "edit_schedule_period",
                "ensure_logs_directory",
                "ensure_tags_initialized",
                "ensure_unique_ids",
                "ensure_user_dir_for_tags",
                "ensure_user_message_files",
                "file_lock",
                "find_pyc_files",
                "find_pycache_dirs",
                "force_restart_logging",
                "format_timestamp",
                "format_timestamp_milliseconds",
                "get_checkins_by_days",
                "get_current_day_names",
                "get_current_time_periods_with_validation",
                "get_flags_dir",
                "get_log_file_info",
                "get_log_level_from_env",
                "get_period_data__time_12h_display_to_24h",
                "get_period_data__time_24h_to_12h_display",
                "get_period_data__validate_and_format_time",
                "get_predefined_options",
                "get_recent_chat_interactions",
                "get_schedule_days",
                "get_schedule_time_periods",
                "get_scheduler_manager",
                "get_timestamp_for_sorting",
                "get_timezone_options",
                "get_user_data_with_metadata",
                "get_user_info_for_schedule_management",
                "get_user_info_for_tracking",
                "get_user_tags",
                "handle_communication_error",
                "handle_configuration_error",
                "handle_file_error",
                "handle_validation_error",
                "is_schedule_period_active",
                "is_valid_category_name",
                "is_valid_discord_id",
                "is_valid_phone",
                "is_valid_string_length",
                "load_and_ensure_ids",
                "load_default_messages",
                "load_user_tags",
                "main",
                "normalize_tag",
                "normalize_tags",
                "now_timestamp_filename",
                "now_timestamp_full",
                "now_timestamp_minute",
                "parse_date_and_time_minute",
                "parse_date_only",
                "parse_tags_from_text",
                "parse_time_only_minute",
                "parse_timestamp",
                "parse_timestamp_full",
                "parse_timestamp_minute",
                "perform_safe_operation",
                "period_name_for_display",
                "period_name_for_storage",
                "process_category_schedule",
                "process_user_schedules",
                "remove_user_tag",
                "run_category_scheduler_standalone",
                "run_full_scheduler_standalone",
                "run_user_scheduler_standalone",
                "safe_file_operation",
                "safe_json_read",
                "safe_json_write",
                "save_user_tags",
                "schedule_all_task_reminders",
                "set_schedule_days",
                "set_schedule_period_active",
                "set_schedule_periods",
                "store_user_response",
                "track_user_response",
                "update_message",
                "validate_new_user_data",
                "validate_personalization_data",
                "validate_schedule_periods__validate_time_format",
                "validate_system_state",
                "validate_tag",
                "validate_user_update"
              ],
              "potentially_unnecessary": [
                "auto_cleanup",
                "checkin_analytics",
                "checkin_dynamic_manager",
                "config",
                "error_handling",
                "file_auditor",
                "file_operations",
                "headless_service",
                "logger",
                "message_management",
                "response_tracking",
                "schedule_management",
                "schedule_utilities",
                "scheduler",
                "service",
                "service_utilities",
                "ui_management",
                "user_data_handlers",
                "user_data_manager",
                "user_data_validation"
              ],
              "cross_module_usage": {
                "setup_logging": {
                  "import_count": 19,
                  "import_locations": [
                    "run_headless_service.py",
                    "core/auto_cleanup.py",
                    "core/service.py",
                    "ui/ui_app_qt.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/message_editor_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_completion_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_edit_dialog.py",
                    "ui/dialogs/task_management_dialog.py",
                    "ui/dialogs/user_analytics_dialog.py",
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/period_row_widget.py",
                    "ui/widgets/tag_widget.py",
                    "ui/widgets/task_settings_widget.py",
                    "ui/widgets/user_profile_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "get_component_logger": {
                  "import_count": 122,
                  "import_locations": [
                    "run_headless_service.py",
                    "ai/cache_manager.py",
                    "ai/chatbot.py",
                    "ai/context_builder.py",
                    "ai/conversation_history.py",
                    "ai/lm_studio_manager.py",
                    "ai/prompt_manager.py",
                    "core/auto_cleanup.py",
                    "core/backup_manager.py",
                    "core/checkin_analytics.py",
                    "core/checkin_dynamic_manager.py",
                    "core/config.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/file_auditor.py",
                    "core/file_locking.py",
                    "core/file_operations.py",
                    "core/headless_service.py",
                    "core/message_analytics.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "core/schedule_utilities.py",
                    "core/schemas.py",
                    "core/service.py",
                    "core/service_utilities.py",
                    "core/tags.py",
                    "core/ui_management.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_validation.py",
                    "notebook/notebook_data_handlers.py",
                    "notebook/notebook_data_manager.py",
                    "notebook/notebook_validation.py",
                    "notebook/schemas.py",
                    "tasks/task_management.py",
                    "ui/ui_app_qt.py",
                    "user/context_manager.py",
                    "user/user_context.py",
                    "user/user_preferences.py",
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/base_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/core/channel_monitor.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/core/retry_manager.py",
                    "communication/core/welcome_manager.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/message_router.py",
                    "communication/communication_channels/base/base_channel.py",
                    "communication/communication_channels/base/command_registry.py",
                    "communication/communication_channels/base/message_formatter.py",
                    "communication/communication_channels/base/rich_formatter.py",
                    "communication/communication_channels/discord/account_flow_handler.py",
                    "communication/communication_channels/discord/api_client.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/task_reminder_view.py",
                    "communication/communication_channels/discord/webhook_handler.py",
                    "communication/communication_channels/discord/webhook_server.py",
                    "communication/communication_channels/discord/welcome_handler.py",
                    "communication/communication_channels/email/bot.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/admin_panel.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/channel_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/message_editor_dialog.py",
                    "ui/dialogs/process_watcher_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_completion_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_edit_dialog.py",
                    "ui/dialogs/task_management_dialog.py",
                    "ui/dialogs/user_analytics_dialog.py",
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/category_selection_widget.py",
                    "ui/widgets/channel_selection_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/period_row_widget.py",
                    "ui/widgets/tag_widget.py",
                    "ui/widgets/task_settings_widget.py",
                    "ui/widgets/user_profile_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "handle_errors": {
                  "import_count": 95,
                  "import_locations": [
                    "run_headless_service.py",
                    "ai/cache_manager.py",
                    "ai/chatbot.py",
                    "ai/context_builder.py",
                    "ai/conversation_history.py",
                    "ai/lm_studio_manager.py",
                    "ai/prompt_manager.py",
                    "core/auto_cleanup.py",
                    "core/backup_manager.py",
                    "core/checkin_analytics.py",
                    "core/checkin_dynamic_manager.py",
                    "core/config.py",
                    "core/file_auditor.py",
                    "core/file_locking.py",
                    "core/file_operations.py",
                    "core/headless_service.py",
                    "core/logger.py",
                    "core/message_analytics.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "core/schedule_utilities.py",
                    "core/schemas.py",
                    "core/service.py",
                    "core/service_utilities.py",
                    "core/tags.py",
                    "core/ui_management.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_validation.py",
                    "notebook/notebook_data_handlers.py",
                    "notebook/notebook_data_manager.py",
                    "notebook/notebook_validation.py",
                    "tasks/task_management.py",
                    "ui/generate_ui_files.py",
                    "ui/ui_app_qt.py",
                    "user/context_manager.py",
                    "user/user_context.py",
                    "user/user_preferences.py",
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/base_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/core/channel_monitor.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/core/retry_manager.py",
                    "communication/core/welcome_manager.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/message_router.py",
                    "communication/communication_channels/base/base_channel.py",
                    "communication/communication_channels/base/command_registry.py",
                    "communication/communication_channels/base/message_formatter.py",
                    "communication/communication_channels/base/rich_formatter.py",
                    "communication/communication_channels/discord/account_flow_handler.py",
                    "communication/communication_channels/discord/api_client.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/task_reminder_view.py",
                    "communication/communication_channels/discord/webhook_handler.py",
                    "communication/communication_channels/discord/webhook_server.py",
                    "communication/communication_channels/discord/welcome_handler.py",
                    "communication/communication_channels/email/bot.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/admin_panel.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/channel_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/message_editor_dialog.py",
                    "ui/dialogs/process_watcher_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_completion_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_edit_dialog.py",
                    "ui/dialogs/task_management_dialog.py",
                    "ui/dialogs/user_analytics_dialog.py",
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/category_selection_widget.py",
                    "ui/widgets/channel_selection_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/period_row_widget.py",
                    "ui/widgets/tag_widget.py",
                    "ui/widgets/task_settings_widget.py",
                    "ui/widgets/user_profile_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "AI_CACHE_RESPONSES": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/cache_manager.py",
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LM_STUDIO_BASE_URL": {
                  "import_count": 3,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/lm_studio_manager.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LM_STUDIO_API_KEY": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/lm_studio_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LM_STUDIO_MODEL": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/lm_studio_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_TIMEOUT_SECONDS": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_SYSTEM_PROMPT_PATH": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/prompt_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_USE_CUSTOM_PROMPT": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/prompt_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_CONNECTION_TEST_TIMEOUT": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/lm_studio_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_MAX_RESPONSE_LENGTH": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_recent_responses": {
                  "import_count": 3,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/context_builder.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "store_chat_interaction": {
                  "import_count": 3,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "get_user_data": {
                  "import_count": 49,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "ai/context_builder.py",
                    "core/backup_manager.py",
                    "core/checkin_analytics.py",
                    "core/checkin_dynamic_manager.py",
                    "core/checkin_dynamic_manager.py",
                    "core/checkin_dynamic_manager.py",
                    "core/response_tracking.py",
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "core/schedule_management.py",
                    "core/service.py",
                    "core/service.py",
                    "core/service.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py",
                    "core/user_data_validation.py",
                    "core/user_data_validation.py",
                    "core/user_data_validation.py",
                    "tasks/task_management.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "user/context_manager.py",
                    "user/user_context.py",
                    "user/user_preferences.py",
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/channel_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/task_management_dialog.py",
                    "ui/dialogs/user_analytics_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/tag_widget.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "get_recent_messages": {
                  "import_count": 5,
                  "import_locations": [
                    "ai/chatbot.py",
                    "core/message_analytics.py",
                    "core/service.py",
                    "user/context_manager.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "TIME_ONLY_MINUTE": {
                  "import_count": 8,
                  "import_locations": [
                    "ai/chatbot.py",
                    "core/checkin_analytics.py",
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "core/schedule_utilities.py",
                    "core/user_data_validation.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "is_user_checkins_enabled": {
                  "import_count": 8,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "now_timestamp_filename": {
                  "import_count": 8,
                  "import_locations": [
                    "ai/conversation_history.py",
                    "core/backup_manager.py",
                    "core/error_handling.py",
                    "core/message_management.py",
                    "core/scheduler.py",
                    "core/service_utilities.py",
                    "core/user_data_manager.py",
                    "ui/dialogs/schedule_editor_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "DATE_ONLY": {
                  "import_count": 8,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/scheduler.py",
                    "core/user_data_validation.py",
                    "tasks/task_management.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "now_timestamp_full": {
                  "import_count": 19,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/backup_manager.py",
                    "core/file_operations.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/service.py",
                    "core/service_utilities.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "notebook/notebook_data_handlers.py",
                    "notebook/notebook_data_manager.py",
                    "notebook/schemas.py",
                    "tasks/task_management.py",
                    "ui/generate_ui_files.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/core/welcome_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "BASE_DATA_DIR": {
                  "import_count": 14,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/auto_cleanup.py",
                    "core/auto_cleanup.py",
                    "core/auto_cleanup.py",
                    "core/scheduler.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "ui/ui_app_qt.py",
                    "communication/core/welcome_manager.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_backups_dir": {
                  "import_count": 2,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_all_user_ids": {
                  "import_count": 9,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/auto_cleanup.py",
                    "core/backup_manager.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "core/user_data_manager.py",
                    "ui/ui_app_qt.py",
                    "communication/command_handlers/account_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "get_logger": {
                  "import_count": 2,
                  "import_locations": [
                    "core/backup_manager.py",
                    "communication/communication_channels/base/base_channel.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "TIMESTAMP_FULL": {
                  "import_count": 16,
                  "import_locations": [
                    "core/backup_manager.py",
                    "core/checkin_analytics.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "notebook/notebook_data_manager.py",
                    "notebook/schemas.py",
                    "ui/ui_app_qt.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "ui/dialogs/process_watcher_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "LOG_MAIN_FILE": {
                  "import_count": 6,
                  "import_locations": [
                    "core/backup_manager.py",
                    "core/service.py",
                    "core/service.py",
                    "core/service.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_checkins_by_days": {
                  "import_count": 2,
                  "import_locations": [
                    "core/checkin_analytics.py",
                    "ui/dialogs/user_analytics_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "load_json_data": {
                  "import_count": 10,
                  "import_locations": [
                    "core/checkin_dynamic_manager.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/service.py",
                    "core/tags.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "notebook/notebook_data_handlers.py",
                    "tasks/task_management.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_operations"
                },
                "update_user_preferences": {
                  "import_count": 6,
                  "import_locations": [
                    "core/checkin_dynamic_manager.py",
                    "core/checkin_dynamic_manager.py",
                    "user/user_preferences.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "ConfigurationError": {
                  "import_count": 2,
                  "import_locations": [
                    "core/config.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "wait_for_network": {
                  "import_count": 2,
                  "import_locations": [
                    "core/error_handling.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "DEFAULT_MESSAGES_DIR_PATH": {
                  "import_count": 2,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/message_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_user_file_path": {
                  "import_count": 9,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/response_tracking.py",
                    "core/tags.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "ensure_user_directory": {
                  "import_count": 2,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_user_data_dir": {
                  "import_count": 16,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/message_management.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "core/service.py",
                    "core/tags.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py",
                    "core/user_data_validation.py",
                    "notebook/notebook_data_handlers.py",
                    "tasks/task_management.py",
                    "communication/core/channel_orchestrator.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "FileOperationError": {
                  "import_count": 3,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/service.py",
                    "notebook/notebook_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "_record_created": {
                  "import_count": 2,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/service_utilities.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_auditor"
                },
                "update_user_index": {
                  "import_count": 12,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/message_management.py",
                    "core/message_management.py",
                    "core/message_management.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "communication/command_handlers/account_handler.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_manager"
                },
                "get_service_processes": {
                  "import_count": 2,
                  "import_locations": [
                    "core/headless_service.py",
                    "ui/dialogs/process_watcher_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "get_flags_dir": {
                  "import_count": 3,
                  "import_locations": [
                    "core/headless_service.py",
                    "core/service.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "save_json_data": {
                  "import_count": 8,
                  "import_locations": [
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/tags.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "notebook/notebook_data_handlers.py",
                    "tasks/task_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_operations"
                },
                "determine_file_path": {
                  "import_count": 3,
                  "import_locations": [
                    "core/message_management.py",
                    "core/user_data_handlers.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_operations"
                },
                "validate_messages_file_dict": {
                  "import_count": 3,
                  "import_locations": [
                    "core/message_management.py",
                    "core/user_data_manager.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schemas"
                },
                "ValidationError": {
                  "import_count": 4,
                  "import_locations": [
                    "core/message_management.py",
                    "core/schedule_management.py",
                    "core/schemas.py",
                    "core/tags.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "get_schedule_time_periods": {
                  "import_count": 8,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/ui_management.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "TIMESTAMP_MINUTE": {
                  "import_count": 3,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/service_utilities.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "SchedulerManager": {
                  "import_count": 5,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.scheduler"
                },
                "cleanup_data_directory": {
                  "import_count": 2,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "cleanup_tests_data_directory": {
                  "import_count": 2,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "update_user_schedules": {
                  "import_count": 2,
                  "import_locations": [
                    "core/schedule_management.py",
                    "core/schedule_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "get_message_categories": {
                  "import_count": 2,
                  "import_locations": [
                    "core/schemas.py",
                    "core/user_data_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "is_valid_discord_id": {
                  "import_count": 2,
                  "import_locations": [
                    "core/schemas.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "USER_INFO_DIR_PATH": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service.py",
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "force_restart_logging": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "get_current_time_periods_with_validation": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "get_current_day_names": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "SCHEDULER_INTERVAL": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service_utilities.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "validate_account_dict": {
                  "import_count": 2,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schemas"
                },
                "validate_preferences_dict": {
                  "import_count": 4,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schemas"
                },
                "validate_schedules_dict": {
                  "import_count": 3,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schemas"
                },
                "UserDataManager": {
                  "import_count": 2,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_manager"
                },
                "safe_json_read": {
                  "import_count": 7,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_locking"
                },
                "_get_user_data_dir": {
                  "import_count": 5,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "ensure_user_message_files": {
                  "import_count": 2,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "get_user_categories": {
                  "import_count": 5,
                  "import_locations": [
                    "core/user_data_manager.py",
                    "core/user_data_manager.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "safe_json_write": {
                  "import_count": 3,
                  "import_locations": [
                    "core/user_data_manager.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_locking"
                },
                "get_recent_checkins": {
                  "import_count": 9,
                  "import_locations": [
                    "core/user_data_manager.py",
                    "user/context_manager.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "ensure_tags_initialized": {
                  "import_count": 2,
                  "import_locations": [
                    "notebook/notebook_data_handlers.py",
                    "notebook/notebook_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "normalize_tags": {
                  "import_count": 2,
                  "import_locations": [
                    "notebook/notebook_data_manager.py",
                    "notebook/schemas.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "is_valid_string_length": {
                  "import_count": 2,
                  "import_locations": [
                    "notebook/notebook_data_manager.py",
                    "notebook/notebook_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "get_scheduler_manager": {
                  "import_count": 3,
                  "import_locations": [
                    "tasks/task_management.py",
                    "tasks/task_management.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service"
                },
                "validate_all_configuration": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "_shared__title_case": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/widgets/category_selection_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "EMAIL_SMTP_SERVER": {
                  "import_count": 4,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "EMAIL_IMAP_SERVER": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "EMAIL_SMTP_USERNAME": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "DISCORD_BOT_TOKEN": {
                  "import_count": 4,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "EMAIL_SMTP_PASSWORD": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "update_user_context": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/dialogs/user_profile_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "save_user_data": {
                  "import_count": 5,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "user/user_context.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "get_active_schedules": {
                  "import_count": 2,
                  "import_locations": [
                    "user/context_manager.py",
                    "user/user_context.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_utilities"
                },
                "get_user_id_by_identifier": {
                  "import_count": 12,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/task_reminder_view.py",
                    "communication/communication_channels/discord/webhook_handler.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "update_user_account": {
                  "import_count": 6,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/communication_channels/discord/webhook_handler.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/channel_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "CheckinAnalytics": {
                  "import_count": 15,
                  "import_locations": [
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/user_analytics_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.checkin_analytics"
                },
                "handle_ai_error": {
                  "import_count": 12,
                  "import_locations": [
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "parse_tags_from_text": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "set_schedule_periods": {
                  "import_count": 6,
                  "import_locations": [
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "store_sent_message": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "get_available_channels": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/core/factory.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "handle_network_error": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "get_channel_class_mapping": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/core/factory.py",
                    "communication/core/factory.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "dynamic_checkin_manager": {
                  "import_count": 10,
                  "import_locations": [
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.checkin_dynamic_manager"
                },
                "DataError": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/base/message_formatter.py",
                    "ui/dialogs/admin_panel.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "validate_schedule_periods": {
                  "import_count": 4,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "is_valid_email": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/channel_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "clear_schedule_periods_cache": {
                  "import_count": 4,
                  "import_locations": [
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "load_period_widgets_for_category": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.ui_management"
                },
                "collect_period_data_from_widgets": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.ui_management"
                },
                "get_predefined_options": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/dynamic_list_container.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                }
              },
              "import_usage_details": {
                "BackupManager": {
                  "import_count": 1,
                  "import_locations": [
                    "create_backup.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.backup_manager"
                },
                "HeadlessServiceManager": {
                  "import_count": 1,
                  "import_locations": [
                    "run_headless_service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.headless_service"
                },
                "setup_logging": {
                  "import_count": 19,
                  "import_locations": [
                    "run_headless_service.py",
                    "core/auto_cleanup.py",
                    "core/service.py",
                    "ui/ui_app_qt.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/message_editor_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_completion_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_edit_dialog.py",
                    "ui/dialogs/task_management_dialog.py",
                    "ui/dialogs/user_analytics_dialog.py",
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/period_row_widget.py",
                    "ui/widgets/tag_widget.py",
                    "ui/widgets/task_settings_widget.py",
                    "ui/widgets/user_profile_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "get_component_logger": {
                  "import_count": 122,
                  "import_locations": [
                    "run_headless_service.py",
                    "ai/cache_manager.py",
                    "ai/chatbot.py",
                    "ai/context_builder.py",
                    "ai/conversation_history.py",
                    "ai/lm_studio_manager.py",
                    "ai/prompt_manager.py",
                    "core/auto_cleanup.py",
                    "core/backup_manager.py",
                    "core/checkin_analytics.py",
                    "core/checkin_dynamic_manager.py",
                    "core/config.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/error_handling.py",
                    "core/file_auditor.py",
                    "core/file_locking.py",
                    "core/file_operations.py",
                    "core/headless_service.py",
                    "core/message_analytics.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "core/schedule_utilities.py",
                    "core/schemas.py",
                    "core/service.py",
                    "core/service_utilities.py",
                    "core/tags.py",
                    "core/ui_management.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_validation.py",
                    "notebook/notebook_data_handlers.py",
                    "notebook/notebook_data_manager.py",
                    "notebook/notebook_validation.py",
                    "notebook/schemas.py",
                    "tasks/task_management.py",
                    "ui/ui_app_qt.py",
                    "user/context_manager.py",
                    "user/user_context.py",
                    "user/user_preferences.py",
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/base_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/core/channel_monitor.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/core/retry_manager.py",
                    "communication/core/welcome_manager.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/message_router.py",
                    "communication/communication_channels/base/base_channel.py",
                    "communication/communication_channels/base/command_registry.py",
                    "communication/communication_channels/base/message_formatter.py",
                    "communication/communication_channels/base/rich_formatter.py",
                    "communication/communication_channels/discord/account_flow_handler.py",
                    "communication/communication_channels/discord/api_client.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/task_reminder_view.py",
                    "communication/communication_channels/discord/webhook_handler.py",
                    "communication/communication_channels/discord/webhook_server.py",
                    "communication/communication_channels/discord/welcome_handler.py",
                    "communication/communication_channels/email/bot.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/admin_panel.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/channel_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/message_editor_dialog.py",
                    "ui/dialogs/process_watcher_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_completion_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_edit_dialog.py",
                    "ui/dialogs/task_management_dialog.py",
                    "ui/dialogs/user_analytics_dialog.py",
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/category_selection_widget.py",
                    "ui/widgets/channel_selection_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/period_row_widget.py",
                    "ui/widgets/tag_widget.py",
                    "ui/widgets/task_settings_widget.py",
                    "ui/widgets/user_profile_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "handle_errors": {
                  "import_count": 95,
                  "import_locations": [
                    "run_headless_service.py",
                    "ai/cache_manager.py",
                    "ai/chatbot.py",
                    "ai/context_builder.py",
                    "ai/conversation_history.py",
                    "ai/lm_studio_manager.py",
                    "ai/prompt_manager.py",
                    "core/auto_cleanup.py",
                    "core/backup_manager.py",
                    "core/checkin_analytics.py",
                    "core/checkin_dynamic_manager.py",
                    "core/config.py",
                    "core/file_auditor.py",
                    "core/file_locking.py",
                    "core/file_operations.py",
                    "core/headless_service.py",
                    "core/logger.py",
                    "core/message_analytics.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "core/schedule_utilities.py",
                    "core/schemas.py",
                    "core/service.py",
                    "core/service_utilities.py",
                    "core/tags.py",
                    "core/ui_management.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_validation.py",
                    "notebook/notebook_data_handlers.py",
                    "notebook/notebook_data_manager.py",
                    "notebook/notebook_validation.py",
                    "tasks/task_management.py",
                    "ui/generate_ui_files.py",
                    "ui/ui_app_qt.py",
                    "user/context_manager.py",
                    "user/user_context.py",
                    "user/user_preferences.py",
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/base_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/core/channel_monitor.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/core/retry_manager.py",
                    "communication/core/welcome_manager.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/message_router.py",
                    "communication/communication_channels/base/base_channel.py",
                    "communication/communication_channels/base/command_registry.py",
                    "communication/communication_channels/base/message_formatter.py",
                    "communication/communication_channels/base/rich_formatter.py",
                    "communication/communication_channels/discord/account_flow_handler.py",
                    "communication/communication_channels/discord/api_client.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/task_reminder_view.py",
                    "communication/communication_channels/discord/webhook_handler.py",
                    "communication/communication_channels/discord/webhook_server.py",
                    "communication/communication_channels/discord/welcome_handler.py",
                    "communication/communication_channels/email/bot.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/admin_panel.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/channel_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/message_editor_dialog.py",
                    "ui/dialogs/process_watcher_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_completion_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_edit_dialog.py",
                    "ui/dialogs/task_management_dialog.py",
                    "ui/dialogs/user_analytics_dialog.py",
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/category_selection_widget.py",
                    "ui/widgets/channel_selection_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/dynamic_list_container.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/period_row_widget.py",
                    "ui/widgets/tag_widget.py",
                    "ui/widgets/task_settings_widget.py",
                    "ui/widgets/user_profile_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "AI_CACHE_RESPONSES": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/cache_manager.py",
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "CONTEXT_CACHE_TTL": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/cache_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_RESPONSE_CACHE_TTL": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/cache_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LM_STUDIO_BASE_URL": {
                  "import_count": 3,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/lm_studio_manager.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LM_STUDIO_API_KEY": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/lm_studio_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LM_STUDIO_MODEL": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/lm_studio_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_TIMEOUT_SECONDS": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_SYSTEM_PROMPT_PATH": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/prompt_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_USE_CUSTOM_PROMPT": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/prompt_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_CONNECTION_TEST_TIMEOUT": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/lm_studio_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_API_CALL_TIMEOUT": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_PERSONALIZED_MESSAGE_TIMEOUT": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_CONTEXTUAL_RESPONSE_TIMEOUT": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_QUICK_RESPONSE_TIMEOUT": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_MAX_RESPONSE_LENGTH": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_MAX_RESPONSE_WORDS": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_MAX_RESPONSE_TOKENS": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_MIN_RESPONSE_LENGTH": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_CHAT_TEMPERATURE": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_COMMAND_TEMPERATURE": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_CLARIFICATION_TEMPERATURE": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_recent_responses": {
                  "import_count": 3,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/context_builder.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "store_chat_interaction": {
                  "import_count": 3,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "get_user_data": {
                  "import_count": 49,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "ai/context_builder.py",
                    "core/backup_manager.py",
                    "core/checkin_analytics.py",
                    "core/checkin_dynamic_manager.py",
                    "core/checkin_dynamic_manager.py",
                    "core/checkin_dynamic_manager.py",
                    "core/response_tracking.py",
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "core/schedule_management.py",
                    "core/service.py",
                    "core/service.py",
                    "core/service.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py",
                    "core/user_data_validation.py",
                    "core/user_data_validation.py",
                    "core/user_data_validation.py",
                    "tasks/task_management.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "user/context_manager.py",
                    "user/user_context.py",
                    "user/user_preferences.py",
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/channel_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/task_management_dialog.py",
                    "ui/dialogs/user_analytics_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/tag_widget.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "get_recent_messages": {
                  "import_count": 5,
                  "import_locations": [
                    "ai/chatbot.py",
                    "core/message_analytics.py",
                    "core/service.py",
                    "user/context_manager.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "TIME_ONLY_MINUTE": {
                  "import_count": 8,
                  "import_locations": [
                    "ai/chatbot.py",
                    "core/checkin_analytics.py",
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "core/schedule_utilities.py",
                    "core/user_data_validation.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "is_user_checkins_enabled": {
                  "import_count": 8,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "parse_timestamp_full": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "now_timestamp_filename": {
                  "import_count": 8,
                  "import_locations": [
                    "ai/conversation_history.py",
                    "core/backup_manager.py",
                    "core/error_handling.py",
                    "core/message_management.py",
                    "core/scheduler.py",
                    "core/service_utilities.py",
                    "core/user_data_manager.py",
                    "ui/dialogs/schedule_editor_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "RetryManager": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/__init__.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.retry_manager"
                },
                "QueuedMessage": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/__init__.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.retry_manager"
                },
                "BotInitializationError": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/__init__.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.channel_orchestrator"
                },
                "MessageSendError": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/__init__.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.channel_orchestrator"
                },
                "CommunicationManager": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/__init__.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.channel_orchestrator"
                },
                "ChannelFactory": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/__init__.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.factory"
                },
                "ChannelMonitor": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/__init__.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.channel_monitor"
                },
                "DATE_ONLY": {
                  "import_count": 8,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/scheduler.py",
                    "core/user_data_validation.py",
                    "tasks/task_management.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "now_timestamp_full": {
                  "import_count": 19,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/backup_manager.py",
                    "core/file_operations.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/service.py",
                    "core/service_utilities.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "notebook/notebook_data_handlers.py",
                    "notebook/notebook_data_manager.py",
                    "notebook/schemas.py",
                    "tasks/task_management.py",
                    "ui/generate_ui_files.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/core/welcome_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "BASE_DATA_DIR": {
                  "import_count": 14,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/auto_cleanup.py",
                    "core/auto_cleanup.py",
                    "core/auto_cleanup.py",
                    "core/scheduler.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "ui/ui_app_qt.py",
                    "communication/core/welcome_manager.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_backups_dir": {
                  "import_count": 2,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_all_user_ids": {
                  "import_count": 9,
                  "import_locations": [
                    "core/auto_cleanup.py",
                    "core/auto_cleanup.py",
                    "core/backup_manager.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "core/user_data_manager.py",
                    "ui/ui_app_qt.py",
                    "communication/command_handlers/account_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "archive_old_messages": {
                  "import_count": 1,
                  "import_locations": [
                    "core/auto_cleanup.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "get_logger": {
                  "import_count": 2,
                  "import_locations": [
                    "core/backup_manager.py",
                    "communication/communication_channels/base/base_channel.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "TIMESTAMP_FULL": {
                  "import_count": 16,
                  "import_locations": [
                    "core/backup_manager.py",
                    "core/checkin_analytics.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "notebook/notebook_data_manager.py",
                    "notebook/schemas.py",
                    "ui/ui_app_qt.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "ui/dialogs/process_watcher_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "LOG_MAIN_FILE": {
                  "import_count": 6,
                  "import_locations": [
                    "core/backup_manager.py",
                    "core/service.py",
                    "core/service.py",
                    "core/service.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LOG_DISCORD_FILE": {
                  "import_count": 1,
                  "import_locations": [
                    "core/backup_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LOG_AI_FILE": {
                  "import_count": 1,
                  "import_locations": [
                    "core/backup_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LOG_USER_ACTIVITY_FILE": {
                  "import_count": 1,
                  "import_locations": [
                    "core/backup_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "LOG_ERRORS_FILE": {
                  "import_count": 1,
                  "import_locations": [
                    "core/backup_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_checkins_by_days": {
                  "import_count": 2,
                  "import_locations": [
                    "core/checkin_analytics.py",
                    "ui/dialogs/user_analytics_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "load_json_data": {
                  "import_count": 10,
                  "import_locations": [
                    "core/checkin_dynamic_manager.py",
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/service.py",
                    "core/tags.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "notebook/notebook_data_handlers.py",
                    "tasks/task_management.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_operations"
                },
                "update_user_preferences": {
                  "import_count": 6,
                  "import_locations": [
                    "core/checkin_dynamic_manager.py",
                    "core/checkin_dynamic_manager.py",
                    "user/user_preferences.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "ConfigurationError": {
                  "import_count": 2,
                  "import_locations": [
                    "core/config.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "handle_configuration_error": {
                  "import_count": 1,
                  "import_locations": [
                    "core/config.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "wait_for_network": {
                  "import_count": 2,
                  "import_locations": [
                    "core/error_handling.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "DEFAULT_MESSAGES_DIR_PATH": {
                  "import_count": 2,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/message_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_user_file_path": {
                  "import_count": 9,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/response_tracking.py",
                    "core/tags.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "ensure_user_directory": {
                  "import_count": 2,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "get_user_data_dir": {
                  "import_count": 16,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/message_management.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "core/service.py",
                    "core/tags.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py",
                    "core/user_data_validation.py",
                    "notebook/notebook_data_handlers.py",
                    "tasks/task_management.py",
                    "communication/core/channel_orchestrator.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "FileOperationError": {
                  "import_count": 3,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/service.py",
                    "notebook/notebook_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "handle_file_error": {
                  "import_count": 1,
                  "import_locations": [
                    "core/file_operations.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "_record_created": {
                  "import_count": 2,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/service_utilities.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_auditor"
                },
                "create_message_file_from_defaults": {
                  "import_count": 1,
                  "import_locations": [
                    "core/file_operations.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "update_message_references": {
                  "import_count": 1,
                  "import_locations": [
                    "core/file_operations.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_manager"
                },
                "update_user_index": {
                  "import_count": 12,
                  "import_locations": [
                    "core/file_operations.py",
                    "core/message_management.py",
                    "core/message_management.py",
                    "core/message_management.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "communication/command_handlers/account_handler.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_manager"
                },
                "get_service_processes": {
                  "import_count": 2,
                  "import_locations": [
                    "core/headless_service.py",
                    "ui/dialogs/process_watcher_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "get_flags_dir": {
                  "import_count": 3,
                  "import_locations": [
                    "core/headless_service.py",
                    "core/service.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "is_headless_service_running": {
                  "import_count": 1,
                  "import_locations": [
                    "core/headless_service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "is_ui_service_running": {
                  "import_count": 1,
                  "import_locations": [
                    "core/headless_service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "save_json_data": {
                  "import_count": 8,
                  "import_locations": [
                    "core/message_management.py",
                    "core/response_tracking.py",
                    "core/tags.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "notebook/notebook_data_handlers.py",
                    "tasks/task_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_operations"
                },
                "determine_file_path": {
                  "import_count": 3,
                  "import_locations": [
                    "core/message_management.py",
                    "core/user_data_handlers.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_operations"
                },
                "validate_messages_file_dict": {
                  "import_count": 3,
                  "import_locations": [
                    "core/message_management.py",
                    "core/user_data_manager.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schemas"
                },
                "ValidationError": {
                  "import_count": 4,
                  "import_locations": [
                    "core/message_management.py",
                    "core/schedule_management.py",
                    "core/schemas.py",
                    "core/tags.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "get_schedule_time_periods": {
                  "import_count": 8,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/ui_management.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "load_and_localize_datetime": {
                  "import_count": 1,
                  "import_locations": [
                    "core/scheduler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "TIMESTAMP_MINUTE": {
                  "import_count": 3,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/service_utilities.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.time_utilities"
                },
                "backup_manager": {
                  "import_count": 1,
                  "import_locations": [
                    "core/scheduler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.backup_manager"
                },
                "suppress_noisy_logging": {
                  "import_count": 1,
                  "import_locations": [
                    "core/scheduler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "SchedulerManager": {
                  "import_count": 5,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.scheduler"
                },
                "cleanup_data_directory": {
                  "import_count": 2,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "cleanup_tests_data_directory": {
                  "import_count": 2,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "compress_old_logs": {
                  "import_count": 1,
                  "import_locations": [
                    "core/scheduler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "cleanup_old_archives": {
                  "import_count": 1,
                  "import_locations": [
                    "core/scheduler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "create_reschedule_request": {
                  "import_count": 1,
                  "import_locations": [
                    "core/schedule_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service_utilities"
                },
                "update_user_schedules": {
                  "import_count": 2,
                  "import_locations": [
                    "core/schedule_management.py",
                    "core/schedule_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "get_message_categories": {
                  "import_count": 2,
                  "import_locations": [
                    "core/schemas.py",
                    "core/user_data_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "is_valid_discord_id": {
                  "import_count": 2,
                  "import_locations": [
                    "core/schemas.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "validate_and_raise_if_invalid": {
                  "import_count": 1,
                  "import_locations": [
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "print_configuration_report": {
                  "import_count": 1,
                  "import_locations": [
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "USER_INFO_DIR_PATH": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service.py",
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "verify_file_access": {
                  "import_count": 1,
                  "import_locations": [
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_operations"
                },
                "start_auditor": {
                  "import_count": 1,
                  "import_locations": [
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_auditor"
                },
                "force_restart_logging": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "get_current_time_periods_with_validation": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "get_current_day_names": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "stop_auditor": {
                  "import_count": 1,
                  "import_locations": [
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_auditor"
                },
                "auto_cleanup_if_needed": {
                  "import_count": 1,
                  "import_locations": [
                    "core/service.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "SCHEDULER_INTERVAL": {
                  "import_count": 2,
                  "import_locations": [
                    "core/service_utilities.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "validate_new_user_data": {
                  "import_count": 1,
                  "import_locations": [
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "validate_user_update": {
                  "import_count": 1,
                  "import_locations": [
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "validate_account_dict": {
                  "import_count": 2,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schemas"
                },
                "validate_preferences_dict": {
                  "import_count": 4,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schemas"
                },
                "validate_schedules_dict": {
                  "import_count": 3,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schemas"
                },
                "load_user_tags": {
                  "import_count": 1,
                  "import_locations": [
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "save_user_tags": {
                  "import_count": 1,
                  "import_locations": [
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "UserDataManager": {
                  "import_count": 2,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_manager"
                },
                "safe_json_read": {
                  "import_count": 7,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_locking"
                },
                "_get_user_data_dir": {
                  "import_count": 5,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py",
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "ensure_user_message_files": {
                  "import_count": 2,
                  "import_locations": [
                    "core/user_data_handlers.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "_get_user_file_path": {
                  "import_count": 1,
                  "import_locations": [
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "_load_json": {
                  "import_count": 1,
                  "import_locations": [
                    "core/user_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_operations"
                },
                "get_user_categories": {
                  "import_count": 5,
                  "import_locations": [
                    "core/user_data_manager.py",
                    "core/user_data_manager.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "current_base_dir": {
                  "import_count": 1,
                  "import_locations": [
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "USER_DATA_LOADERS": {
                  "import_count": 1,
                  "import_locations": [
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "safe_json_write": {
                  "import_count": 3,
                  "import_locations": [
                    "core/user_data_manager.py",
                    "core/user_data_manager.py",
                    "core/user_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_locking"
                },
                "get_recent_checkins": {
                  "import_count": 9,
                  "import_locations": [
                    "core/user_data_manager.py",
                    "user/context_manager.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "ensure_tags_initialized": {
                  "import_count": 2,
                  "import_locations": [
                    "notebook/notebook_data_handlers.py",
                    "notebook/notebook_data_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "normalize_tags": {
                  "import_count": 2,
                  "import_locations": [
                    "notebook/notebook_data_manager.py",
                    "notebook/schemas.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "is_valid_string_length": {
                  "import_count": 2,
                  "import_locations": [
                    "notebook/notebook_data_manager.py",
                    "notebook/notebook_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "normalize_tag": {
                  "import_count": 1,
                  "import_locations": [
                    "notebook/notebook_data_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "is_valid_category_name": {
                  "import_count": 1,
                  "import_locations": [
                    "notebook/notebook_validation.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "validate_tag": {
                  "import_count": 1,
                  "import_locations": [
                    "notebook/schemas.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "get_scheduler_manager": {
                  "import_count": 3,
                  "import_locations": [
                    "tasks/task_management.py",
                    "tasks/task_management.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.service"
                },
                "add_user_tag": {
                  "import_count": 1,
                  "import_locations": [
                    "tasks/task_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "get_user_tags": {
                  "import_count": 1,
                  "import_locations": [
                    "tasks/task_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "remove_user_tag": {
                  "import_count": 1,
                  "import_locations": [
                    "tasks/task_management.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "validate_all_configuration": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "_shared__title_case": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/widgets/category_selection_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "rebuild_user_index": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_manager"
                },
                "run_full_scheduler_standalone": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.scheduler"
                },
                "run_user_scheduler_standalone": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.scheduler"
                },
                "run_category_scheduler_standalone": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.scheduler"
                },
                "toggle_verbose_logging": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "get_verbose_mode": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "get_cleanup_status": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "find_pycache_dirs": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "find_pyc_files": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "calculate_cache_size": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "perform_cleanup": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "update_cleanup_timestamp": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.auto_cleanup"
                },
                "LOG_LEVEL": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "EMAIL_SMTP_SERVER": {
                  "import_count": 4,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "EMAIL_IMAP_SERVER": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "EMAIL_SMTP_USERNAME": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "DISCORD_BOT_TOKEN": {
                  "import_count": 4,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "EMAIL_SMTP_PASSWORD": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "update_user_context": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "ui/dialogs/user_profile_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "save_user_data": {
                  "import_count": 5,
                  "import_locations": [
                    "ui/ui_app_qt.py",
                    "user/user_context.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "get_recent_chat_interactions": {
                  "import_count": 1,
                  "import_locations": [
                    "user/context_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "get_active_schedules": {
                  "import_count": 2,
                  "import_locations": [
                    "user/context_manager.py",
                    "user/user_context.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_utilities"
                },
                "set_schedule_period_active": {
                  "import_count": 1,
                  "import_locations": [
                    "user/user_preferences.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "is_schedule_period_active": {
                  "import_count": 1,
                  "import_locations": [
                    "user/user_preferences.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "get_user_id_by_identifier": {
                  "import_count": 12,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/task_reminder_view.py",
                    "communication/communication_channels/discord/webhook_handler.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "create_new_user": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "update_user_account": {
                  "import_count": 6,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/communication_channels/discord/webhook_handler.py",
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/channel_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "CheckinAnalytics": {
                  "import_count": 15,
                  "import_locations": [
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/user_analytics_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.checkin_analytics"
                },
                "handle_ai_error": {
                  "import_count": 12,
                  "import_locations": [
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "parse_tags_from_text": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.tags"
                },
                "set_schedule_periods": {
                  "import_count": 6,
                  "import_locations": [
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "add_schedule_period": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/command_handlers/schedule_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "store_sent_message": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "get_available_channels": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/core/factory.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "handle_network_error": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "handle_communication_error": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "get_channel_class_mapping": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/core/factory.py",
                    "communication/core/factory.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_RULE_BASED_HIGH_CONFIDENCE_THRESHOLD": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/message_processing/command_parser.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_AI_ENHANCED_CONFIDENCE_THRESHOLD": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/message_processing/command_parser.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_RULE_BASED_FALLBACK_THRESHOLD": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/message_processing/command_parser.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_AI_PARSING_BASE_CONFIDENCE": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/message_processing/command_parser.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_AI_PARSING_PARTIAL_CONFIDENCE": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/message_processing/command_parser.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "AI_COMMAND_PARSING_TIMEOUT": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/message_processing/command_parser.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "dynamic_checkin_manager": {
                  "import_count": 10,
                  "import_locations": [
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/checkin_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.checkin_dynamic_manager"
                },
                "store_user_response": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.response_tracking"
                },
                "DataError": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/base/message_formatter.py",
                    "ui/dialogs/admin_panel.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.error_handling"
                },
                "TIMEZONE_OPTIONS": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/account_flow_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "DISCORD_APPLICATION_ID": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "DISCORD_WEBHOOK_PORT": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "DISCORD_AUTO_NGROK": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "_is_testing_environment": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/webhook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.logger"
                },
                "DISCORD_PUBLIC_KEY": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/webhook_server.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.config"
                },
                "validate_schedule_periods": {
                  "import_count": 4,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "create_user_files": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.file_operations"
                },
                "is_valid_email": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/channel_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "is_valid_phone": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "clear_schedule_periods_cache": {
                  "import_count": 4,
                  "import_locations": [
                    "ui/dialogs/category_management_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "update_channel_preferences": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/channel_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "load_user_messages": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/message_editor_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "add_message": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/message_editor_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "edit_message": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/message_editor_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "delete_message": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/message_editor_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.message_management"
                },
                "load_period_widgets_for_category": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.ui_management"
                },
                "collect_period_data_from_widgets": {
                  "import_count": 3,
                  "import_locations": [
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.ui_management"
                },
                "get_predefined_options": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/dynamic_list_container.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "validate_personalization_data": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/user_profile_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_validation"
                },
                "get_timezone_options": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/channel_selection_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.user_data_handlers"
                },
                "get_period_data__time_24h_to_12h_display": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/period_row_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                },
                "get_period_data__time_12h_display_to_24h": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/period_row_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "core.schedule_management"
                }
              },
              "package_api_items": [
                "AIError",
                "AccountModel",
                "BackupDirectoryRotatingFileHandler",
                "BackupManager",
                "CategoryScheduleModel",
                "ChannelModel",
                "CheckinAnalytics",
                "CommunicationError",
                "ComponentLogger",
                "ConfigValidationError",
                "ConfigurationError",
                "ConfigurationRecovery",
                "DataError",
                "DynamicCheckinManager",
                "ErrorHandler",
                "ErrorRecoveryStrategy",
                "ExcludeLoggerNamesFilter",
                "FeaturesModel",
                "FileAuditor",
                "FileNotFoundRecovery",
                "FileOperationError",
                "HeadlessServiceManager",
                "HeartbeatWarningFilter",
                "InitializationError",
                "InvalidTimeFormatError",
                "JSONDecodeRecovery",
                "MHMError",
                "MHMService",
                "MessageAnalytics",
                "MessageModel",
                "MessagesFileModel",
                "NetworkRecovery",
                "PeriodModel",
                "PreferencesModel",
                "PytestContextLogFormatter",
                "RecoveryError",
                "SchedulerError",
                "SchedulerManager",
                "SchedulesModel",
                "Throttler",
                "UserDataManager",
                "UserInterfaceError",
                "ValidationError",
                "add_message",
                "add_period_widget_to_layout",
                "add_schedule_period",
                "add_user_tag",
                "apply_test_context_formatter_to_all_loggers",
                "archive_old_messages",
                "archive_old_messages_for_all_users",
                "auto_cleanup_if_needed",
                "backup_user_data",
                "build_user_index",
                "calculate_cache_size",
                "cleanup_data_directory",
                "cleanup_old_archives",
                "cleanup_old_backup_files",
                "cleanup_old_logs",
                "cleanup_old_message_archives",
                "cleanup_old_request_files",
                "cleanup_tests_data_directory",
                "clear_all_accumulated_jobs_standalone",
                "clear_log_file_locks",
                "clear_period_widgets_from_layout",
                "clear_schedule_periods_cache",
                "clear_user_caches",
                "collect_period_data_from_widgets",
                "compress_old_logs",
                "create_automatic_backup",
                "create_default_schedule_periods",
                "create_message_file_from_defaults",
                "create_new_user",
                "create_reschedule_request",
                "create_user_files",
                "delete_message",
                "delete_schedule_period",
                "delete_user_completely",
                "determine_file_path",
                "disable_module_logging",
                "edit_message",
                "edit_schedule_period",
                "ensure_all_categories_have_schedules",
                "ensure_category_has_default_schedule",
                "ensure_logs_directory",
                "ensure_tags_initialized",
                "ensure_unique_ids",
                "ensure_user_dir_for_tags",
                "ensure_user_directory",
                "ensure_user_message_files",
                "export_user_data",
                "file_lock",
                "find_pyc_files",
                "find_pycache_dirs",
                "force_restart_logging",
                "format_timestamp",
                "format_timestamp_milliseconds",
                "get_active_schedules",
                "get_all_user_ids",
                "get_all_user_summaries",
                "get_available_channels",
                "get_available_data_types",
                "get_backups_dir",
                "get_channel_class_mapping",
                "get_checkins_by_days",
                "get_cleanup_status",
                "get_component_logger",
                "get_current_active_schedules",
                "get_current_day_names",
                "get_current_time_periods_with_validation",
                "get_data_type_info",
                "get_flags_dir",
                "get_last_cleanup_timestamp",
                "get_log_file_info",
                "get_log_level_from_env",
                "get_logger",
                "get_message_categories",
                "get_period_data__time_12h_display_to_24h",
                "get_period_data__time_24h_to_12h_display",
                "get_period_data__validate_and_format_time",
                "get_predefined_options",
                "get_recent_chat_interactions",
                "get_recent_checkins",
                "get_recent_messages",
                "get_recent_responses",
                "get_schedule_days",
                "get_schedule_time_periods",
                "get_scheduler_manager",
                "get_service_processes",
                "get_timestamp_for_sorting",
                "get_timezone_options",
                "get_user_analytics_summary",
                "get_user_categories",
                "get_user_data",
                "get_user_data_dir",
                "get_user_data_summary",
                "get_user_data_with_metadata",
                "get_user_file_path",
                "get_user_id_by_identifier",
                "get_user_info_for_data_manager",
                "get_user_info_for_schedule_management",
                "get_user_info_for_tracking",
                "get_user_summary",
                "get_user_tags",
                "get_verbose_mode",
                "handle_ai_error",
                "handle_communication_error",
                "handle_configuration_error",
                "handle_errors",
                "handle_file_error",
                "handle_network_error",
                "handle_validation_error",
                "is_headless_service_running",
                "is_schedule_active",
                "is_schedule_period_active",
                "is_service_running",
                "is_ui_service_running",
                "is_user_checkins_enabled",
                "is_valid_category_name",
                "is_valid_discord_id",
                "is_valid_email",
                "is_valid_phone",
                "is_valid_string_length",
                "load_and_ensure_ids",
                "load_and_localize_datetime",
                "load_default_messages",
                "load_json_data",
                "load_period_widgets_for_category",
                "load_user_messages",
                "load_user_tags",
                "main",
                "migrate_legacy_schedules_structure",
                "normalize_tag",
                "normalize_tags",
                "now_timestamp_filename",
                "now_timestamp_full",
                "now_timestamp_minute",
                "parse_date_and_time_minute",
                "parse_date_only",
                "parse_tags_from_text",
                "parse_time_only_minute",
                "parse_timestamp",
                "parse_timestamp_full",
                "parse_timestamp_minute",
                "perform_cleanup",
                "perform_safe_operation",
                "period_name_for_display",
                "period_name_for_storage",
                "print_configuration_report",
                "process_category_schedule",
                "process_user_schedules",
                "rebuild_user_index",
                "record_created",
                "register_data_loader",
                "register_default_loaders",
                "remove_user_tag",
                "run_category_scheduler_standalone",
                "run_full_scheduler_standalone",
                "run_user_scheduler_standalone",
                "safe_file_operation",
                "safe_json_read",
                "safe_json_write",
                "save_json_data",
                "save_user_data",
                "save_user_data_transaction",
                "save_user_tags",
                "schedule_all_task_reminders",
                "set_console_log_level",
                "set_schedule_days",
                "set_schedule_period_active",
                "set_schedule_periods",
                "set_verbose_mode",
                "setup_logging",
                "setup_third_party_error_logging",
                "should_run_cleanup",
                "start_auditor",
                "stop_auditor",
                "store_chat_interaction",
                "store_sent_message",
                "store_user_response",
                "suppress_noisy_logging",
                "toggle_verbose_logging",
                "track_user_response",
                "update_channel_preferences",
                "update_cleanup_timestamp",
                "update_message",
                "update_message_references",
                "update_user_account",
                "update_user_context",
                "update_user_index",
                "update_user_preferences",
                "update_user_schedules",
                "validate_account_dict",
                "validate_ai_configuration",
                "validate_all_configuration",
                "validate_and_raise_if_invalid",
                "validate_communication_channels",
                "validate_core_paths",
                "validate_discord_config",
                "validate_email_config",
                "validate_environment_variables",
                "validate_file_organization_settings",
                "validate_logging_configuration",
                "validate_messages_file_dict",
                "validate_minimum_config",
                "validate_new_user_data",
                "validate_personalization_data",
                "validate_preferences_dict",
                "validate_schedule_periods",
                "validate_schedule_periods__validate_time_format",
                "validate_scheduler_configuration",
                "validate_schedules_dict",
                "validate_system_state",
                "validate_tag",
                "validate_user_update",
                "verify_file_access",
                "wait_for_network"
              ],
              "registry_items": [
                "__enter__",
                "__exit__",
                "__getattr__",
                "__init__",
                "_accept_legacy_shape",
                "_calculate_cache_size__calculate_pyc_files_size",
                "_calculate_cache_size__calculate_pycache_directories_size",
                "_calculate_energy_score",
                "_check_and_fix_logging__check_recent_activity_timestamps",
                "_classify_path",
                "_create_user_files__account_file",
                "_ensure_default_loaders_once",
                "_get_audit_directories",
                "_get_last_interaction",
                "_get_response_log_filename",
                "_get_user_data__load_account",
                "_load_data",
                "_load_default_tags_from_resources",
                "_normalize_path",
                "_parse_timestamp",
                "_shared__title_case",
                "add_period_widget_to_layout",
                "add_schedule_period",
                "can_start_headless_service",
                "clear_schedule_periods_cache",
                "file_lock",
                "format_timestamp",
                "format_timestamp_milliseconds",
                "get_active_schedules",
                "get_checkins_by_days",
                "get_delivery_success_rate",
                "is_valid_category_name"
              ]
            },
            "communication": {
              "package": "communication",
              "current_exports_count": 60,
              "should_export_count": 86,
              "already_exported": [
                "AnalyticsHandler",
                "BaseChannel",
                "BotInitializationError",
                "ChannelConfig",
                "ChannelFactory",
                "ChannelMonitor",
                "ChannelStatus",
                "ChannelType",
                "CheckinHandler",
                "CommandDefinition",
                "CommandRegistry",
                "CommunicationManager",
                "ConversationManager",
                "DiscordAPIClient",
                "DiscordBot",
                "DiscordCommandRegistry",
                "DiscordConnectionStatus",
                "DiscordEventHandler",
                "DiscordRichFormatter",
                "EmailBot",
                "EmailBotError",
                "EmailCommandRegistry",
                "EmailMessageFormatter",
                "EmailRichFormatter",
                "EnhancedCommandParser",
                "EventContext",
                "EventType",
                "HelpHandler",
                "InteractionHandler",
                "InteractionManager",
                "InteractionResponse",
                "MessageData",
                "MessageFormatter",
                "MessageRouter",
                "MessageSendError",
                "MessageType",
                "ParsedCommand",
                "ParsingResult",
                "ProfileHandler",
                "QueuedMessage",
                "RetryManager",
                "RichFormatter",
                "RoutingResult",
                "ScheduleManagementHandler",
                "SendMessageOptions",
                "TaskManagementHandler",
                "TextMessageFormatter",
                "conversation_manager",
                "get_all_handlers",
                "get_command_registry",
                "get_discord_api_client",
                "get_discord_event_handler",
                "get_enhanced_command_parser",
                "get_interaction_handler",
                "get_interaction_manager",
                "get_message_formatter",
                "get_message_router",
                "get_rich_formatter",
                "handle_user_message",
                "parse_command"
              ],
              "missing_exports": [
                "AccountManagementHandler",
                "CheckinFeatureSelect",
                "CreateAccountButton",
                "DiscordWebhookHandler",
                "FLOW_LIST_ITEMS",
                "FLOW_NOTE_BODY",
                "FLOW_TASK_REMINDER",
                "FeatureSelectionView",
                "MessageFeatureSelect",
                "NotebookHandler",
                "TaskFeatureSelect",
                "TimezoneSelect",
                "WebhookServer",
                "clear_welcomed_status",
                "get_checkin_view",
                "get_task_reminder_view",
                "get_welcome_message",
                "get_welcome_message_view",
                "handle_application_authorized",
                "handle_webhook_event",
                "has_been_welcomed",
                "mark_as_welcomed",
                "parse_webhook_event",
                "start_account_creation_flow",
                "start_account_linking_flow",
                "verify_webhook_signature"
              ],
              "potentially_unnecessary": [],
              "cross_module_usage": {
                "CommunicationManager": {
                  "import_count": 10,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/command_handlers/account_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.channel_orchestrator"
                },
                "conversation_manager": {
                  "import_count": 8,
                  "import_locations": [
                    "core/service.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.conversation_flow_manager"
                },
                "InteractionHandler": {
                  "import_count": 7,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.base_handler"
                },
                "InteractionResponse": {
                  "import_count": 9,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/base_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.shared_types"
                },
                "ParsedCommand": {
                  "import_count": 17,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/base_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/communication_channels/discord/account_flow_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.shared_types"
                },
                "_pending_link_operations": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/account_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.account_handler"
                },
                "TaskManagementHandler": {
                  "import_count": 6,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.task_handler"
                },
                "CheckinHandler": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.checkin_handler"
                },
                "ProfileHandler": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.profile_handler"
                },
                "ScheduleManagementHandler": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.schedule_handler"
                },
                "AnalyticsHandler": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.analytics_handler"
                },
                "NotebookHandler": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.notebook_handler"
                },
                "AccountManagementHandler": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/communication_channels/discord/account_flow_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.account_handler"
                },
                "BaseChannel": {
                  "import_count": 5,
                  "import_locations": [
                    "communication/core/channel_monitor.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.base_channel"
                },
                "ChannelConfig": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.base_channel"
                },
                "ChannelStatus": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.base_channel"
                },
                "ChannelFactory": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.factory"
                },
                "handle_user_message": {
                  "import_count": 16,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/task_reminder_view.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.interaction_manager"
                },
                "get_all_handlers": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.interaction_handlers"
                },
                "get_interaction_handler": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.interaction_handlers"
                },
                "ParsingResult": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.command_parser"
                },
                "ChannelType": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.base_channel"
                },
                "get_interaction_manager": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.interaction_manager"
                },
                "has_been_welcomed": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/webhook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.welcome_handler"
                },
                "mark_as_welcomed": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/webhook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.welcome_handler"
                },
                "get_welcome_message": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/webhook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.welcome_handler"
                },
                "start_account_creation_flow": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/welcome_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.account_flow_handler"
                },
                "start_account_linking_flow": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/welcome_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.account_flow_handler"
                },
                "get_rich_formatter": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/event_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.rich_formatter"
                }
              },
              "import_usage_details": {
                "CommunicationManager": {
                  "import_count": 10,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/service.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "ui/ui_app_qt.py",
                    "communication/command_handlers/account_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.channel_orchestrator"
                },
                "conversation_manager": {
                  "import_count": 8,
                  "import_locations": [
                    "core/service.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.conversation_flow_manager"
                },
                "InteractionHandler": {
                  "import_count": 7,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.base_handler"
                },
                "InteractionResponse": {
                  "import_count": 9,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/base_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.shared_types"
                },
                "ParsedCommand": {
                  "import_count": 17,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/base_handler.py",
                    "communication/command_handlers/checkin_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/notebook_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/schedule_handler.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/communication_channels/discord/account_flow_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.shared_types"
                },
                "_pending_link_operations": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py",
                    "communication/command_handlers/account_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.account_handler"
                },
                "_generate_confirmation_code": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.account_handler"
                },
                "_send_confirmation_code": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/command_handlers/account_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.account_handler"
                },
                "TaskManagementHandler": {
                  "import_count": 6,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.task_handler"
                },
                "CheckinHandler": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.checkin_handler"
                },
                "ProfileHandler": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.profile_handler"
                },
                "ScheduleManagementHandler": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.schedule_handler"
                },
                "AnalyticsHandler": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.analytics_handler"
                },
                "NotebookHandler": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.notebook_handler"
                },
                "AccountManagementHandler": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/communication_channels/discord/account_flow_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.account_handler"
                },
                "FLOW_NOTE_BODY": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/command_handlers/notebook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.conversation_flow_manager"
                },
                "FLOW_LIST_ITEMS": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/command_handlers/notebook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.conversation_flow_manager"
                },
                "BaseChannel": {
                  "import_count": 5,
                  "import_locations": [
                    "communication/core/channel_monitor.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.base_channel"
                },
                "ChannelConfig": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/factory.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.base_channel"
                },
                "ChannelStatus": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.base_channel"
                },
                "ChannelFactory": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.factory"
                },
                "RetryManager": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.retry_manager"
                },
                "ChannelMonitor": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.channel_monitor"
                },
                "handle_user_message": {
                  "import_count": 16,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/checkin_view.py",
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/task_reminder_view.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.interaction_manager"
                },
                "get_task_reminder_view": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.task_reminder_view"
                },
                "get_checkin_view": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.checkin_view"
                },
                "get_all_handlers": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.interaction_handlers"
                },
                "get_interaction_handler": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.command_handlers.interaction_handlers"
                },
                "get_enhanced_command_parser": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.command_parser"
                },
                "ParsingResult": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/message_processing/interaction_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.command_parser"
                },
                "FLOW_TASK_REMINDER": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.conversation_flow_manager"
                },
                "ChannelType": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/email/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.base_channel"
                },
                "get_interaction_manager": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.message_processing.interaction_manager"
                },
                "has_been_welcomed": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/webhook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.welcome_handler"
                },
                "mark_as_welcomed": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/webhook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.welcome_handler"
                },
                "get_welcome_message": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/webhook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.welcome_handler"
                },
                "WebhookServer": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.webhook_server"
                },
                "start_account_creation_flow": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/welcome_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.account_flow_handler"
                },
                "start_account_linking_flow": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/bot.py",
                    "communication/communication_channels/discord/welcome_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.account_flow_handler"
                },
                "get_rich_formatter": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/communication_channels/discord/event_handler.py",
                    "communication/communication_channels/discord/event_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.base.rich_formatter"
                },
                "clear_welcomed_status": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/webhook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.welcome_manager"
                },
                "get_welcome_message_view": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/webhook_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.welcome_handler"
                },
                "parse_webhook_event": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/webhook_server.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.webhook_handler"
                },
                "handle_webhook_event": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/webhook_server.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.communication_channels.discord.webhook_handler"
                },
                "_has_been_welcomed": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/welcome_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.welcome_manager"
                },
                "_mark_as_welcomed": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/welcome_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.welcome_manager"
                },
                "_clear_welcomed_status": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/welcome_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.welcome_manager"
                },
                "_get_welcome_message": {
                  "import_count": 1,
                  "import_locations": [
                    "communication/communication_channels/discord/welcome_handler.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "communication.core.welcome_manager"
                }
              },
              "package_api_items": [
                "AccountManagementHandler",
                "AnalyticsHandler",
                "BaseChannel",
                "BotInitializationError",
                "ChannelConfig",
                "ChannelFactory",
                "ChannelMonitor",
                "ChannelStatus",
                "ChannelType",
                "CheckinFeatureSelect",
                "CheckinHandler",
                "CommandDefinition",
                "CommandRegistry",
                "CommunicationManager",
                "ConversationManager",
                "CreateAccountButton",
                "DiscordAPIClient",
                "DiscordBot",
                "DiscordCommandRegistry",
                "DiscordConnectionStatus",
                "DiscordEventHandler",
                "DiscordRichFormatter",
                "DiscordWebhookHandler",
                "EmailBot",
                "EmailBotError",
                "EmailCommandRegistry",
                "EmailMessageFormatter",
                "EmailRichFormatter",
                "EnhancedCommandParser",
                "EventContext",
                "EventType",
                "FeatureSelectionView",
                "HelpHandler",
                "InteractionHandler",
                "InteractionManager",
                "InteractionResponse",
                "MessageData",
                "MessageFeatureSelect",
                "MessageFormatter",
                "MessageRouter",
                "MessageSendError",
                "MessageType",
                "NotebookHandler",
                "ParsedCommand",
                "ParsingResult",
                "ProfileHandler",
                "QueuedMessage",
                "RetryManager",
                "RichFormatter",
                "RoutingResult",
                "ScheduleManagementHandler",
                "SendMessageOptions",
                "TaskFeatureSelect",
                "TaskManagementHandler",
                "TextMessageFormatter",
                "TimezoneSelect",
                "WebhookServer",
                "clear_welcomed_status",
                "get_all_handlers",
                "get_checkin_view",
                "get_command_registry",
                "get_discord_api_client",
                "get_discord_event_handler",
                "get_enhanced_command_parser",
                "get_interaction_handler",
                "get_interaction_manager",
                "get_message_formatter",
                "get_message_router",
                "get_rich_formatter",
                "get_task_reminder_view",
                "get_welcome_message",
                "get_welcome_message_view",
                "handle_application_authorized",
                "handle_user_message",
                "handle_webhook_event",
                "has_been_welcomed",
                "mark_as_welcomed",
                "parse_command",
                "parse_webhook_event",
                "start_account_creation_flow",
                "start_account_linking_flow",
                "verify_webhook_signature"
              ],
              "registry_items": [
                "__getattr__",
                "__init__",
                "__init____setup_event_loop",
                "__post_init__",
                "_attempt_channel_restart",
                "_augment_suggestions",
                "_check_dns_resolution",
                "_complete_checkin",
                "_create_error_response",
                "_find_task_by_identifier",
                "_format_entry_id",
                "_format_entry_response",
                "_format_profile_text",
                "_generate_confirmation_code",
                "_get_field_scale",
                "_get_user_id_by_username",
                "_handle_add_schedule_period",
                "_handle_checkin_status",
                "_handle_commands_list",
                "_handle_continue_checkin",
                "_handle_edit_schedule_period",
                "_handle_examples",
                "_initialize_registry",
                "_load_welcome_tracking",
                "_route_bang_command",
                "_save_welcome_tracking",
                "_set_status",
                "create_channel",
                "create_embed",
                "create_interactive_elements",
                "get_connection_latency",
                "handle_application_authorized"
              ]
            },
            "ui": {
              "package": "ui",
              "current_exports_count": 34,
              "should_export_count": 34,
              "already_exported": [
                "AccountCreatorDialog",
                "AdminPanelDialog",
                "CategoryManagementDialog",
                "CategorySelectionWidget",
                "ChannelManagementDialog",
                "ChannelSelectionWidget",
                "CheckinManagementDialog",
                "CheckinSettingsWidget",
                "DynamicListContainer",
                "DynamicListField",
                "MHMManagerUI",
                "MessageEditDialog",
                "MessageEditorDialog",
                "PeriodRowWidget",
                "ProcessWatcherDialog",
                "ScheduleEditorDialog",
                "ServiceManager",
                "TagWidget",
                "TaskCompletionDialog",
                "TaskCrudDialog",
                "TaskEditDialog",
                "TaskManagementDialog",
                "TaskSettingsWidget",
                "UserAnalyticsDialog",
                "UserProfileDialog",
                "UserProfileSettingsWidget",
                "create_account_dialog",
                "generate_all_ui_files",
                "generate_ui_file",
                "main",
                "open_message_editor_dialog",
                "open_personalization_dialog",
                "open_schedule_editor",
                "open_user_analytics_dialog"
              ],
              "missing_exports": [],
              "potentially_unnecessary": [],
              "cross_module_usage": {
                "PeriodRowWidget": {
                  "import_count": 5,
                  "import_locations": [
                    "core/ui_management.py",
                    "core/ui_management.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.period_row_widget"
                },
                "CategorySelectionWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/category_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.category_selection_widget"
                },
                "ChannelSelectionWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/channel_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.channel_selection_widget"
                },
                "TaskSettingsWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.task_settings_widget"
                },
                "CheckinSettingsWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.checkin_settings_widget"
                },
                "TaskEditDialog": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.task_edit_dialog"
                },
                "TagWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/task_edit_dialog.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.tag_widget"
                },
                "DynamicListContainer": {
                  "import_count": 5,
                  "import_locations": [
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/user_profile_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.dynamic_list_container"
                }
              },
              "import_usage_details": {
                "PeriodRowWidget": {
                  "import_count": 5,
                  "import_locations": [
                    "core/ui_management.py",
                    "core/ui_management.py",
                    "ui/dialogs/schedule_editor_dialog.py",
                    "ui/widgets/checkin_settings_widget.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.period_row_widget"
                },
                "Ui_ui_app_mainwindow": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.admin_panel_pyqt"
                },
                "AccountCreatorDialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.account_creator_dialog"
                },
                "ChannelManagementDialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.channel_management_dialog"
                },
                "CategoryManagementDialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.category_management_dialog"
                },
                "CheckinManagementDialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.checkin_management_dialog"
                },
                "TaskManagementDialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.task_management_dialog"
                },
                "TaskCrudDialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.task_crud_dialog"
                },
                "UserProfileDialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.user_profile_dialog"
                },
                "open_user_analytics_dialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.user_analytics_dialog"
                },
                "open_message_editor_dialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.message_editor_dialog"
                },
                "open_schedule_editor": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.schedule_editor_dialog"
                },
                "ProcessWatcherDialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/ui_app_qt.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.process_watcher_dialog"
                },
                "CategorySelectionWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/category_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.category_selection_widget"
                },
                "ChannelSelectionWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/channel_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.channel_selection_widget"
                },
                "TaskSettingsWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.task_settings_widget"
                },
                "CheckinSettingsWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/checkin_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.checkin_settings_widget"
                },
                "Ui_Dialog_create_account": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.account_creator_dialog_pyqt"
                },
                "open_personalization_dialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.user_profile_dialog"
                },
                "Ui_Dialog_category_management": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/category_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.category_management_dialog_pyqt"
                },
                "Ui_Dialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/channel_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.channel_management_dialog_pyqt"
                },
                "Ui_Dialog_checkin_management": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/checkin_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.checkin_management_dialog_pyqt"
                },
                "Ui_Dialog_message_editor": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/message_editor_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.message_editor_dialog_pyqt"
                },
                "Ui_Dialog_edit_schedule": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/schedule_editor_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.schedule_editor_dialog_pyqt"
                },
                "Ui_Dialog_task_completion": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/task_completion_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.task_completion_dialog_pyqt"
                },
                "Ui_Dialog_task_crud": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.task_crud_dialog_pyqt"
                },
                "TaskEditDialog": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.task_edit_dialog"
                },
                "TaskCompletionDialog": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.dialogs.task_completion_dialog"
                },
                "Ui_Dialog_task_edit": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/task_edit_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.task_edit_dialog_pyqt"
                },
                "TagWidget": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/task_edit_dialog.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.tag_widget"
                },
                "Ui_Dialog_task_management": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.task_management_dialog_pyqt"
                },
                "Ui_Dialog_user_analytics": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/user_analytics_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.user_analytics_dialog_pyqt"
                },
                "Ui_Dialog_user_profile": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/user_profile_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.user_profile_management_dialog_pyqt"
                },
                "UserProfileSettingsWidget": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/user_profile_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.user_profile_settings_widget"
                },
                "DynamicListContainer": {
                  "import_count": 5,
                  "import_locations": [
                    "ui/dialogs/user_profile_dialog.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/dynamic_list_field.py",
                    "ui/widgets/user_profile_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.dynamic_list_container"
                },
                "Ui_Form_category_selection_widget": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/category_selection_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.category_selection_widget_pyqt"
                },
                "Ui_Form_channel_selection": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/channel_selection_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.channel_selection_widget_pyqt"
                },
                "Ui_Form_checkin_settings": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/checkin_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.checkin_settings_widget_pyqt"
                },
                "DynamicListField": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/dynamic_list_container.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.widgets.dynamic_list_field"
                },
                "Ui_Form_dynamic_list_field_template": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/dynamic_list_field.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.dynamic_list_field_template_pyqt"
                },
                "Ui_Form_period_row_template": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/period_row_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.period_row_template_pyqt"
                },
                "Ui_Widget_tag": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/tag_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.tag_widget_pyqt"
                },
                "Ui_Form_task_settings": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.task_settings_widget_pyqt"
                },
                "Ui_Form_user_profile_settings": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/user_profile_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ui.generated.user_profile_settings_widget_pyqt"
                }
              },
              "package_api_items": [
                "AccountCreatorDialog",
                "AdminPanelDialog",
                "CategoryManagementDialog",
                "CategorySelectionWidget",
                "ChannelManagementDialog",
                "ChannelSelectionWidget",
                "CheckinManagementDialog",
                "CheckinSettingsWidget",
                "DynamicListContainer",
                "DynamicListField",
                "MHMManagerUI",
                "MessageEditDialog",
                "MessageEditorDialog",
                "PeriodRowWidget",
                "ProcessWatcherDialog",
                "ScheduleEditorDialog",
                "ServiceManager",
                "TagWidget",
                "TaskCompletionDialog",
                "TaskCrudDialog",
                "TaskEditDialog",
                "TaskManagementDialog",
                "TaskSettingsWidget",
                "UserAnalyticsDialog",
                "UserProfileDialog",
                "UserProfileSettingsWidget",
                "create_account_dialog",
                "generate_all_ui_files",
                "generate_ui_file",
                "main",
                "open_message_editor_dialog",
                "open_personalization_dialog",
                "open_schedule_editor",
                "open_user_analytics_dialog"
              ],
              "registry_items": [
                "__init__",
                "__post_init__",
                "_add_blank_row",
                "_build_features_dict",
                "_check_discord_status",
                "_clear_category_groups",
                "_get_day_checkboxes",
                "_get_personalization_data__ensure_required_fields",
                "_on_delete",
                "_on_save_clicked",
                "_trigger_rescheduling",
                "add_custom_field",
                "add_new_period",
                "add_new_task",
                "add_reminder_period",
                "closeEvent",
                "complete_selected_task",
                "configure_tab_visibility",
                "generate_all_ui_files",
                "generate_ui_file",
                "get_admin_data",
                "get_checkin_settings",
                "get_completion_data",
                "get_selected_categories",
                "get_statistics",
                "get_text",
                "load_user_category_data",
                "on_enable_task_management_toggled",
                "set_selected_categories"
              ]
            },
            "tasks": {
              "package": "tasks",
              "current_exports_count": 20,
              "should_export_count": 20,
              "already_exported": [
                "TaskManagementError",
                "add_user_task_tag",
                "are_tasks_enabled",
                "cleanup_task_reminders",
                "complete_task",
                "create_task",
                "delete_task",
                "ensure_task_directory",
                "get_task_by_id",
                "get_tasks_due_soon",
                "get_user_task_stats",
                "load_active_tasks",
                "load_completed_tasks",
                "remove_user_task_tag",
                "restore_task",
                "save_active_tasks",
                "save_completed_tasks",
                "schedule_task_reminders",
                "setup_default_task_tags",
                "update_task"
              ],
              "missing_exports": [],
              "potentially_unnecessary": [],
              "cross_module_usage": {
                "are_tasks_enabled": {
                  "import_count": 6,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "ui/ui_app_qt.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "load_active_tasks": {
                  "import_count": 9,
                  "import_locations": [
                    "ai/chatbot.py",
                    "core/scheduler.py",
                    "ui/ui_app_qt.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/message_processing/interaction_manager.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "get_user_task_stats": {
                  "import_count": 7,
                  "import_locations": [
                    "ai/chatbot.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "get_tasks_due_soon": {
                  "import_count": 3,
                  "import_locations": [
                    "ai/chatbot.py",
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "get_task_by_id": {
                  "import_count": 14,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "update_task": {
                  "import_count": 5,
                  "import_locations": [
                    "core/scheduler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "ui/dialogs/task_edit_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "load_completed_tasks": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "create_task": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_edit_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "complete_task": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "delete_task": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "add_user_task_tag": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/widgets/tag_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "setup_default_task_tags": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                }
              },
              "import_usage_details": {
                "are_tasks_enabled": {
                  "import_count": 6,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/chatbot.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "ui/ui_app_qt.py",
                    "communication/core/channel_orchestrator.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "load_active_tasks": {
                  "import_count": 9,
                  "import_locations": [
                    "ai/chatbot.py",
                    "core/scheduler.py",
                    "ui/ui_app_qt.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/interaction_handlers.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/message_processing/interaction_manager.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "get_user_task_stats": {
                  "import_count": 7,
                  "import_locations": [
                    "ai/chatbot.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/profile_handler.py",
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/widgets/task_settings_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "get_tasks_due_soon": {
                  "import_count": 3,
                  "import_locations": [
                    "ai/chatbot.py",
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "get_task_by_id": {
                  "import_count": 14,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "core/scheduler.py",
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "update_task": {
                  "import_count": 5,
                  "import_locations": [
                    "core/scheduler.py",
                    "communication/command_handlers/task_handler.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "ui/dialogs/task_edit_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "load_completed_tasks": {
                  "import_count": 3,
                  "import_locations": [
                    "communication/command_handlers/analytics_handler.py",
                    "communication/command_handlers/analytics_handler.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "create_task": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_edit_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "complete_task": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "delete_task": {
                  "import_count": 2,
                  "import_locations": [
                    "communication/command_handlers/task_handler.py",
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "add_user_task_tag": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/widgets/tag_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "setup_default_task_tags": {
                  "import_count": 2,
                  "import_locations": [
                    "ui/dialogs/account_creator_dialog.py",
                    "ui/dialogs/task_management_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "restore_task": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/dialogs/task_crud_dialog.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                },
                "remove_user_task_tag": {
                  "import_count": 1,
                  "import_locations": [
                    "ui/widgets/tag_widget.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "tasks.task_management"
                }
              },
              "package_api_items": [
                "TaskManagementError",
                "add_user_task_tag",
                "are_tasks_enabled",
                "cleanup_task_reminders",
                "complete_task",
                "create_task",
                "delete_task",
                "ensure_task_directory",
                "get_task_by_id",
                "get_tasks_due_soon",
                "get_user_task_stats",
                "load_active_tasks",
                "load_completed_tasks",
                "remove_user_task_tag",
                "restore_task",
                "save_active_tasks",
                "save_completed_tasks",
                "schedule_task_reminders",
                "setup_default_task_tags",
                "update_task"
              ],
              "registry_items": [
                "_calculate_next_due_date"
              ]
            },
            "ai": {
              "package": "ai",
              "current_exports_count": 22,
              "should_export_count": 22,
              "already_exported": [
                "AIChatBotSingleton",
                "CacheEntry",
                "ContextAnalysis",
                "ContextBuilder",
                "ContextCache",
                "ContextData",
                "ConversationHistory",
                "ConversationMessage",
                "ConversationSession",
                "LMStudioManager",
                "PromptManager",
                "PromptTemplate",
                "ResponseCache",
                "ensure_lm_studio_ready",
                "get_ai_chatbot",
                "get_context_builder",
                "get_context_cache",
                "get_conversation_history",
                "get_lm_studio_manager",
                "get_prompt_manager",
                "get_response_cache",
                "is_lm_studio_ready"
              ],
              "missing_exports": [],
              "potentially_unnecessary": [],
              "cross_module_usage": {
                "get_ai_chatbot": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ai.chatbot"
                }
              },
              "import_usage_details": {
                "get_prompt_manager": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ai.prompt_manager"
                },
                "get_response_cache": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ai.cache_manager"
                },
                "is_lm_studio_ready": {
                  "import_count": 1,
                  "import_locations": [
                    "ai/chatbot.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ai.lm_studio_manager"
                },
                "get_ai_chatbot": {
                  "import_count": 4,
                  "import_locations": [
                    "communication/core/channel_orchestrator.py",
                    "communication/message_processing/command_parser.py",
                    "communication/message_processing/conversation_flow_manager.py",
                    "communication/message_processing/interaction_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "ai.chatbot"
                }
              },
              "package_api_items": [
                "AIChatBotSingleton",
                "CacheEntry",
                "ContextAnalysis",
                "ContextBuilder",
                "ContextCache",
                "ContextData",
                "ConversationHistory",
                "ConversationMessage",
                "ConversationSession",
                "LMStudioManager",
                "PromptManager",
                "PromptTemplate",
                "ResponseCache",
                "ensure_lm_studio_ready",
                "get_ai_chatbot",
                "get_context_builder",
                "get_context_cache",
                "get_conversation_history",
                "get_lm_studio_manager",
                "get_prompt_manager",
                "get_response_cache",
                "is_lm_studio_ready"
              ],
              "registry_items": [
                "__init__",
                "__new__",
                "__post_init__",
                "_call_lm_studio_api",
                "_cleanup_lru",
                "_load_custom_prompt",
                "add_prompt_template",
                "ensure_lm_studio_ready"
              ]
            },
            "user": {
              "package": "user",
              "current_exports_count": 4,
              "should_export_count": 4,
              "already_exported": [
                "UserContext",
                "UserContextManager",
                "UserPreferences",
                "user_context_manager"
              ],
              "missing_exports": [],
              "potentially_unnecessary": [],
              "cross_module_usage": {
                "user_context_manager": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/context_builder.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "user.context_manager"
                },
                "UserContext": {
                  "import_count": 4,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "ui/ui_app_qt.py",
                    "user/context_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "user.user_context"
                }
              },
              "import_usage_details": {
                "user_context_manager": {
                  "import_count": 2,
                  "import_locations": [
                    "ai/chatbot.py",
                    "ai/context_builder.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "user.context_manager"
                },
                "UserContext": {
                  "import_count": 4,
                  "import_locations": [
                    "core/scheduler.py",
                    "core/schedule_management.py",
                    "ui/ui_app_qt.py",
                    "user/context_manager.py"
                  ],
                  "import_types": [
                    "from_import"
                  ],
                  "module_path": "user.user_context"
                }
              },
              "package_api_items": [
                "UserContext",
                "UserContextManager",
                "UserPreferences"
              ],
              "registry_items": [
                "__init__",
                "__new__",
                "_get_conversation_history",
                "get_all_preferences",
                "get_instance_context"
              ]
            }
          }
        }
      },
      "timestamp": "2026-01-18T01:46:44"
    },
    "analyze_ascii_compliance": {
      "success": true,
      "data": {
        "files": {},
        "file_count": 0,
        "total_issues": 0
      },
      "timestamp": "2026-01-18T01:46:38"
    },
    "analyze_documentation": {
      "success": true,
      "data": {
        "details": {
          "artifacts": [],
          "consolidation_recommendations": [
            {
              "category": "Development Workflow",
              "files": [
                "ai_development_docs/AI_ARCHITECTURE.md",
                "ai_development_docs/AI_BACKUP_GUIDE.md",
                "ai_development_docs/AI_CHANGELOG.md",
                "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md",
                "ai_development_docs/AI_DOCUMENTATION_GUIDE.md",
                "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
                "ai_development_docs/AI_LEGACY_REMOVAL_GUIDE.md",
                "ai_development_docs/AI_LOGGING_GUIDE.md",
                "ai_development_docs/AI_REFERENCE.md",
                "ai_development_docs/AI_SESSION_STARTER.md",
                "ai_development_docs/AI_TESTING_GUIDE.md",
                "communication/communication_channels/discord/DISCORD_GUIDE.md",
                "communication/COMMUNICATION_GUIDE.md",
                "core/ERROR_HANDLING_GUIDE.md",
                "development_docs/BACKUP_GUIDE.md",
                "development_docs/CHANGELOG_DETAIL.md",
                "development_docs/PLANS.md",
                "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
                "development_tools/DEVELOPMENT_TOOLS_GUIDE.md",
                "DEVELOPMENT_WORKFLOW.md",
                "DOCUMENTATION_GUIDE.md",
                "logs/LOGGING_GUIDE.md",
                "scripts/SCRIPTS_GUIDE.md",
                "tests/DEVELOPMENT_TOOLS_TESTING_GUIDE.md",
                "tests/MANUAL_DISCORD_TEST_GUIDE.md",
                "tests/MANUAL_TESTING_GUIDE.md",
                "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md",
                "tests/TESTING_GUIDE.md",
                "ui/UI_GUIDE.md"
              ],
              "priority": "medium",
              "similarity_score": 0.18686550351292133,
              "suggestion": "Consider consolidating into a single DEVELOPMENT_WORKFLOW.md"
            },
            {
              "category": "Testing",
              "files": [
                "ai_development_docs/AI_TESTING_GUIDE.md",
                "tests/DEVELOPMENT_TOOLS_TESTING_GUIDE.md",
                "tests/MANUAL_TESTING_GUIDE.md",
                "tests/TESTING_GUIDE.md"
              ],
              "priority": "medium",
              "similarity_score": 0.2546576088625333,
              "suggestion": "Consider consolidating testing documentation into a single TESTING_GUIDE.md"
            },
            {
              "category": "High Section Overlap",
              "files": [
                "ai_development_docs/AI_BACKUP_GUIDE.md",
                "development_docs/BACKUP_GUIDE.md"
              ],
              "overlap_count": 13,
              "priority": "high",
              "suggestion": "These files share 13 sections - consider consolidating or clearly differentiating their purposes"
            },
            {
              "category": "High Section Overlap",
              "files": [
                "tests/DEVELOPMENT_TOOLS_TESTING_GUIDE.md",
                "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md"
              ],
              "overlap_count": 3,
              "priority": "medium",
              "suggestion": "These files share 3 sections - consider consolidating or clearly differentiating their purposes"
            }
          ],
          "documents": [
            "ARCHITECTURE.md",
            "DEVELOPMENT_WORKFLOW.md",
            "DOCUMENTATION_GUIDE.md",
            "HOW_TO_RUN.md",
            "PROJECT_VISION.md",
            "README.md",
            "TODO.md",
            "ai_development_docs/AI_ARCHITECTURE.md",
            "ai_development_docs/AI_BACKUP_GUIDE.md",
            "ai_development_docs/AI_CHANGELOG.md",
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md",
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md",
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md",
            "ai_development_docs/AI_LEGACY_REMOVAL_GUIDE.md",
            "ai_development_docs/AI_LOGGING_GUIDE.md",
            "ai_development_docs/AI_REFERENCE.md",
            "ai_development_docs/AI_SESSION_STARTER.md",
            "ai_development_docs/AI_TESTING_GUIDE.md",
            "communication/COMMUNICATION_GUIDE.md",
            "communication/communication_channels/discord/DISCORD_GUIDE.md",
            "core/ERROR_HANDLING_GUIDE.md",
            "development_docs/BACKUP_GUIDE.md",
            "development_docs/CHANGELOG_DETAIL.md",
            "development_docs/PLANS.md",
            "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
            "development_tools/DEVELOPMENT_TOOLS_GUIDE.md",
            "logs/LOGGING_GUIDE.md",
            "scripts/SCRIPTS_GUIDE.md",
            "tests/DEVELOPMENT_TOOLS_TESTING_GUIDE.md",
            "tests/MANUAL_DISCORD_TEST_GUIDE.md",
            "tests/MANUAL_TESTING_GUIDE.md",
            "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md",
            "tests/TESTING_GUIDE.md",
            "ui/UI_GUIDE.md"
          ],
          "duplicates": [],
          "file_purposes": {
            "ARCHITECTURE.md": {
              "content_length": 17488,
              "header_info": "# MHM System Architecture\n\n> **File**: `ARCHITECTURE.md`\n> **Audience**: Human developers building or maintaining the platform  \n> **Purpose**: Explain system design, module responsibilities, and data",
              "section_count": 9,
              "sections": [
                "Introduction",
                "1. Directory Overview",
                "2. User Data Model",
                "2.1. User Data Flow Diagram",
                "3. Data Handling Patterns",
                "4. Key Modules and Responsibilities",
                "5. UI Architecture and Naming Conventions",
                "6. Channel-Agnostic Architecture",
                "7. Development Notes"
              ]
            },
            "DEVELOPMENT_WORKFLOW.md": {
              "content_length": 14260,
              "header_info": "# Development Workflow Guide\n\n> **File**: `DEVELOPMENT_WORKFLOW.md`  \n> **Audience**: Human developer (beginner programmer)  \n> **Purpose**: Safe development practices and day-to-day procedures  \n> **",
              "section_count": 32,
              "sections": [
                "Introduction",
                "Quick Reference",
                "1. Safety First",
                "1.1. Core Safety Rules",
                "1.2. Pre-Change Checklist",
                "2. Virtual Environment Best Practices",
                "2.1. Why It Matters",
                "2.2. Common Commands (PowerShell)",
                "2.3. Troubleshooting",
                "2.4. Configuration and .env"
              ]
            },
            "DOCUMENTATION_GUIDE.md": {
              "content_length": 23706,
              "header_info": "# Documentation Guide\n\n> **File**: `DOCUMENTATION_GUIDE.md`  \n> **Audience**: Developers and contributors  \n> **Purpose**: Organize, author, and maintain project documentation  \n> **Style**: Comprehen",
              "section_count": 34,
              "sections": [
                "Introduction",
                "1. Quick Reference",
                "1.1. Core entry points",
                "1.2. When editing documentation",
                "2. Documentation Categories",
                "2.1. Human-facing documents",
                "2.2. AI-facing documents",
                "2.3. Generated and analytical documentation",
                "3. Standards and Templates",
                "3.1. Metadata for `.md` documentation files"
              ]
            },
            "HOW_TO_RUN.md": {
              "content_length": 7400,
              "header_info": "# How to Run MHM\n\n\n> **File**: `HOW_TO_RUN.md`\n> **Audience**: New users and developers setting up the project  \n> **Purpose**: Step-by-step setup and launch instructions  \n> **Style**: Clear, beginne",
              "section_count": 18,
              "sections": [
                "Introduction",
                "1. Quick Start (Recommended)",
                "1.1. Step 1: Set up Virtual Environment",
                "1.2. Step 2: Install Dependencies",
                "1.3. Step 3: Install the Project in Editable Mode",
                "1.4. Step 4: Configure Environment (Optional)",
                "1.5. Step 5: Launch the Application",
                "2. Alternative Commands",
                "3. Command Reference (Discord and Chat)",
                "4. Alternative Launch Methods"
              ]
            },
            "PROJECT_VISION.md": {
              "content_length": 9875,
              "header_info": "# PROJECT_VISION.md\n\n> **File**: `PROJECT_VISION.md`  \n> **Audience**: Human Developer, AI Collaborators, and Future Contributors  \n> **Purpose**: Overarching vision, mission, priorities, and long-ter",
              "section_count": 42,
              "sections": [
                "Introduction",
                "1. **Core Vision**",
                "2. **Mission Statement**",
                "3. **Core Values**",
                "3.1. **Personalized & Adaptive**",
                "3.2. **Hope-Focused & Supportive**",
                "3.3. **Executive Functioning Support**",
                "4. **Target Users**",
                "4.1. **Primary User: You**",
                "4.2. **Secondary Users (Potential)**"
              ]
            },
            "README.md": {
              "content_length": 10542,
              "header_info": "# Motivational Health Messages (MHM)\n\n> **File**: `README.md`  \n> **Audience**: Developers, contributors, and AI development assistants  \n> **Purpose**: High-level project overview, navigation, and or",
              "section_count": 17,
              "sections": [
                "Introduction",
                "1. Overview",
                "2. Navigation (Start Here)",
                "3. What MHM Currently Supports",
                "4. Communication Channels",
                "5. Project Philosophy",
                "6. Architecture",
                "7. AI Integration (Optional)",
                "8. Project Structure",
                "9. License and Privacy"
              ]
            },
            "TODO.md": {
              "content_length": 22705,
              "header_info": "# TODO.md - MHM Project Tasks\n\n> **File**: `TODO.md`\n> **Audience**: Human Developer (Beginner Programmer) and AI collaborators\n> **Purpose**: Current development priorities and planned improvements  ",
              "section_count": 13,
              "sections": [
                "Introduction",
                "How to Add New TODOs",
                "High Priority",
                "Flow & Communication",
                "AI & Chatbot",
                "Medium Priority",
                "Quality & Operations",
                "Documentation",
                "User Experience Improvements",
                "AI & Conversation"
              ]
            },
            "ai_development_docs/AI_ARCHITECTURE.md": {
              "content_length": 8928,
              "header_info": "# AI Architecture - Quick Reference\n\n> **File**: `ai_development_docs/AI_ARCHITECTURE.md`  \n> **Pair**: [ARCHITECTURE.md](ARCHITECTURE.md)  \n> **Audience**: AI collaborators and tools  \n> **Purpose**:",
              "section_count": 8,
              "sections": [
                "Introduction",
                "1. Directory Overview",
                "2. User Data Model",
                "3. Data Handling Patterns",
                "4. Key Modules and Responsibilities",
                "5. UI Architecture and Naming Conventions",
                "6. Channel-Agnostic Architecture",
                "7. Development Notes"
              ]
            },
            "ai_development_docs/AI_BACKUP_GUIDE.md": {
              "content_length": 6657,
              "header_info": "# AI Backup Guide\n\n> **File**: `ai_development_docs/AI_BACKUP_GUIDE.md`  \n> **Pair**: [BACKUP_GUIDE.md](../development_docs/BACKUP_GUIDE.md)  \n> **Audience**: AI collaborators  \n> **Purpose**: Fast ro",
              "section_count": 15,
              "sections": [
                "Introduction",
                "1. Log Rotation System",
                "2. User Data Backups",
                "3. Message Archives",
                "4. Development Tools Rotation",
                "5. External Archive Directory",
                "6. Compression Files (.gz)",
                "7. Unified Retention Policy",
                "8. Safety Procedures",
                "9. Troubleshooting"
              ]
            },
            "ai_development_docs/AI_CHANGELOG.md": {
              "content_length": 186964,
              "header_info": "# AI Changelog - Brief Summary for AI Context\n\n\n> **File**: `ai_development_docs/AI_CHANGELOG.md`\n> **Audience**: AI collaborators (Cursor, Codex, etc.)\n> **Purpose**: Lightweight summary of recent ch",
              "section_count": 222,
              "sections": [
                "Introduction",
                "Overview",
                "How to Update This File",
                "YYYY-MM-DD - Brief Title **COMPLETED**",
                "Recent Changes (Most Recent First)",
                "2026-01-15 - Ruff rollout and report-generator diagnostics **IN PROGRESS**",
                "2026-01-15 - Test Flags Isolation and Coverage Cache Mapping Refresh **COMPLETED**",
                "2026-01-14 - User Management Retirement Finalized **COMPLETED**",
                "2026-01-14 - Legacy Cleanup and Test Fixes **COMPLETED**",
                "2026-01-14 - Pyright Optional Fixes and Coverage Cache Mapping Fix **COMPLETED**"
              ]
            },
            "ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md": {
              "content_length": 13783,
              "header_info": "# AI Development Workflow Guide\n\n> **File**: `ai_development_docs/AI_DEVELOPMENT_WORKFLOW.md`  \n> **Pair**: [DEVELOPMENT_WORKFLOW.md](DEVELOPMENT_WORKFLOW.md)\n> **Audience**: AI collaborators and tool",
              "section_count": 12,
              "sections": [
                "Introduction",
                "Quick Reference",
                "1. Safety First",
                "2. Virtual Environment Best Practices",
                "3. Development Process",
                "4. Testing Strategy",
                "5. Common Tasks",
                "6. Emergency Procedures",
                "7. Learning Resources",
                "8. Success Tips"
              ]
            },
            "ai_development_docs/AI_DOCUMENTATION_GUIDE.md": {
              "content_length": 8447,
              "header_info": "# AI Documentation Guide\n\n> **File**: `ai_development_docs/AI_DOCUMENTATION_GUIDE.md`  \n> **Pair**: [DOCUMENTATION_GUIDE.md](DOCUMENTATION_GUIDE.md)  \n> **Audience**: AI collaborators and tools  \n> **",
              "section_count": 34,
              "sections": [
                "Introduction",
                "1. Quick Reference",
                "1.1. Decision paths",
                "1.2. When editing documentation",
                "2. Documentation Categories",
                "2.1. Human-facing documents",
                "2.2. AI-facing documents",
                "2.3. Generated and analytical documentation",
                "3. Standards and Templates",
                "3.1. Metadata for `.md` documentation files"
              ]
            },
            "ai_development_docs/AI_ERROR_HANDLING_GUIDE.md": {
              "content_length": 8852,
              "header_info": "# AI Error Handling Guide\n\n> **File**: `ai_development_docs/AI_ERROR_HANDLING_GUIDE.md`  \n> **Pair**: [ERROR_HANDLING_GUIDE.md](core/ERROR_HANDLING_GUIDE.md)  \n> **Audience**: AI collaborators and too",
              "section_count": 12,
              "sections": [
                "Introduction",
                "1. Purpose and Design Principles",
                "2. Architecture Overview",
                "3. Usage Patterns",
                "4. Error Categories and Severity",
                "5. Error Message Guidelines",
                "6. Configuration and Integration",
                "7. Testing Error Handling",
                "8. Monitoring and Debugging",
                "9. Legacy and Migration"
              ]
            },
            "ai_development_docs/AI_LEGACY_REMOVAL_GUIDE.md": {
              "content_length": 6496,
              "header_info": "# AI Legacy Code Removal Guide\n\n> **File**: `ai_development_docs/AI_LEGACY_REMOVAL_GUIDE.md`\n> **Audience**: AI collaborators and developers cleaning up deprecated code paths\n> **Purpose**: Routing an",
              "section_count": 8,
              "sections": [
                "Introduction",
                "Quick Reference",
                "1. Standards",
                "2. Process",
                "3. Checklist",
                "4. Tools",
                "5. Best Practices",
                "6. Success Criteria"
              ]
            },
            "ai_development_docs/AI_LOGGING_GUIDE.md": {
              "content_length": 8089,
              "header_info": "# AI Logging Guide\n\n> **File**: `ai_development_docs/AI_LOGGING_GUIDE.md`  \n> **Pair**: [LOGGING_GUIDE.md](logs/LOGGING_GUIDE.md)  \n> **Audience**: AI assistants and automation tools  \n> **Purpose**: ",
              "section_count": 11,
              "sections": [
                "Introduction",
                "1. Purpose and Scope",
                "2. Logging Architecture",
                "3. Log Levels and When to Use Them",
                "4. Component Log Files and Layout",
                "5. Configuration (Environment Variables)",
                "6. Log Rotation, Backups, and Archival",
                "7. Legacy Compatibility Logging Standard",
                "8. Maintenance and Cleanup",
                "9. Best Practices"
              ]
            },
            "ai_development_docs/AI_REFERENCE.md": {
              "content_length": 3776,
              "header_info": "# AI Reference - Troubleshooting & System Understanding\n\n\n> **File**: `ai_development_docs/AI_REFERENCE.md`\n> **Purpose**: Troubleshooting patterns and deep system understanding for AI collaborators  ",
              "section_count": 13,
              "sections": [
                "Introduction",
                "1. Troubleshooting Patterns",
                "1.1. Documentation appears incomplete",
                "1.2. Code change introduced failures",
                "1.3. PowerShell command failed",
                "1.4. User questions accuracy",
                "2. Communication Patterns",
                "3. System Understanding",
                "3.1. Critical files (do not break)",
                "3.2. Data flow patterns"
              ]
            },
            "ai_development_docs/AI_SESSION_STARTER.md": {
              "content_length": 7119,
              "header_info": "# AI Session Starter\n\n> **File**: `ai_development_docs/AI_SESSION_STARTER.md`\n> **Audience**: AI collaborators working on the MHM codebase and docs\n> **Purpose**: Provide a compact starting point for ",
              "section_count": 8,
              "sections": [
                "Introduction",
                "1. Starting a Session",
                "2. Core AI References",
                "3. Safe Change Rules",
                "4. Typical Tasks and Where to Look",
                "4.1. \"Help me edit documentation\"",
                "4.2. \"Help me refactor or clean up code\"",
                "4.3. \"Help me understand the system\""
              ]
            },
            "ai_development_docs/AI_TESTING_GUIDE.md": {
              "content_length": 11487,
              "header_info": "# AI Testing Guide\n\n> **File**: `ai_development_docs/AI_TESTING_GUIDE.md`  \n> **Pair**: [TESTING_GUIDE.md](tests/TESTING_GUIDE.md)  \n> **Audience**: AI collaborators and tools  \n> **Purpose**: Routing",
              "section_count": 10,
              "sections": [
                "Introduction",
                "1. Purpose and Scope",
                "2. Test Layout and Types",
                "3. Fixtures, Utilities, and Safety",
                "4. Running Automated Tests",
                "5. Parallel Execution and Coverage",
                "6. Writing and Extending Tests",
                "7. Debugging and Troubleshooting",
                "8. Manual and Channel-Specific Testing Overview",
                "9. Memory Profiling and Resource Management"
              ]
            },
            "communication/COMMUNICATION_GUIDE.md": {
              "content_length": 11766,
              "header_info": "# Communication Module - Channel-Agnostic Architecture\n\n> **File**: `communication/COMMUNICATION_GUIDE.md`  \n> **Audience**: Developers and maintainers working on communication channels and message fl",
              "section_count": 16,
              "sections": [
                "Introduction",
                "1. Core Principle",
                "2. Architecture Layers",
                "2.1. High-Level Flow",
                "2.2. Message Models",
                "2.3. Message Processing (`communication/message_processing/`)",
                "2.4. Channel Base and Adapters (`communication/communication_channels/`)",
                "3. Key Patterns",
                "3.1. Error Handling Boundaries",
                "3.2. Logging"
              ]
            },
            "communication/communication_channels/discord/DISCORD_GUIDE.md": {
              "content_length": 15930,
              "header_info": "# Discord Channel Guide\n\n> **File**: `communication/communication_channels/discord/DISCORD_GUIDE.md`  \n> **Audience**: Developers and maintainers working on the Discord channel  \n> **Purpose**: Docume",
              "section_count": 22,
              "sections": [
                "Introduction",
                "1. Purpose and Scope",
                "2. Discord Channel Architecture",
                "3. Bot Lifecycle and Connection Management",
                "3.1. Startup",
                "3.2. Shutdown and Disconnects",
                "4. Message and Interaction Flow",
                "4.1. Messages",
                "4.2. Reactions and Membership Events",
                "4.3. UI Views and Button Callbacks"
              ]
            },
            "core/ERROR_HANDLING_GUIDE.md": {
              "content_length": 17987,
              "header_info": "# Error Handling Guide\n\n> **File**: `core/ERROR_HANDLING_GUIDE.md`  \n> **Audience**: Developers and AI assistants working on MHM  \n> **Purpose**: Centralized error handling patterns, categories, and i",
              "section_count": 24,
              "sections": [
                "Introduction",
                "1. Purpose and Design Principles",
                "2. Architecture Overview",
                "2.1. Core types and hierarchy",
                "2.2. Recovery strategies",
                "2.3. Error handler and helpers",
                "2.4. Decorators and utilities",
                "2.5. Integration with logging",
                "3. Usage Patterns",
                "3.1. Decorated functions (preferred)"
              ]
            },
            "development_docs/BACKUP_GUIDE.md": {
              "content_length": 19874,
              "header_info": "# Backup and Archive System Guide\n\n> **File**: `development_docs/BACKUP_GUIDE.md`  \n> **Pair**: [AI_BACKUP_GUIDE.md](../ai_development_docs/AI_BACKUP_GUIDE.md)  \n> **Audience**: Developers and users  ",
              "section_count": 46,
              "sections": [
                "Introduction",
                "Overview",
                "1. Log Rotation System",
                "1.1. Location",
                "1.2. How It Works",
                "1.3. Restoring Log Files",
                "2. User Data Backups",
                "2.1. Location",
                "2.2. How It Works",
                "2.3. Restoring User Data from Backups"
              ]
            },
            "development_docs/CHANGELOG_DETAIL.md": {
              "content_length": 1093068,
              "header_info": "# CHANGELOG_DETAIL.md - Complete Detailed Changelog History\n\n> **File**: `development_docs/CHANGELOG_DETAIL.md`\n> **Audience**: Developers and contributors  \n> **Purpose**: Full historical record of p",
              "section_count": 375,
              "sections": [
                "Introduction",
                "Overview",
                "How to Update This File",
                "How to Add Changes",
                "YYYY-MM-DD - Brief Title",
                "Recent Changes (Most Recent First)",
                "2016-01-17 - Standardize datetime formatting usage and audit datetime.now() calls",
                "2026-01-15 - Ruff rollout, selective fixes, and report generator diagnostics **IN PROGRESS**",
                "2026-01-15 - Test Flags Isolation, Logging Doc Fix, and Domain Mapping Refresh",
                "2026-01-14 - User Management Retirement Finalized and Dev Tools Cache Cleanup"
              ]
            },
            "development_docs/PLANS.md": {
              "content_length": 27789,
              "header_info": "# MHM Development Plans\n\n\n> **File**: `development_docs/PLANS.md`\n> **Audience**: Human Developer & AI Collaborators  \n> **Purpose**: Consolidated development plans (grouped, interdependent work) with",
              "section_count": 18,
              "sections": [
                "Introduction",
                "[IN PROGRESS] **Plan Maintenance**",
                "**How to Update This Plan**",
                "**Success Criteria**",
                "[ACTIVE] **Current Active Plans**",
                "**Test Suite Performance Optimization Plan** **ON HOLD**",
                "**Error Handling Quality Improvement Plan** **IN PROGRESS**",
                "**Discord App/Bot Capabilities Exploration** **PLANNING**",
                "**Account Management System Improvements** **MOSTLY COMPLETE**",
                "**Mood-Aware Support Calibration** **PLANNING**"
              ]
            },
            "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md": {
              "content_length": 18252,
              "header_info": "# AI Development Tools Guide\n\n> **File**: `development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md`  \n> **Audience**: AI collaborators and automated tooling  \n> **Purpose**: Routing and constraints for all AI",
              "section_count": 11,
              "sections": [
                "Introduction",
                "1. Purpose and Scope",
                "2. Running the Tool Suite",
                "3. Audit Modes and Outputs",
                "3.1. Tier 1: Quick Audit - `audit --quick`",
                "3.2. Tier 2: Standard Audit - `audit` (default)",
                "3.3. Tier 3: Full Audit - `audit --full`",
                "4. Tool Catalog and Tiering",
                "4.1. Naming Conventions",
                "5. Operating Standards and Maintenance"
              ]
            },
            "development_tools/DEVELOPMENT_TOOLS_GUIDE.md": {
              "content_length": 31185,
              "header_info": "# Development Tools Guide\n\n> **File**: `development_tools/DEVELOPMENT_TOOLS_GUIDE.md`\n> **Audience**: Human maintainers and contributors\n> **Purpose**: Detailed reference for every development tool, i",
              "section_count": 17,
              "sections": [
                "Introduction",
                "1. Purpose and Scope",
                "2. Running the Tool Suite",
                "2.1. Global Options",
                "2.2. Command Examples",
                "2.3. Command Summary",
                "2.4. Entry Point Expectations",
                "2.5. Service Architecture",
                "3. Audit Modes and Outputs",
                "4. Tool Catalog and Tiering"
              ]
            },
            "logs/LOGGING_GUIDE.md": {
              "content_length": 10986,
              "header_info": "# Logging Guide\n\n> **File**: `logs/LOGGING_GUIDE.md`  \n> **Audience**: Developers and maintainers  \n> **Purpose**: Describe logging architecture, behavior, and maintenance tasks for MHM  \n> **Style**:",
              "section_count": 32,
              "sections": [
                "Introduction",
                "1. Purpose and Scope",
                "2. Logging Architecture",
                "2.1. Central logger module",
                "2.2. Component loggers",
                "2.3. Format",
                "3. Log Levels and When to Use Them",
                "3.1. DEBUG",
                "3.2. INFO",
                "3.3. WARNING"
              ]
            },
            "scripts/SCRIPTS_GUIDE.md": {
              "content_length": 7268,
              "header_info": "# Scripts Directory\n\n\n> **File**: `scripts/SCRIPTS_GUIDE.md`\n> **Audience**: Developers using MHM utility scripts and tools  \n> **Purpose**: Guide for utility scripts, migration tools, and testing scr",
              "section_count": 14,
              "sections": [
                "Introduction",
                "Quick Reference",
                "1. Structure",
                "2. Available Scripts",
                "2.1. Project Cleanup (`scripts/`)",
                "2.2. Testing Tools (`testing/`)",
                "2.3. AI Testing (`testing/ai/`)",
                "2.4. Debug Tools (`debug/`)",
                "2.5. Utilities (`utilities/`)",
                "2.6. Cleanup Tools (`utilities/cleanup/`)"
              ]
            },
            "tests/DEVELOPMENT_TOOLS_TESTING_GUIDE.md": {
              "content_length": 10411,
              "header_info": "# Development Tools Testing Guide\n\n> **File**: `tests/DEVELOPMENT_TOOLS_TESTING_GUIDE.md`  \n> **Audience**: Developers and AI assistants working on development tools  \n> **Purpose**: Quick reference f",
              "section_count": 21,
              "sections": [
                "Introduction",
                "1. Quick Start",
                "2. Test Suite Structure",
                "3. Prerequisites",
                "4. Test Fixtures and Isolation",
                "4.1. Synthetic Demo Project",
                "4.2. Test Configuration",
                "4.3. Temporary Project Copies",
                "5. Critical Safety Rules",
                "5.1. Never Modify Real Project Files"
              ]
            },
            "tests/MANUAL_DISCORD_TEST_GUIDE.md": {
              "content_length": 5431,
              "header_info": "# Manual Discord Testing Guide\n\n> **File**: `tests/MANUAL_DISCORD_TEST_GUIDE.md`  \n> **Audience**: Developers and AI assistants performing manual Discord testing  \n> **Purpose**: Quick reference for D",
              "section_count": 23,
              "sections": [
                "Introduction",
                "1. Prerequisites",
                "2. Task Reminder Testing",
                "2.1. Basic Flows",
                "3. Notebook Feature Testing",
                "3.1. Important Notes",
                "3.2. Note Creation",
                "3.3. Viewing Entries",
                "3.4. Editing Entries",
                "3.5. Organization"
              ]
            },
            "tests/MANUAL_TESTING_GUIDE.md": {
              "content_length": 11513,
              "header_info": "# Manual Testing Guide\n\n> **File**: `tests/MANUAL_TESTING_GUIDE.md`  \n> **Audience**: Developers and AI assistants performing manual testing  \n> **Purpose**: Canonical manual testing flows and checkli",
              "section_count": 26,
              "sections": [
                "Introduction",
                "1. Purpose and Scope",
                "2. Core Manual Flows",
                "2.1. Application startup and shutdown",
                "2.2. Basic configuration and environment",
                "3. UI Manual Testing",
                "3.1. Layout and visual checks",
                "3.2. Navigation and interaction",
                "3.3. Validation and error feedback",
                "4. Scheduling & Reminder Manual Tests"
              ]
            },
            "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md": {
              "content_length": 4945,
              "header_info": "# System AI Functionality Testing Guide\n\n> **File**: `tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md`  \n> **Audience**: Developers and AI assistants working on MHM AI functionality  \n> **Purpose**: Quick",
              "section_count": 10,
              "sections": [
                "Introduction",
                "1. Quick Start",
                "2. Test Suite Structure",
                "3. Prerequisites",
                "4. Test Data Scenarios",
                "5. Logging Behavior",
                "6. Test Features",
                "7. Test Categories",
                "8. Known Issues",
                "9. Cleanup"
              ]
            },
            "tests/TESTING_GUIDE.md": {
              "content_length": 31270,
              "header_info": "# Testing Guide\n\n> **File**: `tests/TESTING_GUIDE.md`  \n> **Audience**: Developers and AI assistants working on MHM  \n> **Purpose**: Comprehensive testing framework focused on real behavior, integrati",
              "section_count": 44,
              "sections": [
                "Introduction",
                "1. Purpose and Scope",
                "2. Test Layout and Types",
                "2.1. Discovery rules",
                "2.2. Test types",
                "2.3. Where to put new tests",
                "3. Fixtures, Utilities, and Safety",
                "3.1. Shared fixtures and hooks",
                "3.2. Test utilities and helpers",
                "3.3. Filesystem safety"
              ]
            },
            "ui/UI_GUIDE.md": {
              "content_length": 8850,
              "header_info": "# UI Development Guide\n\n> **File**: `ui/UI_GUIDE.md`  \n> **Audience**: Developers working on MHM UI components  \n> **Purpose**: Guide for UI development, generation, and maintenance  \n> **Style**: Tec",
              "section_count": 6,
              "sections": [
                "Introduction",
                "Quick Reference",
                "1. UI Generation",
                "2. File Structure",
                "3. Usage Pattern",
                "4. UI Flow and Signal-Based Updates"
              ]
            }
          },
          "missing": [],
          "placeholders": [],
          "section_overlaps": {
            "1. Log Rotation System": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "1. Purpose and Scope": [
              "ai_development_docs/AI_LOGGING_GUIDE.md",
              "ai_development_docs/AI_TESTING_GUIDE.md",
              "communication/communication_channels/discord/DISCORD_GUIDE.md",
              "development_tools/AI_DEVELOPMENT_TOOLS_GUIDE.md",
              "development_tools/DEVELOPMENT_TOOLS_GUIDE.md",
              "logs/LOGGING_GUIDE.md",
              "tests/MANUAL_TESTING_GUIDE.md",
              "tests/TESTING_GUIDE.md"
            ],
            "1. Quick Start": [
              "tests/DEVELOPMENT_TOOLS_TESTING_GUIDE.md",
              "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md"
            ],
            "10. Best Practices": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "11. Related Documentation": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "2. Test Suite Structure": [
              "tests/DEVELOPMENT_TOOLS_TESTING_GUIDE.md",
              "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md"
            ],
            "2. User Data Backups": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "3. Message Archives": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "3. Prerequisites": [
              "tests/DEVELOPMENT_TOOLS_TESTING_GUIDE.md",
              "tests/SYSTEM_AI_FUNCTIONALITY_TEST_GUIDE.md"
            ],
            "4. Development Tools Rotation": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "5. External Archive Directory": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "6. Compression Files (.gz)": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "7. Unified Retention Policy": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "8. Safety Procedures": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "9.1. Logs Not Rotating": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "9.2. Backups Not Created": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "9.3. Message Archives Empty": [
              "ai_development_docs/AI_BACKUP_GUIDE.md",
              "development_docs/BACKUP_GUIDE.md"
            ],
            "Testing": [
              "development_docs/CHANGELOG_DETAIL.md",
              "TODO.md"
            ]
          }
        },
        "summary": {
          "files_affected": 0,
          "total_issues": 0
        }
      },
      "timestamp": "2026-01-18T01:46:28"
    },
    "analyze_documentation_sync": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 1,
          "files_affected": 1,
          "status": "FAIL"
        },
        "details": {
          "paired_doc_issues": 0,
          "path_drift_issues": 1,
          "ascii_compliance_issues": 0,
          "heading_numbering_issues": 0,
          "missing_address_issues": 0,
          "unconverted_link_issues": 0,
          "path_drift_files": [
            "ai_development_docs\\AI_SESSION_STARTER.md"
          ],
          "paired_docs": {},
          "path_drift": {
            "files": {
              "ai_development_docs\\AI_SESSION_STARTER.md": 1
            },
            "total_issues": 1,
            "detailed_issues": {
              "ai_development_docs\\AI_SESSION_STARTER.md": [
                "Missing file: shared/standard_exclusions.py"
              ]
            }
          },
          "ascii_compliance": {
            "files": {},
            "file_count": 0,
            "total_issues": 0
          },
          "heading_numbering": {
            "files": {},
            "file_count": 0,
            "total_issues": 0
          },
          "missing_addresses": {
            "files": {},
            "file_count": 0,
            "total_issues": 0
          },
          "unconverted_links": {
            "files": {},
            "file_count": 0,
            "total_issues": 0
          }
        }
      },
      "timestamp": "2026-01-18T01:46:43"
    },
    "analyze_heading_numbering": {
      "success": true,
      "data": {
        "files": {},
        "file_count": 0,
        "total_issues": 0
      },
      "timestamp": "2026-01-18T01:46:40"
    },
    "analyze_missing_addresses": {
      "success": true,
      "data": {
        "files": {},
        "file_count": 0,
        "total_issues": 0
      },
      "timestamp": "2026-01-18T01:46:42"
    },
    "analyze_path_drift": {
      "success": true,
      "data": {
        "files": {
          "ai_development_docs\\AI_SESSION_STARTER.md": 1
        },
        "total_issues": 1,
        "detailed_issues": {
          "ai_development_docs\\AI_SESSION_STARTER.md": [
            "Missing file: shared/standard_exclusions.py"
          ]
        }
      },
      "timestamp": "2026-01-18T01:46:37"
    },
    "analyze_unconverted_links": {
      "success": true,
      "data": {
        "files": {},
        "file_count": 0,
        "total_issues": 0
      },
      "timestamp": "2026-01-18T01:46:43"
    },
    "analyze_error_handling": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 6,
          "files_affected": 2
        },
        "details": {
          "total_functions": 1482,
          "functions_with_try_except": 613,
          "functions_with_error_handling": 1476,
          "functions_with_decorators": 1463,
          "functions_missing_error_handling": 6,
          "analyze_error_handling": 99.59514170040485,
          "error_patterns": {
            "handle_errors_decorator": 1467,
            "try_except": 713,
            "handle_ai_error": 16,
            "handle_network_error": 5,
            "handle_communication_error": 4,
            "DataError": 12,
            "ConfigurationError": 8,
            "handle_configuration_error": 4,
            "ValidationError": 16,
            "error_handler_usage": 8,
            "handle_validation_error": 2,
            "FileOperationError": 8,
            "CommunicationError": 2,
            "AIError": 2,
            "handle_file_error": 3,
            "safe_file_operation": 1,
            "MHMError": 2
          },
          "missing_error_handling": [
            {
              "file": "communication\\message_processing\\conversation_flow_manager.py",
              "function": "_date_str",
              "line": 1944,
              "quality": "none"
            },
            {
              "file": "core\\time_utilities.py",
              "function": "now_timestamp_full",
              "line": 61,
              "quality": "none"
            },
            {
              "file": "core\\time_utilities.py",
              "function": "now_timestamp_minute",
              "line": 66,
              "quality": "none"
            },
            {
              "file": "core\\time_utilities.py",
              "function": "now_timestamp_filename",
              "line": 71,
              "quality": "none"
            },
            {
              "file": "core\\time_utilities.py",
              "function": "format_timestamp",
              "line": 81,
              "quality": "none"
            },
            {
              "file": "core\\time_utilities.py",
              "function": "format_timestamp_milliseconds",
              "line": 88,
              "quality": "none"
            }
          ],
          "error_handling_quality": {
            "excellent": 1463,
            "none": 6,
            "good": 13
          },
          "recommendations": [
            "Add error handling to 6 functions",
            "Replace basic try-except with @handle_errors decorator where appropriate",
            "Priority: Add error handling to 6 critical functions"
          ],
          "phase1_candidates": [],
          "phase1_total": 0,
          "phase1_by_priority": {},
          "phase2_exceptions": [],
          "phase2_total": 0,
          "phase2_by_type": {},
          "worst_modules": [
            {
              "module": "time_utilities.py",
              "coverage": 54.54545454545454,
              "missing": 5,
              "total": 11
            },
            {
              "module": "message_processing/conversation_flow_manager.py",
              "coverage": 97.2972972972973,
              "missing": 1,
              "total": 37
            },
            {
              "module": "cache_manager.py",
              "coverage": 100.0,
              "missing": 0,
              "total": 19
            },
            {
              "module": "chatbot.py",
              "coverage": 100.0,
              "missing": 0,
              "total": 28
            },
            {
              "module": "context_builder.py",
              "coverage": 100.0,
              "missing": 0,
              "total": 12
            }
          ]
        }
      },
      "timestamp": "2026-01-18T01:46:34"
    },
    "analyze_test_coverage": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 0,
          "files_affected": 0
        },
        "details": {
          "overall": {
            "coverage": 72.1,
            "statements": 28451,
            "covered": 20499,
            "missed": 7952,
            "generated": "2026-01-18T01:55:37.736566"
          },
          "modules": [
            {
              "module": "communication",
              "coverage": 68.9,
              "missed": 2833
            },
            {
              "module": "ui",
              "coverage": 70.8,
              "missed": 1966
            },
            {
              "module": "core",
              "coverage": 73.6,
              "missed": 2658
            },
            {
              "module": "ai",
              "coverage": 77.7,
              "missed": 384
            },
            {
              "module": "tasks",
              "coverage": 81.6,
              "missed": 99
            },
            {
              "module": "user",
              "coverage": 95.8,
              "missed": 12
            }
          ],
          "worst_files": [
            {
              "path": "core/time_utilities.py",
              "coverage": 32.1,
              "missing": 55
            },
            {
              "path": "ui/widgets/checkin_settings_widget.py",
              "coverage": 43.8,
              "missing": 372
            },
            {
              "path": "communication/communication_channels/discord/bot.py",
              "coverage": 45.1,
              "missing": 696
            },
            {
              "path": "communication/command_handlers/notebook_handler.py",
              "coverage": 51.9,
              "missing": 250
            },
            {
              "path": "ui/ui_app_qt.py",
              "coverage": 53.2,
              "missing": 650
            }
          ]
        }
      },
      "timestamp": "2026-01-18T01:55:40"
    },
    "analyze_test_markers": {
      "success": true,
      "data": {
        "missing_count": 0,
        "missing": []
      },
      "timestamp": "2026-01-18T01:55:42"
    },
    "generate_dev_tools_coverage": {
      "success": true,
      "data": {
        "overall": {
          "overall_coverage": 40.7,
          "total_statements": 20945,
          "total_missed": 12427
        },
        "modules": {
          "development_tools\\__init__.py": {
            "statements": 3,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 3
          },
          "development_tools\\ai_work\\__init__.py": {
            "statements": 0,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 0
          },
          "development_tools\\ai_work\\analyze_ai_work.py": {
            "statements": 215,
            "missed": 35,
            "coverage": 84,
            "missing_lines": [
              "27",
              "38",
              "206",
              "209",
              "212",
              "213",
              "251",
              "252",
              "253",
              "264",
              "265",
              "266",
              "278",
              "279",
              "280",
              "301",
              "307",
              "342",
              "343",
              "344",
              "345",
              "347",
              "348",
              "349",
              "350",
              "351",
              "354",
              "356",
              "358",
              "359",
              "360",
              "361",
              "362",
              "378",
              "382"
            ],
            "covered": 180
          },
          "development_tools\\config\\__init__.py": {
            "statements": 1,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 1
          },
          "development_tools\\config\\analyze_config.py": {
            "statements": 235,
            "missed": 75,
            "coverage": 68,
            "missing_lines": [
              "63",
              "156",
              "157",
              "187",
              "197",
              "203",
              "204",
              "230",
              "231",
              "253",
              "254",
              "280",
              "281",
              "296",
              "297",
              "302",
              "303",
              "317",
              "318",
              "341",
              "342",
              "343",
              "344",
              "348",
              "349",
              "350",
              "351",
              "356",
              "357",
              "359",
              "363",
              "365",
              "366",
              "367",
              "368",
              "369",
              "370",
              "374",
              "375",
              "376",
              "377",
              "380",
              "381",
              "382",
              "383",
              "386",
              "387",
              "388",
              "389",
              "460",
              "461",
              "462",
              "463",
              "464",
              "499",
              "500",
              "501",
              "504",
              "505",
              "510",
              "515",
              "520",
              "521",
              "525",
              "526",
              "527",
              "530",
              "531",
              "532",
              "536",
              "537",
              "538",
              "542",
              "543",
              "544"
            ],
            "covered": 160
          },
          "development_tools\\config\\config.py": {
            "statements": 320,
            "missed": 56,
            "coverage": 82,
            "missing_lines": [
              "50",
              "53",
              "54",
              "55",
              "62",
              "64",
              "65",
              "66",
              "92",
              "360",
              "363",
              "375",
              "386",
              "407",
              "418",
              "428",
              "453",
              "463",
              "473",
              "483",
              "493",
              "503",
              "513",
              "523",
              "534",
              "545",
              "550",
              "551",
              "553",
              "555",
              "556",
              "557",
              "558",
              "560",
              "561",
              "562",
              "634",
              "675",
              "687",
              "688",
              "689",
              "691",
              "692",
              "696",
              "697",
              "698",
              "699",
              "700",
              "725",
              "753",
              "798",
              "830",
              "855",
              "889",
              "898",
              "900"
            ],
            "covered": 264
          },
          "development_tools\\docs\\__init__.py": {
            "statements": 0,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 0
          },
          "development_tools\\docs\\analyze_ascii_compliance.py": {
            "statements": 105,
            "missed": 40,
            "coverage": 62,
            "missing_lines": [
              "27",
              "35",
              "41",
              "42",
              "162",
              "182",
              "195",
              "196",
              "197",
              "198",
              "231",
              "232",
              "233",
              "238",
              "239",
              "241",
              "256",
              "259",
              "263",
              "265",
              "268",
              "269",
              "271",
              "272",
              "273",
              "276",
              "279",
              "280",
              "281",
              "282",
              "285",
              "286",
              "287",
              "288",
              "290",
              "291",
              "292",
              "293",
              "295",
              "297"
            ],
            "covered": 65
          },
          "development_tools\\docs\\analyze_documentation.py": {
            "statements": 301,
            "missed": 166,
            "coverage": 45,
            "missing_lines": [
              "17",
              "22",
              "29",
              "41",
              "164",
              "165",
              "166",
              "167",
              "178",
              "179",
              "180",
              "181",
              "182",
              "183",
              "184",
              "185",
              "186",
              "187",
              "214",
              "215",
              "216",
              "217",
              "225",
              "232",
              "233",
              "234",
              "235",
              "236",
              "250",
              "251",
              "252",
              "253",
              "256",
              "257",
              "258",
              "259",
              "261",
              "262",
              "267",
              "281",
              "282",
              "283",
              "284",
              "285",
              "288",
              "289",
              "290",
              "292",
              "293",
              "294",
              "297",
              "298",
              "301",
              "304",
              "305",
              "307",
              "312",
              "314",
              "316",
              "317",
              "320",
              "323",
              "325",
              "332",
              "346",
              "349",
              "352",
              "360",
              "362",
              "363",
              "373",
              "380",
              "381",
              "382",
              "392",
              "395",
              "396",
              "397",
              "408",
              "409",
              "410",
              "411",
              "412",
              "422",
              "427",
              "428",
              "431",
              "432",
              "433",
              "435",
              "436",
              "439",
              "440",
              "441",
              "442",
              "443",
              "444",
              "445",
              "447",
              "448",
              "449",
              "450",
              "452",
              "460",
              "461",
              "463",
              "464",
              "465",
              "466",
              "469",
              "470",
              "471",
              "473",
              "474",
              "475",
              "476",
              "477",
              "478",
              "479",
              "480",
              "481",
              "482",
              "485",
              "486",
              "487",
              "489",
              "494",
              "498",
              "517",
              "518",
              "519",
              "520",
              "521",
              "524",
              "526",
              "530",
              "532",
              "535",
              "537",
              "541",
              "545",
              "546",
              "547",
              "548",
              "549",
              "552",
              "554",
              "555",
              "556",
              "557",
              "558",
              "559",
              "560",
              "561",
              "574",
              "577",
              "578",
              "616",
              "617",
              "618",
              "619",
              "620",
              "623",
              "636",
              "641",
              "658"
            ],
            "covered": 135
          },
          "development_tools\\docs\\analyze_documentation_sync.py": {
            "statements": 87,
            "missed": 34,
            "coverage": 61,
            "missing_lines": [
              "34",
              "43",
              "44",
              "64",
              "72",
              "118",
              "119",
              "157",
              "158",
              "159",
              "160",
              "161",
              "164",
              "165",
              "166",
              "167",
              "168",
              "169",
              "171",
              "172",
              "174",
              "175",
              "177",
              "184",
              "186",
              "189",
              "194",
              "198",
              "200",
              "204",
              "206",
              "208",
              "211",
              "219"
            ],
            "covered": 53
          },
          "development_tools\\docs\\analyze_heading_numbering.py": {
            "statements": 200,
            "missed": 66,
            "coverage": 67,
            "missing_lines": [
              "27",
              "35",
              "41",
              "42",
              "74",
              "125",
              "126",
              "127",
              "159",
              "160",
              "167",
              "171",
              "174",
              "178",
              "193",
              "198",
              "203",
              "204",
              "205",
              "225",
              "230",
              "231",
              "268",
              "273",
              "277",
              "280",
              "284",
              "287",
              "291",
              "325",
              "335",
              "340",
              "355",
              "356",
              "357",
              "358",
              "395",
              "396",
              "397",
              "402",
              "403",
              "405",
              "420",
              "423",
              "427",
              "429",
              "432",
              "433",
              "435",
              "436",
              "437",
              "440",
              "443",
              "444",
              "445",
              "446",
              "449",
              "450",
              "451",
              "452",
              "454",
              "455",
              "456",
              "457",
              "459",
              "461"
            ],
            "covered": 134
          },
          "development_tools\\docs\\analyze_missing_addresses.py": {
            "statements": 118,
            "missed": 51,
            "coverage": 57,
            "missing_lines": [
              "26",
              "34",
              "40",
              "41",
              "73",
              "127",
              "143",
              "150",
              "151",
              "152",
              "154",
              "155",
              "181",
              "182",
              "183",
              "184",
              "211",
              "214",
              "215",
              "216",
              "217",
              "218",
              "219",
              "222",
              "223",
              "224",
              "225",
              "227",
              "229",
              "242",
              "245",
              "249",
              "251",
              "254",
              "255",
              "257",
              "258",
              "259",
              "262",
              "265",
              "266",
              "267",
              "268",
              "271",
              "272",
              "273",
              "274",
              "275",
              "276",
              "278",
              "280"
            ],
            "covered": 67
          },
          "development_tools\\docs\\analyze_path_drift.py": {
            "statements": 381,
            "missed": 130,
            "coverage": 66,
            "missing_lines": [
              "28",
              "49",
              "50",
              "51",
              "68",
              "69",
              "122",
              "123",
              "173",
              "178",
              "186",
              "194",
              "206",
              "210",
              "214",
              "218",
              "301",
              "302",
              "304",
              "308",
              "312",
              "316",
              "324",
              "328",
              "364",
              "368",
              "392",
              "400",
              "419",
              "448",
              "452",
              "456",
              "469",
              "500",
              "502",
              "504",
              "505",
              "507",
              "508",
              "512",
              "516",
              "541",
              "545",
              "558",
              "571",
              "580",
              "583",
              "586",
              "587",
              "591",
              "592",
              "598",
              "602",
              "614",
              "619",
              "630",
              "638",
              "642",
              "646",
              "658",
              "668",
              "669",
              "670",
              "671",
              "672",
              "685",
              "692",
              "693",
              "696",
              "697",
              "700",
              "701",
              "704",
              "705",
              "708",
              "709",
              "712",
              "713",
              "717",
              "720",
              "721",
              "724",
              "725",
              "726",
              "727",
              "729",
              "754",
              "755",
              "756",
              "776",
              "812",
              "860",
              "863",
              "866",
              "875",
              "876",
              "879",
              "882",
              "885",
              "913",
              "914",
              "915",
              "917",
              "983",
              "984",
              "986",
              "1023",
              "1026",
              "1028",
              "1030",
              "1031",
              "1033",
              "1034",
              "1036",
              "1037",
              "1041",
              "1042",
              "1043",
              "1044",
              "1045",
              "1047",
              "1048",
              "1049",
              "1050",
              "1051",
              "1052",
              "1053",
              "1054",
              "1056",
              "1058"
            ],
            "covered": 251
          },
          "development_tools\\docs\\analyze_unconverted_links.py": {
            "statements": 276,
            "missed": 134,
            "coverage": 51,
            "missing_lines": [
              "28",
              "36",
              "42",
              "43",
              "75",
              "93",
              "94",
              "97",
              "98",
              "101",
              "102",
              "103",
              "105",
              "108",
              "139",
              "165",
              "174",
              "175",
              "202",
              "207",
              "240",
              "242",
              "248",
              "250",
              "252",
              "254",
              "256",
              "265",
              "270",
              "287",
              "288",
              "290",
              "296",
              "297",
              "306",
              "307",
              "308",
              "309",
              "314",
              "315",
              "316",
              "317",
              "334",
              "336",
              "338",
              "340",
              "345",
              "351",
              "352",
              "353",
              "380",
              "381",
              "384",
              "385",
              "387",
              "388",
              "389",
              "390",
              "391",
              "393",
              "394",
              "395",
              "397",
              "398",
              "403",
              "404",
              "405",
              "407",
              "408",
              "409",
              "410",
              "413",
              "414",
              "416",
              "417",
              "418",
              "419",
              "420",
              "421",
              "424",
              "425",
              "426",
              "429",
              "430",
              "431",
              "432",
              "435",
              "436",
              "439",
              "441",
              "442",
              "445",
              "448",
              "451",
              "452",
              "453",
              "454",
              "458",
              "461",
              "462",
              "463",
              "464",
              "467",
              "468",
              "501",
              "502",
              "503",
              "508",
              "509",
              "511",
              "526",
              "529",
              "533",
              "535",
              "538",
              "539",
              "541",
              "542",
              "543",
              "546",
              "549",
              "550",
              "551",
              "552",
              "555",
              "556",
              "557",
              "558",
              "559",
              "560",
              "561",
              "562",
              "564",
              "566"
            ],
            "covered": 142
          },
          "development_tools\\docs\\fix_documentation.py": {
            "statements": 57,
            "missed": 45,
            "coverage": 21,
            "missing_lines": [
              "27",
              "37",
              "38",
              "41",
              "42",
              "45",
              "52",
              "54",
              "55",
              "60",
              "65",
              "70",
              "75",
              "76",
              "82",
              "84",
              "85",
              "87",
              "88",
              "89",
              "90",
              "91",
              "92",
              "96",
              "97",
              "98",
              "99",
              "100",
              "101",
              "105",
              "106",
              "107",
              "108",
              "109",
              "110",
              "114",
              "115",
              "116",
              "117",
              "118",
              "119",
              "123",
              "130",
              "131",
              "133"
            ],
            "covered": 12
          },
          "development_tools\\docs\\fix_documentation_addresses.py": {
            "statements": 95,
            "missed": 29,
            "coverage": 69,
            "missing_lines": [
              "25",
              "33",
              "34",
              "35",
              "84",
              "88",
              "91",
              "92",
              "93",
              "94",
              "95",
              "116",
              "117",
              "118",
              "124",
              "149",
              "151",
              "157",
              "158",
              "159",
              "160",
              "167",
              "169",
              "172",
              "178",
              "180",
              "181",
              "183",
              "187"
            ],
            "covered": 66
          },
          "development_tools\\docs\\fix_documentation_ascii.py": {
            "statements": 60,
            "missed": 21,
            "coverage": 65,
            "missing_lines": [
              "24",
              "32",
              "33",
              "34",
              "130",
              "131",
              "132",
              "135",
              "136",
              "137",
              "138",
              "141",
              "142",
              "157",
              "159",
              "162",
              "168",
              "170",
              "171",
              "173",
              "177"
            ],
            "covered": 39
          },
          "development_tools\\docs\\fix_documentation_headings.py": {
            "statements": 268,
            "missed": 91,
            "coverage": 66,
            "missing_lines": [
              "25",
              "33",
              "34",
              "35",
              "58",
              "59",
              "60",
              "61",
              "63",
              "64",
              "65",
              "68",
              "87",
              "88",
              "94",
              "95",
              "96",
              "97",
              "98",
              "99",
              "103",
              "104",
              "115",
              "145",
              "147",
              "155",
              "156",
              "157",
              "158",
              "159",
              "166",
              "167",
              "168",
              "169",
              "179",
              "181",
              "182",
              "183",
              "196",
              "210",
              "227",
              "230",
              "231",
              "232",
              "233",
              "234",
              "237",
              "238",
              "239",
              "240",
              "241",
              "252",
              "255",
              "256",
              "257",
              "258",
              "259",
              "261",
              "264",
              "265",
              "266",
              "267",
              "268",
              "288",
              "291",
              "294",
              "299",
              "300",
              "303",
              "304",
              "305",
              "306",
              "307",
              "308",
              "311",
              "312",
              "313",
              "316",
              "317",
              "318",
              "321",
              "322",
              "323",
              "326",
              "327",
              "328",
              "331",
              "333",
              "342",
              "343",
              "344"
            ],
            "covered": 177
          },
          "development_tools\\docs\\fix_documentation_links.py": {
            "statements": 150,
            "missed": 44,
            "coverage": 71,
            "missing_lines": [
              "25",
              "34",
              "35",
              "36",
              "37",
              "71",
              "72",
              "74",
              "139",
              "140",
              "166",
              "167",
              "170",
              "175",
              "176",
              "177",
              "186",
              "187",
              "190",
              "191",
              "194",
              "195",
              "196",
              "197",
              "200",
              "204",
              "205",
              "206",
              "209",
              "213",
              "214",
              "218",
              "221",
              "222",
              "223",
              "256",
              "257",
              "260",
              "261",
              "262",
              "263",
              "264",
              "265",
              "266"
            ],
            "covered": 106
          },
          "development_tools\\docs\\fix_version_sync.py": {
            "statements": 410,
            "missed": 340,
            "coverage": 17,
            "missing_lines": [
              "21",
              "23",
              "24",
              "25",
              "26",
              "31",
              "77",
              "82",
              "83",
              "84",
              "85",
              "86",
              "91",
              "92",
              "93",
              "94",
              "97",
              "98",
              "101",
              "102",
              "103",
              "104",
              "105",
              "110",
              "112",
              "113",
              "115",
              "120",
              "123",
              "124",
              "125",
              "128",
              "132",
              "134",
              "136",
              "138",
              "140",
              "142",
              "144",
              "156",
              "157",
              "158",
              "165",
              "167",
              "169",
              "171",
              "173",
              "178",
              "180",
              "183",
              "184",
              "185",
              "186",
              "187",
              "189",
              "191",
              "192",
              "195",
              "196",
              "197",
              "199",
              "201",
              "202",
              "203",
              "207",
              "209",
              "217",
              "218",
              "219",
              "220",
              "222",
              "227",
              "230",
              "234",
              "235",
              "237",
              "243",
              "251",
              "259",
              "261",
              "262",
              "263",
              "264",
              "268",
              "269",
              "270",
              "272",
              "278",
              "280",
              "294",
              "307",
              "308",
              "321",
              "383",
              "384",
              "395",
              "408",
              "409",
              "418",
              "420",
              "421",
              "423",
              "424",
              "425",
              "428",
              "429",
              "430",
              "431",
              "432",
              "433",
              "435",
              "436",
              "443",
              "445",
              "446",
              "449",
              "450",
              "453",
              "454",
              "456",
              "457",
              "464",
              "471",
              "472",
              "481",
              "483",
              "485",
              "486",
              "488",
              "490",
              "491",
              "493",
              "494",
              "497",
              "498",
              "499",
              "500",
              "501",
              "502",
              "504",
              "505",
              "508",
              "509",
              "510",
              "512",
              "513",
              "516",
              "517",
              "520",
              "521",
              "522",
              "523",
              "524",
              "525",
              "526",
              "529",
              "530",
              "533",
              "534",
              "535",
              "537",
              "539",
              "540",
              "541",
              "542",
              "543",
              "545",
              "548",
              "551",
              "552",
              "553",
              "554",
              "558",
              "560",
              "561",
              "562",
              "563",
              "565",
              "566",
              "567",
              "568",
              "569",
              "570",
              "571",
              "573",
              "574",
              "576",
              "578",
              "579",
              "580",
              "582",
              "583",
              "584",
              "585",
              "586",
              "588",
              "589",
              "591",
              "592",
              "593",
              "594",
              "595",
              "597",
              "598",
              "599",
              "600",
              "601",
              "603",
              "612",
              "614",
              "615",
              "618",
              "619",
              "620",
              "624",
              "625",
              "626",
              "627",
              "628",
              "630",
              "633",
              "634",
              "637",
              "638",
              "640",
              "646",
              "647",
              "652",
              "653",
              "655",
              "657",
              "658",
              "659",
              "660",
              "663",
              "665",
              "666",
              "667",
              "669",
              "670",
              "673",
              "675",
              "677",
              "680",
              "681",
              "682",
              "683",
              "684",
              "686",
              "687",
              "690",
              "692",
              "693",
              "696",
              "698",
              "702",
              "707",
              "711",
              "712",
              "713",
              "714",
              "715",
              "717",
              "720",
              "721",
              "723",
              "725",
              "726",
              "727",
              "728",
              "732",
              "736",
              "737",
              "739",
              "742",
              "743",
              "744",
              "746",
              "747",
              "748",
              "752",
              "757",
              "759",
              "761",
              "762",
              "763",
              "764",
              "765",
              "767",
              "769",
              "771",
              "772",
              "773",
              "774",
              "775",
              "777",
              "778",
              "781",
              "782",
              "783",
              "785",
              "787",
              "788",
              "789",
              "790",
              "792",
              "797",
              "799",
              "801",
              "802",
              "803",
              "804",
              "805",
              "807",
              "809",
              "811",
              "812",
              "813",
              "814",
              "816",
              "817",
              "818",
              "819",
              "820",
              "821",
              "824",
              "826",
              "828",
              "830",
              "831",
              "832",
              "833",
              "835",
              "836",
              "837",
              "838",
              "840",
              "841",
              "842",
              "843"
            ],
            "covered": 70
          },
          "development_tools\\docs\\generate_directory_tree.py": {
            "statements": 130,
            "missed": 21,
            "coverage": 84,
            "missing_lines": [
              "27",
              "37",
              "43",
              "44",
              "72",
              "155",
              "156",
              "160",
              "162",
              "192",
              "193",
              "194",
              "198",
              "200",
              "201",
              "240",
              "242",
              "243",
              "244",
              "245",
              "246"
            ],
            "covered": 109
          },
          "development_tools\\error_handling\\__init__.py": {
            "statements": 0,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 0
          },
          "development_tools\\error_handling\\analyze_error_handling.py": {
            "statements": 592,
            "missed": 287,
            "coverage": 52,
            "missing_lines": [
              "25",
              "31",
              "32",
              "33",
              "150",
              "152",
              "285",
              "286",
              "288",
              "289",
              "290",
              "291",
              "293",
              "295",
              "296",
              "298",
              "299",
              "300",
              "302",
              "304",
              "306",
              "308",
              "311",
              "314",
              "315",
              "316",
              "318",
              "323",
              "324",
              "326",
              "327",
              "331",
              "332",
              "338",
              "339",
              "340",
              "341",
              "342",
              "343",
              "346",
              "353",
              "356",
              "357",
              "359",
              "360",
              "364",
              "366",
              "367",
              "368",
              "370",
              "372",
              "373",
              "377",
              "378",
              "385",
              "393",
              "394",
              "395",
              "396",
              "400",
              "455",
              "456",
              "461",
              "463",
              "465",
              "466",
              "469",
              "470",
              "471",
              "473",
              "475",
              "476",
              "479",
              "480",
              "481",
              "483",
              "485",
              "486",
              "487",
              "488",
              "489",
              "490",
              "491",
              "492",
              "493",
              "495",
              "496",
              "498",
              "500",
              "563",
              "564",
              "565",
              "567",
              "568",
              "569",
              "600",
              "601",
              "603",
              "605",
              "606",
              "607",
              "608",
              "609",
              "610",
              "611",
              "613",
              "614",
              "615",
              "616",
              "617",
              "618",
              "619",
              "620",
              "627",
              "628",
              "629",
              "630",
              "631",
              "633",
              "634",
              "637",
              "638",
              "643",
              "644",
              "707",
              "711",
              "715",
              "735",
              "736",
              "737",
              "740",
              "743",
              "750",
              "753",
              "754",
              "755",
              "757",
              "758",
              "761",
              "763",
              "765",
              "769",
              "770",
              "771",
              "772",
              "775",
              "776",
              "777",
              "778",
              "779",
              "780",
              "781",
              "782",
              "783",
              "784",
              "785",
              "786",
              "787",
              "788",
              "789",
              "790",
              "792",
              "796",
              "797",
              "798",
              "801",
              "802",
              "805",
              "806",
              "809",
              "810",
              "813",
              "814",
              "815",
              "818",
              "819",
              "820",
              "821",
              "823",
              "827",
              "829",
              "830",
              "832",
              "833",
              "835",
              "836",
              "837",
              "838",
              "840",
              "844",
              "845",
              "848",
              "849",
              "852",
              "853",
              "854",
              "855",
              "858",
              "860",
              "861",
              "863",
              "864",
              "866",
              "868",
              "876",
              "886",
              "887",
              "897",
              "899",
              "910",
              "911",
              "917",
              "924",
              "925",
              "965",
              "966",
              "969",
              "970",
              "971",
              "974",
              "976",
              "977",
              "979",
              "980",
              "982",
              "983",
              "985",
              "986",
              "987",
              "989",
              "990",
              "992",
              "993",
              "995",
              "996",
              "1004",
              "1007",
              "1008",
              "1011",
              "1013",
              "1014",
              "1015",
              "1016",
              "1018",
              "1030",
              "1031",
              "1032",
              "1035",
              "1036",
              "1041",
              "1042",
              "1043",
              "1058",
              "1121",
              "1123",
              "1124",
              "1125",
              "1126",
              "1127",
              "1128",
              "1130",
              "1133",
              "1134",
              "1135",
              "1136",
              "1139",
              "1140",
              "1141",
              "1145",
              "1146",
              "1147",
              "1150",
              "1151",
              "1152",
              "1153",
              "1155",
              "1156",
              "1158",
              "1159",
              "1162",
              "1164",
              "1167",
              "1168",
              "1169",
              "1170",
              "1171",
              "1173"
            ],
            "covered": 305
          },
          "development_tools\\error_handling\\generate_error_handling_report.py": {
            "statements": 109,
            "missed": 4,
            "coverage": 96,
            "missing_lines": [
              "21",
              "29",
              "30",
              "205"
            ],
            "covered": 105
          },
          "development_tools\\functions\\__init__.py": {
            "statements": 0,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 0
          },
          "development_tools\\functions\\analyze_function_patterns.py": {
            "statements": 87,
            "missed": 32,
            "coverage": 63,
            "missing_lines": [
              "19",
              "26",
              "27",
              "82",
              "94",
              "127",
              "128",
              "140",
              "250",
              "251",
              "253",
              "256",
              "259",
              "263",
              "266",
              "267",
              "268",
              "271",
              "273",
              "276",
              "278",
              "279",
              "281",
              "282",
              "283",
              "284",
              "285",
              "286",
              "287",
              "289",
              "290",
              "291"
            ],
            "covered": 55
          },
          "development_tools\\functions\\analyze_function_registry.py": {
            "statements": 420,
            "missed": 42,
            "coverage": 90,
            "missing_lines": [
              "20",
              "26",
              "27",
              "28",
              "31",
              "32",
              "33",
              "34",
              "35",
              "126",
              "147",
              "148",
              "149",
              "206",
              "222",
              "240",
              "243",
              "367",
              "371",
              "408",
              "433",
              "434",
              "435",
              "436",
              "475",
              "599",
              "600",
              "601",
              "602",
              "603",
              "604",
              "653",
              "654",
              "658",
              "659",
              "662",
              "663",
              "709",
              "710",
              "712",
              "713",
              "717"
            ],
            "covered": 378
          },
          "development_tools\\functions\\analyze_functions.py": {
            "statements": 354,
            "missed": 92,
            "coverage": 74,
            "missing_lines": [
              "21",
              "35",
              "36",
              "37",
              "38",
              "111",
              "112",
              "115",
              "116",
              "120",
              "121",
              "122",
              "126",
              "127",
              "141",
              "148",
              "160",
              "175",
              "178",
              "179",
              "180",
              "181",
              "183",
              "189",
              "191",
              "193",
              "225",
              "227",
              "256",
              "292",
              "294",
              "313",
              "315",
              "418",
              "420",
              "491",
              "493",
              "505",
              "519",
              "552",
              "603",
              "606",
              "609",
              "610",
              "612",
              "615",
              "616",
              "617",
              "618",
              "621",
              "622",
              "623",
              "624",
              "626",
              "629",
              "630",
              "631",
              "632",
              "633",
              "635",
              "636",
              "639",
              "641",
              "642",
              "644",
              "678",
              "684",
              "687",
              "698",
              "699",
              "700",
              "702",
              "704",
              "705",
              "708",
              "709",
              "712",
              "713",
              "714",
              "719",
              "724",
              "725",
              "726",
              "729",
              "730",
              "760",
              "763",
              "768",
              "771",
              "803",
              "812",
              "815"
            ],
            "covered": 262
          },
          "development_tools\\functions\\analyze_package_exports.py": {
            "statements": 314,
            "missed": 314,
            "coverage": 0,
            "missing_lines": [
              "21",
              "22",
              "23",
              "24",
              "25",
              "26",
              "31",
              "32",
              "33",
              "35",
              "36",
              "37",
              "38",
              "39",
              "40",
              "42",
              "45",
              "48",
              "49",
              "50",
              "52",
              "55",
              "56",
              "57",
              "58",
              "59",
              "62",
              "64",
              "65",
              "66",
              "68",
              "70",
              "77",
              "79",
              "80",
              "81",
              "82",
              "92",
              "93",
              "94",
              "95",
              "105",
              "107",
              "109",
              "110",
              "114",
              "116",
              "117",
              "119",
              "120",
              "121",
              "125",
              "126",
              "128",
              "129",
              "130",
              "131",
              "132",
              "133",
              "134",
              "135",
              "136",
              "137",
              "138",
              "141",
              "142",
              "144",
              "145",
              "146",
              "147",
              "150",
              "152",
              "154",
              "155",
              "157",
              "159",
              "161",
              "162",
              "165",
              "166",
              "170",
              "171",
              "173",
              "174",
              "178",
              "180",
              "181",
              "183",
              "186",
              "188",
              "198",
              "199",
              "200",
              "203",
              "204",
              "206",
              "208",
              "209",
              "212",
              "213",
              "214",
              "217",
              "219",
              "220",
              "222",
              "223",
              "225",
              "226",
              "227",
              "230",
              "231",
              "233",
              "234",
              "235",
              "236",
              "239",
              "240",
              "241",
              "248",
              "251",
              "253",
              "255",
              "256",
              "258",
              "259",
              "260",
              "263",
              "264",
              "266",
              "267",
              "268",
              "269",
              "270",
              "271",
              "272",
              "275",
              "276",
              "278",
              "281",
              "282",
              "283",
              "284",
              "286",
              "287",
              "289",
              "290",
              "291",
              "292",
              "295",
              "297",
              "299",
              "300",
              "302",
              "304",
              "305",
              "306",
              "310",
              "312",
              "313",
              "316",
              "317",
              "320",
              "321",
              "322",
              "323",
              "325",
              "326",
              "327",
              "328",
              "331",
              "333",
              "334",
              "335",
              "338",
              "339",
              "342",
              "343",
              "344",
              "347",
              "348",
              "349",
              "350",
              "351",
              "352",
              "355",
              "356",
              "357",
              "358",
              "361",
              "364",
              "367",
              "376",
              "381",
              "384",
              "388",
              "391",
              "395",
              "398",
              "407",
              "410",
              "413",
              "423",
              "426",
              "429",
              "432",
              "434",
              "449",
              "452",
              "454",
              "456",
              "457",
              "460",
              "461",
              "462",
              "464",
              "465",
              "466",
              "467",
              "469",
              "470",
              "471",
              "472",
              "474",
              "476",
              "477",
              "479",
              "480",
              "481",
              "484",
              "486",
              "487",
              "488",
              "491",
              "493",
              "494",
              "495",
              "497",
              "500",
              "502",
              "503",
              "504",
              "506",
              "507",
              "508",
              "509",
              "510",
              "512",
              "513",
              "516",
              "517",
              "518",
              "519",
              "520",
              "521",
              "522",
              "524",
              "525",
              "528",
              "533",
              "534",
              "535",
              "536",
              "538",
              "539",
              "542",
              "545",
              "546",
              "547",
              "548",
              "552",
              "553",
              "554",
              "555",
              "556",
              "558",
              "561",
              "563",
              "565",
              "568",
              "574",
              "575",
              "581",
              "583",
              "585",
              "586",
              "587",
              "589",
              "590",
              "591",
              "592",
              "593",
              "595",
              "596",
              "598",
              "599",
              "600",
              "601",
              "602",
              "603",
              "604",
              "605",
              "606",
              "608",
              "611",
              "612",
              "613",
              "614",
              "616",
              "617",
              "619",
              "620",
              "622",
              "623",
              "624"
            ],
            "covered": 0
          },
          "development_tools\\functions\\generate_function_docstrings.py": {
            "statements": 232,
            "missed": 79,
            "coverage": 66,
            "missing_lines": [
              "25",
              "30",
              "31",
              "51",
              "53",
              "69",
              "125",
              "127",
              "139",
              "145",
              "146",
              "147",
              "149",
              "150",
              "156",
              "158",
              "160",
              "167",
              "168",
              "169",
              "170",
              "171",
              "172",
              "173",
              "174",
              "176",
              "179",
              "182",
              "236",
              "239",
              "247",
              "249",
              "250",
              "292",
              "331",
              "332",
              "333",
              "334",
              "335",
              "339",
              "340",
              "360",
              "364",
              "367",
              "369",
              "373",
              "374",
              "375",
              "387",
              "388",
              "389",
              "391",
              "393",
              "394",
              "396",
              "399",
              "401",
              "406",
              "409",
              "410",
              "412",
              "413",
              "414",
              "416",
              "417",
              "418",
              "419",
              "420",
              "423",
              "426",
              "427",
              "428",
              "429",
              "430",
              "433",
              "435",
              "436",
              "439",
              "441"
            ],
            "covered": 153
          },
          "development_tools\\functions\\generate_function_registry.py": {
            "statements": 482,
            "missed": 392,
            "coverage": 19,
            "missing_lines": [
              "25",
              "31",
              "39",
              "40",
              "42",
              "43",
              "60",
              "67",
              "71",
              "75",
              "79",
              "83",
              "93",
              "94",
              "96",
              "97",
              "98",
              "99",
              "100",
              "102",
              "104",
              "106",
              "107",
              "108",
              "109",
              "110",
              "111",
              "112",
              "114",
              "116",
              "117",
              "118",
              "119",
              "120",
              "121",
              "122",
              "123",
              "124",
              "125",
              "126",
              "128",
              "130",
              "131",
              "133",
              "134",
              "137",
              "144",
              "146",
              "147",
              "342",
              "343",
              "344",
              "345",
              "352",
              "358",
              "366",
              "367",
              "368",
              "373",
              "376",
              "377",
              "380",
              "383",
              "386",
              "389",
              "394",
              "397",
              "400",
              "405",
              "417",
              "428",
              "440",
              "454",
              "470",
              "471",
              "472",
              "473",
              "474",
              "475",
              "477",
              "478",
              "479",
              "480",
              "481",
              "482",
              "484",
              "485",
              "486",
              "488",
              "492",
              "498",
              "577",
              "582",
              "583",
              "585",
              "586",
              "587",
              "588",
              "590",
              "591",
              "597",
              "599",
              "611",
              "612",
              "613",
              "615",
              "616",
              "618",
              "625",
              "627",
              "628",
              "629",
              "630",
              "632",
              "633",
              "635",
              "638",
              "644",
              "646",
              "648",
              "649",
              "650",
              "661",
              "663",
              "670",
              "673",
              "767",
              "779",
              "780",
              "781",
              "782",
              "784",
              "786",
              "787",
              "788",
              "791",
              "792",
              "793",
              "794",
              "795",
              "796",
              "797",
              "798",
              "799",
              "801",
              "802",
              "803",
              "806",
              "807",
              "808",
              "811",
              "812",
              "814",
              "815",
              "821",
              "822",
              "824",
              "828",
              "839",
              "847",
              "848",
              "849",
              "852",
              "854",
              "855",
              "856",
              "857",
              "859",
              "860",
              "861",
              "863",
              "866",
              "872",
              "874",
              "876",
              "877",
              "880",
              "883",
              "884",
              "885",
              "887",
              "890",
              "891",
              "896",
              "897",
              "903",
              "904",
              "905",
              "907",
              "914",
              "917",
              "918",
              "919",
              "920",
              "921",
              "922",
              "923",
              "924",
              "925",
              "926",
              "929",
              "930",
              "931",
              "933",
              "934",
              "935",
              "940",
              "941",
              "942",
              "947",
              "948",
              "949",
              "954",
              "955",
              "958",
              "959",
              "960",
              "961",
              "962",
              "963",
              "964",
              "967",
              "968",
              "969",
              "972",
              "973",
              "975",
              "977",
              "978",
              "979",
              "982",
              "989",
              "990",
              "991",
              "992",
              "993",
              "994",
              "997",
              "1001",
              "1002",
              "1005",
              "1011",
              "1012",
              "1013",
              "1014",
              "1017",
              "1018",
              "1019",
              "1022",
              "1038",
              "1039",
              "1041",
              "1042",
              "1043",
              "1044",
              "1047",
              "1056",
              "1059",
              "1060",
              "1061",
              "1070",
              "1071",
              "1072",
              "1082",
              "1088",
              "1089",
              "1091",
              "1092",
              "1093",
              "1095",
              "1096",
              "1097",
              "1101",
              "1103",
              "1108",
              "1111",
              "1112",
              "1113",
              "1114",
              "1116",
              "1118",
              "1119",
              "1121",
              "1122",
              "1123",
              "1124",
              "1129",
              "1141",
              "1142",
              "1144",
              "1145",
              "1146",
              "1147",
              "1148",
              "1149",
              "1154",
              "1155",
              "1156",
              "1157",
              "1158",
              "1159",
              "1163",
              "1174",
              "1177",
              "1178",
              "1179",
              "1180",
              "1181",
              "1184",
              "1186",
              "1187",
              "1188",
              "1193",
              "1194",
              "1195",
              "1196",
              "1200",
              "1201",
              "1203",
              "1205",
              "1206",
              "1207",
              "1212",
              "1213",
              "1214",
              "1215",
              "1216",
              "1221",
              "1222",
              "1223",
              "1226",
              "1236",
              "1238",
              "1240",
              "1241",
              "1242",
              "1247",
              "1248",
              "1251",
              "1260",
              "1261",
              "1263",
              "1264",
              "1266",
              "1267",
              "1270",
              "1272",
              "1277",
              "1286",
              "1291",
              "1300",
              "1301",
              "1302",
              "1309",
              "1315",
              "1323",
              "1330",
              "1339",
              "1340",
              "1341",
              "1342",
              "1346",
              "1347",
              "1348",
              "1351",
              "1354",
              "1355",
              "1356",
              "1357",
              "1358",
              "1359",
              "1360",
              "1361",
              "1362",
              "1363",
              "1364",
              "1365",
              "1368",
              "1369",
              "1370",
              "1371",
              "1372",
              "1373",
              "1374",
              "1375",
              "1376",
              "1377",
              "1378",
              "1379",
              "1380",
              "1384",
              "1385"
            ],
            "covered": 90
          },
          "development_tools\\imports\\__init__.py": {
            "statements": 0,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 0
          },
          "development_tools\\imports\\analyze_dependency_patterns.py": {
            "statements": 285,
            "missed": 264,
            "coverage": 7,
            "missing_lines": [
              "17",
              "28",
              "42",
              "52",
              "53",
              "54",
              "57",
              "58",
              "68",
              "69",
              "79",
              "80",
              "90",
              "91",
              "100",
              "101",
              "109",
              "113",
              "119",
              "121",
              "122",
              "125",
              "127",
              "129",
              "130",
              "131",
              "136",
              "138",
              "139",
              "140",
              "141",
              "143",
              "149",
              "152",
              "153",
              "154",
              "155",
              "158",
              "161",
              "164",
              "165",
              "166",
              "167",
              "168",
              "169",
              "170",
              "171",
              "172",
              "174",
              "177",
              "178",
              "181",
              "182",
              "183",
              "184",
              "185",
              "186",
              "188",
              "192",
              "195",
              "198",
              "199",
              "200",
              "201",
              "202",
              "205",
              "206",
              "209",
              "214",
              "215",
              "216",
              "217",
              "218",
              "219",
              "222",
              "225",
              "226",
              "227",
              "228",
              "229",
              "230",
              "231",
              "233",
              "239",
              "240",
              "242",
              "245",
              "248",
              "253",
              "257",
              "260",
              "262",
              "263",
              "265",
              "267",
              "268",
              "270",
              "272",
              "274",
              "275",
              "276",
              "277",
              "279",
              "285",
              "288",
              "289",
              "290",
              "291",
              "292",
              "293",
              "294",
              "295",
              "296",
              "299",
              "300",
              "301",
              "302",
              "303",
              "304",
              "305",
              "306",
              "307",
              "310",
              "311",
              "312",
              "315",
              "316",
              "321",
              "322",
              "323",
              "326",
              "327",
              "328",
              "329",
              "330",
              "331",
              "332",
              "333",
              "335",
              "341",
              "343",
              "344",
              "347",
              "350",
              "351",
              "352",
              "354",
              "355",
              "358",
              "361",
              "364",
              "365",
              "368",
              "369",
              "370",
              "371",
              "372",
              "373",
              "374",
              "375",
              "376",
              "378",
              "387",
              "388",
              "389",
              "391",
              "395",
              "398",
              "399",
              "400",
              "401",
              "404",
              "405",
              "406",
              "409",
              "411",
              "412",
              "413",
              "414",
              "415",
              "417",
              "418",
              "419",
              "420",
              "421",
              "423",
              "424",
              "425",
              "426",
              "427",
              "428",
              "431",
              "436",
              "439",
              "440",
              "441",
              "443",
              "444",
              "445",
              "446",
              "447",
              "449",
              "452",
              "453",
              "454",
              "455",
              "456",
              "458",
              "459",
              "460",
              "463",
              "464",
              "465",
              "466",
              "469",
              "470",
              "471",
              "473",
              "474",
              "475",
              "476",
              "477",
              "478",
              "480",
              "481",
              "482",
              "483",
              "484",
              "485",
              "487",
              "490",
              "491",
              "492",
              "493",
              "494",
              "495",
              "498",
              "499",
              "500",
              "501",
              "503",
              "504",
              "505",
              "506",
              "507",
              "508",
              "510",
              "511",
              "512",
              "513",
              "514",
              "515",
              "517",
              "518",
              "519",
              "520",
              "521",
              "522",
              "523",
              "525",
              "535",
              "536",
              "543",
              "544"
            ],
            "covered": 21
          },
          "development_tools\\imports\\analyze_module_dependencies.py": {
            "statements": 274,
            "missed": 165,
            "coverage": 40,
            "missing_lines": [
              "19",
              "29",
              "30",
              "125",
              "126",
              "130",
              "142",
              "143",
              "145",
              "146",
              "148",
              "149",
              "150",
              "153",
              "154",
              "157",
              "159",
              "160",
              "161",
              "162",
              "165",
              "168",
              "171",
              "173",
              "174",
              "175",
              "178",
              "179",
              "180",
              "181",
              "182",
              "183",
              "185",
              "186",
              "187",
              "189",
              "190",
              "191",
              "192",
              "193",
              "195",
              "198",
              "199",
              "200",
              "201",
              "202",
              "203",
              "204",
              "205",
              "206",
              "208",
              "209",
              "214",
              "215",
              "220",
              "221",
              "222",
              "224",
              "225",
              "226",
              "227",
              "228",
              "231",
              "250",
              "253",
              "254",
              "255",
              "280",
              "281",
              "282",
              "284",
              "286",
              "289",
              "290",
              "292",
              "293",
              "295",
              "296",
              "299",
              "300",
              "302",
              "304",
              "307",
              "308",
              "378",
              "380",
              "382",
              "403",
              "404",
              "405",
              "408",
              "409",
              "410",
              "411",
              "412",
              "414",
              "415",
              "416",
              "417",
              "419",
              "420",
              "422",
              "423",
              "427",
              "428",
              "429",
              "430",
              "435",
              "436",
              "438",
              "439",
              "442",
              "445",
              "446",
              "447",
              "448",
              "451",
              "454",
              "457",
              "458",
              "459",
              "460",
              "463",
              "464",
              "465",
              "467",
              "468",
              "469",
              "470",
              "471",
              "472",
              "473",
              "476",
              "477",
              "478",
              "480",
              "481",
              "482",
              "490",
              "492",
              "493",
              "494",
              "495",
              "496",
              "499",
              "502",
              "503",
              "504",
              "505",
              "508",
              "509",
              "510",
              "511",
              "512",
              "513",
              "514",
              "515",
              "516",
              "518",
              "519",
              "520",
              "525",
              "528",
              "529",
              "532"
            ],
            "covered": 109
          },
          "development_tools\\imports\\analyze_module_imports.py": {
            "statements": 274,
            "missed": 113,
            "coverage": 59,
            "missing_lines": [
              "18",
              "27",
              "28",
              "51",
              "70",
              "72",
              "108",
              "147",
              "148",
              "149",
              "150",
              "161",
              "163",
              "189",
              "239",
              "240",
              "241",
              "242",
              "243",
              "257",
              "258",
              "259",
              "283",
              "293",
              "294",
              "323",
              "324",
              "325",
              "326",
              "327",
              "328",
              "329",
              "330",
              "331",
              "332",
              "333",
              "334",
              "335",
              "336",
              "337",
              "338",
              "339",
              "340",
              "342",
              "347",
              "348",
              "349",
              "350",
              "351",
              "352",
              "353",
              "354",
              "355",
              "356",
              "357",
              "358",
              "359",
              "360",
              "361",
              "362",
              "363",
              "364",
              "365",
              "366",
              "367",
              "368",
              "369",
              "370",
              "371",
              "372",
              "373",
              "374",
              "375",
              "376",
              "377",
              "378",
              "379",
              "380",
              "381",
              "382",
              "383",
              "384",
              "385",
              "387",
              "392",
              "393",
              "396",
              "397",
              "400",
              "401",
              "404",
              "405",
              "407",
              "412",
              "413",
              "416",
              "417",
              "420",
              "421",
              "424",
              "425",
              "429",
              "434",
              "435",
              "436",
              "437",
              "439",
              "444",
              "449",
              "453",
              "455",
              "457",
              "459"
            ],
            "covered": 161
          },
          "development_tools\\imports\\analyze_unused_imports.py": {
            "statements": 401,
            "missed": 192,
            "coverage": 52,
            "missing_lines": [
              "34",
              "37",
              "38",
              "39",
              "40",
              "41",
              "42",
              "52",
              "53",
              "55",
              "57",
              "67",
              "72",
              "73",
              "74",
              "75",
              "76",
              "77",
              "79",
              "81",
              "82",
              "83",
              "84",
              "166",
              "167",
              "168",
              "169",
              "170",
              "173",
              "174",
              "175",
              "178",
              "179",
              "182",
              "183",
              "186",
              "187",
              "189",
              "193",
              "196",
              "197",
              "198",
              "200",
              "239",
              "249",
              "250",
              "251",
              "252",
              "259",
              "260",
              "277",
              "285",
              "286",
              "289",
              "303",
              "304",
              "307",
              "313",
              "314",
              "317",
              "323",
              "324",
              "327",
              "331",
              "332",
              "333",
              "346",
              "350",
              "381",
              "392",
              "393",
              "397",
              "398",
              "399",
              "400",
              "411",
              "421",
              "422",
              "424",
              "433",
              "439",
              "444",
              "445",
              "448",
              "452",
              "453",
              "456",
              "490",
              "496",
              "497",
              "498",
              "501",
              "505",
              "506",
              "509",
              "520",
              "571",
              "586",
              "588",
              "599",
              "629",
              "632",
              "633",
              "634",
              "636",
              "645",
              "652",
              "671",
              "675",
              "679",
              "680",
              "681",
              "684",
              "688",
              "691",
              "695",
              "696",
              "697",
              "700",
              "702",
              "710",
              "743",
              "744",
              "747",
              "756",
              "757",
              "760",
              "774",
              "775",
              "778",
              "791",
              "818",
              "820",
              "824",
              "825",
              "828",
              "832",
              "836",
              "837",
              "840",
              "843",
              "847",
              "848",
              "851",
              "853",
              "877",
              "878",
              "888",
              "890",
              "892",
              "893",
              "894",
              "895",
              "896",
              "897",
              "899",
              "901",
              "902",
              "903",
              "905",
              "915",
              "938",
              "950",
              "951",
              "975",
              "976",
              "977",
              "991",
              "1008",
              "1019",
              "1054",
              "1055",
              "1059",
              "1061",
              "1064",
              "1067",
              "1072",
              "1073",
              "1076",
              "1082",
              "1085",
              "1088",
              "1089",
              "1092",
              "1094",
              "1095",
              "1098",
              "1099",
              "1100",
              "1101",
              "1102",
              "1106"
            ],
            "covered": 209
          },
          "development_tools\\imports\\generate_module_dependencies.py": {
            "statements": 360,
            "missed": 173,
            "coverage": 52,
            "missing_lines": [
              "17",
              "34",
              "35",
              "42",
              "45",
              "194",
              "210",
              "219",
              "224",
              "264",
              "323",
              "324",
              "325",
              "326",
              "327",
              "328",
              "329",
              "330",
              "331",
              "332",
              "354",
              "355",
              "356",
              "362",
              "363",
              "364",
              "365",
              "366",
              "367",
              "394",
              "401",
              "403",
              "404",
              "405",
              "406",
              "420",
              "431",
              "432",
              "434",
              "435",
              "480",
              "505",
              "511",
              "517",
              "523",
              "524",
              "533",
              "536",
              "537",
              "538",
              "539",
              "540",
              "542",
              "546",
              "548",
              "549",
              "551",
              "552",
              "553",
              "560",
              "561",
              "563",
              "564",
              "565",
              "566",
              "568",
              "569",
              "572",
              "573",
              "574",
              "576",
              "577",
              "579",
              "580",
              "582",
              "586",
              "588",
              "590",
              "593",
              "594",
              "596",
              "597",
              "598",
              "599",
              "600",
              "604",
              "606",
              "608",
              "613",
              "615",
              "616",
              "617",
              "620",
              "623",
              "625",
              "626",
              "629",
              "630",
              "633",
              "636",
              "639",
              "641",
              "642",
              "694",
              "708",
              "710",
              "711",
              "716",
              "721",
              "722",
              "723",
              "724",
              "725",
              "726",
              "727",
              "729",
              "730",
              "734",
              "735",
              "738",
              "743",
              "748",
              "750",
              "751",
              "752",
              "763",
              "764",
              "772",
              "777",
              "778",
              "783",
              "788",
              "789",
              "790",
              "795",
              "800",
              "801",
              "802",
              "805",
              "806",
              "811",
              "814",
              "816",
              "821",
              "822",
              "823",
              "824",
              "826",
              "827",
              "834",
              "837",
              "843",
              "844",
              "847",
              "848",
              "853",
              "856",
              "857",
              "858",
              "859",
              "861",
              "862",
              "864",
              "865",
              "867",
              "868",
              "874",
              "879",
              "880",
              "886",
              "888",
              "889",
              "894"
            ],
            "covered": 187
          },
          "development_tools\\imports\\generate_unused_imports_report.py": {
            "statements": 165,
            "missed": 55,
            "coverage": 67,
            "missing_lines": [
              "31",
              "41",
              "42",
              "43",
              "44",
              "50",
              "51",
              "117",
              "208",
              "212",
              "216",
              "223",
              "224",
              "227",
              "228",
              "231",
              "232",
              "235",
              "236",
              "299",
              "302",
              "307",
              "313",
              "316",
              "319",
              "322",
              "323",
              "325",
              "328",
              "330",
              "331",
              "332",
              "333",
              "335",
              "336",
              "339",
              "346",
              "347",
              "350",
              "353",
              "354",
              "357",
              "358",
              "361",
              "362",
              "371",
              "374",
              "375",
              "377",
              "386",
              "387",
              "388",
              "389",
              "390",
              "392"
            ],
            "covered": 110
          },
          "development_tools\\legacy\\__init__.py": {
            "statements": 0,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 0
          },
          "development_tools\\legacy\\analyze_legacy_references.py": {
            "statements": 335,
            "missed": 157,
            "coverage": 53,
            "missing_lines": [
              "27",
              "35",
              "41",
              "42",
              "73",
              "86",
              "119",
              "120",
              "121",
              "122",
              "124",
              "132",
              "134",
              "138",
              "146",
              "148",
              "153",
              "159",
              "160",
              "162",
              "163",
              "168",
              "174",
              "175",
              "177",
              "209",
              "212",
              "214",
              "215",
              "217",
              "229",
              "231",
              "232",
              "233",
              "234",
              "235",
              "236",
              "238",
              "249",
              "264",
              "289",
              "291",
              "293",
              "294",
              "295",
              "296",
              "298",
              "301",
              "302",
              "304",
              "305",
              "306",
              "314",
              "315",
              "316",
              "324",
              "325",
              "326",
              "328",
              "329",
              "332",
              "334",
              "336",
              "337",
              "338",
              "339",
              "346",
              "348",
              "350",
              "351",
              "352",
              "353",
              "355",
              "358",
              "359",
              "361",
              "362",
              "363",
              "371",
              "372",
              "373",
              "422",
              "468",
              "474",
              "489",
              "490",
              "491",
              "498",
              "499",
              "500",
              "502",
              "503",
              "504",
              "505",
              "515",
              "516",
              "518",
              "519",
              "520",
              "565",
              "567",
              "569",
              "573",
              "587",
              "591",
              "595",
              "599",
              "633",
              "636",
              "642",
              "649",
              "651",
              "654",
              "655",
              "657",
              "658",
              "660",
              "661",
              "662",
              "663",
              "664",
              "665",
              "666",
              "668",
              "670",
              "673",
              "674",
              "676",
              "677",
              "680",
              "681",
              "682",
              "683",
              "684",
              "685",
              "686",
              "688",
              "689",
              "690",
              "696",
              "698",
              "699",
              "700",
              "701",
              "703",
              "704",
              "705",
              "706",
              "708",
              "711",
              "714",
              "715",
              "716",
              "718",
              "719",
              "720",
              "721"
            ],
            "covered": 178
          },
          "development_tools\\legacy\\fix_legacy_references.py": {
            "statements": 160,
            "missed": 100,
            "coverage": 38,
            "missing_lines": [
              "36",
              "46",
              "47",
              "50",
              "58",
              "59",
              "93",
              "111",
              "145",
              "146",
              "181",
              "183",
              "184",
              "185",
              "188",
              "189",
              "192",
              "195",
              "198",
              "200",
              "201",
              "204",
              "205",
              "206",
              "207",
              "209",
              "210",
              "211",
              "212",
              "214",
              "215",
              "216",
              "217",
              "221",
              "224",
              "225",
              "226",
              "229",
              "231",
              "232",
              "233",
              "235",
              "236",
              "238",
              "243",
              "246",
              "252",
              "255",
              "261",
              "266",
              "272",
              "279",
              "282",
              "283",
              "285",
              "286",
              "288",
              "289",
              "291",
              "292",
              "293",
              "294",
              "295",
              "296",
              "297",
              "299",
              "301",
              "303",
              "304",
              "306",
              "307",
              "310",
              "311",
              "312",
              "313",
              "314",
              "315",
              "316",
              "318",
              "319",
              "320",
              "326",
              "328",
              "329",
              "330",
              "331",
              "333",
              "334",
              "335",
              "336",
              "338",
              "341",
              "342",
              "345",
              "346",
              "348",
              "349",
              "350",
              "351",
              "352"
            ],
            "covered": 60
          },
          "development_tools\\legacy\\generate_legacy_reference_report.py": {
            "statements": 106,
            "missed": 35,
            "coverage": 67,
            "missing_lines": [
              "27",
              "36",
              "42",
              "43",
              "121",
              "133",
              "153",
              "187",
              "188",
              "192",
              "195",
              "198",
              "200",
              "208",
              "209",
              "211",
              "216",
              "219",
              "225",
              "232",
              "234",
              "237",
              "238",
              "239",
              "242",
              "244",
              "245",
              "247",
              "250",
              "253",
              "256",
              "257",
              "259",
              "260",
              "262"
            ],
            "covered": 71
          },
          "development_tools\\reports\\__init__.py": {
            "statements": 0,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 0
          },
          "development_tools\\reports\\analyze_system_signals.py": {
            "statements": 424,
            "missed": 295,
            "coverage": 30,
            "missing_lines": [
              "24",
              "34",
              "35",
              "49",
              "59",
              "64",
              "72",
              "79",
              "111",
              "112",
              "113",
              "137",
              "138",
              "139",
              "140",
              "143",
              "144",
              "145",
              "148",
              "149",
              "150",
              "153",
              "154",
              "155",
              "158",
              "159",
              "160",
              "163",
              "164",
              "165",
              "168",
              "171",
              "172",
              "173",
              "176",
              "180",
              "181",
              "184",
              "189",
              "190",
              "192",
              "193",
              "196",
              "198",
              "203",
              "204",
              "205",
              "206",
              "207",
              "208",
              "210",
              "211",
              "212",
              "215",
              "216",
              "217",
              "220",
              "223",
              "224",
              "227",
              "232",
              "236",
              "239",
              "244",
              "247",
              "250",
              "254",
              "257",
              "258",
              "259",
              "260",
              "261",
              "262",
              "264",
              "265",
              "266",
              "269",
              "270",
              "271",
              "274",
              "278",
              "279",
              "282",
              "285",
              "286",
              "290",
              "291",
              "302",
              "303",
              "304",
              "306",
              "307",
              "308",
              "311",
              "314",
              "315",
              "318",
              "323",
              "324",
              "325",
              "328",
              "329",
              "333",
              "336",
              "337",
              "339",
              "354",
              "362",
              "366",
              "371",
              "372",
              "374",
              "375",
              "377",
              "378",
              "380",
              "381",
              "385",
              "402",
              "407",
              "410",
              "411",
              "414",
              "422",
              "432",
              "433",
              "436",
              "437",
              "438",
              "443",
              "446",
              "470",
              "471",
              "494",
              "502",
              "504",
              "505",
              "520",
              "522",
              "523",
              "529",
              "530",
              "531",
              "532",
              "533",
              "534",
              "548",
              "549",
              "552",
              "553",
              "554",
              "592",
              "593",
              "601",
              "602",
              "603",
              "609",
              "610",
              "612",
              "618",
              "619",
              "620",
              "622",
              "623",
              "624",
              "625",
              "628",
              "632",
              "636",
              "637",
              "638",
              "639",
              "640",
              "641",
              "644",
              "645",
              "646",
              "647",
              "648",
              "650",
              "651",
              "652",
              "653",
              "655",
              "659",
              "666",
              "667",
              "668",
              "669",
              "670",
              "671",
              "672",
              "673",
              "676",
              "677",
              "678",
              "679",
              "680",
              "681",
              "682",
              "684",
              "689",
              "691",
              "694",
              "695",
              "697",
              "699",
              "700",
              "702",
              "703",
              "705",
              "707",
              "708",
              "709",
              "710",
              "711",
              "712",
              "714",
              "717",
              "722",
              "723",
              "724",
              "725",
              "726",
              "729",
              "730",
              "731",
              "732",
              "733",
              "736",
              "737",
              "738",
              "739",
              "740",
              "743",
              "744",
              "745",
              "746",
              "747",
              "748",
              "750",
              "751",
              "752",
              "753",
              "754",
              "756",
              "757",
              "758",
              "759",
              "760",
              "763",
              "764",
              "765",
              "766",
              "767",
              "770",
              "771",
              "772",
              "773",
              "774",
              "777",
              "778",
              "779",
              "782",
              "783",
              "786",
              "789",
              "790",
              "791",
              "792",
              "793",
              "794",
              "795",
              "796",
              "799",
              "800",
              "801",
              "802",
              "803",
              "804",
              "805",
              "806",
              "807",
              "810",
              "811",
              "812",
              "813",
              "814",
              "817",
              "818",
              "819",
              "820",
              "821",
              "822",
              "824"
            ],
            "covered": 129
          },
          "development_tools\\reports\\decision_support.py": {
            "statements": 107,
            "missed": 13,
            "coverage": 88,
            "missing_lines": [
              "22",
              "23",
              "24",
              "27",
              "28",
              "29",
              "30",
              "31",
              "124",
              "135",
              "146",
              "153",
              "160"
            ],
            "covered": 94
          },
          "development_tools\\reports\\generate_consolidated_report.py": {
            "statements": 26,
            "missed": 4,
            "coverage": 85,
            "missing_lines": [
              "20",
              "26",
              "27",
              "28"
            ],
            "covered": 22
          },
          "development_tools\\reports\\quick_status.py": {
            "statements": 238,
            "missed": 157,
            "coverage": 34,
            "missing_lines": [
              "22",
              "36",
              "47",
              "52",
              "60",
              "68",
              "111",
              "115",
              "121",
              "122",
              "123",
              "124",
              "125",
              "128",
              "129",
              "130",
              "135",
              "136",
              "137",
              "138",
              "139",
              "142",
              "143",
              "144",
              "147",
              "155",
              "156",
              "157",
              "159",
              "163",
              "166",
              "167",
              "168",
              "171",
              "172",
              "175",
              "176",
              "177",
              "178",
              "179",
              "180",
              "181",
              "182",
              "185",
              "188",
              "189",
              "191",
              "195",
              "198",
              "199",
              "200",
              "201",
              "202",
              "203",
              "206",
              "207",
              "210",
              "211",
              "214",
              "215",
              "218",
              "219",
              "221",
              "230",
              "246",
              "247",
              "248",
              "249",
              "250",
              "251",
              "285",
              "294",
              "298",
              "307",
              "315",
              "316",
              "317",
              "318",
              "329",
              "330",
              "333",
              "339",
              "340",
              "342",
              "344",
              "345",
              "347",
              "348",
              "349",
              "352",
              "356",
              "358",
              "360",
              "361",
              "364",
              "365",
              "366",
              "367",
              "368",
              "371",
              "372",
              "373",
              "374",
              "377",
              "378",
              "379",
              "380",
              "381",
              "382",
              "385",
              "386",
              "387",
              "390",
              "391",
              "394",
              "395",
              "396",
              "397",
              "398",
              "399",
              "400",
              "403",
              "404",
              "405",
              "406",
              "409",
              "410",
              "411",
              "414",
              "415",
              "416",
              "417",
              "418",
              "419",
              "420",
              "423",
              "424",
              "425",
              "426",
              "430",
              "431",
              "433",
              "438",
              "440",
              "441",
              "442",
              "443",
              "445",
              "446",
              "448",
              "449",
              "450",
              "451",
              "453",
              "455",
              "456",
              "457"
            ],
            "covered": 81
          },
          "development_tools\\run_dev_tools.py": {
            "statements": 6,
            "missed": 6,
            "coverage": 0,
            "missing_lines": [
              "11",
              "12",
              "15",
              "16",
              "17",
              "22"
            ],
            "covered": 0
          },
          "development_tools\\run_development_tools.py": {
            "statements": 125,
            "missed": 51,
            "coverage": 59,
            "missing_lines": [
              "23",
              "25",
              "26",
              "27",
              "28",
              "29",
              "30",
              "52",
              "53",
              "56",
              "57",
              "58",
              "59",
              "60",
              "61",
              "62",
              "63",
              "64",
              "65",
              "66",
              "67",
              "72",
              "73",
              "75",
              "76",
              "77",
              "78",
              "79",
              "80",
              "83",
              "84",
              "89",
              "90",
              "91",
              "92",
              "93",
              "94",
              "97",
              "98",
              "102",
              "186",
              "193",
              "194",
              "195",
              "196",
              "198",
              "199",
              "224",
              "225",
              "226",
              "227"
            ],
            "covered": 74
          },
          "development_tools\\shared\\__init__.py": {
            "statements": 4,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 4
          },
          "development_tools\\shared\\cli_interface.py": {
            "statements": 262,
            "missed": 204,
            "coverage": 22,
            "missing_lines": [
              "20",
              "35",
              "36",
              "71",
              "72",
              "94",
              "95",
              "96",
              "97",
              "98",
              "105",
              "106",
              "107",
              "108",
              "109",
              "110",
              "112",
              "113",
              "118",
              "119",
              "120",
              "121",
              "122",
              "129",
              "130",
              "132",
              "133",
              "134",
              "136",
              "137",
              "138",
              "140",
              "141",
              "142",
              "146",
              "147",
              "148",
              "151",
              "152",
              "153",
              "155",
              "156",
              "164",
              "165",
              "172",
              "173",
              "174",
              "176",
              "177",
              "178",
              "182",
              "183",
              "184",
              "185",
              "186",
              "187",
              "191",
              "192",
              "193",
              "194",
              "195",
              "196",
              "201",
              "202",
              "203",
              "204",
              "205",
              "206",
              "208",
              "209",
              "213",
              "214",
              "215",
              "216",
              "217",
              "218",
              "220",
              "221",
              "226",
              "227",
              "232",
              "237",
              "242",
              "247",
              "248",
              "254",
              "255",
              "256",
              "257",
              "260",
              "261",
              "262",
              "263",
              "264",
              "265",
              "266",
              "267",
              "268",
              "270",
              "271",
              "272",
              "273",
              "274",
              "275",
              "276",
              "279",
              "281",
              "282",
              "286",
              "287",
              "288",
              "289",
              "290",
              "291",
              "293",
              "294",
              "298",
              "299",
              "300",
              "301",
              "302",
              "303",
              "305",
              "306",
              "311",
              "312",
              "313",
              "314",
              "316",
              "317",
              "318",
              "319",
              "321",
              "322",
              "323",
              "330",
              "331",
              "332",
              "333",
              "335",
              "336",
              "339",
              "340",
              "342",
              "343",
              "344",
              "349",
              "350",
              "355",
              "360",
              "363",
              "368",
              "374",
              "375",
              "378",
              "381",
              "384",
              "385",
              "388",
              "389",
              "390",
              "393",
              "394",
              "396",
              "397",
              "398",
              "399",
              "400",
              "403",
              "404",
              "405",
              "408",
              "409",
              "410",
              "411",
              "415",
              "416",
              "417",
              "418",
              "421",
              "422",
              "423",
              "425",
              "435",
              "436",
              "437",
              "438",
              "439",
              "440",
              "442",
              "445",
              "447",
              "448",
              "459",
              "462",
              "466",
              "467",
              "468",
              "469",
              "470",
              "471",
              "473",
              "474",
              "622"
            ],
            "covered": 58
          },
          "development_tools\\shared\\common.py": {
            "statements": 103,
            "missed": 49,
            "coverage": 52,
            "missing_lines": [
              "22",
              "24",
              "25",
              "27",
              "28",
              "34",
              "35",
              "47",
              "49",
              "76",
              "78",
              "93",
              "95",
              "96",
              "97",
              "98",
              "99",
              "105",
              "111",
              "112",
              "113",
              "117",
              "118",
              "149",
              "162",
              "163",
              "166",
              "167",
              "168",
              "169",
              "170",
              "171",
              "172",
              "173",
              "174",
              "176",
              "177",
              "178",
              "179",
              "180",
              "181",
              "182",
              "183",
              "185",
              "186",
              "187",
              "188",
              "189",
              "197"
            ],
            "covered": 54
          },
          "development_tools\\shared\\constants.py": {
            "statements": 107,
            "missed": 24,
            "coverage": 78,
            "missing_lines": [
              "19",
              "21",
              "22",
              "24",
              "25",
              "32",
              "33",
              "54",
              "55",
              "56",
              "64",
              "72",
              "80",
              "332",
              "341",
              "363",
              "365",
              "366",
              "368",
              "370",
              "371",
              "386",
              "401",
              "404"
            ],
            "covered": 83
          },
          "development_tools\\shared\\exclusion_utilities.py": {
            "statements": 65,
            "missed": 3,
            "coverage": 95,
            "missing_lines": [
              "45",
              "46",
              "122"
            ],
            "covered": 62
          },
          "development_tools\\shared\\export_code_snapshot.py": {
            "statements": 104,
            "missed": 104,
            "coverage": 0,
            "missing_lines": [
              "13",
              "15",
              "16",
              "17",
              "18",
              "19",
              "21",
              "22",
              "28",
              "30",
              "33",
              "45",
              "47",
              "48",
              "50",
              "51",
              "53",
              "54",
              "55",
              "56",
              "58",
              "61",
              "69",
              "70",
              "73",
              "75",
              "77",
              "78",
              "80",
              "83",
              "84",
              "85",
              "86",
              "87",
              "89",
              "91",
              "92",
              "94",
              "96",
              "97",
              "100",
              "103",
              "104",
              "106",
              "107",
              "108",
              "109",
              "110",
              "112",
              "113",
              "114",
              "115",
              "116",
              "118",
              "119",
              "120",
              "122",
              "123",
              "124",
              "125",
              "127",
              "128",
              "129",
              "130",
              "131",
              "132",
              "133",
              "134",
              "135",
              "136",
              "137",
              "138",
              "141",
              "142",
              "146",
              "151",
              "156",
              "162",
              "167",
              "172",
              "179",
              "181",
              "182",
              "183",
              "184",
              "186",
              "188",
              "189",
              "190",
              "192",
              "200",
              "202",
              "205",
              "207",
              "208",
              "210",
              "213",
              "214",
              "216",
              "217",
              "219",
              "221",
              "223",
              "224"
            ],
            "covered": 0
          },
          "development_tools\\shared\\file_rotation.py": {
            "statements": 343,
            "missed": 200,
            "coverage": 42,
            "missing_lines": [
              "29",
              "61",
              "73",
              "74",
              "75",
              "107",
              "109",
              "113",
              "114",
              "116",
              "117",
              "120",
              "196",
              "197",
              "208",
              "210",
              "211",
              "238",
              "239",
              "241",
              "242",
              "243",
              "244",
              "246",
              "257",
              "262",
              "263",
              "270",
              "273",
              "274",
              "275",
              "276",
              "277",
              "278",
              "280",
              "300",
              "301",
              "303",
              "304",
              "305",
              "310",
              "311",
              "312",
              "313",
              "314",
              "315",
              "316",
              "339",
              "342",
              "345",
              "346",
              "347",
              "348",
              "353",
              "354",
              "355",
              "357",
              "358",
              "365",
              "366",
              "368",
              "369",
              "370",
              "375",
              "376",
              "377",
              "378",
              "379",
              "380",
              "381",
              "382",
              "384",
              "385",
              "386",
              "387",
              "388",
              "389",
              "390",
              "394",
              "395",
              "401",
              "402",
              "404",
              "407",
              "410",
              "416",
              "417",
              "419",
              "422",
              "423",
              "424",
              "427",
              "428",
              "429",
              "432",
              "433",
              "434",
              "439",
              "440",
              "441",
              "443",
              "444",
              "447",
              "448",
              "452",
              "454",
              "458",
              "461",
              "462",
              "465",
              "469",
              "473",
              "476",
              "478",
              "482",
              "484",
              "485",
              "488",
              "492",
              "497",
              "500",
              "502",
              "503",
              "506",
              "530",
              "534",
              "537",
              "538",
              "543",
              "544",
              "545",
              "546",
              "547",
              "573",
              "576",
              "579",
              "581",
              "582",
              "586",
              "587",
              "589",
              "590",
              "592",
              "595",
              "602",
              "605",
              "608",
              "609",
              "611",
              "614",
              "615",
              "617",
              "621",
              "622",
              "623",
              "624",
              "625",
              "626",
              "628",
              "629",
              "633",
              "634",
              "635",
              "639",
              "641",
              "643",
              "644",
              "647",
              "650",
              "651",
              "655",
              "657",
              "659",
              "660",
              "662",
              "663",
              "666",
              "668",
              "669",
              "673",
              "676",
              "678",
              "679",
              "681",
              "689",
              "705",
              "706",
              "708",
              "721",
              "726",
              "753",
              "756",
              "759",
              "760",
              "761",
              "764",
              "765",
              "767",
              "768",
              "770"
            ],
            "covered": 143
          },
          "development_tools\\shared\\fix_project_cleanup.py": {
            "statements": 318,
            "missed": 88,
            "coverage": 72,
            "missing_lines": [
              "33",
              "100",
              "101",
              "102",
              "103",
              "149",
              "150",
              "151",
              "155",
              "156",
              "157",
              "158",
              "159",
              "160",
              "161",
              "162",
              "163",
              "164",
              "165",
              "166",
              "168",
              "169",
              "170",
              "187",
              "188",
              "189",
              "206",
              "207",
              "208",
              "216",
              "217",
              "218",
              "219",
              "220",
              "222",
              "223",
              "224",
              "229",
              "230",
              "231",
              "232",
              "233",
              "235",
              "236",
              "237",
              "271",
              "273",
              "276",
              "277",
              "278",
              "286",
              "300",
              "301",
              "302",
              "324",
              "325",
              "326",
              "344",
              "345",
              "346",
              "347",
              "348",
              "349",
              "350",
              "352",
              "353",
              "354",
              "359",
              "360",
              "361",
              "362",
              "363",
              "365",
              "366",
              "367",
              "373",
              "374",
              "375",
              "376",
              "377",
              "378",
              "380",
              "381",
              "382",
              "407",
              "408",
              "409",
              "529"
            ],
            "covered": 230
          },
          "development_tools\\shared\\mtime_cache.py": {
            "statements": 139,
            "missed": 34,
            "coverage": 76,
            "missing_lines": [
              "39",
              "40",
              "71",
              "104",
              "105",
              "119",
              "120",
              "146",
              "147",
              "151",
              "153",
              "157",
              "158",
              "159",
              "172",
              "173",
              "193",
              "202",
              "209",
              "210",
              "211",
              "216",
              "242",
              "243",
              "244",
              "253",
              "255",
              "268",
              "273",
              "274",
              "308",
              "310",
              "314",
              "323"
            ],
            "covered": 105
          },
          "development_tools\\shared\\output_storage.py": {
            "statements": 221,
            "missed": 113,
            "coverage": 49,
            "missing_lines": [
              "34",
              "35",
              "37",
              "38",
              "40",
              "41",
              "42",
              "43",
              "46",
              "47",
              "48",
              "49",
              "50",
              "51",
              "52",
              "53",
              "54",
              "55",
              "56",
              "57",
              "58",
              "59",
              "60",
              "61",
              "62",
              "63",
              "66",
              "72",
              "73",
              "75",
              "76",
              "78",
              "102",
              "107",
              "163",
              "164",
              "168",
              "169",
              "170",
              "190",
              "211",
              "212",
              "213",
              "216",
              "218",
              "224",
              "225",
              "226",
              "228",
              "236",
              "272",
              "277",
              "304",
              "305",
              "306",
              "328",
              "333",
              "380",
              "385",
              "398",
              "399",
              "400",
              "415",
              "416",
              "418",
              "420",
              "421",
              "424",
              "437",
              "439",
              "440",
              "441",
              "442",
              "443",
              "444",
              "445",
              "446",
              "447",
              "448",
              "449",
              "450",
              "451",
              "452",
              "453",
              "455",
              "456",
              "457",
              "458",
              "461",
              "462",
              "465",
              "466",
              "467",
              "469",
              "471",
              "472",
              "473",
              "475",
              "476",
              "477",
              "478",
              "479",
              "480",
              "481",
              "482",
              "484",
              "485",
              "487",
              "488",
              "489",
              "490",
              "491",
              "493"
            ],
            "covered": 108
          },
          "development_tools\\shared\\result_format.py": {
            "statements": 308,
            "missed": 261,
            "coverage": 15,
            "missing_lines": [
              "45",
              "48",
              "64",
              "71",
              "72",
              "89",
              "90",
              "91",
              "94",
              "95",
              "96",
              "99",
              "100",
              "101",
              "104",
              "105",
              "106",
              "109",
              "110",
              "111",
              "114",
              "115",
              "116",
              "119",
              "120",
              "121",
              "124",
              "125",
              "126",
              "129",
              "130",
              "131",
              "134",
              "135",
              "136",
              "139",
              "140",
              "143",
              "144",
              "145",
              "148",
              "149",
              "152",
              "186",
              "193",
              "194",
              "211",
              "219",
              "220",
              "221",
              "224",
              "225",
              "230",
              "233",
              "234",
              "237",
              "238",
              "241",
              "242",
              "243",
              "245",
              "251",
              "252",
              "253",
              "256",
              "262",
              "265",
              "266",
              "268",
              "269",
              "272",
              "273",
              "280",
              "282",
              "288",
              "289",
              "290",
              "293",
              "303",
              "306",
              "307",
              "308",
              "310",
              "316",
              "317",
              "318",
              "322",
              "329",
              "331",
              "334",
              "335",
              "337",
              "343",
              "344",
              "345",
              "348",
              "354",
              "357",
              "358",
              "359",
              "360",
              "361",
              "363",
              "366",
              "367",
              "369",
              "375",
              "376",
              "377",
              "381",
              "382",
              "384",
              "389",
              "395",
              "397",
              "400",
              "401",
              "403",
              "409",
              "410",
              "411",
              "414",
              "419",
              "422",
              "423",
              "425",
              "431",
              "432",
              "433",
              "437",
              "438",
              "441",
              "442",
              "443",
              "446",
              "447",
              "449",
              "451",
              "454",
              "455",
              "457",
              "463",
              "464",
              "465",
              "468",
              "469",
              "475",
              "477",
              "480",
              "481",
              "483",
              "489",
              "490",
              "491",
              "496",
              "497",
              "498",
              "500",
              "502",
              "505",
              "506",
              "508",
              "514",
              "515",
              "516",
              "520",
              "521",
              "522",
              "523",
              "525",
              "526",
              "527",
              "528",
              "529",
              "530",
              "531",
              "532",
              "533",
              "537",
              "542",
              "543",
              "545",
              "547",
              "550",
              "551",
              "553",
              "559",
              "560",
              "561",
              "564",
              "566",
              "569",
              "570",
              "572",
              "578",
              "579",
              "580",
              "584",
              "585",
              "586",
              "587",
              "589",
              "597",
              "600",
              "601",
              "603",
              "609",
              "610",
              "611",
              "615",
              "618",
              "619",
              "620",
              "621",
              "622",
              "623",
              "624",
              "625",
              "627",
              "633",
              "636",
              "637",
              "639",
              "645",
              "646",
              "647",
              "650",
              "653",
              "654",
              "655",
              "658",
              "659",
              "666",
              "672",
              "674",
              "680",
              "681",
              "684",
              "685",
              "686",
              "687",
              "688",
              "689",
              "690",
              "691",
              "692",
              "695",
              "696",
              "697",
              "698",
              "699",
              "700",
              "701",
              "702",
              "703",
              "705",
              "707",
              "710",
              "711",
              "720",
              "722"
            ],
            "covered": 47
          },
          "development_tools\\shared\\service\\__init__.py": {
            "statements": 3,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 3
          },
          "development_tools\\shared\\service\\audit_orchestration.py": {
            "statements": 735,
            "missed": 398,
            "coverage": 46,
            "missing_lines": [
              "34",
              "35",
              "37",
              "38",
              "39",
              "40",
              "55",
              "57",
              "74",
              "75",
              "76",
              "78",
              "79",
              "80",
              "81",
              "82",
              "83",
              "84",
              "86",
              "87",
              "88",
              "89",
              "90",
              "91",
              "92",
              "94",
              "95",
              "96",
              "97",
              "98",
              "99",
              "110",
              "111",
              "117",
              "118",
              "120",
              "121",
              "122",
              "123",
              "133",
              "134",
              "135",
              "137",
              "138",
              "160",
              "161",
              "168",
              "169",
              "178",
              "182",
              "183",
              "184",
              "185",
              "186",
              "190",
              "191",
              "192",
              "193",
              "194",
              "195",
              "196",
              "197",
              "198",
              "203",
              "204",
              "215",
              "216",
              "220",
              "227",
              "235",
              "236",
              "243",
              "244",
              "246",
              "248",
              "249",
              "250",
              "251",
              "252",
              "261",
              "263",
              "264",
              "265",
              "269",
              "270",
              "271",
              "276",
              "277",
              "278",
              "283",
              "284",
              "285",
              "291",
              "298",
              "299",
              "303",
              "304",
              "308",
              "309",
              "335",
              "336",
              "337",
              "338",
              "339",
              "340",
              "354",
              "355",
              "356",
              "357",
              "363",
              "421",
              "422",
              "423",
              "424",
              "425",
              "426",
              "438",
              "439",
              "440",
              "471",
              "472",
              "494",
              "495",
              "498",
              "499",
              "500",
              "502",
              "504",
              "505",
              "506",
              "507",
              "510",
              "521",
              "522",
              "525",
              "532",
              "555",
              "557",
              "559",
              "560",
              "561",
              "562",
              "563",
              "564",
              "565",
              "566",
              "567",
              "570",
              "571",
              "573",
              "575",
              "576",
              "577",
              "582",
              "583",
              "584",
              "585",
              "586",
              "587",
              "590",
              "591",
              "592",
              "593",
              "594",
              "596",
              "598",
              "599",
              "600",
              "602",
              "603",
              "605",
              "606",
              "608",
              "610",
              "619",
              "620",
              "621",
              "631",
              "632",
              "636",
              "639",
              "644",
              "650",
              "656",
              "663",
              "664",
              "665",
              "667",
              "670",
              "672",
              "674",
              "675",
              "676",
              "677",
              "678",
              "679",
              "680",
              "681",
              "682",
              "684",
              "685",
              "691",
              "692",
              "693",
              "694",
              "695",
              "696",
              "697",
              "698",
              "699",
              "700",
              "701",
              "702",
              "703",
              "705",
              "706",
              "707",
              "708",
              "710",
              "711",
              "714",
              "715",
              "716",
              "717",
              "718",
              "719",
              "722",
              "723",
              "724",
              "725",
              "726",
              "727",
              "728",
              "729",
              "730",
              "731",
              "732",
              "733",
              "735",
              "736",
              "737",
              "738",
              "740",
              "741",
              "742",
              "743",
              "744",
              "745",
              "747",
              "748",
              "750",
              "752",
              "756",
              "757",
              "758",
              "759",
              "760",
              "762",
              "764",
              "765",
              "771",
              "772",
              "778",
              "779",
              "785",
              "786",
              "787",
              "793",
              "794",
              "800",
              "801",
              "802",
              "808",
              "809",
              "810",
              "820",
              "837",
              "843",
              "844",
              "845",
              "850",
              "851",
              "852",
              "853",
              "854",
              "855",
              "856",
              "857",
              "859",
              "863",
              "864",
              "865",
              "866",
              "867",
              "873",
              "874",
              "875",
              "876",
              "877",
              "878",
              "879",
              "880",
              "881",
              "882",
              "883",
              "884",
              "885",
              "886",
              "887",
              "888",
              "889",
              "890",
              "891",
              "892",
              "893",
              "894",
              "895",
              "896",
              "897",
              "898",
              "899",
              "912",
              "930",
              "931",
              "932",
              "934",
              "935",
              "938",
              "939",
              "966",
              "967",
              "968",
              "969",
              "971",
              "972",
              "974",
              "975",
              "977",
              "980",
              "981",
              "983",
              "984",
              "985",
              "987",
              "988",
              "989",
              "990",
              "995",
              "997",
              "1002",
              "1005",
              "1006",
              "1007",
              "1012",
              "1013",
              "1017",
              "1018",
              "1020",
              "1051",
              "1053",
              "1054",
              "1055",
              "1063",
              "1072",
              "1073",
              "1074",
              "1075",
              "1076",
              "1077",
              "1078",
              "1079",
              "1080",
              "1081",
              "1082",
              "1096",
              "1101",
              "1102",
              "1103",
              "1118",
              "1122",
              "1125",
              "1126",
              "1142",
              "1143",
              "1144",
              "1145",
              "1146",
              "1147",
              "1161",
              "1162",
              "1169",
              "1186",
              "1187",
              "1200",
              "1204",
              "1205",
              "1206",
              "1207"
            ],
            "covered": 337
          },
          "development_tools\\shared\\service\\commands.py": {
            "statements": 604,
            "missed": 510,
            "coverage": 16,
            "missing_lines": [
              "23",
              "24",
              "25",
              "26",
              "29",
              "30",
              "31",
              "32",
              "33",
              "35",
              "36",
              "37",
              "38",
              "39",
              "42",
              "43",
              "44",
              "45",
              "46",
              "48",
              "49",
              "50",
              "51",
              "52",
              "57",
              "58",
              "59",
              "60",
              "61",
              "62",
              "65",
              "66",
              "67",
              "68",
              "69",
              "70",
              "71",
              "73",
              "74",
              "75",
              "77",
              "78",
              "94",
              "95",
              "103",
              "104",
              "109",
              "110",
              "134",
              "135",
              "137",
              "138",
              "139",
              "140",
              "141",
              "142",
              "145",
              "148",
              "149",
              "153",
              "154",
              "155",
              "156",
              "157",
              "158",
              "159",
              "160",
              "161",
              "162",
              "163",
              "164",
              "165",
              "169",
              "170",
              "171",
              "172",
              "173",
              "174",
              "175",
              "177",
              "178",
              "182",
              "183",
              "186",
              "188",
              "189",
              "190",
              "191",
              "192",
              "193",
              "195",
              "196",
              "199",
              "202",
              "203",
              "204",
              "207",
              "208",
              "209",
              "210",
              "213",
              "214",
              "216",
              "217",
              "218",
              "219",
              "220",
              "221",
              "228",
              "229",
              "230",
              "231",
              "232",
              "233",
              "234",
              "235",
              "236",
              "242",
              "243",
              "244",
              "245",
              "246",
              "251",
              "252",
              "253",
              "254",
              "255",
              "259",
              "260",
              "262",
              "264",
              "265",
              "266",
              "267",
              "269",
              "271",
              "272",
              "273",
              "274",
              "275",
              "276",
              "278",
              "279",
              "280",
              "281",
              "282",
              "283",
              "284",
              "286",
              "287",
              "288",
              "290",
              "291",
              "292",
              "293",
              "294",
              "295",
              "296",
              "298",
              "299",
              "300",
              "301",
              "302",
              "303",
              "304",
              "306",
              "307",
              "308",
              "309",
              "310",
              "311",
              "312",
              "314",
              "315",
              "319",
              "320",
              "321",
              "322",
              "323",
              "325",
              "326",
              "330",
              "331",
              "332",
              "333",
              "334",
              "335",
              "336",
              "337",
              "338",
              "339",
              "340",
              "341",
              "342",
              "343",
              "344",
              "345",
              "346",
              "347",
              "348",
              "350",
              "351",
              "352",
              "353",
              "354",
              "356",
              "357",
              "358",
              "359",
              "360",
              "364",
              "365",
              "366",
              "368",
              "369",
              "370",
              "371",
              "372",
              "373",
              "375",
              "381",
              "382",
              "386",
              "387",
              "388",
              "390",
              "391",
              "392",
              "393",
              "394",
              "397",
              "398",
              "399",
              "401",
              "402",
              "403",
              "404",
              "405",
              "408",
              "410",
              "411",
              "413",
              "414",
              "415",
              "416",
              "417",
              "421",
              "422",
              "423",
              "424",
              "425",
              "427",
              "428",
              "435",
              "436",
              "438",
              "439",
              "442",
              "443",
              "444",
              "445",
              "447",
              "448",
              "449",
              "450",
              "451",
              "453",
              "454",
              "462",
              "463",
              "465",
              "467",
              "471",
              "472",
              "473",
              "492",
              "496",
              "501",
              "503",
              "507",
              "508",
              "509",
              "510",
              "511",
              "515",
              "516",
              "521",
              "522",
              "523",
              "524",
              "525",
              "526",
              "527",
              "528",
              "529",
              "530",
              "531",
              "533",
              "535",
              "539",
              "540",
              "541",
              "542",
              "543",
              "544",
              "545",
              "546",
              "547",
              "548",
              "549",
              "550",
              "551",
              "552",
              "553",
              "554",
              "555",
              "556",
              "557",
              "558",
              "559",
              "560",
              "561",
              "563",
              "564",
              "565",
              "566",
              "567",
              "568",
              "570",
              "574",
              "575",
              "576",
              "577",
              "578",
              "580",
              "581",
              "585",
              "586",
              "587",
              "588",
              "590",
              "591",
              "600",
              "601",
              "602",
              "603",
              "604",
              "605",
              "607",
              "608",
              "609",
              "611",
              "612",
              "613",
              "614",
              "619",
              "623",
              "624",
              "625",
              "632",
              "633",
              "634",
              "635",
              "636",
              "637",
              "639",
              "640",
              "644",
              "646",
              "647",
              "648",
              "650",
              "661",
              "672",
              "673",
              "674",
              "675",
              "681",
              "682",
              "683",
              "684",
              "685",
              "686",
              "687",
              "688",
              "689",
              "690",
              "694",
              "695",
              "696",
              "697",
              "698",
              "699",
              "700",
              "701",
              "702",
              "703",
              "704",
              "705",
              "706",
              "708",
              "709",
              "710",
              "711",
              "715",
              "716",
              "717",
              "721",
              "722",
              "723",
              "727",
              "728",
              "729",
              "733",
              "737",
              "738",
              "739",
              "741",
              "744",
              "745",
              "746",
              "747",
              "748",
              "749",
              "751",
              "754",
              "755",
              "757",
              "758",
              "759",
              "760",
              "766",
              "767",
              "768",
              "769",
              "770",
              "771",
              "772",
              "773",
              "774",
              "775",
              "777",
              "780",
              "781",
              "783",
              "784",
              "785",
              "786",
              "792",
              "793",
              "794",
              "795",
              "796",
              "797",
              "798",
              "799",
              "800",
              "801",
              "803",
              "806",
              "807",
              "809",
              "810",
              "811",
              "812",
              "818",
              "819",
              "820",
              "821",
              "822",
              "823",
              "824",
              "825",
              "826",
              "827",
              "829",
              "832",
              "833",
              "835",
              "836",
              "837",
              "838",
              "844",
              "845",
              "846",
              "847",
              "848",
              "849",
              "850",
              "851",
              "852",
              "853",
              "855",
              "858",
              "859",
              "860",
              "861",
              "862",
              "866",
              "867",
              "868",
              "869",
              "870",
              "871",
              "872",
              "873",
              "874",
              "875",
              "876",
              "877"
            ],
            "covered": 94
          },
          "development_tools\\shared\\service\\core.py": {
            "statements": 44,
            "missed": 2,
            "coverage": 95,
            "missing_lines": [
              "41",
              "92"
            ],
            "covered": 42
          },
          "development_tools\\shared\\service\\data_freshness_audit.py": {
            "statements": 99,
            "missed": 99,
            "coverage": 0,
            "missing_lines": [
              "25",
              "26",
              "27",
              "30",
              "36",
              "38",
              "41",
              "44",
              "51",
              "52",
              "54",
              "55",
              "56",
              "58",
              "61",
              "68",
              "70",
              "71",
              "72",
              "75",
              "76",
              "77",
              "78",
              "80",
              "82",
              "83",
              "84",
              "85",
              "86",
              "87",
              "88",
              "89",
              "91",
              "92",
              "93",
              "94",
              "96",
              "97",
              "99",
              "102",
              "104",
              "107",
              "108",
              "109",
              "110",
              "113",
              "114",
              "115",
              "116",
              "118",
              "121",
              "127",
              "130",
              "136",
              "137",
              "138",
              "140",
              "144",
              "146",
              "147",
              "148",
              "150",
              "151",
              "153",
              "154",
              "156",
              "159",
              "161",
              "164",
              "171",
              "172",
              "173",
              "175",
              "176",
              "177",
              "179",
              "180",
              "183",
              "184",
              "188",
              "189",
              "190",
              "191",
              "192",
              "193",
              "195",
              "198",
              "210",
              "211",
              "213",
              "215",
              "223",
              "224",
              "227",
              "228",
              "231",
              "232",
              "235",
              "243"
            ],
            "covered": 0
          },
          "development_tools\\shared\\service\\data_loading.py": {
            "statements": 843,
            "missed": 679,
            "coverage": 19,
            "missing_lines": [
              "54",
              "55",
              "56",
              "57",
              "59",
              "60",
              "61",
              "74",
              "75",
              "89",
              "90",
              "91",
              "92",
              "94",
              "95",
              "97",
              "98",
              "99",
              "100",
              "102",
              "103",
              "105",
              "108",
              "109",
              "111",
              "112",
              "114",
              "115",
              "116",
              "119",
              "120",
              "121",
              "140",
              "141",
              "166",
              "170",
              "176",
              "218",
              "235",
              "236",
              "237",
              "238",
              "239",
              "240",
              "241",
              "242",
              "243",
              "248",
              "249",
              "250",
              "251",
              "252",
              "254",
              "256",
              "258",
              "259",
              "260",
              "261",
              "263",
              "264",
              "265",
              "266",
              "267",
              "288",
              "300",
              "301",
              "302",
              "304",
              "305",
              "306",
              "307",
              "308",
              "309",
              "310",
              "311",
              "313",
              "315",
              "316",
              "317",
              "318",
              "321",
              "339",
              "344",
              "345",
              "346",
              "349",
              "350",
              "351",
              "353",
              "356",
              "357",
              "358",
              "359",
              "360",
              "361",
              "362",
              "363",
              "364",
              "365",
              "366",
              "367",
              "368",
              "369",
              "371",
              "372",
              "392",
              "393",
              "401",
              "402",
              "404",
              "409",
              "410",
              "411",
              "412",
              "413",
              "414",
              "415",
              "416",
              "417",
              "418",
              "419",
              "420",
              "421",
              "422",
              "423",
              "424",
              "425",
              "426",
              "427",
              "428",
              "429",
              "430",
              "431",
              "432",
              "433",
              "434",
              "435",
              "436",
              "437",
              "439",
              "440",
              "445",
              "446",
              "448",
              "449",
              "450",
              "451",
              "452",
              "453",
              "455",
              "456",
              "461",
              "462",
              "463",
              "464",
              "465",
              "482",
              "483",
              "484",
              "486",
              "487",
              "488",
              "490",
              "491",
              "492",
              "493",
              "494",
              "495",
              "496",
              "497",
              "498",
              "499",
              "500",
              "501",
              "502",
              "503",
              "504",
              "505",
              "506",
              "507",
              "508",
              "509",
              "510",
              "511",
              "512",
              "513",
              "514",
              "515",
              "516",
              "517",
              "518",
              "519",
              "520",
              "521",
              "522",
              "523",
              "525",
              "526",
              "527",
              "528",
              "529",
              "530",
              "531",
              "532",
              "533",
              "534",
              "535",
              "536",
              "537",
              "538",
              "539",
              "540",
              "543",
              "544",
              "555",
              "556",
              "557",
              "558",
              "559",
              "560",
              "561",
              "562",
              "563",
              "564",
              "565",
              "566",
              "567",
              "568",
              "569",
              "570",
              "571",
              "572",
              "574",
              "575",
              "576",
              "590",
              "591",
              "592",
              "593",
              "594",
              "595",
              "596",
              "597",
              "598",
              "599",
              "600",
              "601",
              "602",
              "603",
              "604",
              "606",
              "607",
              "608",
              "609",
              "610",
              "611",
              "612",
              "619",
              "626",
              "627",
              "628",
              "629",
              "630",
              "631",
              "632",
              "633",
              "634",
              "635",
              "636",
              "637",
              "638",
              "639",
              "640",
              "641",
              "642",
              "643",
              "644",
              "645",
              "646",
              "647",
              "648",
              "649",
              "650",
              "651",
              "657",
              "658",
              "659",
              "670",
              "671",
              "672",
              "673",
              "682",
              "683",
              "684",
              "685",
              "686",
              "687",
              "688",
              "689",
              "690",
              "691",
              "692",
              "693",
              "694",
              "695",
              "699",
              "708",
              "709",
              "711",
              "712",
              "713",
              "714",
              "716",
              "717",
              "718",
              "719",
              "720",
              "721",
              "722",
              "723",
              "725",
              "726",
              "727",
              "728",
              "729",
              "730",
              "731",
              "732",
              "733",
              "734",
              "738",
              "746",
              "747",
              "748",
              "749",
              "750",
              "751",
              "752",
              "753",
              "754",
              "755",
              "756",
              "757",
              "758",
              "759",
              "760",
              "761",
              "762",
              "763",
              "764",
              "765",
              "766",
              "767",
              "768",
              "769",
              "770",
              "771",
              "772",
              "773",
              "774",
              "775",
              "776",
              "777",
              "778",
              "779",
              "780",
              "781",
              "782",
              "783",
              "784",
              "785",
              "786",
              "790",
              "791",
              "792",
              "793",
              "794",
              "795",
              "796",
              "797",
              "799",
              "800",
              "801",
              "802",
              "803",
              "804",
              "805",
              "809",
              "810",
              "811",
              "812",
              "813",
              "814",
              "815",
              "816",
              "817",
              "818",
              "819",
              "820",
              "821",
              "822",
              "823",
              "824",
              "825",
              "826",
              "827",
              "828",
              "829",
              "834",
              "835",
              "836",
              "837",
              "838",
              "843",
              "844",
              "845",
              "850",
              "851",
              "852",
              "854",
              "855",
              "856",
              "857",
              "858",
              "859",
              "860",
              "861",
              "862",
              "863",
              "864",
              "865",
              "866",
              "867",
              "868",
              "869",
              "870",
              "871",
              "872",
              "873",
              "874",
              "875",
              "880",
              "881",
              "882",
              "883",
              "884",
              "885",
              "886",
              "888",
              "889",
              "890",
              "891",
              "892",
              "893",
              "894",
              "895",
              "896",
              "897",
              "898",
              "899",
              "900",
              "901",
              "902",
              "903",
              "904",
              "905",
              "906",
              "907",
              "908",
              "909",
              "910",
              "911",
              "912",
              "917",
              "918",
              "919",
              "920",
              "921",
              "922",
              "923",
              "925",
              "926",
              "927",
              "928",
              "929",
              "930",
              "931",
              "932",
              "933",
              "934",
              "935",
              "936",
              "937",
              "938",
              "939",
              "940",
              "941",
              "942",
              "943",
              "944",
              "945",
              "946",
              "947",
              "948",
              "953",
              "954",
              "955",
              "956",
              "957",
              "958",
              "959",
              "961",
              "962",
              "963",
              "964",
              "965",
              "966",
              "967",
              "968",
              "969",
              "970",
              "971",
              "972",
              "973",
              "974",
              "975",
              "976",
              "977",
              "982",
              "983",
              "984",
              "985",
              "986",
              "987",
              "988",
              "990",
              "991",
              "992",
              "993",
              "994",
              "995",
              "996",
              "997",
              "998",
              "999",
              "1000",
              "1001",
              "1002",
              "1003",
              "1004",
              "1005",
              "1006",
              "1007",
              "1008",
              "1009",
              "1010",
              "1011",
              "1015",
              "1027",
              "1028",
              "1029",
              "1030",
              "1031",
              "1032",
              "1034",
              "1035",
              "1036",
              "1037",
              "1038",
              "1039",
              "1040",
              "1042",
              "1043",
              "1044",
              "1045",
              "1047",
              "1048",
              "1049",
              "1050",
              "1052",
              "1053",
              "1054",
              "1055",
              "1057",
              "1058",
              "1059",
              "1060",
              "1062",
              "1063",
              "1065",
              "1066",
              "1070",
              "1075",
              "1076",
              "1077",
              "1078",
              "1079",
              "1080",
              "1081",
              "1082",
              "1083",
              "1084",
              "1085",
              "1086",
              "1087",
              "1088",
              "1089",
              "1090",
              "1091",
              "1092",
              "1093",
              "1094",
              "1095",
              "1115",
              "1118",
              "1119",
              "1120",
              "1121",
              "1122",
              "1123",
              "1124",
              "1127",
              "1129",
              "1130",
              "1133",
              "1134",
              "1137",
              "1138",
              "1139",
              "1141",
              "1142",
              "1143",
              "1145",
              "1146",
              "1147",
              "1148",
              "1149",
              "1151",
              "1154",
              "1156",
              "1157",
              "1158",
              "1161",
              "1164",
              "1166",
              "1167",
              "1170",
              "1171",
              "1172",
              "1173",
              "1174",
              "1175",
              "1176",
              "1177",
              "1178",
              "1185",
              "1186",
              "1187",
              "1189",
              "1190",
              "1191",
              "1192",
              "1193",
              "1195",
              "1198",
              "1199",
              "1200",
              "1201",
              "1202",
              "1203",
              "1205",
              "1206",
              "1207",
              "1208",
              "1209",
              "1210",
              "1212",
              "1214",
              "1217"
            ],
            "covered": 164
          },
          "development_tools\\shared\\service\\report_generation.py": {
            "statements": 2972,
            "missed": 1880,
            "coverage": 37,
            "missing_lines": [
              "30",
              "32",
              "37",
              "38",
              "45",
              "46",
              "50",
              "51",
              "54",
              "55",
              "58",
              "65",
              "66",
              "69",
              "70",
              "71",
              "73",
              "74",
              "77",
              "78",
              "87",
              "88",
              "89",
              "90",
              "91",
              "92",
              "93",
              "94",
              "95",
              "104",
              "112",
              "113",
              "115",
              "116",
              "117",
              "127",
              "132",
              "133",
              "134",
              "135",
              "136",
              "137",
              "139",
              "140",
              "152",
              "156",
              "158",
              "160",
              "161",
              "162",
              "163",
              "164",
              "166",
              "167",
              "171",
              "172",
              "173",
              "174",
              "175",
              "176",
              "177",
              "178",
              "179",
              "183",
              "212",
              "214",
              "221",
              "222",
              "231",
              "239",
              "240",
              "241",
              "243",
              "247",
              "257",
              "274",
              "275",
              "276",
              "277",
              "278",
              "279",
              "281",
              "282",
              "283",
              "284",
              "285",
              "286",
              "287",
              "288",
              "289",
              "290",
              "291",
              "292",
              "293",
              "294",
              "295",
              "296",
              "297",
              "298",
              "299",
              "300",
              "301",
              "302",
              "303",
              "304",
              "305",
              "310",
              "326",
              "328",
              "329",
              "330",
              "347",
              "348",
              "349",
              "350",
              "351",
              "352",
              "353",
              "354",
              "355",
              "357",
              "358",
              "359",
              "360",
              "361",
              "362",
              "363",
              "364",
              "365",
              "366",
              "367",
              "371",
              "372",
              "373",
              "374",
              "376",
              "377",
              "378",
              "379",
              "380",
              "381",
              "382",
              "387",
              "400",
              "401",
              "408",
              "409",
              "410",
              "411",
              "412",
              "413",
              "414",
              "415",
              "417",
              "418",
              "419",
              "420",
              "421",
              "428",
              "433",
              "434",
              "436",
              "439",
              "444",
              "454",
              "455",
              "456",
              "457",
              "458",
              "459",
              "460",
              "461",
              "462",
              "463",
              "464",
              "465",
              "469",
              "470",
              "474",
              "475",
              "476",
              "477",
              "486",
              "487",
              "488",
              "489",
              "490",
              "491",
              "492",
              "496",
              "497",
              "502",
              "509",
              "511",
              "516",
              "517",
              "518",
              "519",
              "526",
              "530",
              "531",
              "532",
              "543",
              "550",
              "561",
              "595",
              "599",
              "603",
              "618",
              "619",
              "621",
              "622",
              "623",
              "624",
              "625",
              "633",
              "641",
              "642",
              "645",
              "646",
              "654",
              "655",
              "656",
              "658",
              "661",
              "668",
              "669",
              "670",
              "671",
              "673",
              "674",
              "675",
              "676",
              "681",
              "688",
              "699",
              "712",
              "721",
              "731",
              "734",
              "735",
              "736",
              "738",
              "744",
              "749",
              "750",
              "751",
              "752",
              "753",
              "754",
              "755",
              "757",
              "763",
              "764",
              "765",
              "770",
              "774",
              "779",
              "788",
              "789",
              "791",
              "792",
              "793",
              "796",
              "797",
              "805",
              "806",
              "807",
              "808",
              "809",
              "812",
              "813",
              "820",
              "821",
              "823",
              "824",
              "825",
              "827",
              "828",
              "830",
              "831",
              "835",
              "836",
              "838",
              "839",
              "840",
              "841",
              "843",
              "844",
              "845",
              "846",
              "847",
              "848",
              "849",
              "851",
              "852",
              "856",
              "857",
              "859",
              "860",
              "861",
              "862",
              "864",
              "865",
              "866",
              "867",
              "868",
              "874",
              "875",
              "876",
              "879",
              "880",
              "881",
              "882",
              "884",
              "886",
              "887",
              "888",
              "889",
              "891",
              "892",
              "893",
              "894",
              "895",
              "898",
              "899",
              "900",
              "901",
              "902",
              "903",
              "904",
              "905",
              "906",
              "907",
              "908",
              "909",
              "910",
              "911",
              "913",
              "914",
              "915",
              "916",
              "917",
              "918",
              "919",
              "920",
              "921",
              "922",
              "924",
              "925",
              "926",
              "934",
              "935",
              "939",
              "940",
              "941",
              "943",
              "944",
              "945",
              "946",
              "947",
              "948",
              "949",
              "950",
              "954",
              "955",
              "956",
              "957",
              "958",
              "959",
              "960",
              "965",
              "966",
              "967",
              "968",
              "969",
              "971",
              "972",
              "974",
              "975",
              "976",
              "977",
              "979",
              "980",
              "987",
              "988",
              "989",
              "990",
              "992",
              "993",
              "994",
              "995",
              "996",
              "997",
              "998",
              "999",
              "1000",
              "1001",
              "1002",
              "1003",
              "1004",
              "1005",
              "1007",
              "1008",
              "1010",
              "1011",
              "1012",
              "1021",
              "1022",
              "1023",
              "1024",
              "1025",
              "1027",
              "1028",
              "1030",
              "1031",
              "1032",
              "1033",
              "1035",
              "1036",
              "1037",
              "1038",
              "1040",
              "1042",
              "1044",
              "1045",
              "1046",
              "1048",
              "1053",
              "1054",
              "1055",
              "1056",
              "1057",
              "1059",
              "1060",
              "1062",
              "1063",
              "1064",
              "1066",
              "1067",
              "1068",
              "1069",
              "1070",
              "1071",
              "1073",
              "1085",
              "1086",
              "1087",
              "1091",
              "1094",
              "1095",
              "1097",
              "1100",
              "1101",
              "1103",
              "1104",
              "1105",
              "1106",
              "1107",
              "1108",
              "1109",
              "1110",
              "1111",
              "1112",
              "1114",
              "1116",
              "1118",
              "1120",
              "1138",
              "1149",
              "1150",
              "1157",
              "1176",
              "1177",
              "1178",
              "1179",
              "1181",
              "1182",
              "1183",
              "1185",
              "1187",
              "1188",
              "1189",
              "1190",
              "1191",
              "1194",
              "1195",
              "1196",
              "1199",
              "1200",
              "1201",
              "1204",
              "1205",
              "1206",
              "1207",
              "1208",
              "1209",
              "1210",
              "1211",
              "1212",
              "1213",
              "1215",
              "1219",
              "1220",
              "1222",
              "1223",
              "1224",
              "1225",
              "1227",
              "1228",
              "1229",
              "1230",
              "1231",
              "1252",
              "1260",
              "1261",
              "1263",
              "1264",
              "1265",
              "1277",
              "1278",
              "1279",
              "1280",
              "1281",
              "1282",
              "1284",
              "1285",
              "1293",
              "1294",
              "1295",
              "1296",
              "1297",
              "1298",
              "1304",
              "1312",
              "1313",
              "1317",
              "1318",
              "1319",
              "1320",
              "1321",
              "1322",
              "1323",
              "1324",
              "1325",
              "1372",
              "1382",
              "1383",
              "1385",
              "1386",
              "1387",
              "1388",
              "1391",
              "1392",
              "1393",
              "1394",
              "1395",
              "1396",
              "1398",
              "1400",
              "1413",
              "1414",
              "1415",
              "1417",
              "1421",
              "1425",
              "1428",
              "1430",
              "1434",
              "1439",
              "1445",
              "1446",
              "1447",
              "1448",
              "1449",
              "1451",
              "1452",
              "1453",
              "1463",
              "1464",
              "1465",
              "1466",
              "1467",
              "1468",
              "1469",
              "1470",
              "1471",
              "1475",
              "1488",
              "1489",
              "1490",
              "1491",
              "1492",
              "1493",
              "1494",
              "1495",
              "1501",
              "1502",
              "1503",
              "1504",
              "1505",
              "1506",
              "1507",
              "1508",
              "1509",
              "1510",
              "1511",
              "1512",
              "1513",
              "1522",
              "1523",
              "1524",
              "1526",
              "1527",
              "1528",
              "1531",
              "1532",
              "1533",
              "1535",
              "1536",
              "1537",
              "1546",
              "1547",
              "1548",
              "1549",
              "1550",
              "1551",
              "1576",
              "1577",
              "1581",
              "1582",
              "1591",
              "1592",
              "1593",
              "1594",
              "1595",
              "1596",
              "1598",
              "1607",
              "1612",
              "1635",
              "1636",
              "1637",
              "1640",
              "1641",
              "1644",
              "1647",
              "1650",
              "1653",
              "1669",
              "1670",
              "1671",
              "1672",
              "1692",
              "1694",
              "1699",
              "1719",
              "1743",
              "1746",
              "1747",
              "1753",
              "1754",
              "1755",
              "1756",
              "1757",
              "1758",
              "1759",
              "1760",
              "1762",
              "1764",
              "1765",
              "1766",
              "1768",
              "1769",
              "1771",
              "1774",
              "1777",
              "1781",
              "1790",
              "1793",
              "1794",
              "1795",
              "1796",
              "1797",
              "1798",
              "1799",
              "1801",
              "1802",
              "1803",
              "1805",
              "1809",
              "1810",
              "1811",
              "1812",
              "1813",
              "1814",
              "1815",
              "1816",
              "1817",
              "1818",
              "1819",
              "1821",
              "1822",
              "1823",
              "1826",
              "1829",
              "1833",
              "1846",
              "1848",
              "1849",
              "1850",
              "1853",
              "1854",
              "1855",
              "1856",
              "1858",
              "1859",
              "1860",
              "1861",
              "1862",
              "1864",
              "1865",
              "1866",
              "1867",
              "1868",
              "1871",
              "1872",
              "1875",
              "1877",
              "1878",
              "1879",
              "1880",
              "1883",
              "1884",
              "1885",
              "1888",
              "1889",
              "1890",
              "1891",
              "1892",
              "1894",
              "1895",
              "1896",
              "1899",
              "1903",
              "1906",
              "1909",
              "1921",
              "1922",
              "1923",
              "1924",
              "1925",
              "1930",
              "1931",
              "1932",
              "1933",
              "1934",
              "1935",
              "1937",
              "1938",
              "1939",
              "1940",
              "1941",
              "1942",
              "1944",
              "1946",
              "1947",
              "1948",
              "1949",
              "1951",
              "1954",
              "1957",
              "1960",
              "1963",
              "1972",
              "1976",
              "1983",
              "1991",
              "1992",
              "1993",
              "1994",
              "1995",
              "1996",
              "2000",
              "2001",
              "2002",
              "2003",
              "2004",
              "2005",
              "2006",
              "2007",
              "2016",
              "2017",
              "2018",
              "2019",
              "2020",
              "2021",
              "2024",
              "2027",
              "2030",
              "2033",
              "2043",
              "2044",
              "2045",
              "2046",
              "2049",
              "2050",
              "2052",
              "2053",
              "2059",
              "2060",
              "2063",
              "2069",
              "2070",
              "2071",
              "2072",
              "2073",
              "2074",
              "2077",
              "2078",
              "2081",
              "2082",
              "2085",
              "2086",
              "2087",
              "2090",
              "2091",
              "2094",
              "2095",
              "2098",
              "2099",
              "2102",
              "2103",
              "2106",
              "2109",
              "2112",
              "2125",
              "2130",
              "2131",
              "2132",
              "2133",
              "2134",
              "2135",
              "2136",
              "2137",
              "2139",
              "2140",
              "2146",
              "2147",
              "2148",
              "2149",
              "2150",
              "2151",
              "2154",
              "2157",
              "2160",
              "2163",
              "2173",
              "2174",
              "2175",
              "2176",
              "2177",
              "2180",
              "2181",
              "2183",
              "2184",
              "2185",
              "2186",
              "2187",
              "2190",
              "2192",
              "2193",
              "2196",
              "2197",
              "2198",
              "2199",
              "2200",
              "2201",
              "2202",
              "2203",
              "2204",
              "2206",
              "2207",
              "2212",
              "2213",
              "2214",
              "2215",
              "2216",
              "2217",
              "2218",
              "2219",
              "2221",
              "2224",
              "2227",
              "2230",
              "2234",
              "2244",
              "2245",
              "2246",
              "2247",
              "2250",
              "2251",
              "2254",
              "2255",
              "2256",
              "2259",
              "2260",
              "2264",
              "2267",
              "2268",
              "2273",
              "2274",
              "2275",
              "2276",
              "2277",
              "2278",
              "2279",
              "2280",
              "2281",
              "2283",
              "2285",
              "2286",
              "2287",
              "2289",
              "2292",
              "2295",
              "2300",
              "2301",
              "2302",
              "2307",
              "2310",
              "2312",
              "2325",
              "2327",
              "2329",
              "2330",
              "2331",
              "2333",
              "2334",
              "2335",
              "2336",
              "2337",
              "2338",
              "2339",
              "2340",
              "2342",
              "2343",
              "2344",
              "2345",
              "2346",
              "2347",
              "2348",
              "2350",
              "2351",
              "2353",
              "2354",
              "2359",
              "2360",
              "2361",
              "2362",
              "2363",
              "2364",
              "2366",
              "2367",
              "2369",
              "2371",
              "2372",
              "2373",
              "2376",
              "2377",
              "2380",
              "2383",
              "2386",
              "2389",
              "2396",
              "2397",
              "2398",
              "2403",
              "2404",
              "2405",
              "2406",
              "2407",
              "2408",
              "2409",
              "2411",
              "2413",
              "2414",
              "2415",
              "2418",
              "2421",
              "2424",
              "2427",
              "2441",
              "2449",
              "2450",
              "2451",
              "2452",
              "2459",
              "2464",
              "2465",
              "2466",
              "2467",
              "2468",
              "2472",
              "2473",
              "2474",
              "2475",
              "2476",
              "2477",
              "2481",
              "2482",
              "2483",
              "2484",
              "2485",
              "2486",
              "2487",
              "2488",
              "2490",
              "2492",
              "2495",
              "2496",
              "2497",
              "2498",
              "2499",
              "2500",
              "2501",
              "2502",
              "2504",
              "2506",
              "2508",
              "2511",
              "2512",
              "2513",
              "2514",
              "2515",
              "2516",
              "2517",
              "2519",
              "2521",
              "2529",
              "2536",
              "2550",
              "2557",
              "2566",
              "2567",
              "2568",
              "2569",
              "2570",
              "2571",
              "2572",
              "2573",
              "2574",
              "2583",
              "2584",
              "2600",
              "2601",
              "2603",
              "2605",
              "2606",
              "2607",
              "2612",
              "2613",
              "2614",
              "2617",
              "2621",
              "2622",
              "2625",
              "2633",
              "2634",
              "2635",
              "2637",
              "2638",
              "2639",
              "2640",
              "2641",
              "2642",
              "2643",
              "2644",
              "2645",
              "2646",
              "2649",
              "2653",
              "2654",
              "2673",
              "2681",
              "2682",
              "2684",
              "2685",
              "2686",
              "2697",
              "2698",
              "2699",
              "2700",
              "2701",
              "2702",
              "2704",
              "2705",
              "2714",
              "2720",
              "2721",
              "2722",
              "2723",
              "2724",
              "2725",
              "2726",
              "2727",
              "2728",
              "2734",
              "2736",
              "2737",
              "2738",
              "2739",
              "2740",
              "2742",
              "2743",
              "2766",
              "2767",
              "2809",
              "2811",
              "2821",
              "2825",
              "2853",
              "2871",
              "2873",
              "2875",
              "2890",
              "2891",
              "2892",
              "2893",
              "2894",
              "2896",
              "2897",
              "2898",
              "2899",
              "2900",
              "2901",
              "2902",
              "2903",
              "2904",
              "2906",
              "2907",
              "2908",
              "2909",
              "2910",
              "2911",
              "2912",
              "2913",
              "2914",
              "2915",
              "2916",
              "2917",
              "2918",
              "2919",
              "2924",
              "2929",
              "2930",
              "2932",
              "2933",
              "2934",
              "2942",
              "2943",
              "2944",
              "2946",
              "2949",
              "2954",
              "2955",
              "2956",
              "2958",
              "2959",
              "2960",
              "2961",
              "2962",
              "2963",
              "2965",
              "2966",
              "2967",
              "2968",
              "2969",
              "2970",
              "2971",
              "2974",
              "2975",
              "2976",
              "2977",
              "2978",
              "2986",
              "2995",
              "3010",
              "3037",
              "3040",
              "3042",
              "3046",
              "3051",
              "3065",
              "3066",
              "3068",
              "3072",
              "3073",
              "3074",
              "3075",
              "3077",
              "3080",
              "3081",
              "3082",
              "3083",
              "3085",
              "3086",
              "3087",
              "3089",
              "3090",
              "3092",
              "3094",
              "3095",
              "3096",
              "3107",
              "3112",
              "3113",
              "3114",
              "3115",
              "3116",
              "3117",
              "3119",
              "3120",
              "3121",
              "3122",
              "3123",
              "3124",
              "3125",
              "3126",
              "3127",
              "3128",
              "3135",
              "3137",
              "3138",
              "3139",
              "3140",
              "3141",
              "3142",
              "3143",
              "3145",
              "3146",
              "3147",
              "3148",
              "3159",
              "3160",
              "3165",
              "3166",
              "3167",
              "3168",
              "3170",
              "3172",
              "3173",
              "3174",
              "3175",
              "3176",
              "3177",
              "3178",
              "3179",
              "3180",
              "3181",
              "3182",
              "3190",
              "3191",
              "3192",
              "3193",
              "3195",
              "3197",
              "3198",
              "3199",
              "3200",
              "3201",
              "3202",
              "3203",
              "3204",
              "3205",
              "3206",
              "3207",
              "3220",
              "3222",
              "3225",
              "3235",
              "3236",
              "3241",
              "3242",
              "3243",
              "3244",
              "3246",
              "3248",
              "3249",
              "3250",
              "3251",
              "3252",
              "3253",
              "3254",
              "3255",
              "3256",
              "3257",
              "3258",
              "3268",
              "3272",
              "3273",
              "3274",
              "3275",
              "3276",
              "3277",
              "3278",
              "3279",
              "3280",
              "3281",
              "3282",
              "3294",
              "3295",
              "3296",
              "3301",
              "3302",
              "3317",
              "3318",
              "3320",
              "3321",
              "3331",
              "3332",
              "3333",
              "3334",
              "3335",
              "3336",
              "3337",
              "3338",
              "3339",
              "3340",
              "3341",
              "3342",
              "3343",
              "3346",
              "3347",
              "3358",
              "3359",
              "3360",
              "3362",
              "3363",
              "3364",
              "3366",
              "3367",
              "3368",
              "3373",
              "3374",
              "3375",
              "3376",
              "3377",
              "3378",
              "3379",
              "3380",
              "3381",
              "3382",
              "3384",
              "3385",
              "3387",
              "3388",
              "3389",
              "3392",
              "3394",
              "3395",
              "3396",
              "3397",
              "3399",
              "3400",
              "3401",
              "3402",
              "3403",
              "3404",
              "3405",
              "3407",
              "3408",
              "3410",
              "3411",
              "3412",
              "3413",
              "3414",
              "3415",
              "3417",
              "3418",
              "3419",
              "3420",
              "3421",
              "3422",
              "3424",
              "3425",
              "3426",
              "3427",
              "3428",
              "3429",
              "3432",
              "3434",
              "3435",
              "3436",
              "3437",
              "3439",
              "3440",
              "3442",
              "3443",
              "3445",
              "3447",
              "3448",
              "3449",
              "3450",
              "3451",
              "3452",
              "3454",
              "3455",
              "3456",
              "3457",
              "3458",
              "3459",
              "3460",
              "3462",
              "3463",
              "3464",
              "3465",
              "3466",
              "3468",
              "3469",
              "3470",
              "3480",
              "3481",
              "3484",
              "3485",
              "3486",
              "3487",
              "3488",
              "3489",
              "3490",
              "3491",
              "3492",
              "3494",
              "3495",
              "3499",
              "3501",
              "3502",
              "3503",
              "3507",
              "3508",
              "3511",
              "3512",
              "3513",
              "3514",
              "3515",
              "3516",
              "3517",
              "3518",
              "3519",
              "3520",
              "3521",
              "3522",
              "3526",
              "3529",
              "3530",
              "3531",
              "3532",
              "3533",
              "3534",
              "3535",
              "3537",
              "3538",
              "3540",
              "3541",
              "3542",
              "3543",
              "3544",
              "3545",
              "3546",
              "3547",
              "3548",
              "3549",
              "3550",
              "3551",
              "3553",
              "3554",
              "3555",
              "3556",
              "3557",
              "3558",
              "3560",
              "3561",
              "3564",
              "3574",
              "3575",
              "3576",
              "3577",
              "3578",
              "3579",
              "3580",
              "3581",
              "3582",
              "3583",
              "3584",
              "3585",
              "3586",
              "3589",
              "3590",
              "3591",
              "3594",
              "3595",
              "3596",
              "3597",
              "3598",
              "3600",
              "3601",
              "3602",
              "3603",
              "3605",
              "3607",
              "3610",
              "3611",
              "3612",
              "3613",
              "3614",
              "3616",
              "3617",
              "3619",
              "3620",
              "3621",
              "3623",
              "3624",
              "3625",
              "3626",
              "3627",
              "3628",
              "3629",
              "3632",
              "3633",
              "3634",
              "3636",
              "3638",
              "3639",
              "3640",
              "3642",
              "3643",
              "3644",
              "3645",
              "3646",
              "3647",
              "3649",
              "3650",
              "3651",
              "3652",
              "3654",
              "3655",
              "3656",
              "3657",
              "3659",
              "3669",
              "3670",
              "3671",
              "3672",
              "3673",
              "3674",
              "3675",
              "3676",
              "3677",
              "3678",
              "3679",
              "3681",
              "3682",
              "3684",
              "3685",
              "3687",
              "3688",
              "3689",
              "3698",
              "3699",
              "3700",
              "3701",
              "3712",
              "3717",
              "3720",
              "3721",
              "3722",
              "3723",
              "3724",
              "3725",
              "3726",
              "3727",
              "3728",
              "3730",
              "3732",
              "3733",
              "3734",
              "3735",
              "3736",
              "3737",
              "3742",
              "3743",
              "3745",
              "3746",
              "3747",
              "3748",
              "3749",
              "3750",
              "3767",
              "3768",
              "3772",
              "3773",
              "3774",
              "3775",
              "3776",
              "3777",
              "3778",
              "3782",
              "3783",
              "3784",
              "3785",
              "3786",
              "3787",
              "3788",
              "3789",
              "3790",
              "3791",
              "3792",
              "3793",
              "3794",
              "3795",
              "3796",
              "3797",
              "3798",
              "3799",
              "3800",
              "3801",
              "3802",
              "3803",
              "3804",
              "3805",
              "3806",
              "3807",
              "3820",
              "3821",
              "3822",
              "3823",
              "3829",
              "3830",
              "3831",
              "3832",
              "3833",
              "3834",
              "3835",
              "3836",
              "3843",
              "3845",
              "3847",
              "3849",
              "3854",
              "3855",
              "3856",
              "3857",
              "3858",
              "3859",
              "3860",
              "3864",
              "3865",
              "3866",
              "3874",
              "3875",
              "3876",
              "3884",
              "3885",
              "3886",
              "3887",
              "3888",
              "3889",
              "3890",
              "3891",
              "3892",
              "3894",
              "3896",
              "3897",
              "3898",
              "3899",
              "3907",
              "3908",
              "3910",
              "3911",
              "3912",
              "3913",
              "3914",
              "3915",
              "3920",
              "3921",
              "3922",
              "3923",
              "3924",
              "3925",
              "3926",
              "3927",
              "3928",
              "3929",
              "3930",
              "3939",
              "3940",
              "3941",
              "3946",
              "3947",
              "3948",
              "3949",
              "3950",
              "3951",
              "3952",
              "3953",
              "3955",
              "3956",
              "3957",
              "3958",
              "3959",
              "3960",
              "3966",
              "3967",
              "3968",
              "3975",
              "3976",
              "3977",
              "3978",
              "3995",
              "3997",
              "4000",
              "4004",
              "4011",
              "4023",
              "4024",
              "4025",
              "4026",
              "4027",
              "4028",
              "4037",
              "4038",
              "4039",
              "4040",
              "4042",
              "4043",
              "4044",
              "4049",
              "4050",
              "4052",
              "4053",
              "4054",
              "4055",
              "4056",
              "4063",
              "4064",
              "4065",
              "4066",
              "4068",
              "4069",
              "4070",
              "4072",
              "4073",
              "4075",
              "4076",
              "4077",
              "4078",
              "4079",
              "4080",
              "4081",
              "4082",
              "4083",
              "4095",
              "4096",
              "4097",
              "4101",
              "4104",
              "4105",
              "4107",
              "4110",
              "4111",
              "4113",
              "4114",
              "4115",
              "4116",
              "4117",
              "4118",
              "4119",
              "4120",
              "4121",
              "4122",
              "4124",
              "4126",
              "4128",
              "4130",
              "4135",
              "4136",
              "4137",
              "4138",
              "4139",
              "4140",
              "4142",
              "4143",
              "4144",
              "4145",
              "4146",
              "4148",
              "4149",
              "4150",
              "4151",
              "4152",
              "4153",
              "4155",
              "4156",
              "4157",
              "4158",
              "4159",
              "4160",
              "4161",
              "4163",
              "4164",
              "4178",
              "4179",
              "4180",
              "4181",
              "4183",
              "4184",
              "4185",
              "4186",
              "4187",
              "4194",
              "4195",
              "4209",
              "4223",
              "4224",
              "4225",
              "4246",
              "4247",
              "4257",
              "4258",
              "4262",
              "4264",
              "4265",
              "4266",
              "4267",
              "4268",
              "4269",
              "4270",
              "4272",
              "4273",
              "4274",
              "4282",
              "4283",
              "4287",
              "4288",
              "4292",
              "4293",
              "4305",
              "4306",
              "4307",
              "4308",
              "4309",
              "4310",
              "4311",
              "4312",
              "4313",
              "4314",
              "4315",
              "4316",
              "4317",
              "4318",
              "4322",
              "4323",
              "4324",
              "4325",
              "4326",
              "4327",
              "4328",
              "4329",
              "4330",
              "4331",
              "4332",
              "4333",
              "4334",
              "4335",
              "4336",
              "4337",
              "4338",
              "4339",
              "4340",
              "4341"
            ],
            "covered": 1092
          },
          "development_tools\\shared\\service\\tool_wrappers.py": {
            "statements": 827,
            "missed": 733,
            "coverage": 11,
            "missing_lines": [
              "68",
              "75",
              "95",
              "96",
              "108",
              "123",
              "124",
              "128",
              "129",
              "137",
              "138",
              "142",
              "143",
              "145",
              "146",
              "147",
              "148",
              "149",
              "150",
              "151",
              "152",
              "154",
              "155",
              "156",
              "157",
              "158",
              "163",
              "164",
              "169",
              "170",
              "171",
              "172",
              "173",
              "178",
              "179",
              "180",
              "181",
              "182",
              "183",
              "184",
              "185",
              "186",
              "187",
              "188",
              "189",
              "190",
              "191",
              "192",
              "193",
              "194",
              "195",
              "196",
              "197",
              "198",
              "199",
              "200",
              "201",
              "202",
              "203",
              "204",
              "205",
              "206",
              "207",
              "208",
              "210",
              "211",
              "212",
              "213",
              "214",
              "215",
              "219",
              "220",
              "221",
              "222",
              "223",
              "224",
              "225",
              "226",
              "227",
              "234",
              "235",
              "236",
              "237",
              "238",
              "239",
              "240",
              "241",
              "242",
              "243",
              "244",
              "245",
              "246",
              "250",
              "251",
              "252",
              "253",
              "254",
              "255",
              "256",
              "257",
              "258",
              "259",
              "260",
              "261",
              "262",
              "263",
              "264",
              "266",
              "267",
              "268",
              "269",
              "270",
              "271",
              "272",
              "273",
              "274",
              "275",
              "276",
              "277",
              "278",
              "279",
              "280",
              "287",
              "289",
              "296",
              "297",
              "298",
              "299",
              "300",
              "301",
              "302",
              "303",
              "304",
              "313",
              "318",
              "319",
              "342",
              "343",
              "344",
              "351",
              "352",
              "353",
              "354",
              "355",
              "356",
              "363",
              "364",
              "368",
              "369",
              "370",
              "377",
              "378",
              "379",
              "380",
              "381",
              "382",
              "383",
              "384",
              "385",
              "392",
              "393",
              "397",
              "398",
              "399",
              "406",
              "407",
              "408",
              "409",
              "410",
              "411",
              "412",
              "413",
              "414",
              "415",
              "416",
              "417",
              "418",
              "419",
              "420",
              "421",
              "422",
              "423",
              "424",
              "425",
              "431",
              "432",
              "433",
              "434",
              "435",
              "447",
              "448",
              "449",
              "453",
              "454",
              "455",
              "462",
              "463",
              "464",
              "465",
              "466",
              "467",
              "468",
              "469",
              "471",
              "472",
              "473",
              "474",
              "475",
              "476",
              "477",
              "478",
              "479",
              "480",
              "481",
              "482",
              "483",
              "484",
              "485",
              "486",
              "488",
              "489",
              "490",
              "494",
              "495",
              "496",
              "497",
              "498",
              "499",
              "500",
              "501",
              "502",
              "505",
              "506",
              "507",
              "508",
              "509",
              "510",
              "511",
              "512",
              "515",
              "516",
              "517",
              "519",
              "520",
              "521",
              "522",
              "523",
              "524",
              "528",
              "529",
              "530",
              "531",
              "532",
              "533",
              "555",
              "556",
              "557",
              "558",
              "559",
              "560",
              "561",
              "562",
              "563",
              "570",
              "571",
              "573",
              "574",
              "575",
              "576",
              "577",
              "578",
              "586",
              "592",
              "593",
              "594",
              "603",
              "604",
              "605",
              "606",
              "608",
              "609",
              "614",
              "615",
              "616",
              "617",
              "618",
              "619",
              "620",
              "621",
              "622",
              "623",
              "624",
              "625",
              "626",
              "627",
              "629",
              "630",
              "632",
              "633",
              "634",
              "635",
              "636",
              "637",
              "644",
              "645",
              "646",
              "647",
              "648",
              "649",
              "650",
              "651",
              "652",
              "653",
              "654",
              "655",
              "656",
              "657",
              "658",
              "659",
              "664",
              "671",
              "672",
              "673",
              "674",
              "676",
              "677",
              "678",
              "679",
              "680",
              "681",
              "682",
              "683",
              "684",
              "685",
              "686",
              "687",
              "688",
              "689",
              "690",
              "691",
              "692",
              "693",
              "694",
              "696",
              "697",
              "698",
              "702",
              "704",
              "705",
              "706",
              "709",
              "710",
              "711",
              "712",
              "713",
              "714",
              "716",
              "717",
              "725",
              "726",
              "727",
              "728",
              "730",
              "731",
              "732",
              "733",
              "734",
              "735",
              "737",
              "739",
              "741",
              "742",
              "743",
              "744",
              "745",
              "753",
              "754",
              "755",
              "756",
              "758",
              "764",
              "765",
              "766",
              "767",
              "768",
              "769",
              "775",
              "776",
              "777",
              "778",
              "779",
              "780",
              "793",
              "796",
              "797",
              "798",
              "799",
              "800",
              "808",
              "809",
              "810",
              "811",
              "812",
              "813",
              "814",
              "821",
              "822",
              "825",
              "826",
              "827",
              "828",
              "829",
              "830",
              "831",
              "832",
              "833",
              "842",
              "843",
              "844",
              "845",
              "846",
              "847",
              "848",
              "849",
              "850",
              "851",
              "854",
              "866",
              "867",
              "868",
              "869",
              "871",
              "873",
              "874",
              "875",
              "877",
              "884",
              "885",
              "886",
              "895",
              "897",
              "898",
              "899",
              "906",
              "912",
              "913",
              "920",
              "921",
              "922",
              "925",
              "926",
              "929",
              "930",
              "931",
              "932",
              "933",
              "934",
              "936",
              "939",
              "940",
              "941",
              "942",
              "943",
              "944",
              "945",
              "946",
              "947",
              "949",
              "950",
              "951",
              "952",
              "953",
              "954",
              "955",
              "956",
              "957",
              "959",
              "962",
              "963",
              "964",
              "965",
              "966",
              "967",
              "968",
              "970",
              "971",
              "972",
              "973",
              "975",
              "976",
              "977",
              "978",
              "979",
              "980",
              "981",
              "982",
              "983",
              "984",
              "986",
              "987",
              "988",
              "989",
              "990",
              "991",
              "992",
              "996",
              "997",
              "998",
              "1000",
              "1001",
              "1008",
              "1014",
              "1015",
              "1017",
              "1018",
              "1019",
              "1025",
              "1026",
              "1027",
              "1033",
              "1037",
              "1039",
              "1041",
              "1043",
              "1045",
              "1047",
              "1049",
              "1051",
              "1053",
              "1055",
              "1057",
              "1059",
              "1061",
              "1063",
              "1065",
              "1067",
              "1069",
              "1071",
              "1073",
              "1077",
              "1079",
              "1081",
              "1084",
              "1086",
              "1088",
              "1090",
              "1092",
              "1094",
              "1096",
              "1098",
              "1100",
              "1102",
              "1104",
              "1106",
              "1108",
              "1110",
              "1112",
              "1114",
              "1116",
              "1120",
              "1122",
              "1124",
              "1126",
              "1128",
              "1130",
              "1138",
              "1140",
              "1142",
              "1144",
              "1146",
              "1148",
              "1192",
              "1194",
              "1196",
              "1198",
              "1200",
              "1202",
              "1204",
              "1206",
              "1208",
              "1222",
              "1224",
              "1228",
              "1230",
              "1232",
              "1234",
              "1236",
              "1238",
              "1254",
              "1266",
              "1268",
              "1270",
              "1288",
              "1290",
              "1292",
              "1294",
              "1296",
              "1298",
              "1300",
              "1312",
              "1314",
              "1324",
              "1326",
              "1328",
              "1330",
              "1332",
              "1334",
              "1336",
              "1338",
              "1340",
              "1342",
              "1344",
              "1346",
              "1348",
              "1350",
              "1354",
              "1356",
              "1360",
              "1362",
              "1364",
              "1366",
              "1368",
              "1370",
              "1384",
              "1386",
              "1388",
              "1390",
              "1391",
              "1392",
              "1393",
              "1394",
              "1395",
              "1396",
              "1397",
              "1398",
              "1399",
              "1400",
              "1401",
              "1402",
              "1408",
              "1416",
              "1418",
              "1420",
              "1422",
              "1426",
              "1428",
              "1430",
              "1432",
              "1434",
              "1436",
              "1438",
              "1440",
              "1442",
              "1444",
              "1446",
              "1448",
              "1450",
              "1452",
              "1454",
              "1456",
              "1458",
              "1460",
              "1462",
              "1466",
              "1468",
              "1470",
              "1480",
              "1482",
              "1484",
              "1490",
              "1492",
              "1494",
              "1496",
              "1498",
              "1500",
              "1504",
              "1506",
              "1522",
              "1524",
              "1526",
              "1528",
              "1532",
              "1534",
              "1536",
              "1538",
              "1540",
              "1542",
              "1546",
              "1550",
              "1554",
              "1556",
              "1558",
              "1560",
              "1562",
              "1578",
              "1580",
              "1582",
              "1584",
              "1588",
              "1600",
              "1602",
              "1604",
              "1606",
              "1608",
              "1610",
              "1622",
              "1624",
              "1626",
              "1628",
              "1630",
              "1632",
              "1650",
              "1652",
              "1654",
              "1656",
              "1658",
              "1660",
              "1662",
              "1664",
              "1666",
              "1672",
              "1696",
              "1698",
              "1700",
              "1702",
              "1706",
              "1710",
              "1712",
              "1714",
              "1718",
              "1732",
              "1734",
              "1736"
            ],
            "covered": 94
          },
          "development_tools\\shared\\service\\utilities.py": {
            "statements": 363,
            "missed": 246,
            "coverage": 32,
            "missing_lines": [
              "21",
              "24",
              "35",
              "36",
              "41",
              "42",
              "44",
              "46",
              "47",
              "48",
              "49",
              "50",
              "52",
              "57",
              "58",
              "60",
              "64",
              "66",
              "67",
              "69",
              "70",
              "71",
              "72",
              "74",
              "85",
              "92",
              "93",
              "94",
              "111",
              "115",
              "121",
              "128",
              "130",
              "134",
              "136",
              "137",
              "138",
              "142",
              "143",
              "144",
              "145",
              "146",
              "147",
              "148",
              "153",
              "158",
              "159",
              "160",
              "161",
              "162",
              "163",
              "164",
              "165",
              "166",
              "167",
              "168",
              "169",
              "170",
              "171",
              "172",
              "173",
              "174",
              "175",
              "176",
              "177",
              "178",
              "179",
              "180",
              "181",
              "182",
              "183",
              "184",
              "185",
              "186",
              "187",
              "188",
              "189",
              "190",
              "191",
              "192",
              "193",
              "194",
              "195",
              "196",
              "197",
              "198",
              "199",
              "200",
              "201",
              "202",
              "203",
              "204",
              "206",
              "207",
              "208",
              "209",
              "210",
              "212",
              "214",
              "218",
              "219",
              "220",
              "221",
              "222",
              "223",
              "225",
              "226",
              "227",
              "228",
              "229",
              "230",
              "231",
              "232",
              "233",
              "235",
              "237",
              "238",
              "239",
              "240",
              "241",
              "242",
              "243",
              "244",
              "245",
              "246",
              "247",
              "248",
              "249",
              "250",
              "251",
              "252",
              "253",
              "254",
              "255",
              "256",
              "257",
              "258",
              "259",
              "260",
              "261",
              "262",
              "263",
              "265",
              "266",
              "267",
              "268",
              "269",
              "270",
              "271",
              "272",
              "273",
              "274",
              "275",
              "276",
              "277",
              "278",
              "279",
              "280",
              "281",
              "282",
              "283",
              "284",
              "285",
              "286",
              "287",
              "293",
              "310",
              "312",
              "314",
              "319",
              "320",
              "325",
              "326",
              "331",
              "332",
              "337",
              "338",
              "343",
              "344",
              "346",
              "347",
              "348",
              "349",
              "350",
              "351",
              "352",
              "353",
              "354",
              "355",
              "356",
              "357",
              "358",
              "359",
              "360",
              "361",
              "362",
              "368",
              "369",
              "370",
              "371",
              "372",
              "373",
              "379",
              "381",
              "392",
              "393",
              "394",
              "395",
              "396",
              "399",
              "401",
              "406",
              "407",
              "408",
              "426",
              "427",
              "428",
              "429",
              "430",
              "431",
              "432",
              "433",
              "434",
              "435",
              "436",
              "437",
              "438",
              "439",
              "440",
              "441",
              "442",
              "443",
              "444",
              "445",
              "446",
              "447",
              "448",
              "449",
              "453",
              "454",
              "455",
              "456",
              "457",
              "458",
              "459",
              "460"
            ],
            "covered": 117
          },
          "development_tools\\shared\\standard_exclusions.py": {
            "statements": 109,
            "missed": 18,
            "coverage": 83,
            "missing_lines": [
              "28",
              "30",
              "31",
              "33",
              "34",
              "41",
              "43",
              "113",
              "114",
              "115",
              "123",
              "138",
              "216",
              "289",
              "294",
              "313",
              "355",
              "389"
            ],
            "covered": 91
          },
          "development_tools\\shared\\tool_guide.py": {
            "statements": 131,
            "missed": 131,
            "coverage": 0,
            "missing_lines": [
              "10",
              "11",
              "12",
              "15",
              "16",
              "17",
              "19",
              "21",
              "22",
              "23",
              "24",
              "26",
              "28",
              "35",
              "195",
              "197",
              "198",
              "199",
              "200",
              "201",
              "202",
              "203",
              "204",
              "205",
              "206",
              "207",
              "210",
              "212",
              "213",
              "214",
              "215",
              "216",
              "217",
              "218",
              "220",
              "221",
              "222",
              "223",
              "225",
              "226",
              "227",
              "228",
              "230",
              "231",
              "232",
              "234",
              "236",
              "237",
              "238",
              "240",
              "241",
              "242",
              "243",
              "244",
              "245",
              "246",
              "247",
              "248",
              "250",
              "251",
              "252",
              "253",
              "254",
              "255",
              "258",
              "260",
              "262",
              "264",
              "265",
              "266",
              "267",
              "281",
              "283",
              "286",
              "288",
              "289",
              "291",
              "293",
              "294",
              "295",
              "296",
              "297",
              "298",
              "299",
              "301",
              "303",
              "304",
              "307",
              "309",
              "310",
              "311",
              "313",
              "315",
              "316",
              "319",
              "320",
              "321",
              "322",
              "323",
              "324",
              "325",
              "328",
              "329",
              "331",
              "332",
              "334",
              "335",
              "338",
              "339",
              "341",
              "343",
              "344",
              "346",
              "347",
              "348",
              "349",
              "350",
              "351",
              "352",
              "353",
              "354",
              "356",
              "357",
              "358",
              "359",
              "361",
              "362",
              "363",
              "364",
              "365",
              "366"
            ],
            "covered": 0
          },
          "development_tools\\shared\\tool_metadata.py": {
            "statements": 26,
            "missed": 6,
            "coverage": 77,
            "missing_lines": [
              "435",
              "436",
              "437",
              "443",
              "449",
              "455"
            ],
            "covered": 20
          },
          "development_tools\\tests\\__init__.py": {
            "statements": 0,
            "missed": 0,
            "coverage": 100,
            "missing_lines": [],
            "covered": 0
          },
          "development_tools\\tests\\analyze_test_coverage.py": {
            "statements": 200,
            "missed": 110,
            "coverage": 45,
            "missing_lines": [
              "27",
              "34",
              "36",
              "37",
              "38",
              "43",
              "45",
              "46",
              "47",
              "48",
              "79",
              "109",
              "110",
              "120",
              "121",
              "130",
              "131",
              "132",
              "133",
              "155",
              "156",
              "157",
              "158",
              "175",
              "178",
              "183",
              "184",
              "185",
              "186",
              "187",
              "188",
              "190",
              "192",
              "217",
              "223",
              "224",
              "225",
              "226",
              "227",
              "229",
              "230",
              "231",
              "233",
              "234",
              "235",
              "236",
              "238",
              "239",
              "243",
              "245",
              "297",
              "298",
              "299",
              "300",
              "302",
              "309",
              "310",
              "311",
              "312",
              "315",
              "322",
              "323",
              "324",
              "325",
              "328",
              "331",
              "332",
              "334",
              "336",
              "337",
              "338",
              "340",
              "341",
              "344",
              "345",
              "346",
              "347",
              "350",
              "352",
              "359",
              "365",
              "366",
              "367",
              "368",
              "372",
              "377",
              "379",
              "380",
              "381",
              "382",
              "384",
              "386",
              "389",
              "390",
              "391",
              "392",
              "394",
              "395",
              "396",
              "397",
              "399",
              "400",
              "401",
              "404",
              "407",
              "408",
              "409",
              "410",
              "412",
              "414"
            ],
            "covered": 90
          },
          "development_tools\\tests\\analyze_test_markers.py": {
            "statements": 243,
            "missed": 243,
            "coverage": 0,
            "missing_lines": [
              "11",
              "12",
              "13",
              "14",
              "15",
              "18",
              "19",
              "20",
              "23",
              "24",
              "25",
              "26",
              "28",
              "31",
              "33",
              "35",
              "38",
              "41",
              "42",
              "43",
              "45",
              "47",
              "48",
              "49",
              "51",
              "52",
              "54",
              "57",
              "60",
              "61",
              "63",
              "64",
              "66",
              "68",
              "70",
              "72",
              "78",
              "80",
              "82",
              "83",
              "84",
              "85",
              "86",
              "87",
              "88",
              "89",
              "90",
              "91",
              "93",
              "95",
              "96",
              "97",
              "105",
              "106",
              "107",
              "108",
              "109",
              "110",
              "112",
              "113",
              "114",
              "116",
              "118",
              "119",
              "121",
              "128",
              "130",
              "131",
              "133",
              "134",
              "136",
              "138",
              "145",
              "146",
              "147",
              "149",
              "150",
              "151",
              "152",
              "153",
              "154",
              "157",
              "158",
              "159",
              "162",
              "163",
              "164",
              "165",
              "168",
              "169",
              "171",
              "172",
              "173",
              "176",
              "177",
              "180",
              "185",
              "188",
              "190",
              "191",
              "194",
              "195",
              "196",
              "198",
              "199",
              "205",
              "212",
              "219",
              "220",
              "221",
              "222",
              "229",
              "233",
              "235",
              "238",
              "244",
              "246",
              "247",
              "250",
              "253",
              "254",
              "255",
              "260",
              "266",
              "267",
              "274",
              "275",
              "276",
              "278",
              "281",
              "284",
              "285",
              "287",
              "288",
              "289",
              "290",
              "291",
              "292",
              "293",
              "294",
              "295",
              "296",
              "297",
              "298",
              "299",
              "300",
              "301",
              "302",
              "304",
              "305",
              "306",
              "307",
              "308",
              "309",
              "310",
              "311",
              "312",
              "313",
              "314",
              "316",
              "317",
              "318",
              "319",
              "320",
              "322",
              "323",
              "324",
              "325",
              "326",
              "327",
              "328",
              "329",
              "331",
              "332",
              "333",
              "334",
              "335",
              "336",
              "340",
              "341",
              "343",
              "344",
              "345",
              "346",
              "347",
              "348",
              "349",
              "350",
              "351",
              "352",
              "353",
              "354",
              "355",
              "357",
              "358",
              "359",
              "360",
              "361",
              "362",
              "363",
              "364",
              "365",
              "366",
              "367",
              "368",
              "369",
              "372",
              "373",
              "375",
              "378",
              "381",
              "386",
              "388",
              "391",
              "392",
              "394",
              "396",
              "397",
              "398",
              "399",
              "401",
              "403",
              "404",
              "405",
              "406",
              "407",
              "408",
              "409",
              "410",
              "411",
              "414",
              "415",
              "416",
              "418",
              "431",
              "432",
              "433",
              "435",
              "436",
              "437",
              "439",
              "440",
              "442"
            ],
            "covered": 0
          },
          "development_tools\\tests\\dev_tools_coverage_cache.py": {
            "statements": 127,
            "missed": 94,
            "coverage": 26,
            "missing_lines": [
              "20",
              "29",
              "30",
              "36",
              "37",
              "63",
              "64",
              "65",
              "66",
              "67",
              "68",
              "69",
              "70",
              "71",
              "72",
              "73",
              "74",
              "75",
              "76",
              "77",
              "78",
              "80",
              "82",
              "83",
              "84",
              "85",
              "86",
              "87",
              "88",
              "89",
              "90",
              "91",
              "92",
              "93",
              "94",
              "95",
              "96",
              "98",
              "99",
              "100",
              "101",
              "102",
              "113",
              "114",
              "115",
              "116",
              "117",
              "118",
              "121",
              "125",
              "126",
              "127",
              "129",
              "130",
              "131",
              "132",
              "133",
              "134",
              "135",
              "136",
              "137",
              "138",
              "139",
              "140",
              "141",
              "143",
              "144",
              "145",
              "147",
              "148",
              "149",
              "150",
              "151",
              "152",
              "153",
              "154",
              "155",
              "156",
              "158",
              "159",
              "160",
              "161",
              "162",
              "163",
              "165",
              "166",
              "167",
              "168",
              "172",
              "173",
              "177",
              "183",
              "184",
              "185"
            ],
            "covered": 33
          },
          "development_tools\\tests\\domain_mapper.py": {
            "statements": 132,
            "missed": 107,
            "coverage": 19,
            "missing_lines": [
              "27",
              "28",
              "79",
              "80",
              "83",
              "84",
              "85",
              "86",
              "87",
              "88",
              "91",
              "92",
              "93",
              "94",
              "96",
              "108",
              "109",
              "111",
              "112",
              "113",
              "114",
              "115",
              "117",
              "129",
              "130",
              "132",
              "133",
              "134",
              "135",
              "136",
              "137",
              "139",
              "151",
              "152",
              "154",
              "156",
              "157",
              "158",
              "160",
              "161",
              "164",
              "165",
              "168",
              "169",
              "172",
              "173",
              "174",
              "181",
              "182",
              "184",
              "185",
              "193",
              "194",
              "196",
              "197",
              "201",
              "202",
              "203",
              "205",
              "206",
              "207",
              "209",
              "210",
              "222",
              "225",
              "226",
              "227",
              "228",
              "230",
              "231",
              "234",
              "235",
              "237",
              "238",
              "239",
              "240",
              "241",
              "243",
              "255",
              "256",
              "257",
              "258",
              "259",
              "260",
              "272",
              "273",
              "276",
              "277",
              "278",
              "279",
              "280",
              "282",
              "283",
              "286",
              "298",
              "299",
              "300",
              "301",
              "302",
              "303",
              "315",
              "316",
              "318",
              "360",
              "361",
              "362",
              "364"
            ],
            "covered": 25
          },
          "development_tools\\tests\\fix_test_markers.py": {
            "statements": 35,
            "missed": 35,
            "coverage": 0,
            "missing_lines": [
              "11",
              "12",
              "13",
              "16",
              "17",
              "18",
              "21",
              "22",
              "23",
              "24",
              "25",
              "26",
              "28",
              "31",
              "33",
              "36",
              "47",
              "48",
              "51",
              "52",
              "54",
              "57",
              "62",
              "64",
              "66",
              "68",
              "69",
              "71",
              "73",
              "74",
              "76",
              "77",
              "78",
              "79",
              "83"
            ],
            "covered": 0
          },
          "development_tools\\tests\\generate_test_coverage_report.py": {
            "statements": 243,
            "missed": 130,
            "coverage": 47,
            "missing_lines": [
              "25",
              "33",
              "35",
              "36",
              "37",
              "96",
              "97",
              "100",
              "105",
              "132",
              "133",
              "135",
              "153",
              "154",
              "225",
              "226",
              "227",
              "228",
              "230",
              "231",
              "232",
              "240",
              "252",
              "256",
              "261",
              "262",
              "264",
              "266",
              "268",
              "269",
              "277",
              "278",
              "279",
              "280",
              "288",
              "289",
              "290",
              "297",
              "298",
              "299",
              "307",
              "310",
              "311",
              "313",
              "315",
              "317",
              "322",
              "323",
              "326",
              "327",
              "329",
              "336",
              "339",
              "341",
              "342",
              "343",
              "344",
              "345",
              "347",
              "350",
              "353",
              "354",
              "359",
              "368",
              "370",
              "378",
              "379",
              "380",
              "382",
              "383",
              "384",
              "385",
              "394",
              "417",
              "427",
              "428",
              "429",
              "451",
              "452",
              "453",
              "456",
              "479",
              "480",
              "481",
              "482",
              "483",
              "484",
              "487",
              "489",
              "491",
              "513",
              "515",
              "518",
              "524",
              "529",
              "533",
              "537",
              "538",
              "542",
              "543",
              "545",
              "546",
              "547",
              "548",
              "553",
              "556",
              "557",
              "558",
              "559",
              "560",
              "561",
              "564",
              "565",
              "566",
              "567",
              "568",
              "569",
              "572",
              "575",
              "576",
              "577",
              "582",
              "583",
              "584",
              "585",
              "587",
              "588",
              "590",
              "591",
              "593"
            ],
            "covered": 113
          },
          "development_tools\\tests\\run_test_coverage.py": {
            "statements": 1626,
            "missed": 1291,
            "coverage": 21,
            "missing_lines": [
              "39",
              "46",
              "49",
              "50",
              "51",
              "59",
              "61",
              "62",
              "63",
              "64",
              "65",
              "66",
              "122",
              "128",
              "139",
              "157",
              "164",
              "177",
              "184",
              "252",
              "253",
              "254",
              "257",
              "258",
              "259",
              "270",
              "271",
              "272",
              "275",
              "281",
              "282",
              "286",
              "287",
              "288",
              "289",
              "293",
              "294",
              "295",
              "300",
              "301",
              "302",
              "308",
              "309",
              "313",
              "314",
              "315",
              "316",
              "320",
              "321",
              "322",
              "344",
              "345",
              "346",
              "396",
              "411",
              "412",
              "413",
              "414",
              "415",
              "417",
              "418",
              "422",
              "423",
              "424",
              "425",
              "427",
              "428",
              "429",
              "431",
              "453",
              "464",
              "467",
              "468",
              "471",
              "474",
              "475",
              "477",
              "478",
              "479",
              "482",
              "485",
              "486",
              "487",
              "488",
              "489",
              "492",
              "493",
              "494",
              "495",
              "496",
              "497",
              "498",
              "499",
              "500",
              "501",
              "502",
              "505",
              "506",
              "511",
              "512",
              "517",
              "522",
              "523",
              "525",
              "530",
              "532",
              "533",
              "534",
              "536",
              "537",
              "539",
              "541",
              "542",
              "543",
              "545",
              "546",
              "547",
              "548",
              "550",
              "551",
              "552",
              "553",
              "555",
              "561",
              "570",
              "572",
              "573",
              "575",
              "576",
              "582",
              "583",
              "584",
              "586",
              "587",
              "588",
              "589",
              "590",
              "591",
              "592",
              "593",
              "595",
              "596",
              "597",
              "599",
              "600",
              "604",
              "665",
              "669",
              "672",
              "673",
              "676",
              "677",
              "684",
              "685",
              "686",
              "687",
              "691",
              "696",
              "697",
              "701",
              "702",
              "703",
              "705",
              "706",
              "709",
              "710",
              "713",
              "714",
              "717",
              "720",
              "724",
              "725",
              "727",
              "729",
              "730",
              "740",
              "741",
              "742",
              "743",
              "744",
              "745",
              "746",
              "747",
              "748",
              "749",
              "751",
              "752",
              "753",
              "754",
              "755",
              "759",
              "760",
              "771",
              "772",
              "773",
              "786",
              "787",
              "791",
              "812",
              "813",
              "814",
              "815",
              "823",
              "824",
              "825",
              "826",
              "830",
              "831",
              "836",
              "838",
              "839",
              "840",
              "841",
              "847",
              "848",
              "849",
              "850",
              "851",
              "853",
              "854",
              "857",
              "862",
              "863",
              "866",
              "867",
              "873",
              "885",
              "893",
              "894",
              "897",
              "946",
              "968",
              "979",
              "980",
              "981",
              "1009",
              "1083",
              "1086",
              "1087",
              "1088",
              "1089",
              "1090",
              "1091",
              "1095",
              "1103",
              "1104",
              "1105",
              "1108",
              "1110",
              "1111",
              "1112",
              "1115",
              "1118",
              "1122",
              "1125",
              "1126",
              "1129",
              "1130",
              "1133",
              "1134",
              "1137",
              "1140",
              "1141",
              "1144",
              "1145",
              "1149",
              "1164",
              "1190",
              "1196",
              "1197",
              "1198",
              "1215",
              "1220",
              "1221",
              "1223",
              "1224",
              "1230",
              "1232",
              "1235",
              "1236",
              "1245",
              "1252",
              "1253",
              "1254",
              "1257",
              "1260",
              "1261",
              "1264",
              "1265",
              "1267",
              "1268",
              "1269",
              "1276",
              "1277",
              "1278",
              "1279",
              "1280",
              "1281",
              "1284",
              "1285",
              "1286",
              "1289",
              "1290",
              "1291",
              "1292",
              "1294",
              "1295",
              "1297",
              "1298",
              "1300",
              "1303",
              "1304",
              "1305",
              "1308",
              "1309",
              "1312",
              "1313",
              "1320",
              "1321",
              "1324",
              "1337",
              "1338",
              "1339",
              "1340",
              "1347",
              "1350",
              "1353",
              "1356",
              "1357",
              "1358",
              "1363",
              "1364",
              "1365",
              "1366",
              "1372",
              "1373",
              "1374",
              "1375",
              "1376",
              "1377",
              "1378",
              "1379",
              "1382",
              "1383",
              "1385",
              "1386",
              "1387",
              "1388",
              "1390",
              "1391",
              "1395",
              "1396",
              "1399",
              "1400",
              "1402",
              "1403",
              "1409",
              "1420",
              "1421",
              "1424",
              "1427",
              "1428",
              "1433",
              "1434",
              "1438",
              "1439",
              "1440",
              "1445",
              "1448",
              "1452",
              "1453",
              "1454",
              "1457",
              "1460",
              "1463",
              "1464",
              "1469",
              "1470",
              "1471",
              "1472",
              "1473",
              "1476",
              "1481",
              "1482",
              "1483",
              "1492",
              "1493",
              "1498",
              "1499",
              "1500",
              "1501",
              "1505",
              "1506",
              "1509",
              "1510",
              "1514",
              "1515",
              "1516",
              "1517",
              "1519",
              "1524",
              "1525",
              "1526",
              "1541",
              "1543",
              "1544",
              "1545",
              "1546",
              "1552",
              "1553",
              "1554",
              "1558",
              "1561",
              "1564",
              "1570",
              "1582",
              "1586",
              "1587",
              "1591",
              "1592",
              "1593",
              "1603",
              "1604",
              "1609",
              "1630",
              "1632",
              "1633",
              "1637",
              "1639",
              "1640",
              "1643",
              "1645",
              "1649",
              "1653",
              "1654",
              "1655",
              "1661",
              "1662",
              "1665",
              "1669",
              "1670",
              "1673",
              "1683",
              "1686",
              "1689",
              "1690",
              "1693",
              "1694",
              "1695",
              "1698",
              "1713",
              "1714",
              "1715",
              "1720",
              "1721",
              "1727",
              "1731",
              "1733",
              "1734",
              "1737",
              "1740",
              "1744",
              "1747",
              "1750",
              "1753",
              "1756",
              "1759",
              "1762",
              "1765",
              "1768",
              "1771",
              "1775",
              "1779",
              "1780",
              "1781",
              "1795",
              "1796",
              "1799",
              "1800",
              "1803",
              "1807",
              "1812",
              "1816",
              "1817",
              "1820",
              "1824",
              "1827",
              "1830",
              "1833",
              "1836",
              "1839",
              "1842",
              "1845",
              "1848",
              "1851",
              "1854",
              "1855",
              "1858",
              "1859",
              "1863",
              "1867",
              "1871",
              "1872",
              "1873",
              "1874",
              "1878",
              "1881",
              "1882",
              "1885",
              "1887",
              "1892",
              "1895",
              "1898",
              "1902",
              "1903",
              "1907",
              "1910",
              "1914",
              "1918",
              "1923",
              "1924",
              "1929",
              "1930",
              "1935",
              "1938",
              "1939",
              "1944",
              "1945",
              "1946",
              "1951",
              "1952",
              "1953",
              "1957",
              "1958",
              "1961",
              "1964",
              "1965",
              "1972",
              "1973",
              "1976",
              "1983",
              "1984",
              "1986",
              "1990",
              "1998",
              "1999",
              "2000",
              "2004",
              "2005",
              "2006",
              "2007",
              "2011",
              "2018",
              "2019",
              "2025",
              "2026",
              "2032",
              "2039",
              "2040",
              "2043",
              "2050",
              "2051",
              "2057",
              "2058",
              "2059",
              "2060",
              "2061",
              "2062",
              "2063",
              "2064",
              "2067",
              "2068",
              "2069",
              "2073",
              "2074",
              "2075",
              "2078",
              "2079",
              "2086",
              "2087",
              "2091",
              "2094",
              "2097",
              "2098",
              "2100",
              "2101",
              "2102",
              "2103",
              "2104",
              "2107",
              "2108",
              "2111",
              "2114",
              "2115",
              "2116",
              "2117",
              "2118",
              "2119",
              "2120",
              "2123",
              "2124",
              "2128",
              "2129",
              "2132",
              "2133",
              "2134",
              "2137",
              "2140",
              "2141",
              "2145",
              "2146",
              "2150",
              "2154",
              "2158",
              "2159",
              "2160",
              "2164",
              "2165",
              "2171",
              "2172",
              "2173",
              "2179",
              "2181",
              "2182",
              "2186",
              "2195",
              "2196",
              "2199",
              "2202",
              "2203",
              "2206",
              "2207",
              "2212",
              "2221",
              "2222",
              "2223",
              "2226",
              "2227",
              "2231",
              "2235",
              "2240",
              "2241",
              "2244",
              "2246",
              "2247",
              "2248",
              "2251",
              "2252",
              "2255",
              "2259",
              "2262",
              "2266",
              "2267",
              "2268",
              "2269",
              "2273",
              "2277",
              "2282",
              "2283",
              "2284",
              "2285",
              "2286",
              "2287",
              "2288",
              "2289",
              "2290",
              "2291",
              "2295",
              "2296",
              "2297",
              "2298",
              "2299",
              "2300",
              "2301",
              "2304",
              "2309",
              "2314",
              "2322",
              "2323",
              "2324",
              "2325",
              "2326",
              "2330",
              "2331",
              "2332",
              "2333",
              "2338",
              "2340",
              "2341",
              "2342",
              "2343",
              "2344",
              "2345",
              "2346",
              "2347",
              "2348",
              "2353",
              "2357",
              "2358",
              "2362",
              "2372",
              "2373",
              "2380",
              "2381",
              "2382",
              "2387",
              "2388",
              "2389",
              "2392",
              "2393",
              "2396",
              "2397",
              "2403",
              "2406",
              "2407",
              "2408",
              "2409",
              "2414",
              "2416",
              "2417",
              "2418",
              "2421",
              "2424",
              "2433",
              "2434",
              "2437",
              "2438",
              "2439",
              "2444",
              "2447",
              "2448",
              "2449",
              "2452",
              "2459",
              "2465",
              "2468",
              "2477",
              "2482",
              "2490",
              "2491",
              "2492",
              "2493",
              "2496",
              "2499",
              "2500",
              "2501",
              "2506",
              "2509",
              "2510",
              "2511",
              "2516",
              "2523",
              "2528",
              "2534",
              "2537",
              "2540",
              "2543",
              "2546",
              "2549",
              "2550",
              "2551",
              "2554",
              "2557",
              "2558",
              "2559",
              "2562",
              "2565",
              "2566",
              "2569",
              "2574",
              "2575",
              "2578",
              "2579",
              "2585",
              "2588",
              "2591",
              "2592",
              "2593",
              "2596",
              "2597",
              "2598",
              "2603",
              "2609",
              "2610",
              "2615",
              "2622",
              "2632",
              "2637",
              "2640",
              "2643",
              "2647",
              "2648",
              "2652",
              "2653",
              "2658",
              "2661",
              "2665",
              "2666",
              "2672",
              "2675",
              "2676",
              "2687",
              "2696",
              "2699",
              "2702",
              "2711",
              "2713",
              "2718",
              "2719",
              "2728",
              "2729",
              "2735",
              "2739",
              "2742",
              "2747",
              "2750",
              "2754",
              "2755",
              "2756",
              "2762",
              "2767",
              "2768",
              "2775",
              "2779",
              "2782",
              "2785",
              "2786",
              "2789",
              "2794",
              "2795",
              "2796",
              "2800",
              "2801",
              "2803",
              "2804",
              "2807",
              "2808",
              "2809",
              "2826",
              "2827",
              "2829",
              "2830",
              "2831",
              "2832",
              "2833",
              "2841",
              "2842",
              "2846",
              "2854",
              "2855",
              "2856",
              "2857",
              "2858",
              "2859",
              "2867",
              "2868",
              "2871",
              "2872",
              "2873",
              "2878",
              "2879",
              "2880",
              "2881",
              "2904",
              "2905",
              "2907",
              "2908",
              "2909",
              "2910",
              "2912",
              "2913",
              "2915",
              "2930",
              "2931",
              "2932",
              "2933",
              "2936",
              "2939",
              "2940",
              "2941",
              "2942",
              "2943",
              "2944",
              "2945",
              "2946",
              "2948",
              "2957",
              "2958",
              "2961",
              "2962",
              "2963",
              "2964",
              "2967",
              "2968",
              "2969",
              "2970",
              "2973",
              "2974",
              "2975",
              "2977",
              "2981",
              "2982",
              "2985",
              "2986",
              "2988",
              "2989",
              "2991",
              "2993",
              "2994",
              "2995",
              "2996",
              "3001",
              "3003",
              "3004",
              "3008",
              "3009",
              "3010",
              "3013",
              "3015",
              "3016",
              "3017",
              "3018",
              "3019",
              "3020",
              "3021",
              "3022",
              "3024",
              "3026",
              "3028",
              "3029",
              "3030",
              "3035",
              "3036",
              "3039",
              "3040",
              "3046",
              "3058",
              "3067",
              "3068",
              "3074",
              "3076",
              "3078",
              "3098",
              "3099",
              "3103",
              "3105",
              "3107",
              "3108",
              "3109",
              "3112",
              "3114",
              "3115",
              "3118",
              "3120",
              "3122",
              "3126",
              "3129",
              "3130",
              "3133",
              "3136",
              "3137",
              "3147",
              "3148",
              "3149",
              "3150",
              "3152",
              "3155",
              "3156",
              "3157",
              "3158",
              "3159",
              "3160",
              "3161",
              "3164",
              "3167",
              "3170",
              "3171",
              "3172",
              "3173",
              "3175",
              "3176",
              "3177",
              "3178",
              "3181",
              "3184",
              "3187",
              "3188",
              "3189",
              "3190",
              "3192",
              "3194",
              "3195",
              "3200",
              "3205",
              "3206",
              "3208",
              "3209",
              "3210",
              "3211",
              "3214",
              "3215",
              "3216",
              "3217",
              "3218",
              "3219",
              "3225",
              "3226",
              "3227",
              "3228",
              "3229",
              "3234",
              "3235",
              "3237",
              "3238",
              "3240",
              "3241",
              "3243",
              "3244",
              "3248",
              "3249",
              "3254",
              "3255",
              "3257",
              "3258",
              "3262",
              "3265",
              "3266",
              "3267",
              "3269",
              "3274",
              "3278",
              "3279",
              "3282",
              "3283",
              "3285",
              "3286",
              "3290",
              "3294",
              "3295",
              "3298",
              "3299",
              "3302",
              "3309",
              "3310",
              "3311",
              "3312",
              "3313",
              "3316",
              "3317",
              "3318",
              "3319",
              "3320",
              "3323",
              "3325",
              "3327",
              "3328",
              "3329",
              "3333",
              "3335",
              "3348",
              "3349",
              "3350",
              "3352",
              "3353",
              "3363",
              "3364",
              "3366",
              "3367",
              "3369",
              "3370",
              "3372",
              "3373",
              "3374",
              "3375",
              "3376",
              "3377",
              "3378",
              "3380",
              "3381",
              "3382",
              "3383",
              "3384",
              "3385",
              "3386",
              "3388",
              "3389",
              "3392",
              "3393",
              "3394",
              "3395",
              "3396",
              "3397",
              "3398",
              "3399",
              "3400",
              "3404",
              "3405",
              "3411",
              "3412",
              "3414",
              "3418",
              "3420",
              "3421",
              "3423",
              "3424",
              "3425",
              "3426",
              "3427",
              "3428",
              "3431",
              "3432",
              "3433",
              "3435",
              "3436",
              "3437",
              "3438",
              "3439",
              "3440",
              "3444",
              "3445",
              "3466",
              "3471",
              "3472",
              "3473",
              "3474",
              "3477",
              "3478",
              "3479",
              "3480",
              "3484",
              "3485",
              "3486",
              "3487",
              "3488",
              "3489",
              "3490",
              "3491",
              "3496",
              "3503",
              "3504",
              "3505",
              "3506",
              "3507",
              "3508",
              "3509",
              "3510",
              "3511",
              "3512",
              "3513",
              "3514",
              "3519",
              "3525",
              "3526",
              "3527",
              "3528",
              "3529",
              "3530",
              "3531",
              "3532",
              "3533",
              "3534",
              "3535",
              "3536",
              "3537",
              "3538",
              "3539",
              "3540",
              "3541",
              "3545",
              "3546",
              "3551",
              "3552",
              "3553",
              "3554",
              "3555",
              "3556",
              "3557",
              "3558",
              "3559",
              "3560",
              "3561",
              "3564",
              "3565",
              "3568",
              "3569",
              "3570",
              "3571",
              "3572",
              "3573",
              "3575",
              "3576",
              "3577",
              "3579",
              "3638",
              "3639",
              "3640",
              "3643",
              "3644",
              "3645",
              "3656",
              "3657",
              "3658",
              "3659",
              "3660",
              "3661",
              "3662",
              "3663",
              "3664",
              "3685",
              "3688",
              "3689",
              "3690",
              "3691",
              "3692",
              "3693",
              "3694",
              "3695",
              "3702",
              "3704",
              "3705",
              "3706",
              "3707",
              "3708",
              "3714",
              "3715",
              "3717",
              "3719",
              "3721",
              "3723",
              "3724",
              "3726",
              "3727",
              "3732",
              "3733",
              "3742",
              "3743",
              "3746",
              "3750",
              "3752",
              "3758",
              "3760",
              "3762",
              "3763",
              "3764",
              "3765",
              "3766",
              "3769",
              "3770",
              "3771",
              "3773",
              "3776",
              "3779",
              "3780",
              "3786",
              "3792",
              "3794",
              "3796",
              "3799",
              "3801",
              "3802",
              "3805",
              "3808",
              "3809",
              "3812",
              "3813",
              "3816",
              "3819",
              "3820",
              "3825",
              "3830",
              "3834",
              "3835",
              "3838",
              "3842",
              "3847",
              "3850",
              "3855",
              "3858",
              "3863",
              "3868",
              "3873",
              "3879",
              "3882",
              "3883",
              "3888",
              "3892",
              "3894",
              "3895",
              "3896",
              "3898",
              "3899"
            ],
            "covered": 335
          },
          "development_tools\\tests\\test_file_coverage_cache.py": {
            "statements": 321,
            "missed": 235,
            "coverage": 27,
            "missing_lines": [
              "35",
              "44",
              "45",
              "51",
              "52",
              "110",
              "113",
              "114",
              "115",
              "116",
              "118",
              "123",
              "124",
              "125",
              "126",
              "127",
              "128",
              "129",
              "130",
              "131",
              "132",
              "133",
              "134",
              "135",
              "136",
              "137",
              "138",
              "140",
              "142",
              "143",
              "144",
              "145",
              "146",
              "147",
              "148",
              "149",
              "150",
              "151",
              "153",
              "154",
              "157",
              "160",
              "161",
              "162",
              "163",
              "164",
              "166",
              "167",
              "168",
              "169",
              "170",
              "181",
              "182",
              "183",
              "184",
              "185",
              "186",
              "189",
              "193",
              "194",
              "195",
              "197",
              "198",
              "199",
              "200",
              "201",
              "202",
              "203",
              "204",
              "205",
              "206",
              "207",
              "208",
              "209",
              "211",
              "212",
              "213",
              "215",
              "216",
              "217",
              "218",
              "219",
              "220",
              "221",
              "222",
              "223",
              "224",
              "226",
              "228",
              "229",
              "230",
              "231",
              "232",
              "233",
              "234",
              "236",
              "237",
              "238",
              "239",
              "254",
              "255",
              "256",
              "257",
              "259",
              "261",
              "262",
              "263",
              "264",
              "265",
              "266",
              "267",
              "268",
              "270",
              "284",
              "287",
              "288",
              "289",
              "290",
              "291",
              "302",
              "320",
              "342",
              "343",
              "344",
              "346",
              "347",
              "348",
              "352",
              "354",
              "355",
              "357",
              "358",
              "360",
              "364",
              "365",
              "368",
              "369",
              "370",
              "371",
              "373",
              "376",
              "377",
              "379",
              "391",
              "392",
              "395",
              "396",
              "397",
              "398",
              "399",
              "400",
              "404",
              "405",
              "406",
              "407",
              "408",
              "411",
              "412",
              "416",
              "417",
              "419",
              "420",
              "422",
              "437",
              "438",
              "440",
              "441",
              "443",
              "444",
              "447",
              "448",
              "454",
              "459",
              "460",
              "462",
              "463",
              "464",
              "466",
              "467",
              "469",
              "470",
              "483",
              "485",
              "486",
              "488",
              "489",
              "491",
              "498",
              "499",
              "501",
              "502",
              "503",
              "505",
              "506",
              "508",
              "522",
              "523",
              "525",
              "526",
              "529",
              "530",
              "531",
              "533",
              "535",
              "537",
              "555",
              "556",
              "557",
              "558",
              "559",
              "574",
              "581",
              "584",
              "588",
              "590",
              "591",
              "592",
              "598",
              "607",
              "608",
              "609",
              "618",
              "619",
              "625",
              "626",
              "627",
              "628",
              "630",
              "649",
              "650",
              "653",
              "654",
              "655",
              "657",
              "668"
            ],
            "covered": 86
          }
        },
        "coverage_collected": true,
        "output_file": "C:\\Users\\Julie\\projects\\MHM\\MHM\\development_tools\\tests\\jsons\\coverage_dev_tools.json",
        "html_dir": null
      },
      "timestamp": "2026-01-18T01:52:40"
    },
    "analyze_dependency_patterns": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 0,
          "files_affected": 0
        },
        "details": {
          "core_dependencies": [
            {
              "file": "core/auto_cleanup.py",
              "local_imports": 12,
              "third_party_imports": 0,
              "modules": [
                "core.error_handling",
                "core.logger",
                "core.time_utilities",
                "core.config",
                "core.logger",
                "core.config",
                "core.config",
                "core.config",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.message_management",
                "core.config"
              ]
            },
            {
              "file": "core/backup_manager.py",
              "local_imports": 7,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.config",
                "core.error_handling",
                "core.user_data_handlers",
                "core.time_utilities",
                "core.config",
                "core.config"
              ]
            },
            {
              "file": "core/checkin_analytics.py",
              "local_imports": 5,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.response_tracking",
                "core.error_handling",
                "core.time_utilities",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "core/checkin_dynamic_manager.py",
              "local_imports": 6,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.file_operations",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "core/config.py",
              "local_imports": 2,
              "third_party_imports": 1,
              "modules": [
                "core.error_handling",
                "core.logger"
              ]
            },
            {
              "file": "core/error_handling.py",
              "local_imports": 21,
              "third_party_imports": 0,
              "modules": [
                "core.time_utilities",
                "core.service_utilities",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger"
              ]
            },
            {
              "file": "core/file_auditor.py",
              "local_imports": 2,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "core/file_locking.py",
              "local_imports": 2,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "core/file_operations.py",
              "local_imports": 7,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.config",
                "core.error_handling",
                "core.time_utilities",
                "core.file_auditor",
                "core.message_management",
                "core.user_data_manager"
              ]
            },
            {
              "file": "core/headless_service.py",
              "local_imports": 3,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.service_utilities"
              ]
            },
            {
              "file": "core/logger.py",
              "local_imports": 7,
              "third_party_imports": 0,
              "modules": [
                "core.error_handling",
                "core.config",
                "core.config",
                "core.config",
                "core.config",
                "core.config",
                "core.config"
              ]
            },
            {
              "file": "core/message_analytics.py",
              "local_imports": 3,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.message_management",
                "core.error_handling"
              ]
            },
            {
              "file": "core/message_management.py",
              "local_imports": 9,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.config",
                "core.file_operations",
                "core.schemas",
                "core.error_handling",
                "core.time_utilities",
                "core.user_data_manager",
                "core.user_data_manager",
                "core.user_data_manager"
              ]
            },
            {
              "file": "core/response_tracking.py",
              "local_imports": 5,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.user_data_handlers",
                "core.file_operations",
                "core.error_handling",
                "core.time_utilities"
              ]
            },
            {
              "file": "core/scheduler.py",
              "local_imports": 29,
              "third_party_imports": 3,
              "modules": [
                "core.user_data_handlers",
                "core.schedule_management",
                "core.service_utilities",
                "core.time_utilities",
                "core.logger",
                "user.user_context",
                "core.error_handling",
                "core.user_data_handlers",
                "core.backup_manager",
                "core.logger",
                "communication.core.channel_orchestrator",
                "core.scheduler",
                "communication.core.channel_orchestrator",
                "core.scheduler",
                "communication.core.channel_orchestrator",
                "core.scheduler",
                "tasks.task_management",
                "communication.core.channel_orchestrator",
                "communication.core.channel_orchestrator",
                "core.auto_cleanup",
                "core.config",
                "tasks.task_management",
                "core.schedule_management",
                "tasks.task_management",
                "tasks.task_management",
                "core.user_data_handlers",
                "tasks.task_management",
                "core.logger",
                "tasks.task_management"
              ]
            },
            {
              "file": "core/schedule_management.py",
              "local_imports": 9,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.user_data_handlers",
                "core.service_utilities",
                "core.time_utilities",
                "user.user_context",
                "core.error_handling",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "core/schedule_utilities.py",
              "local_imports": 3,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.time_utilities"
              ]
            },
            {
              "file": "core/schemas.py",
              "local_imports": 4,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.message_management",
                "core.user_data_validation"
              ]
            },
            {
              "file": "core/service.py",
              "local_imports": 25,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.config",
                "communication.core.channel_orchestrator",
                "core.config",
                "core.scheduler",
                "core.service_utilities",
                "core.time_utilities",
                "core.file_operations",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.error_handling",
                "core.file_auditor",
                "core.config",
                "core.config",
                "core.logger",
                "core.schedule_management",
                "core.message_management",
                "core.config",
                "core.file_operations",
                "communication.message_processing.conversation_flow_manager",
                "core.user_data_handlers",
                "core.file_auditor",
                "core.auto_cleanup",
                "core.auto_cleanup",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "core/service_utilities.py",
              "local_imports": 5,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.time_utilities",
                "core.config",
                "core.error_handling",
                "core.file_auditor"
              ]
            },
            {
              "file": "core/tags.py",
              "local_imports": 4,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config",
                "core.file_operations"
              ]
            },
            {
              "file": "core/time_utilities.py",
              "local_imports": 0,
              "third_party_imports": 0,
              "modules": []
            },
            {
              "file": "core/ui_management.py",
              "local_imports": 5,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "ui.widgets.period_row_widget",
                "core.schedule_management",
                "ui.widgets.period_row_widget"
              ]
            },
            {
              "file": "core/user_data_handlers.py",
              "local_imports": 45,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config",
                "core.file_operations",
                "core.time_utilities",
                "core.user_data_validation",
                "core.schemas",
                "core.config",
                "core.config",
                "core.user_data_manager",
                "core.user_data_manager",
                "core.user_data_manager",
                "core.tags",
                "core.tags",
                "core.file_operations",
                "core.config",
                "core.user_data_manager",
                "core.user_data_manager",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.user_data_manager",
                "core.config",
                "core.user_data_manager",
                "core.file_operations",
                "core.config",
                "core.user_data_manager",
                "core.config",
                "core.message_management",
                "core.config",
                "core.config",
                "core.config",
                "core.schemas",
                "core.schemas",
                "core.schemas",
                "core.config",
                "core.file_operations"
              ]
            },
            {
              "file": "core/user_data_manager.py",
              "local_imports": 19,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.config",
                "core.file_operations",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.schemas",
                "core.error_handling",
                "core.time_utilities",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.config",
                "core.user_data_handlers",
                "core.message_management",
                "core.file_locking",
                "core.file_locking",
                "core.file_locking",
                "core.response_tracking",
                "core.response_tracking"
              ]
            },
            {
              "file": "core/user_data_validation.py",
              "local_imports": 11,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.time_utilities",
                "core.config",
                "core.schemas",
                "core.message_management",
                "core.schemas",
                "core.user_data_handlers",
                "core.schemas",
                "core.user_data_handlers",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "core/__init__.py",
              "local_imports": 0,
              "third_party_imports": 34,
              "modules": []
            }
          ],
          "communication_dependencies": [
            {
              "file": "ai/cache_manager.py",
              "local_imports": 3,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config"
              ]
            },
            {
              "file": "ai/chatbot.py",
              "local_imports": 21,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.config",
                "core.response_tracking",
                "core.user_data_handlers",
                "user.context_manager",
                "ai.prompt_manager",
                "ai.cache_manager",
                "core.error_handling",
                "core.message_management",
                "core.time_utilities",
                "core.response_tracking",
                "tasks.task_management",
                "core.response_tracking",
                "core.response_tracking",
                "core.response_tracking",
                "tasks.task_management",
                "ai.lm_studio_manager",
                "core.user_data_handlers",
                "core.response_tracking",
                "core.response_tracking",
                "core.time_utilities"
              ]
            },
            {
              "file": "ai/context_builder.py",
              "local_imports": 5,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.response_tracking",
                "core.user_data_handlers",
                "user.context_manager"
              ]
            },
            {
              "file": "ai/conversation_history.py",
              "local_imports": 3,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.time_utilities"
              ]
            },
            {
              "file": "ai/lm_studio_manager.py",
              "local_imports": 3,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config"
              ]
            },
            {
              "file": "ai/prompt_manager.py",
              "local_imports": 3,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config"
              ]
            },
            {
              "file": "ai/__init__.py",
              "local_imports": 0,
              "third_party_imports": 6,
              "modules": []
            },
            {
              "file": "communication/__init__.py",
              "local_imports": 5,
              "third_party_imports": 40,
              "modules": [
                "core.retry_manager",
                "core.channel_orchestrator",
                "core.channel_orchestrator",
                "core.factory",
                "core.channel_monitor"
              ]
            },
            {
              "file": "communication/command_handlers/account_handler.py",
              "local_imports": 11,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_manager",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.account_handler",
                "communication.core.channel_orchestrator",
                "communication.command_handlers.account_handler",
                "communication.command_handlers.account_handler"
              ]
            },
            {
              "file": "communication/command_handlers/analytics_handler.py",
              "local_imports": 33,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.user_data_handlers",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.response_tracking",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "tasks.task_management",
                "core.checkin_analytics",
                "tasks.task_management",
                "core.checkin_analytics",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.checkin_analytics",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/command_handlers/base_handler.py",
              "local_imports": 3,
              "third_party_imports": 0,
              "modules": [
                "communication.command_handlers.shared_types",
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/command_handlers/checkin_handler.py",
              "local_imports": 7,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.response_tracking",
                "core.time_utilities",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "communication.message_processing.conversation_flow_manager"
              ]
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "local_imports": 22,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.user_data_handlers",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.base_handler",
                "tasks.task_management",
                "core.response_tracking",
                "core.response_tracking",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.checkin_handler",
                "communication.command_handlers.profile_handler",
                "communication.command_handlers.schedule_handler",
                "communication.command_handlers.analytics_handler",
                "communication.command_handlers.notebook_handler",
                "communication.command_handlers.account_handler",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.checkin_handler",
                "communication.command_handlers.profile_handler",
                "communication.command_handlers.schedule_handler",
                "communication.command_handlers.analytics_handler",
                "communication.command_handlers.notebook_handler",
                "communication.command_handlers.account_handler"
              ]
            },
            {
              "file": "communication/command_handlers/notebook_handler.py",
              "local_imports": 9,
              "third_party_imports": 3,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.time_utilities",
                "core.tags",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "communication.message_processing.conversation_flow_manager",
                "communication.message_processing.conversation_flow_manager",
                "core.tags"
              ]
            },
            {
              "file": "communication/command_handlers/profile_handler.py",
              "local_imports": 7,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.user_data_handlers",
                "core.response_tracking",
                "tasks.task_management",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types"
              ]
            },
            {
              "file": "communication/command_handlers/schedule_handler.py",
              "local_imports": 11,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "core.schedule_management",
                "core.user_data_handlers",
                "core.schedule_management",
                "core.schedule_management",
                "core.user_data_handlers",
                "core.schedule_management",
                "core.schedule_management"
              ]
            },
            {
              "file": "communication/command_handlers/shared_types.py",
              "local_imports": 0,
              "third_party_imports": 0,
              "modules": []
            },
            {
              "file": "communication/command_handlers/task_handler.py",
              "local_imports": 8,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.time_utilities",
                "tasks.task_management",
                "communication.message_processing.conversation_flow_manager",
                "core.checkin_analytics",
                "core.user_data_handlers",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "communication/command_handlers/__init__.py",
              "local_imports": 0,
              "third_party_imports": 3,
              "modules": []
            },
            {
              "file": "communication/communication_channels/__init__.py",
              "local_imports": 0,
              "third_party_imports": 0,
              "modules": []
            },
            {
              "file": "communication/core/channel_monitor.py",
              "local_imports": 3,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.communication_channels.base.base_channel"
              ]
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "local_imports": 32,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.communication_channels.base.base_channel",
                "communication.core.factory",
                "communication.core.retry_manager",
                "communication.core.channel_monitor",
                "core.user_data_handlers",
                "core.message_management",
                "core.schedule_management",
                "core.file_operations",
                "core.config",
                "core.service_utilities",
                "core.config",
                "communication.core.factory",
                "tasks.task_management",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.conversation_flow_manager",
                "communication.message_processing.conversation_flow_manager",
                "ai.chatbot",
                "core.message_management",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.logger",
                "core.schemas",
                "communication.communication_channels.discord.task_reminder_view",
                "communication.communication_channels.discord.checkin_view",
                "core.message_management",
                "core.message_management"
              ]
            },
            {
              "file": "communication/core/factory.py",
              "local_imports": 5,
              "third_party_imports": 0,
              "modules": [
                "communication.communication_channels.base.base_channel",
                "core.logger",
                "core.error_handling",
                "core.config",
                "core.config"
              ]
            },
            {
              "file": "communication/core/retry_manager.py",
              "local_imports": 2,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/core/welcome_manager.py",
              "local_imports": 4,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config",
                "core.time_utilities"
              ]
            },
            {
              "file": "communication/core/__init__.py",
              "local_imports": 0,
              "third_party_imports": 3,
              "modules": []
            },
            {
              "file": "communication/message_processing/command_parser.py",
              "local_imports": 8,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config",
                "ai.chatbot",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.interaction_handlers",
                "core.logger",
                "core.tags"
              ]
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "local_imports": 37,
              "third_party_imports": 3,
              "modules": [
                "ai.chatbot",
                "core.logger",
                "core.user_data_handlers",
                "core.response_tracking",
                "core.error_handling",
                "core.time_utilities",
                "core.config",
                "core.time_utilities",
                "core.logger",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "core.checkin_dynamic_manager",
                "core.time_utilities",
                "core.checkin_dynamic_manager",
                "core.response_tracking",
                "core.logger",
                "tasks.task_management",
                "tasks.task_management",
                "core.time_utilities",
                "tasks.task_management",
                "tasks.task_management",
                "core.time_utilities",
                "core.tags",
                "communication.message_processing.interaction_manager",
                "tasks.task_management",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.profile_handler",
                "communication.message_processing.interaction_manager",
                "communication.command_handlers.analytics_handler",
                "communication.command_handlers.analytics_handler",
                "communication.command_handlers.schedule_handler",
                "communication.command_handlers.interaction_handlers"
              ]
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "local_imports": 22,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config",
                "communication.message_processing.command_parser",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.interaction_handlers",
                "ai.chatbot",
                "communication.message_processing.conversation_flow_manager",
                "core.time_utilities",
                "tasks.task_management",
                "core.response_tracking",
                "core.time_utilities",
                "core.user_data_handlers",
                "core.response_tracking",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.shared_types",
                "communication.message_processing.command_parser",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.shared_types"
              ]
            },
            {
              "file": "communication/message_processing/message_router.py",
              "local_imports": 2,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/message_processing/__init__.py",
              "local_imports": 0,
              "third_party_imports": 1,
              "modules": []
            },
            {
              "file": "communication/communication_channels/base/base_channel.py",
              "local_imports": 2,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/communication_channels/base/command_registry.py",
              "local_imports": 2,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/communication_channels/base/message_formatter.py",
              "local_imports": 2,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/communication_channels/base/rich_formatter.py",
              "local_imports": 2,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "local_imports": 5,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.account_handler",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "communication/communication_channels/discord/api_client.py",
              "local_imports": 2,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/communication_channels/discord/bot.py",
              "local_imports": 21,
              "third_party_imports": 6,
              "modules": [
                "core.config",
                "core.logger",
                "communication.communication_channels.base.base_channel",
                "core.user_data_handlers",
                "core.error_handling",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "communication.communication_channels.discord.welcome_handler",
                "communication.communication_channels.discord.webhook_server",
                "core.config",
                "communication.communication_channels.discord.welcome_handler",
                "core.user_data_handlers",
                "communication.communication_channels.discord.welcome_handler",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "communication.communication_channels.discord.account_flow_handler",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager"
              ]
            },
            {
              "file": "communication/communication_channels/discord/checkin_view.py",
              "local_imports": 6,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.message_processing.interaction_manager",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "communication/communication_channels/discord/event_handler.py",
              "local_imports": 6,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager",
                "communication.communication_channels.base.rich_formatter",
                "communication.communication_channels.base.rich_formatter"
              ]
            },
            {
              "file": "communication/communication_channels/discord/task_reminder_view.py",
              "local_imports": 4,
              "third_party_imports": 1,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.message_processing.interaction_manager",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "communication/communication_channels/discord/webhook_handler.py",
              "local_imports": 8,
              "third_party_imports": 0,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.core.welcome_manager",
                "communication.communication_channels.discord.welcome_handler",
                "core.user_data_handlers",
                "communication.core.welcome_manager",
                "core.user_data_handlers",
                "communication.communication_channels.discord.welcome_handler"
              ]
            },
            {
              "file": "communication/communication_channels/discord/webhook_server.py",
              "local_imports": 4,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.communication_channels.discord.webhook_handler",
                "core.config"
              ]
            },
            {
              "file": "communication/communication_channels/discord/welcome_handler.py",
              "local_imports": 5,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.core.welcome_manager",
                "communication.communication_channels.discord.account_flow_handler",
                "communication.communication_channels.discord.account_flow_handler"
              ]
            },
            {
              "file": "communication/communication_channels/email/bot.py",
              "local_imports": 4,
              "third_party_imports": 0,
              "modules": [
                "core.config",
                "core.logger",
                "communication.communication_channels.base.base_channel",
                "core.error_handling"
              ]
            }
          ],
          "ui_dependencies": [
            {
              "file": "ui/generate_ui_files.py",
              "local_imports": 2,
              "third_party_imports": 0,
              "modules": [
                "core.error_handling",
                "core.time_utilities"
              ]
            },
            {
              "file": "ui/ui_app_qt.py",
              "local_imports": 44,
              "third_party_imports": 6,
              "modules": [
                "core.time_utilities",
                "core.logger",
                "core.config",
                "core.error_handling",
                "core.service_utilities",
                "user.user_context",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_validation",
                "core.config",
                "ui.generated.admin_panel_pyqt",
                "core.user_data_manager",
                "core.scheduler",
                "core.scheduler",
                "core.scheduler",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.logger",
                "core.config",
                "core.auto_cleanup",
                "core.auto_cleanup",
                "core.config",
                "core.config",
                "core.config",
                "core.config",
                "ui.dialogs.account_creator_dialog",
                "communication.core.channel_orchestrator",
                "ui.dialogs.channel_management_dialog",
                "ui.dialogs.category_management_dialog",
                "ui.dialogs.checkin_management_dialog",
                "ui.dialogs.task_management_dialog",
                "ui.dialogs.task_crud_dialog",
                "ui.dialogs.user_profile_dialog",
                "core.user_data_handlers",
                "ui.dialogs.user_analytics_dialog",
                "ui.dialogs.message_editor_dialog",
                "ui.dialogs.schedule_editor_dialog",
                "core.user_data_handlers",
                "tasks.task_management",
                "core.scheduler",
                "communication.core.channel_orchestrator",
                "ui.dialogs.process_watcher_dialog",
                "core.user_data_handlers",
                "communication.core.channel_orchestrator"
              ]
            },
            {
              "file": "ui/__init__.py",
              "local_imports": 0,
              "third_party_imports": 28,
              "modules": []
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "local_imports": 22,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.user_data_validation",
                "core.user_data_handlers",
                "core.error_handling",
                "ui.widgets.category_selection_widget",
                "ui.widgets.channel_selection_widget",
                "ui.widgets.task_settings_widget",
                "ui.widgets.checkin_settings_widget",
                "ui.generated.account_creator_dialog_pyqt",
                "ui.dialogs.user_profile_dialog",
                "core.file_operations",
                "core.user_data_handlers",
                "core.config",
                "tasks.task_management",
                "tasks.task_management",
                "core.user_data_manager",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.service",
                "core.user_data_validation",
                "core.user_data_validation",
                "core.user_data_validation"
              ]
            },
            {
              "file": "ui/dialogs/admin_panel.py",
              "local_imports": 2,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "local_imports": 7,
              "third_party_imports": 2,
              "modules": [
                "ui.generated.category_management_dialog_pyqt",
                "ui.widgets.category_selection_widget",
                "core.logger",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.error_handling",
                "core.schedule_management"
              ]
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "local_imports": 6,
              "third_party_imports": 2,
              "modules": [
                "ui.generated.channel_management_dialog_pyqt",
                "ui.widgets.channel_selection_widget",
                "core.logger",
                "core.user_data_validation",
                "core.user_data_handlers",
                "core.error_handling"
              ]
            },
            {
              "file": "ui/dialogs/checkin_management_dialog.py",
              "local_imports": 8,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.schedule_management",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.error_handling",
                "core.user_data_validation",
                "ui.widgets.checkin_settings_widget",
                "ui.generated.checkin_management_dialog_pyqt"
              ]
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "local_imports": 4,
              "third_party_imports": 3,
              "modules": [
                "ui.generated.message_editor_dialog_pyqt",
                "core.message_management",
                "core.error_handling",
                "core.logger"
              ]
            },
            {
              "file": "ui/dialogs/process_watcher_dialog.py",
              "local_imports": 4,
              "third_party_imports": 4,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.time_utilities",
                "core.service_utilities"
              ]
            },
            {
              "file": "ui/dialogs/schedule_editor_dialog.py",
              "local_imports": 9,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.schedule_management",
                "core.ui_management",
                "core.error_handling",
                "core.user_data_validation",
                "core.time_utilities",
                "ui.widgets.period_row_widget",
                "ui.generated.schedule_editor_dialog_pyqt",
                "core.config"
              ]
            },
            {
              "file": "ui/dialogs/task_completion_dialog.py",
              "local_imports": 3,
              "third_party_imports": 2,
              "modules": [
                "ui.generated.task_completion_dialog_pyqt",
                "core.error_handling",
                "core.logger"
              ]
            },
            {
              "file": "ui/dialogs/task_crud_dialog.py",
              "local_imports": 13,
              "third_party_imports": 2,
              "modules": [
                "ui.generated.task_crud_dialog_pyqt",
                "tasks.task_management",
                "core.error_handling",
                "core.logger",
                "ui.dialogs.task_edit_dialog",
                "tasks.task_management",
                "ui.dialogs.task_edit_dialog",
                "tasks.task_management",
                "ui.dialogs.task_completion_dialog",
                "tasks.task_management",
                "tasks.task_management",
                "tasks.task_management",
                "tasks.task_management"
              ]
            },
            {
              "file": "ui/dialogs/task_edit_dialog.py",
              "local_imports": 5,
              "third_party_imports": 2,
              "modules": [
                "ui.generated.task_edit_dialog_pyqt",
                "tasks.task_management",
                "core.error_handling",
                "core.logger",
                "ui.widgets.tag_widget"
              ]
            },
            {
              "file": "ui/dialogs/task_management_dialog.py",
              "local_imports": 9,
              "third_party_imports": 2,
              "modules": [
                "ui.generated.task_management_dialog_pyqt",
                "ui.widgets.task_settings_widget",
                "core.schedule_management",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "core.user_data_validation",
                "tasks.task_management"
              ]
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "local_imports": 6,
              "third_party_imports": 3,
              "modules": [
                "ui.generated.user_analytics_dialog_pyqt",
                "core.checkin_analytics",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "core.response_tracking"
              ]
            },
            {
              "file": "ui/dialogs/user_profile_dialog.py",
              "local_imports": 8,
              "third_party_imports": 2,
              "modules": [
                "ui.generated.user_profile_management_dialog_pyqt",
                "core.logger",
                "core.user_data_handlers",
                "core.user_data_validation",
                "core.error_handling",
                "ui.widgets.user_profile_settings_widget",
                "ui.widgets.dynamic_list_container",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "ui/dialogs/__init__.py",
              "local_imports": 0,
              "third_party_imports": 7,
              "modules": []
            },
            {
              "file": "ui/widgets/category_selection_widget.py",
              "local_imports": 4,
              "third_party_imports": 1,
              "modules": [
                "ui.generated.category_selection_widget_pyqt",
                "core.user_data_validation",
                "core.error_handling",
                "core.logger"
              ]
            },
            {
              "file": "ui/widgets/channel_selection_widget.py",
              "local_imports": 4,
              "third_party_imports": 1,
              "modules": [
                "ui.generated.channel_selection_widget_pyqt",
                "core.user_data_handlers",
                "core.logger",
                "core.error_handling"
              ]
            },
            {
              "file": "ui/widgets/checkin_settings_widget.py",
              "local_imports": 14,
              "third_party_imports": 5,
              "modules": [
                "ui.generated.checkin_settings_widget_pyqt",
                "core.ui_management",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "ui.widgets.period_row_widget",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager"
              ]
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "local_imports": 10,
              "third_party_imports": 3,
              "modules": [
                "ui.widgets.dynamic_list_field",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger"
              ]
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "local_imports": 6,
              "third_party_imports": 3,
              "modules": [
                "ui.generated.dynamic_list_field_template_pyqt",
                "core.logger",
                "core.error_handling",
                "ui.widgets.dynamic_list_container",
                "ui.widgets.dynamic_list_container",
                "ui.widgets.dynamic_list_container"
              ]
            },
            {
              "file": "ui/widgets/period_row_widget.py",
              "local_imports": 4,
              "third_party_imports": 2,
              "modules": [
                "core.logger",
                "core.schedule_management",
                "core.error_handling",
                "ui.generated.period_row_template_pyqt"
              ]
            },
            {
              "file": "ui/widgets/tag_widget.py",
              "local_imports": 5,
              "third_party_imports": 2,
              "modules": [
                "ui.generated.tag_widget_pyqt",
                "tasks.task_management",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger"
              ]
            },
            {
              "file": "ui/widgets/task_settings_widget.py",
              "local_imports": 8,
              "third_party_imports": 1,
              "modules": [
                "ui.generated.task_settings_widget_pyqt",
                "tasks.task_management",
                "core.ui_management",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "ui.widgets.period_row_widget",
                "ui.widgets.tag_widget"
              ]
            },
            {
              "file": "ui/widgets/user_profile_settings_widget.py",
              "local_imports": 4,
              "third_party_imports": 3,
              "modules": [
                "ui.generated.user_profile_settings_widget_pyqt",
                "core.logger",
                "core.error_handling",
                "ui.widgets.dynamic_list_container"
              ]
            },
            {
              "file": "ui/widgets/__init__.py",
              "local_imports": 0,
              "third_party_imports": 7,
              "modules": []
            }
          ],
          "third_party_dependencies": [
            {
              "file": "ai/chatbot.py",
              "dependencies": [
                "requests",
                "psutil"
              ]
            },
            {
              "file": "ai/lm_studio_manager.py",
              "dependencies": [
                "requests"
              ]
            },
            {
              "file": "ai/__init__.py",
              "dependencies": [
                "chatbot",
                "cache_manager",
                "context_builder",
                "prompt_manager",
                "conversation_history",
                "lm_studio_manager"
              ]
            },
            {
              "file": "communication/__init__.py",
              "dependencies": [
                "communication_channels.base.base_channel",
                "message_processing.command_parser",
                "command_handlers.shared_types",
                "message_processing.interaction_manager",
                "message_processing.conversation_flow_manager",
                "command_handlers.base_handler",
                "command_handlers.analytics_handler",
                "command_handlers.task_handler",
                "command_handlers.checkin_handler",
                "command_handlers.profile_handler",
                "command_handlers.schedule_handler",
                "command_handlers.interaction_handlers",
                "communication_channels.discord.bot",
                "communication_channels.email.bot",
                "communication_channels.discord.bot",
                "communication_channels.discord.event_handler",
                "communication_channels.discord.api_client",
                "communication_channels.email.bot",
                "message_processing.message_router",
                "message_processing.message_router",
                "message_processing.message_router",
                "message_processing.message_router",
                "communication_channels.discord.event_handler",
                "communication_channels.base.rich_formatter",
                "communication_channels.discord.api_client",
                "communication_channels.base.command_registry",
                "communication_channels.base.message_formatter",
                "communication_channels.base.rich_formatter",
                "communication_channels.base.rich_formatter",
                "communication_channels.base.rich_formatter",
                "communication_channels.base.message_formatter",
                "communication_channels.base.message_formatter",
                "communication_channels.base.message_formatter",
                "communication_channels.base.command_registry",
                "communication_channels.base.command_registry",
                "communication_channels.base.command_registry",
                "communication_channels.discord.event_handler",
                "communication_channels.discord.event_handler",
                "communication_channels.discord.api_client",
                "communication_channels.discord.api_client"
              ]
            },
            {
              "file": "communication/command_handlers/notebook_handler.py",
              "dependencies": [
                "notebook.notebook_data_manager",
                "notebook.schemas",
                "notebook.notebook_validation"
              ]
            },
            {
              "file": "communication/command_handlers/task_handler.py",
              "dependencies": [
                "base_handler"
              ]
            },
            {
              "file": "communication/command_handlers/__init__.py",
              "dependencies": [
                "shared_types",
                "base_handler",
                "interaction_handlers"
              ]
            },
            {
              "file": "communication/core/__init__.py",
              "dependencies": [
                "retry_manager",
                "factory",
                "channel_monitor"
              ]
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "dependencies": [
                "notebook.notebook_data_manager",
                "notebook.notebook_data_manager",
                "notebook.notebook_data_manager"
              ]
            },
            {
              "file": "communication/message_processing/__init__.py",
              "dependencies": [
                "interaction_manager"
              ]
            },
            {
              "file": "communication/communication_channels/base/command_registry.py",
              "dependencies": [
                "discord"
              ]
            },
            {
              "file": "communication/communication_channels/base/rich_formatter.py",
              "dependencies": [
                "discord"
              ]
            },
            {
              "file": "communication/communication_channels/discord/account_flow_handler.py",
              "dependencies": [
                "discord"
              ]
            },
            {
              "file": "communication/communication_channels/discord/api_client.py",
              "dependencies": [
                "discord"
              ]
            },
            {
              "file": "communication/communication_channels/discord/bot.py",
              "dependencies": [
                "discord",
                "discord",
                "discord.ext",
                "psutil",
                "aiohttp",
                "dns.resolver"
              ]
            },
            {
              "file": "communication/communication_channels/discord/checkin_view.py",
              "dependencies": [
                "discord"
              ]
            },
            {
              "file": "communication/communication_channels/discord/event_handler.py",
              "dependencies": [
                "discord"
              ]
            },
            {
              "file": "communication/communication_channels/discord/task_reminder_view.py",
              "dependencies": [
                "discord"
              ]
            },
            {
              "file": "communication/communication_channels/discord/webhook_server.py",
              "dependencies": [
                "nacl.signing",
                "nacl.exceptions"
              ]
            },
            {
              "file": "communication/communication_channels/discord/welcome_handler.py",
              "dependencies": [
                "discord",
                "discord"
              ]
            },
            {
              "file": "core/config.py",
              "dependencies": [
                "dotenv"
              ]
            },
            {
              "file": "core/headless_service.py",
              "dependencies": [
                "psutil"
              ]
            },
            {
              "file": "core/scheduler.py",
              "dependencies": [
                "schedule",
                "pytz",
                "pytz"
              ]
            },
            {
              "file": "core/schemas.py",
              "dependencies": [
                "pydantic",
                "pytz"
              ]
            },
            {
              "file": "core/service.py",
              "dependencies": [
                "psutil"
              ]
            },
            {
              "file": "core/service_utilities.py",
              "dependencies": [
                "psutil",
                "pytz"
              ]
            },
            {
              "file": "core/user_data_handlers.py",
              "dependencies": [
                "pytz"
              ]
            },
            {
              "file": "core/__init__.py",
              "dependencies": [
                "logger",
                "error_handling",
                "user_data_handlers",
                "file_operations",
                "message_management",
                "response_tracking",
                "user_data_validation",
                "error_handling",
                "config",
                "ui_management",
                "user_data_handlers",
                "schemas",
                "schemas",
                "config",
                "config",
                "error_handling",
                "config",
                "service",
                "service_utilities",
                "headless_service",
                "file_operations",
                "file_auditor",
                "schedule_utilities",
                "auto_cleanup",
                "backup_manager",
                "checkin_dynamic_manager",
                "checkin_analytics",
                "error_handling",
                "config",
                "checkin_dynamic_manager",
                "user_data_manager",
                "user_data_handlers",
                "scheduler",
                "schedule_management"
              ]
            },
            {
              "file": "tasks/__init__.py",
              "dependencies": [
                "task_management"
              ]
            },
            {
              "file": "ui/ui_app_qt.py",
              "dependencies": [
                "psutil",
                "run_mhm",
                "PySide6.QtWidgets",
                "PySide6.QtCore",
                "PySide6.QtGui",
                "PySide6.QtWidgets"
              ]
            },
            {
              "file": "ui/__init__.py",
              "dependencies": [
                "ui_app_qt",
                "dialogs.account_creator_dialog",
                "dialogs.user_profile_dialog",
                "dialogs.task_management_dialog",
                "dialogs.checkin_management_dialog",
                "dialogs.category_management_dialog",
                "dialogs.channel_management_dialog",
                "dialogs.process_watcher_dialog",
                "dialogs.task_completion_dialog",
                "dialogs.task_crud_dialog",
                "dialogs.admin_panel",
                "dialogs.message_editor_dialog",
                "dialogs.schedule_editor_dialog",
                "dialogs.user_analytics_dialog",
                "dialogs.user_profile_dialog",
                "dialogs.account_creator_dialog",
                "widgets.dynamic_list_container",
                "widgets.period_row_widget",
                "widgets.category_selection_widget",
                "widgets.channel_selection_widget",
                "widgets.task_settings_widget",
                "widgets.checkin_settings_widget",
                "widgets.tag_widget",
                "widgets.dynamic_list_field",
                "widgets.user_profile_settings_widget",
                "dialogs.task_edit_dialog",
                "generate_ui_files",
                "ui_app_qt"
              ]
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/admin_panel.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/checkin_management_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/message_editor_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore",
                "PySide6.QtGui"
              ]
            },
            {
              "file": "ui/dialogs/process_watcher_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore",
                "PySide6.QtGui",
                "psutil"
              ]
            },
            {
              "file": "ui/dialogs/schedule_editor_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtWidgets"
              ]
            },
            {
              "file": "ui/dialogs/task_completion_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/task_crud_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/task_edit_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/task_management_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore",
                "PySide6.QtGui"
              ]
            },
            {
              "file": "ui/dialogs/user_profile_dialog.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/dialogs/__init__.py",
              "dependencies": [
                "account_creator_dialog",
                "user_profile_dialog",
                "task_management_dialog",
                "checkin_management_dialog",
                "schedule_editor_dialog",
                "message_editor_dialog",
                "user_analytics_dialog"
              ]
            },
            {
              "file": "ui/widgets/category_selection_widget.py",
              "dependencies": [
                "PySide6.QtWidgets"
              ]
            },
            {
              "file": "ui/widgets/channel_selection_widget.py",
              "dependencies": [
                "PySide6.QtWidgets"
              ]
            },
            {
              "file": "ui/widgets/checkin_settings_widget.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore",
                "PySide6.QtCore",
                "PySide6.QtWidgets",
                "PySide6.QtWidgets"
              ]
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore",
                "PySide6.QtWidgets"
              ]
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore",
                "PySide6.QtWidgets"
              ]
            },
            {
              "file": "ui/widgets/period_row_widget.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/widgets/tag_widget.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore"
              ]
            },
            {
              "file": "ui/widgets/task_settings_widget.py",
              "dependencies": [
                "PySide6.QtWidgets"
              ]
            },
            {
              "file": "ui/widgets/user_profile_settings_widget.py",
              "dependencies": [
                "PySide6.QtWidgets",
                "PySide6.QtCore",
                "PySide6.QtWidgets"
              ]
            },
            {
              "file": "ui/widgets/__init__.py",
              "dependencies": [
                "task_settings_widget",
                "checkin_settings_widget",
                "user_profile_settings_widget",
                "channel_selection_widget",
                "category_selection_widget",
                "tag_widget",
                "period_row_widget"
              ]
            },
            {
              "file": "user/__init__.py",
              "dependencies": [
                "context_manager",
                "user_context",
                "user_preferences"
              ]
            }
          ],
          "circular_dependencies": [
            [
              "communication/command_handlers/account_handler.py",
              "communication/command_handlers/account_handler.py"
            ],
            [
              "communication/command_handlers/task_handler.py",
              "communication/message_processing/conversation_flow_manager.py"
            ],
            [
              "communication/message_processing/conversation_flow_manager.py",
              "communication/message_processing/interaction_manager.py"
            ],
            [
              "core/config.py",
              "core/logger.py"
            ],
            [
              "core/error_handling.py",
              "core/service_utilities.py"
            ],
            [
              "core/error_handling.py",
              "core/logger.py"
            ],
            [
              "core/file_operations.py",
              "core/message_management.py"
            ],
            [
              "core/file_operations.py",
              "core/user_data_manager.py"
            ],
            [
              "core/message_management.py",
              "core/schemas.py"
            ],
            [
              "core/message_management.py",
              "core/user_data_manager.py"
            ],
            [
              "core/scheduler.py",
              "core/scheduler.py"
            ],
            [
              "core/schemas.py",
              "core/user_data_validation.py"
            ],
            [
              "core/user_data_handlers.py",
              "core/user_data_validation.py"
            ],
            [
              "core/user_data_handlers.py",
              "core/user_data_manager.py"
            ],
            [
              "ui/widgets/dynamic_list_container.py",
              "ui/widgets/dynamic_list_field.py"
            ]
          ],
          "high_coupling": [
            {
              "file": "ai/chatbot.py",
              "import_count": 21,
              "modules": [
                "core.logger",
                "core.config",
                "core.response_tracking",
                "core.user_data_handlers",
                "user.context_manager",
                "ai.prompt_manager",
                "ai.cache_manager",
                "core.error_handling",
                "core.message_management",
                "core.time_utilities",
                "core.response_tracking",
                "tasks.task_management",
                "core.response_tracking",
                "core.response_tracking",
                "core.response_tracking",
                "tasks.task_management",
                "ai.lm_studio_manager",
                "core.user_data_handlers",
                "core.response_tracking",
                "core.response_tracking",
                "core.time_utilities"
              ]
            },
            {
              "file": "communication/command_handlers/account_handler.py",
              "import_count": 11,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_manager",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.account_handler",
                "communication.core.channel_orchestrator",
                "communication.command_handlers.account_handler",
                "communication.command_handlers.account_handler"
              ]
            },
            {
              "file": "communication/command_handlers/analytics_handler.py",
              "import_count": 33,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.user_data_handlers",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "core.response_tracking",
                "core.checkin_analytics",
                "core.checkin_analytics",
                "tasks.task_management",
                "core.checkin_analytics",
                "tasks.task_management",
                "core.checkin_analytics",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.checkin_analytics",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling"
              ]
            },
            {
              "file": "communication/command_handlers/checkin_handler.py",
              "import_count": 7,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.response_tracking",
                "core.time_utilities",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "communication.message_processing.conversation_flow_manager"
              ]
            },
            {
              "file": "communication/command_handlers/interaction_handlers.py",
              "import_count": 22,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.user_data_handlers",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.base_handler",
                "tasks.task_management",
                "core.response_tracking",
                "core.response_tracking",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.checkin_handler",
                "communication.command_handlers.profile_handler",
                "communication.command_handlers.schedule_handler",
                "communication.command_handlers.analytics_handler",
                "communication.command_handlers.notebook_handler",
                "communication.command_handlers.account_handler",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.checkin_handler",
                "communication.command_handlers.profile_handler",
                "communication.command_handlers.schedule_handler",
                "communication.command_handlers.analytics_handler",
                "communication.command_handlers.notebook_handler",
                "communication.command_handlers.account_handler"
              ]
            },
            {
              "file": "communication/command_handlers/notebook_handler.py",
              "import_count": 9,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.time_utilities",
                "core.tags",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "communication.message_processing.conversation_flow_manager",
                "communication.message_processing.conversation_flow_manager",
                "core.tags"
              ]
            },
            {
              "file": "communication/command_handlers/profile_handler.py",
              "import_count": 7,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.user_data_handlers",
                "core.response_tracking",
                "tasks.task_management",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types"
              ]
            },
            {
              "file": "communication/command_handlers/schedule_handler.py",
              "import_count": 11,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.command_handlers.base_handler",
                "communication.command_handlers.shared_types",
                "core.schedule_management",
                "core.user_data_handlers",
                "core.schedule_management",
                "core.schedule_management",
                "core.user_data_handlers",
                "core.schedule_management",
                "core.schedule_management"
              ]
            },
            {
              "file": "communication/command_handlers/task_handler.py",
              "import_count": 8,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.time_utilities",
                "tasks.task_management",
                "communication.message_processing.conversation_flow_manager",
                "core.checkin_analytics",
                "core.user_data_handlers",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "communication/core/channel_orchestrator.py",
              "import_count": 32,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.communication_channels.base.base_channel",
                "communication.core.factory",
                "communication.core.retry_manager",
                "communication.core.channel_monitor",
                "core.user_data_handlers",
                "core.message_management",
                "core.schedule_management",
                "core.file_operations",
                "core.config",
                "core.service_utilities",
                "core.config",
                "communication.core.factory",
                "tasks.task_management",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.conversation_flow_manager",
                "communication.message_processing.conversation_flow_manager",
                "ai.chatbot",
                "core.message_management",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.error_handling",
                "core.logger",
                "core.schemas",
                "communication.communication_channels.discord.task_reminder_view",
                "communication.communication_channels.discord.checkin_view",
                "core.message_management",
                "core.message_management"
              ]
            },
            {
              "file": "communication/message_processing/command_parser.py",
              "import_count": 8,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config",
                "ai.chatbot",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.interaction_handlers",
                "core.logger",
                "core.tags"
              ]
            },
            {
              "file": "communication/message_processing/conversation_flow_manager.py",
              "import_count": 37,
              "modules": [
                "ai.chatbot",
                "core.logger",
                "core.user_data_handlers",
                "core.response_tracking",
                "core.error_handling",
                "core.time_utilities",
                "core.config",
                "core.time_utilities",
                "core.logger",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "core.checkin_dynamic_manager",
                "core.time_utilities",
                "core.checkin_dynamic_manager",
                "core.response_tracking",
                "core.logger",
                "tasks.task_management",
                "tasks.task_management",
                "core.time_utilities",
                "tasks.task_management",
                "tasks.task_management",
                "core.time_utilities",
                "core.tags",
                "communication.message_processing.interaction_manager",
                "tasks.task_management",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.profile_handler",
                "communication.message_processing.interaction_manager",
                "communication.command_handlers.analytics_handler",
                "communication.command_handlers.analytics_handler",
                "communication.command_handlers.schedule_handler",
                "communication.command_handlers.interaction_handlers"
              ]
            },
            {
              "file": "communication/message_processing/interaction_manager.py",
              "import_count": 22,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config",
                "communication.message_processing.command_parser",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.interaction_handlers",
                "ai.chatbot",
                "communication.message_processing.conversation_flow_manager",
                "core.time_utilities",
                "tasks.task_management",
                "core.response_tracking",
                "core.time_utilities",
                "core.user_data_handlers",
                "core.response_tracking",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.task_handler",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.shared_types",
                "communication.message_processing.command_parser",
                "communication.command_handlers.shared_types",
                "communication.command_handlers.shared_types"
              ]
            },
            {
              "file": "communication/communication_channels/discord/bot.py",
              "import_count": 21,
              "modules": [
                "core.config",
                "core.logger",
                "communication.communication_channels.base.base_channel",
                "core.user_data_handlers",
                "core.error_handling",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "communication.communication_channels.discord.welcome_handler",
                "communication.communication_channels.discord.webhook_server",
                "core.config",
                "communication.communication_channels.discord.welcome_handler",
                "core.user_data_handlers",
                "communication.communication_channels.discord.welcome_handler",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager",
                "communication.message_processing.interaction_manager",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "communication.communication_channels.discord.account_flow_handler",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager"
              ]
            },
            {
              "file": "communication/communication_channels/discord/checkin_view.py",
              "import_count": 6,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.message_processing.interaction_manager",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "communication/communication_channels/discord/event_handler.py",
              "import_count": 6,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.user_data_handlers",
                "communication.message_processing.interaction_manager",
                "communication.communication_channels.base.rich_formatter",
                "communication.communication_channels.base.rich_formatter"
              ]
            },
            {
              "file": "communication/communication_channels/discord/webhook_handler.py",
              "import_count": 8,
              "modules": [
                "core.logger",
                "core.error_handling",
                "communication.core.welcome_manager",
                "communication.communication_channels.discord.welcome_handler",
                "core.user_data_handlers",
                "communication.core.welcome_manager",
                "core.user_data_handlers",
                "communication.communication_channels.discord.welcome_handler"
              ]
            },
            {
              "file": "core/auto_cleanup.py",
              "import_count": 12,
              "modules": [
                "core.error_handling",
                "core.logger",
                "core.time_utilities",
                "core.config",
                "core.logger",
                "core.config",
                "core.config",
                "core.config",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.message_management",
                "core.config"
              ]
            },
            {
              "file": "core/backup_manager.py",
              "import_count": 7,
              "modules": [
                "core.logger",
                "core.config",
                "core.error_handling",
                "core.user_data_handlers",
                "core.time_utilities",
                "core.config",
                "core.config"
              ]
            },
            {
              "file": "core/checkin_dynamic_manager.py",
              "import_count": 6,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.file_operations",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "core/error_handling.py",
              "import_count": 21,
              "modules": [
                "core.time_utilities",
                "core.service_utilities",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger"
              ]
            },
            {
              "file": "core/file_operations.py",
              "import_count": 7,
              "modules": [
                "core.logger",
                "core.config",
                "core.error_handling",
                "core.time_utilities",
                "core.file_auditor",
                "core.message_management",
                "core.user_data_manager"
              ]
            },
            {
              "file": "core/logger.py",
              "import_count": 7,
              "modules": [
                "core.error_handling",
                "core.config",
                "core.config",
                "core.config",
                "core.config",
                "core.config",
                "core.config"
              ]
            },
            {
              "file": "core/message_management.py",
              "import_count": 9,
              "modules": [
                "core.logger",
                "core.config",
                "core.file_operations",
                "core.schemas",
                "core.error_handling",
                "core.time_utilities",
                "core.user_data_manager",
                "core.user_data_manager",
                "core.user_data_manager"
              ]
            },
            {
              "file": "core/scheduler.py",
              "import_count": 29,
              "modules": [
                "core.user_data_handlers",
                "core.schedule_management",
                "core.service_utilities",
                "core.time_utilities",
                "core.logger",
                "user.user_context",
                "core.error_handling",
                "core.user_data_handlers",
                "core.backup_manager",
                "core.logger",
                "communication.core.channel_orchestrator",
                "core.scheduler",
                "communication.core.channel_orchestrator",
                "core.scheduler",
                "communication.core.channel_orchestrator",
                "core.scheduler",
                "tasks.task_management",
                "communication.core.channel_orchestrator",
                "communication.core.channel_orchestrator",
                "core.auto_cleanup",
                "core.config",
                "tasks.task_management",
                "core.schedule_management",
                "tasks.task_management",
                "tasks.task_management",
                "core.user_data_handlers",
                "tasks.task_management",
                "core.logger",
                "tasks.task_management"
              ]
            },
            {
              "file": "core/schedule_management.py",
              "import_count": 9,
              "modules": [
                "core.logger",
                "core.user_data_handlers",
                "core.service_utilities",
                "core.time_utilities",
                "user.user_context",
                "core.error_handling",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "core/service.py",
              "import_count": 25,
              "modules": [
                "core.logger",
                "core.config",
                "communication.core.channel_orchestrator",
                "core.config",
                "core.scheduler",
                "core.service_utilities",
                "core.time_utilities",
                "core.file_operations",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.error_handling",
                "core.file_auditor",
                "core.config",
                "core.config",
                "core.logger",
                "core.schedule_management",
                "core.message_management",
                "core.config",
                "core.file_operations",
                "communication.message_processing.conversation_flow_manager",
                "core.user_data_handlers",
                "core.file_auditor",
                "core.auto_cleanup",
                "core.auto_cleanup",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "core/user_data_handlers.py",
              "import_count": 45,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.config",
                "core.file_operations",
                "core.time_utilities",
                "core.user_data_validation",
                "core.schemas",
                "core.config",
                "core.config",
                "core.user_data_manager",
                "core.user_data_manager",
                "core.user_data_manager",
                "core.tags",
                "core.tags",
                "core.file_operations",
                "core.config",
                "core.user_data_manager",
                "core.user_data_manager",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.file_locking",
                "core.config",
                "core.user_data_manager",
                "core.config",
                "core.user_data_manager",
                "core.file_operations",
                "core.config",
                "core.user_data_manager",
                "core.config",
                "core.message_management",
                "core.config",
                "core.config",
                "core.config",
                "core.schemas",
                "core.schemas",
                "core.schemas",
                "core.config",
                "core.file_operations"
              ]
            },
            {
              "file": "core/user_data_manager.py",
              "import_count": 19,
              "modules": [
                "core.logger",
                "core.config",
                "core.file_operations",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.schemas",
                "core.error_handling",
                "core.time_utilities",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.config",
                "core.user_data_handlers",
                "core.message_management",
                "core.file_locking",
                "core.file_locking",
                "core.file_locking",
                "core.response_tracking",
                "core.response_tracking"
              ]
            },
            {
              "file": "core/user_data_validation.py",
              "import_count": 11,
              "modules": [
                "core.logger",
                "core.error_handling",
                "core.time_utilities",
                "core.config",
                "core.schemas",
                "core.message_management",
                "core.schemas",
                "core.user_data_handlers",
                "core.schemas",
                "core.user_data_handlers",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "tasks/task_management.py",
              "import_count": 11,
              "modules": [
                "core.logger",
                "core.file_operations",
                "core.error_handling",
                "core.config",
                "core.user_data_handlers",
                "core.time_utilities",
                "core.service",
                "core.service",
                "core.tags",
                "core.tags",
                "core.tags"
              ]
            },
            {
              "file": "ui/ui_app_qt.py",
              "import_count": 44,
              "modules": [
                "core.time_utilities",
                "core.logger",
                "core.config",
                "core.error_handling",
                "core.service_utilities",
                "user.user_context",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.user_data_validation",
                "core.config",
                "ui.generated.admin_panel_pyqt",
                "core.user_data_manager",
                "core.scheduler",
                "core.scheduler",
                "core.scheduler",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.logger",
                "core.config",
                "core.auto_cleanup",
                "core.auto_cleanup",
                "core.config",
                "core.config",
                "core.config",
                "core.config",
                "ui.dialogs.account_creator_dialog",
                "communication.core.channel_orchestrator",
                "ui.dialogs.channel_management_dialog",
                "ui.dialogs.category_management_dialog",
                "ui.dialogs.checkin_management_dialog",
                "ui.dialogs.task_management_dialog",
                "ui.dialogs.task_crud_dialog",
                "ui.dialogs.user_profile_dialog",
                "core.user_data_handlers",
                "ui.dialogs.user_analytics_dialog",
                "ui.dialogs.message_editor_dialog",
                "ui.dialogs.schedule_editor_dialog",
                "core.user_data_handlers",
                "tasks.task_management",
                "core.scheduler",
                "communication.core.channel_orchestrator",
                "ui.dialogs.process_watcher_dialog",
                "core.user_data_handlers",
                "communication.core.channel_orchestrator"
              ]
            },
            {
              "file": "ui/dialogs/account_creator_dialog.py",
              "import_count": 22,
              "modules": [
                "core.logger",
                "core.user_data_validation",
                "core.user_data_handlers",
                "core.error_handling",
                "ui.widgets.category_selection_widget",
                "ui.widgets.channel_selection_widget",
                "ui.widgets.task_settings_widget",
                "ui.widgets.checkin_settings_widget",
                "ui.generated.account_creator_dialog_pyqt",
                "ui.dialogs.user_profile_dialog",
                "core.file_operations",
                "core.user_data_handlers",
                "core.config",
                "tasks.task_management",
                "tasks.task_management",
                "core.user_data_manager",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.service",
                "core.user_data_validation",
                "core.user_data_validation",
                "core.user_data_validation"
              ]
            },
            {
              "file": "ui/dialogs/category_management_dialog.py",
              "import_count": 7,
              "modules": [
                "ui.generated.category_management_dialog_pyqt",
                "ui.widgets.category_selection_widget",
                "core.logger",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.error_handling",
                "core.schedule_management"
              ]
            },
            {
              "file": "ui/dialogs/channel_management_dialog.py",
              "import_count": 6,
              "modules": [
                "ui.generated.channel_management_dialog_pyqt",
                "ui.widgets.channel_selection_widget",
                "core.logger",
                "core.user_data_validation",
                "core.user_data_handlers",
                "core.error_handling"
              ]
            },
            {
              "file": "ui/dialogs/checkin_management_dialog.py",
              "import_count": 8,
              "modules": [
                "core.logger",
                "core.schedule_management",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.error_handling",
                "core.user_data_validation",
                "ui.widgets.checkin_settings_widget",
                "ui.generated.checkin_management_dialog_pyqt"
              ]
            },
            {
              "file": "ui/dialogs/schedule_editor_dialog.py",
              "import_count": 9,
              "modules": [
                "core.logger",
                "core.schedule_management",
                "core.ui_management",
                "core.error_handling",
                "core.user_data_validation",
                "core.time_utilities",
                "ui.widgets.period_row_widget",
                "ui.generated.schedule_editor_dialog_pyqt",
                "core.config"
              ]
            },
            {
              "file": "ui/dialogs/task_crud_dialog.py",
              "import_count": 13,
              "modules": [
                "ui.generated.task_crud_dialog_pyqt",
                "tasks.task_management",
                "core.error_handling",
                "core.logger",
                "ui.dialogs.task_edit_dialog",
                "tasks.task_management",
                "ui.dialogs.task_edit_dialog",
                "tasks.task_management",
                "ui.dialogs.task_completion_dialog",
                "tasks.task_management",
                "tasks.task_management",
                "tasks.task_management",
                "tasks.task_management"
              ]
            },
            {
              "file": "ui/dialogs/task_management_dialog.py",
              "import_count": 9,
              "modules": [
                "ui.generated.task_management_dialog_pyqt",
                "ui.widgets.task_settings_widget",
                "core.schedule_management",
                "core.user_data_handlers",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "core.user_data_validation",
                "tasks.task_management"
              ]
            },
            {
              "file": "ui/dialogs/user_analytics_dialog.py",
              "import_count": 6,
              "modules": [
                "ui.generated.user_analytics_dialog_pyqt",
                "core.checkin_analytics",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "core.response_tracking"
              ]
            },
            {
              "file": "ui/dialogs/user_profile_dialog.py",
              "import_count": 8,
              "modules": [
                "ui.generated.user_profile_management_dialog_pyqt",
                "core.logger",
                "core.user_data_handlers",
                "core.user_data_validation",
                "core.error_handling",
                "ui.widgets.user_profile_settings_widget",
                "ui.widgets.dynamic_list_container",
                "core.user_data_handlers"
              ]
            },
            {
              "file": "ui/widgets/checkin_settings_widget.py",
              "import_count": 14,
              "modules": [
                "ui.generated.checkin_settings_widget_pyqt",
                "core.ui_management",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "ui.widgets.period_row_widget",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager",
                "core.checkin_dynamic_manager"
              ]
            },
            {
              "file": "ui/widgets/dynamic_list_container.py",
              "import_count": 10,
              "modules": [
                "ui.widgets.dynamic_list_field",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger",
                "core.logger"
              ]
            },
            {
              "file": "ui/widgets/dynamic_list_field.py",
              "import_count": 6,
              "modules": [
                "ui.generated.dynamic_list_field_template_pyqt",
                "core.logger",
                "core.error_handling",
                "ui.widgets.dynamic_list_container",
                "ui.widgets.dynamic_list_container",
                "ui.widgets.dynamic_list_container"
              ]
            },
            {
              "file": "ui/widgets/task_settings_widget.py",
              "import_count": 8,
              "modules": [
                "ui.generated.task_settings_widget_pyqt",
                "tasks.task_management",
                "core.ui_management",
                "core.user_data_handlers",
                "core.error_handling",
                "core.logger",
                "ui.widgets.period_row_widget",
                "ui.widgets.tag_widget"
              ]
            },
            {
              "file": "user/context_manager.py",
              "import_count": 7,
              "modules": [
                "core.logger",
                "core.user_data_handlers",
                "core.response_tracking",
                "core.message_management",
                "core.schedule_utilities",
                "core.error_handling",
                "user.user_context"
              ]
            }
          ]
        }
      },
      "timestamp": "2026-01-18T01:46:33"
    },
    "analyze_module_dependencies": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 0,
          "files_affected": 0
        },
        "details": {
          "files_scanned": 108,
          "total_imports": 1472,
          "documented_dependencies": 108,
          "standard_library": 408,
          "third_party": 230,
          "local_imports": 834,
          "missing_files": [],
          "missing_sections": []
        }
      },
      "timestamp": "2026-01-18T01:46:35"
    },
    "analyze_module_imports": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 0,
          "files_affected": 0
        },
        "details": {
          "ai/cache_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "hashlib",
                  "as_name": null,
                  "imported_items": [
                    "hashlib"
                  ]
                },
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Optional",
                    "Tuple",
                    "Any"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass",
                    "field"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "AI_CACHE_RESPONSES",
                    "CONTEXT_CACHE_TTL",
                    "AI_RESPONSE_CACHE_TTL"
                  ]
                }
              ]
            },
            "total_imports": 8
          },
          "ai/chatbot.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                },
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "collections",
                  "as_name": null,
                  "imported_items": [
                    "collections"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "date"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "requests",
                  "as_name": null,
                  "imported_items": [
                    "requests"
                  ]
                },
                {
                  "module": "psutil",
                  "as_name": null,
                  "imported_items": [
                    "psutil"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "LM_STUDIO_BASE_URL",
                    "LM_STUDIO_API_KEY",
                    "LM_STUDIO_MODEL",
                    "AI_TIMEOUT_SECONDS",
                    "AI_CACHE_RESPONSES",
                    "AI_SYSTEM_PROMPT_PATH",
                    "AI_USE_CUSTOM_PROMPT",
                    "AI_CONNECTION_TEST_TIMEOUT",
                    "AI_API_CALL_TIMEOUT",
                    "AI_PERSONALIZED_MESSAGE_TIMEOUT",
                    "AI_CONTEXTUAL_RESPONSE_TIMEOUT",
                    "AI_QUICK_RESPONSE_TIMEOUT",
                    "AI_MAX_RESPONSE_LENGTH",
                    "AI_MAX_RESPONSE_WORDS",
                    "AI_MAX_RESPONSE_TOKENS",
                    "AI_MIN_RESPONSE_LENGTH",
                    "AI_CHAT_TEMPERATURE",
                    "AI_COMMAND_TEMPERATURE",
                    "AI_CLARIFICATION_TEMPERATURE"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_responses",
                    "store_chat_interaction"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "user.context_manager",
                  "as_name": null,
                  "imported_items": [
                    "user_context_manager"
                  ]
                },
                {
                  "module": "ai.prompt_manager",
                  "as_name": null,
                  "imported_items": [
                    "get_prompt_manager"
                  ]
                },
                {
                  "module": "ai.cache_manager",
                  "as_name": null,
                  "imported_items": [
                    "get_response_cache"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_messages"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIME_ONLY_MINUTE"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "is_user_checkins_enabled"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "are_tasks_enabled"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "is_user_checkins_enabled"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "is_user_checkins_enabled"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "is_user_checkins_enabled"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "load_active_tasks",
                    "get_user_task_stats",
                    "get_tasks_due_soon",
                    "are_tasks_enabled"
                  ]
                },
                {
                  "module": "ai.lm_studio_manager",
                  "as_name": null,
                  "imported_items": [
                    "is_lm_studio_ready"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "store_chat_interaction"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "store_chat_interaction"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "parse_timestamp_full"
                  ]
                }
              ]
            },
            "total_imports": 33
          },
          "ai/context_builder.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List",
                    "Optional"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_responses"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "user.context_manager",
                  "as_name": null,
                  "imported_items": [
                    "user_context_manager"
                  ]
                }
              ]
            },
            "total_imports": 8
          },
          "ai/conversation_history.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List",
                    "Optional"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_filename"
                  ]
                }
              ]
            },
            "total_imports": 6
          },
          "ai/lm_studio_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "subprocess",
                  "as_name": null,
                  "imported_items": [
                    "subprocess"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "requests",
                  "as_name": null,
                  "imported_items": [
                    "requests"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "LM_STUDIO_BASE_URL",
                    "LM_STUDIO_API_KEY",
                    "LM_STUDIO_MODEL",
                    "AI_CONNECTION_TEST_TIMEOUT"
                  ]
                }
              ]
            },
            "total_imports": 5
          },
          "ai/prompt_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Optional"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "AI_SYSTEM_PROMPT_PATH",
                    "AI_USE_CUSTOM_PROMPT"
                  ]
                }
              ]
            },
            "total_imports": 6
          },
          "ai/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "chatbot",
                  "as_name": null,
                  "imported_items": [
                    "AIChatBotSingleton",
                    "get_ai_chatbot"
                  ]
                },
                {
                  "module": "cache_manager",
                  "as_name": null,
                  "imported_items": [
                    "ResponseCache",
                    "get_response_cache",
                    "get_context_cache",
                    "CacheEntry",
                    "ContextCache"
                  ]
                },
                {
                  "module": "context_builder",
                  "as_name": null,
                  "imported_items": [
                    "ContextBuilder",
                    "get_context_builder",
                    "ContextData",
                    "ContextAnalysis"
                  ]
                },
                {
                  "module": "prompt_manager",
                  "as_name": null,
                  "imported_items": [
                    "PromptManager",
                    "get_prompt_manager",
                    "PromptTemplate"
                  ]
                },
                {
                  "module": "conversation_history",
                  "as_name": null,
                  "imported_items": [
                    "ConversationHistory",
                    "get_conversation_history",
                    "ConversationMessage",
                    "ConversationSession"
                  ]
                },
                {
                  "module": "lm_studio_manager",
                  "as_name": null,
                  "imported_items": [
                    "LMStudioManager",
                    "get_lm_studio_manager",
                    "is_lm_studio_ready",
                    "ensure_lm_studio_ready"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 6
          },
          "communication/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "communication_channels.base.base_channel",
                  "as_name": null,
                  "imported_items": [
                    "BaseChannel",
                    "ChannelStatus",
                    "ChannelType",
                    "ChannelConfig"
                  ]
                },
                {
                  "module": "message_processing.command_parser",
                  "as_name": null,
                  "imported_items": [
                    "EnhancedCommandParser",
                    "ParsingResult",
                    "get_enhanced_command_parser",
                    "parse_command"
                  ]
                },
                {
                  "module": "command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message",
                    "get_interaction_manager",
                    "InteractionManager",
                    "CommandDefinition"
                  ]
                },
                {
                  "module": "message_processing.conversation_flow_manager",
                  "as_name": null,
                  "imported_items": [
                    "conversation_manager",
                    "ConversationManager"
                  ]
                },
                {
                  "module": "command_handlers.base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler"
                  ]
                },
                {
                  "module": "command_handlers.analytics_handler",
                  "as_name": null,
                  "imported_items": [
                    "AnalyticsHandler"
                  ]
                },
                {
                  "module": "command_handlers.task_handler",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementHandler"
                  ]
                },
                {
                  "module": "command_handlers.checkin_handler",
                  "as_name": null,
                  "imported_items": [
                    "CheckinHandler"
                  ]
                },
                {
                  "module": "command_handlers.profile_handler",
                  "as_name": null,
                  "imported_items": [
                    "ProfileHandler"
                  ]
                },
                {
                  "module": "command_handlers.schedule_handler",
                  "as_name": null,
                  "imported_items": [
                    "ScheduleManagementHandler"
                  ]
                },
                {
                  "module": "command_handlers.interaction_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_handlers",
                    "get_interaction_handler",
                    "HelpHandler"
                  ]
                },
                {
                  "module": "communication_channels.discord.bot",
                  "as_name": null,
                  "imported_items": [
                    "DiscordBot"
                  ]
                },
                {
                  "module": "communication_channels.email.bot",
                  "as_name": null,
                  "imported_items": [
                    "EmailBot"
                  ]
                },
                {
                  "module": "communication_channels.discord.bot",
                  "as_name": null,
                  "imported_items": [
                    "DiscordConnectionStatus"
                  ]
                },
                {
                  "module": "communication_channels.discord.event_handler",
                  "as_name": null,
                  "imported_items": [
                    "DiscordEventHandler"
                  ]
                },
                {
                  "module": "communication_channels.discord.api_client",
                  "as_name": null,
                  "imported_items": [
                    "DiscordAPIClient"
                  ]
                },
                {
                  "module": "communication_channels.email.bot",
                  "as_name": null,
                  "imported_items": [
                    "EmailBotError"
                  ]
                },
                {
                  "module": "message_processing.message_router",
                  "as_name": null,
                  "imported_items": [
                    "MessageRouter"
                  ]
                },
                {
                  "module": "message_processing.message_router",
                  "as_name": null,
                  "imported_items": [
                    "MessageType"
                  ]
                },
                {
                  "module": "message_processing.message_router",
                  "as_name": null,
                  "imported_items": [
                    "RoutingResult"
                  ]
                },
                {
                  "module": "message_processing.message_router",
                  "as_name": null,
                  "imported_items": [
                    "get_message_router"
                  ]
                },
                {
                  "module": "communication_channels.discord.event_handler",
                  "as_name": null,
                  "imported_items": [
                    "get_discord_event_handler"
                  ]
                },
                {
                  "module": "communication_channels.base.rich_formatter",
                  "as_name": null,
                  "imported_items": [
                    "get_rich_formatter"
                  ]
                },
                {
                  "module": "communication_channels.discord.api_client",
                  "as_name": null,
                  "imported_items": [
                    "get_discord_api_client"
                  ]
                },
                {
                  "module": "communication_channels.base.command_registry",
                  "as_name": null,
                  "imported_items": [
                    "get_command_registry"
                  ]
                },
                {
                  "module": "communication_channels.base.message_formatter",
                  "as_name": null,
                  "imported_items": [
                    "get_message_formatter"
                  ]
                },
                {
                  "module": "communication_channels.base.rich_formatter",
                  "as_name": null,
                  "imported_items": [
                    "RichFormatter"
                  ]
                },
                {
                  "module": "communication_channels.base.rich_formatter",
                  "as_name": null,
                  "imported_items": [
                    "DiscordRichFormatter"
                  ]
                },
                {
                  "module": "communication_channels.base.rich_formatter",
                  "as_name": null,
                  "imported_items": [
                    "EmailRichFormatter"
                  ]
                },
                {
                  "module": "communication_channels.base.message_formatter",
                  "as_name": null,
                  "imported_items": [
                    "MessageFormatter"
                  ]
                },
                {
                  "module": "communication_channels.base.message_formatter",
                  "as_name": null,
                  "imported_items": [
                    "TextMessageFormatter"
                  ]
                },
                {
                  "module": "communication_channels.base.message_formatter",
                  "as_name": null,
                  "imported_items": [
                    "EmailMessageFormatter"
                  ]
                },
                {
                  "module": "communication_channels.base.command_registry",
                  "as_name": null,
                  "imported_items": [
                    "CommandRegistry"
                  ]
                },
                {
                  "module": "communication_channels.base.command_registry",
                  "as_name": null,
                  "imported_items": [
                    "DiscordCommandRegistry"
                  ]
                },
                {
                  "module": "communication_channels.base.command_registry",
                  "as_name": null,
                  "imported_items": [
                    "EmailCommandRegistry"
                  ]
                },
                {
                  "module": "communication_channels.discord.event_handler",
                  "as_name": null,
                  "imported_items": [
                    "EventContext"
                  ]
                },
                {
                  "module": "communication_channels.discord.event_handler",
                  "as_name": null,
                  "imported_items": [
                    "EventType"
                  ]
                },
                {
                  "module": "communication_channels.discord.api_client",
                  "as_name": null,
                  "imported_items": [
                    "MessageData"
                  ]
                },
                {
                  "module": "communication_channels.discord.api_client",
                  "as_name": null,
                  "imported_items": [
                    "SendMessageOptions"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.retry_manager",
                  "as_name": null,
                  "imported_items": [
                    "RetryManager",
                    "QueuedMessage"
                  ]
                },
                {
                  "module": "core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "BotInitializationError",
                    "MessageSendError"
                  ]
                },
                {
                  "module": "core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "core.factory",
                  "as_name": null,
                  "imported_items": [
                    "ChannelFactory"
                  ]
                },
                {
                  "module": "core.channel_monitor",
                  "as_name": null,
                  "imported_items": [
                    "ChannelMonitor"
                  ]
                }
              ]
            },
            "total_imports": 45
          },
          "communication/command_handlers/account_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List",
                    "Optional"
                  ]
                },
                {
                  "module": "secrets",
                  "as_name": null,
                  "imported_items": [
                    "secrets"
                  ]
                },
                {
                  "module": "string",
                  "as_name": null,
                  "imported_items": [
                    "string"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier",
                    "create_new_user"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "get_all_user_ids",
                    "update_user_account"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "communication.command_handlers.base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.command_handlers.account_handler",
                  "as_name": null,
                  "imported_items": [
                    "_pending_link_operations"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "communication.command_handlers.account_handler",
                  "as_name": null,
                  "imported_items": [
                    "_generate_confirmation_code",
                    "_send_confirmation_code"
                  ]
                },
                {
                  "module": "communication.command_handlers.account_handler",
                  "as_name": null,
                  "imported_items": [
                    "_pending_link_operations"
                  ]
                }
              ]
            },
            "total_imports": 14
          },
          "communication/command_handlers/analytics_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List"
                  ]
                },
                {
                  "module": "collections",
                  "as_name": null,
                  "imported_items": [
                    "Counter"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.command_handlers.base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_checkins"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_user_task_stats",
                    "load_active_tasks",
                    "load_completed_tasks"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_user_task_stats",
                    "load_active_tasks",
                    "load_completed_tasks"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                }
              ]
            },
            "total_imports": 35
          },
          "communication/command_handlers/base_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "abc",
                  "as_name": null,
                  "imported_items": [
                    "ABC",
                    "abstractmethod"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "List",
                    "Optional"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 5
          },
          "communication/command_handlers/checkin_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "date"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "is_user_checkins_enabled",
                    "get_recent_checkins"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_FULL"
                  ]
                },
                {
                  "module": "communication.command_handlers.base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.message_processing.conversation_flow_manager",
                  "as_name": null,
                  "imported_items": [
                    "conversation_manager"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "communication/command_handlers/interaction_handlers.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional",
                    "Any"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "save_user_data"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.command_handlers.base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "load_active_tasks"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "is_user_checkins_enabled"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_checkins"
                  ]
                },
                {
                  "module": "communication.command_handlers.task_handler",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.checkin_handler",
                  "as_name": null,
                  "imported_items": [
                    "CheckinHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.profile_handler",
                  "as_name": null,
                  "imported_items": [
                    "ProfileHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.schedule_handler",
                  "as_name": null,
                  "imported_items": [
                    "ScheduleManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.analytics_handler",
                  "as_name": null,
                  "imported_items": [
                    "AnalyticsHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.notebook_handler",
                  "as_name": null,
                  "imported_items": [
                    "NotebookHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.account_handler",
                  "as_name": null,
                  "imported_items": [
                    "AccountManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.task_handler",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.checkin_handler",
                  "as_name": null,
                  "imported_items": [
                    "CheckinHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.profile_handler",
                  "as_name": null,
                  "imported_items": [
                    "ProfileHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.schedule_handler",
                  "as_name": null,
                  "imported_items": [
                    "ScheduleManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.analytics_handler",
                  "as_name": null,
                  "imported_items": [
                    "AnalyticsHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.notebook_handler",
                  "as_name": null,
                  "imported_items": [
                    "NotebookHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.account_handler",
                  "as_name": null,
                  "imported_items": [
                    "AccountManagementHandler"
                  ]
                }
              ]
            },
            "total_imports": 23
          },
          "communication/command_handlers/notebook_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Any",
                    "Optional"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "notebook.notebook_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "get_entry",
                    "list_recent",
                    "append_to_entry_body",
                    "set_entry_body",
                    "add_tags",
                    "remove_tags",
                    "search_entries",
                    "pin_entry",
                    "archive_entry",
                    "add_list_item",
                    "toggle_list_item_done",
                    "remove_list_item",
                    "set_group",
                    "list_by_group",
                    "list_pinned",
                    "list_inbox",
                    "list_by_tag",
                    "list_archived",
                    "create_note",
                    "create_list",
                    "create_journal"
                  ]
                },
                {
                  "module": "notebook.schemas",
                  "as_name": null,
                  "imported_items": [
                    "Entry"
                  ]
                },
                {
                  "module": "notebook.notebook_validation",
                  "as_name": null,
                  "imported_items": [
                    "format_short_id"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_full",
                    "TIMESTAMP_FULL"
                  ]
                },
                {
                  "module": "core.tags",
                  "as_name": null,
                  "imported_items": [
                    "parse_tags_from_text"
                  ]
                },
                {
                  "module": "communication.command_handlers.base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.message_processing.conversation_flow_manager",
                  "as_name": null,
                  "imported_items": [
                    "FLOW_NOTE_BODY",
                    "conversation_manager"
                  ]
                },
                {
                  "module": "communication.message_processing.conversation_flow_manager",
                  "as_name": null,
                  "imported_items": [
                    "FLOW_LIST_ITEMS",
                    "conversation_manager"
                  ]
                },
                {
                  "module": "core.tags",
                  "as_name": null,
                  "imported_items": [
                    "parse_tags_from_text"
                  ]
                }
              ]
            },
            "total_imports": 13
          },
          "communication/command_handlers/profile_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "save_user_data"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_checkins"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_user_task_stats"
                  ]
                },
                {
                  "module": "communication.command_handlers.base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                }
              ]
            },
            "total_imports": 8
          },
          "communication/command_handlers/schedule_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.command_handlers.base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_schedule_time_periods"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_categories"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_schedule_time_periods",
                    "set_schedule_periods"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_schedule_time_periods"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_categories"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "add_schedule_period",
                    "get_schedule_time_periods",
                    "set_schedule_periods"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_schedule_time_periods",
                    "set_schedule_periods"
                  ]
                }
              ]
            },
            "total_imports": 12
          },
          "communication/command_handlers/shared_types.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List",
                    "Optional"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                }
              ],
              "third_party": [],
              "local": []
            },
            "total_imports": 2
          },
          "communication/command_handlers/task_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional",
                    "Any"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler",
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "DATE_ONLY"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "create_task",
                    "load_active_tasks",
                    "complete_task",
                    "delete_task",
                    "update_task",
                    "get_user_task_stats",
                    "get_tasks_due_soon"
                  ]
                },
                {
                  "module": "communication.message_processing.conversation_flow_manager",
                  "as_name": null,
                  "imported_items": [
                    "conversation_manager"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                }
              ]
            },
            "total_imports": 14
          },
          "communication/command_handlers/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "base_handler",
                  "as_name": null,
                  "imported_items": [
                    "InteractionHandler"
                  ]
                },
                {
                  "module": "interaction_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_handlers",
                    "get_interaction_handler"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 3
          },
          "communication/communication_channels/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [],
              "local": []
            },
            "total_imports": 0
          },
          "communication/core/channel_monitor.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "Optional"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.communication_channels.base.base_channel",
                  "as_name": null,
                  "imported_items": [
                    "BaseChannel"
                  ]
                }
              ]
            },
            "total_imports": 7
          },
          "communication/core/channel_orchestrator.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                },
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional",
                    "Any"
                  ]
                },
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                },
                {
                  "module": "uuid",
                  "as_name": null,
                  "imported_items": [
                    "uuid"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                },
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                },
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.communication_channels.base.base_channel",
                  "as_name": null,
                  "imported_items": [
                    "BaseChannel",
                    "ChannelConfig",
                    "ChannelStatus"
                  ]
                },
                {
                  "module": "communication.core.factory",
                  "as_name": null,
                  "imported_items": [
                    "ChannelFactory"
                  ]
                },
                {
                  "module": "communication.core.retry_manager",
                  "as_name": null,
                  "imported_items": [
                    "RetryManager"
                  ]
                },
                {
                  "module": "communication.core.channel_monitor",
                  "as_name": null,
                  "imported_items": [
                    "ChannelMonitor"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "store_sent_message"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_current_time_periods_with_validation",
                    "get_current_day_names"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "determine_file_path",
                    "load_json_data"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "EMAIL_SMTP_SERVER",
                    "DISCORD_BOT_TOKEN",
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.service_utilities",
                  "as_name": null,
                  "imported_items": [
                    "wait_for_network"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_available_channels"
                  ]
                },
                {
                  "module": "communication.core.factory",
                  "as_name": null,
                  "imported_items": [
                    "ChannelFactory"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id",
                    "are_tasks_enabled"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "communication.message_processing.conversation_flow_manager",
                  "as_name": null,
                  "imported_items": [
                    "conversation_manager"
                  ]
                },
                {
                  "module": "communication.message_processing.conversation_flow_manager",
                  "as_name": null,
                  "imported_items": [
                    "conversation_manager"
                  ]
                },
                {
                  "module": "ai.chatbot",
                  "as_name": null,
                  "imported_items": [
                    "get_ai_chatbot"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_messages"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_network_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_network_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_network_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_network_error"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_communication_error"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "force_restart_logging"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_messages_file_dict"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.task_reminder_view",
                  "as_name": null,
                  "imported_items": [
                    "get_task_reminder_view"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.checkin_view",
                  "as_name": null,
                  "imported_items": [
                    "get_checkin_view"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "store_sent_message"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "store_sent_message"
                  ]
                }
              ]
            },
            "total_imports": 46
          },
          "communication/core/factory.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Type",
                    "Optional"
                  ]
                },
                {
                  "module": "importlib",
                  "as_name": null,
                  "imported_items": [
                    "importlib"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "communication.communication_channels.base.base_channel",
                  "as_name": null,
                  "imported_items": [
                    "BaseChannel",
                    "ChannelConfig"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_available_channels",
                    "get_channel_class_mapping"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_available_channels",
                    "get_channel_class_mapping"
                  ]
                }
              ]
            },
            "total_imports": 7
          },
          "communication/core/retry_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "queue",
                  "as_name": null,
                  "imported_items": [
                    "queue"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 7
          },
          "communication/core/welcome_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "Optional"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_full"
                  ]
                }
              ]
            },
            "total_imports": 8
          },
          "communication/core/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "retry_manager",
                  "as_name": null,
                  "imported_items": [
                    "RetryManager",
                    "QueuedMessage"
                  ]
                },
                {
                  "module": "factory",
                  "as_name": null,
                  "imported_items": [
                    "ChannelFactory"
                  ]
                },
                {
                  "module": "channel_monitor",
                  "as_name": null,
                  "imported_items": [
                    "ChannelMonitor"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 3
          },
          "communication/message_processing/command_parser.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional",
                    "Any"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "AI_RULE_BASED_HIGH_CONFIDENCE_THRESHOLD",
                    "AI_AI_ENHANCED_CONFIDENCE_THRESHOLD",
                    "AI_RULE_BASED_FALLBACK_THRESHOLD",
                    "AI_AI_PARSING_BASE_CONFIDENCE",
                    "AI_AI_PARSING_PARTIAL_CONFIDENCE",
                    "AI_COMMAND_PARSING_TIMEOUT"
                  ]
                },
                {
                  "module": "ai.chatbot",
                  "as_name": null,
                  "imported_items": [
                    "get_ai_chatbot"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.command_handlers.interaction_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_handlers"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.tags",
                  "as_name": null,
                  "imported_items": [
                    "parse_tags_from_text"
                  ]
                }
              ]
            },
            "total_imports": 12
          },
          "communication/message_processing/conversation_flow_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "notebook.notebook_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "create_note"
                  ]
                },
                {
                  "module": "notebook.notebook_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "create_note"
                  ]
                },
                {
                  "module": "notebook.notebook_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "create_list"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ai.chatbot",
                  "as_name": null,
                  "imported_items": [
                    "get_ai_chatbot"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "is_user_checkins_enabled",
                    "get_recent_checkins"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_FULL",
                    "DATE_ONLY",
                    "TIME_ONLY_MINUTE",
                    "now_timestamp_full"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_FULL"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_FULL",
                    "now_timestamp_full"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "store_user_response"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_MINUTE",
                    "now_timestamp_full"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id",
                    "update_task"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "DATE_ONLY"
                  ]
                },
                {
                  "module": "core.tags",
                  "as_name": null,
                  "imported_items": [
                    "parse_tags_from_text"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "update_task"
                  ]
                },
                {
                  "module": "communication.command_handlers.task_handler",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.command_handlers.task_handler",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.profile_handler",
                  "as_name": null,
                  "imported_items": [
                    "ProfileHandler"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "communication.command_handlers.analytics_handler",
                  "as_name": null,
                  "imported_items": [
                    "AnalyticsHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.analytics_handler",
                  "as_name": null,
                  "imported_items": [
                    "AnalyticsHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.schedule_handler",
                  "as_name": null,
                  "imported_items": [
                    "ScheduleManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.interaction_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_interaction_handler"
                  ]
                }
              ]
            },
            "total_imports": 57
          },
          "communication/message_processing/interaction_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional",
                    "Dict",
                    "Any",
                    "List"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "AI_MAX_RESPONSE_LENGTH"
                  ]
                },
                {
                  "module": "communication.message_processing.command_parser",
                  "as_name": null,
                  "imported_items": [
                    "get_enhanced_command_parser",
                    "ParsingResult"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "InteractionResponse",
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.command_handlers.interaction_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_interaction_handler",
                    "get_all_handlers"
                  ]
                },
                {
                  "module": "ai.chatbot",
                  "as_name": null,
                  "imported_items": [
                    "get_ai_chatbot"
                  ]
                },
                {
                  "module": "communication.message_processing.conversation_flow_manager",
                  "as_name": null,
                  "imported_items": [
                    "conversation_manager",
                    "FLOW_TASK_REMINDER"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "DATE_ONLY",
                    "TIME_ONLY_MINUTE"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "load_active_tasks"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_checkins",
                    "is_user_checkins_enabled"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_FULL"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_categories"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_checkins"
                  ]
                },
                {
                  "module": "communication.command_handlers.task_handler",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.command_handlers.task_handler",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementHandler"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.message_processing.command_parser",
                  "as_name": null,
                  "imported_items": [
                    "ParsingResult"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "ParsedCommand"
                  ]
                }
              ]
            },
            "total_imports": 32
          },
          "communication/message_processing/message_router.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional",
                    "Dict",
                    "List"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                },
                {
                  "module": "enum",
                  "as_name": null,
                  "imported_items": [
                    "Enum"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 5
          },
          "communication/message_processing/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "InteractionManager",
                    "handle_user_message",
                    "get_interaction_manager",
                    "CommandDefinition"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 1
          },
          "communication/communication_channels/base/base_channel.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "abc",
                  "as_name": null,
                  "imported_items": [
                    "ABC",
                    "abstractmethod"
                  ]
                },
                {
                  "module": "enum",
                  "as_name": null,
                  "imported_items": [
                    "Enum"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional",
                    "Dict",
                    "Any",
                    "List"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger",
                    "get_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 6
          },
          "communication/communication_channels/base/command_registry.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional",
                    "Callable"
                  ]
                },
                {
                  "module": "abc",
                  "as_name": null,
                  "imported_items": [
                    "ABC",
                    "abstractmethod"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "app_commands"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 6
          },
          "communication/communication_channels/base/message_formatter.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List",
                    "Optional"
                  ]
                },
                {
                  "module": "abc",
                  "as_name": null,
                  "imported_items": [
                    "ABC",
                    "abstractmethod"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors",
                    "DataError"
                  ]
                }
              ]
            },
            "total_imports": 4
          },
          "communication/communication_channels/base/rich_formatter.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List"
                  ]
                },
                {
                  "module": "abc",
                  "as_name": null,
                  "imported_items": [
                    "ABC",
                    "abstractmethod"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "discord"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 5
          },
          "communication/communication_channels/discord/account_flow_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "discord"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.command_handlers.shared_types",
                  "as_name": null,
                  "imported_items": [
                    "ParsedCommand"
                  ]
                },
                {
                  "module": "communication.command_handlers.account_handler",
                  "as_name": null,
                  "imported_items": [
                    "AccountManagementHandler"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "TIMEZONE_OPTIONS"
                  ]
                }
              ]
            },
            "total_imports": 7
          },
          "communication/communication_channels/discord/api_client.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List",
                    "Optional",
                    "Union"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "discord"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 7
          },
          "communication/communication_channels/discord/bot.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                },
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "List",
                    "Dict",
                    "Any",
                    "Optional"
                  ]
                },
                {
                  "module": "queue",
                  "as_name": null,
                  "imported_items": [
                    "queue"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "socket",
                  "as_name": null,
                  "imported_items": [
                    "socket"
                  ]
                },
                {
                  "module": "enum",
                  "as_name": null,
                  "imported_items": [
                    "enum"
                  ]
                },
                {
                  "module": "contextlib",
                  "as_name": null,
                  "imported_items": [
                    "contextlib"
                  ]
                },
                {
                  "module": "subprocess",
                  "as_name": null,
                  "imported_items": [
                    "subprocess"
                  ]
                },
                {
                  "module": "shutil",
                  "as_name": null,
                  "imported_items": [
                    "shutil"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "gc",
                  "as_name": null,
                  "imported_items": [
                    "gc"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "discord"
                  ]
                },
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "app_commands"
                  ]
                },
                {
                  "module": "discord.ext",
                  "as_name": null,
                  "imported_items": [
                    "commands"
                  ]
                },
                {
                  "module": "psutil",
                  "as_name": null,
                  "imported_items": [
                    "psutil"
                  ]
                },
                {
                  "module": "aiohttp",
                  "as_name": null,
                  "imported_items": [
                    "aiohttp"
                  ]
                },
                {
                  "module": "dns.resolver",
                  "as_name": null,
                  "imported_items": [
                    "dns.resolver"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "DISCORD_BOT_TOKEN",
                    "DISCORD_APPLICATION_ID"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "communication.communication_channels.base.base_channel",
                  "as_name": null,
                  "imported_items": [
                    "BaseChannel",
                    "ChannelType",
                    "ChannelStatus",
                    "ChannelConfig"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "get_interaction_manager",
                    "handle_user_message"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "get_interaction_manager"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.welcome_handler",
                  "as_name": null,
                  "imported_items": [
                    "has_been_welcomed",
                    "mark_as_welcomed",
                    "get_welcome_message"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.webhook_server",
                  "as_name": null,
                  "imported_items": [
                    "WebhookServer"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "DISCORD_WEBHOOK_PORT",
                    "DISCORD_AUTO_NGROK"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.welcome_handler",
                  "as_name": null,
                  "imported_items": [
                    "has_been_welcomed",
                    "mark_as_welcomed",
                    "get_welcome_message"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.welcome_handler",
                  "as_name": null,
                  "imported_items": [
                    "has_been_welcomed",
                    "mark_as_welcomed",
                    "get_welcome_message"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "save_user_data"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.account_flow_handler",
                  "as_name": null,
                  "imported_items": [
                    "start_account_creation_flow",
                    "start_account_linking_flow"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                }
              ]
            },
            "total_imports": 39
          },
          "communication/communication_channels/discord/checkin_view.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "discord"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                }
              ]
            },
            "total_imports": 8
          },
          "communication/communication_channels/discord/event_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List",
                    "Optional",
                    "Callable"
                  ]
                },
                {
                  "module": "dataclasses",
                  "as_name": null,
                  "imported_items": [
                    "dataclass"
                  ]
                },
                {
                  "module": "enum",
                  "as_name": null,
                  "imported_items": [
                    "Enum"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "discord"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "communication.communication_channels.base.rich_formatter",
                  "as_name": null,
                  "imported_items": [
                    "get_rich_formatter"
                  ]
                },
                {
                  "module": "communication.communication_channels.base.rich_formatter",
                  "as_name": null,
                  "imported_items": [
                    "get_rich_formatter"
                  ]
                }
              ]
            },
            "total_imports": 11
          },
          "communication/communication_channels/discord/task_reminder_view.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "discord"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.message_processing.interaction_manager",
                  "as_name": null,
                  "imported_items": [
                    "handle_user_message"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                }
              ]
            },
            "total_imports": 6
          },
          "communication/communication_channels/discord/webhook_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "Optional"
                  ]
                },
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                },
                {
                  "module": "concurrent.futures",
                  "as_name": null,
                  "imported_items": [
                    "Future"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger",
                    "_is_testing_environment"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.core.welcome_manager",
                  "as_name": null,
                  "imported_items": [
                    "has_been_welcomed",
                    "mark_as_welcomed"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.welcome_handler",
                  "as_name": null,
                  "imported_items": [
                    "get_welcome_message"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "communication.core.welcome_manager",
                  "as_name": null,
                  "imported_items": [
                    "clear_welcomed_status"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "update_user_account"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.welcome_handler",
                  "as_name": null,
                  "imported_items": [
                    "get_welcome_message_view"
                  ]
                }
              ]
            },
            "total_imports": 12
          },
          "communication/communication_channels/discord/webhook_server.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "http.server",
                  "as_name": null,
                  "imported_items": [
                    "HTTPServer",
                    "BaseHTTPRequestHandler"
                  ]
                },
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "nacl.signing",
                  "as_name": null,
                  "imported_items": [
                    "VerifyKey"
                  ]
                },
                {
                  "module": "nacl.exceptions",
                  "as_name": null,
                  "imported_items": [
                    "BadSignatureError"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.webhook_handler",
                  "as_name": null,
                  "imported_items": [
                    "parse_webhook_event",
                    "handle_webhook_event"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "DISCORD_PUBLIC_KEY"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "communication/communication_channels/discord/welcome_handler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "TYPE_CHECKING",
                    "Optional"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "discord"
                  ]
                },
                {
                  "module": "discord",
                  "as_name": null,
                  "imported_items": [
                    "discord"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "communication.core.welcome_manager",
                  "as_name": null,
                  "imported_items": [
                    "has_been_welcomed",
                    "mark_as_welcomed",
                    "clear_welcomed_status",
                    "get_welcome_message"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.account_flow_handler",
                  "as_name": null,
                  "imported_items": [
                    "start_account_creation_flow"
                  ]
                },
                {
                  "module": "communication.communication_channels.discord.account_flow_handler",
                  "as_name": null,
                  "imported_items": [
                    "start_account_linking_flow"
                  ]
                }
              ]
            },
            "total_imports": 8
          },
          "communication/communication_channels/email/bot.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "smtplib",
                  "as_name": null,
                  "imported_items": [
                    "smtplib"
                  ]
                },
                {
                  "module": "imaplib",
                  "as_name": null,
                  "imported_items": [
                    "imaplib"
                  ]
                },
                {
                  "module": "email",
                  "as_name": null,
                  "imported_items": [
                    "email"
                  ]
                },
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "email.mime.text",
                  "as_name": null,
                  "imported_items": [
                    "MIMEText"
                  ]
                },
                {
                  "module": "email.header",
                  "as_name": null,
                  "imported_items": [
                    "decode_header"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "List",
                    "Dict",
                    "Any",
                    "Optional",
                    "Tuple"
                  ]
                },
                {
                  "module": "socket",
                  "as_name": null,
                  "imported_items": [
                    "socket"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "EMAIL_SMTP_SERVER",
                    "EMAIL_IMAP_SERVER",
                    "EMAIL_SMTP_USERNAME",
                    "EMAIL_SMTP_PASSWORD"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "communication.communication_channels.base.base_channel",
                  "as_name": null,
                  "imported_items": [
                    "BaseChannel",
                    "ChannelType",
                    "ChannelStatus",
                    "ChannelConfig"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors",
                    "ConfigurationError"
                  ]
                }
              ]
            },
            "total_imports": 15
          },
          "core/auto_cleanup.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "shutil",
                  "as_name": null,
                  "imported_items": [
                    "shutil"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "sys",
                  "as_name": null,
                  "imported_items": [
                    "sys"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "DATE_ONLY",
                    "now_timestamp_full"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_backups_dir"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_user_ids"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_user_ids"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "archive_old_messages"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                }
              ]
            },
            "total_imports": 19
          },
          "core/backup_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "shutil",
                  "as_name": null,
                  "imported_items": [
                    "shutil"
                  ]
                },
                {
                  "module": "zipfile",
                  "as_name": null,
                  "imported_items": [
                    "zipfile"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional",
                    "Tuple"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "traceback",
                  "as_name": null,
                  "imported_items": [
                    "traceback"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_logger",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "core.config"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "get_all_user_ids"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_filename",
                    "now_timestamp_full",
                    "TIMESTAMP_FULL"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "LOG_MAIN_FILE",
                    "LOG_DISCORD_FILE",
                    "LOG_AI_FILE",
                    "LOG_USER_ACTIVITY_FILE",
                    "LOG_ERRORS_FILE"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": "config",
                  "imported_items": [
                    "core.config"
                  ]
                }
              ]
            },
            "total_imports": 17
          },
          "core/checkin_analytics.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "statistics",
                  "as_name": null,
                  "imported_items": [
                    "statistics"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_checkins_by_days"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_FULL",
                    "TIME_ONLY_MINUTE"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "core/checkin_dynamic_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "Optional",
                    "Tuple"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "update_user_preferences"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "update_user_preferences"
                  ]
                }
              ]
            },
            "total_imports": 11
          },
          "core/config.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Tuple",
                    "Optional"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "dotenv",
                  "as_name": null,
                  "imported_items": [
                    "load_dotenv"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "ConfigurationError",
                    "handle_configuration_error",
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                }
              ]
            },
            "total_imports": 6
          },
          "core/error_handling.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "sys",
                  "as_name": null,
                  "imported_items": [
                    "sys"
                  ]
                },
                {
                  "module": "traceback",
                  "as_name": null,
                  "imported_items": [
                    "traceback"
                  ]
                },
                {
                  "module": "logging",
                  "as_name": null,
                  "imported_items": [
                    "logging"
                  ]
                },
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "functools",
                  "as_name": null,
                  "imported_items": [
                    "functools"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional",
                    "Dict",
                    "Any",
                    "Callable",
                    "List"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "asyncio",
                  "as_name": null,
                  "imported_items": [
                    "asyncio"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "shutil",
                  "as_name": null,
                  "imported_items": [
                    "shutil"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_filename"
                  ]
                },
                {
                  "module": "core.service_utilities",
                  "as_name": null,
                  "imported_items": [
                    "wait_for_network"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                }
              ]
            },
            "total_imports": 38
          },
          "core/file_auditor.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "__future__",
                  "as_name": null,
                  "imported_items": [
                    "annotations"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "traceback",
                  "as_name": null,
                  "imported_items": [
                    "traceback"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Optional",
                    "List"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 6
          },
          "core/file_locking.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "sys",
                  "as_name": null,
                  "imported_items": [
                    "sys"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "contextlib",
                  "as_name": null,
                  "imported_items": [
                    "contextmanager",
                    "suppress"
                  ]
                },
                {
                  "module": "fcntl",
                  "as_name": null,
                  "imported_items": [
                    "fcntl"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "tempfile",
                  "as_name": null,
                  "imported_items": [
                    "tempfile"
                  ]
                },
                {
                  "module": "shutil",
                  "as_name": null,
                  "imported_items": [
                    "shutil"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 12
          },
          "core/file_operations.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "time",
                  "as_name": "_t",
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "shutil",
                  "as_name": "_sh",
                  "imported_items": [
                    "shutil"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "DEFAULT_MESSAGES_DIR_PATH",
                    "get_user_file_path",
                    "ensure_user_directory",
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "FileOperationError",
                    "handle_file_error",
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_full"
                  ]
                },
                {
                  "module": "core.file_auditor",
                  "as_name": null,
                  "imported_items": [
                    "record_created"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "create_message_file_from_defaults"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_message_references",
                    "update_user_index"
                  ]
                }
              ]
            },
            "total_imports": 15
          },
          "core/headless_service.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "sys",
                  "as_name": null,
                  "imported_items": [
                    "sys"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "subprocess",
                  "as_name": null,
                  "imported_items": [
                    "subprocess"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Optional"
                  ]
                },
                {
                  "module": "argparse",
                  "as_name": null,
                  "imported_items": [
                    "argparse"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "psutil",
                  "as_name": null,
                  "imported_items": [
                    "psutil"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.service_utilities",
                  "as_name": null,
                  "imported_items": [
                    "get_service_processes",
                    "get_flags_dir",
                    "is_headless_service_running",
                    "is_ui_service_running"
                  ]
                }
              ]
            },
            "total_imports": 13
          },
          "core/logger.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "logging",
                  "as_name": null,
                  "imported_items": [
                    "logging"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "shutil",
                  "as_name": null,
                  "imported_items": [
                    "shutil"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "gzip",
                  "as_name": null,
                  "imported_items": [
                    "gzip"
                  ]
                },
                {
                  "module": "sys",
                  "as_name": null,
                  "imported_items": [
                    "sys"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "logging.handlers",
                  "as_name": null,
                  "imported_items": [
                    "RotatingFileHandler",
                    "TimedRotatingFileHandler"
                  ]
                },
                {
                  "module": "glob",
                  "as_name": null,
                  "imported_items": [
                    "glob"
                  ]
                },
                {
                  "module": "glob",
                  "as_name": null,
                  "imported_items": [
                    "glob"
                  ]
                },
                {
                  "module": "glob",
                  "as_name": null,
                  "imported_items": [
                    "glob"
                  ]
                },
                {
                  "module": "glob",
                  "as_name": null,
                  "imported_items": [
                    "glob"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": "config",
                  "imported_items": [
                    "core.config"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": "config",
                  "imported_items": [
                    "core.config"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": "config",
                  "imported_items": [
                    "core.config"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": "config",
                  "imported_items": [
                    "core.config"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": "config",
                  "imported_items": [
                    "core.config"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": "config",
                  "imported_items": [
                    "core.config"
                  ]
                }
              ]
            },
            "total_imports": 20
          },
          "core/message_analytics.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "collections",
                  "as_name": null,
                  "imported_items": [
                    "defaultdict"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_messages"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 4
          },
          "core/message_management.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "uuid",
                  "as_name": null,
                  "imported_items": [
                    "uuid"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timezone",
                    "timedelta"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "List",
                    "Dict",
                    "Any",
                    "Optional"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "DEFAULT_MESSAGES_DIR_PATH",
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data",
                    "save_json_data",
                    "determine_file_path"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_messages_file_dict"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "ValidationError",
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_filename",
                    "now_timestamp_full",
                    "TIMESTAMP_FULL"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                }
              ]
            },
            "total_imports": 15
          },
          "core/response_tracking.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Any"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "timedelta"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data",
                    "save_json_data",
                    "get_user_file_path"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_full",
                    "TIMESTAMP_FULL"
                  ]
                }
              ]
            },
            "total_imports": 8
          },
          "core/scheduler.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "calendar",
                  "as_name": null,
                  "imported_items": [
                    "calendar"
                  ]
                },
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                },
                {
                  "module": "subprocess",
                  "as_name": null,
                  "imported_items": [
                    "subprocess"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "List",
                    "Dict",
                    "Any"
                  ]
                },
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "schedule",
                  "as_name": null,
                  "imported_items": [
                    "schedule"
                  ]
                },
                {
                  "module": "pytz",
                  "as_name": null,
                  "imported_items": [
                    "pytz"
                  ]
                },
                {
                  "module": "pytz",
                  "as_name": null,
                  "imported_items": [
                    "pytz"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_user_ids"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_schedule_time_periods"
                  ]
                },
                {
                  "module": "core.service_utilities",
                  "as_name": null,
                  "imported_items": [
                    "load_and_localize_datetime"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_filename",
                    "TIMESTAMP_FULL",
                    "DATE_ONLY",
                    "TIME_ONLY_MINUTE",
                    "TIMESTAMP_MINUTE"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "user.user_context",
                  "as_name": null,
                  "imported_items": [
                    "UserContext"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.backup_manager",
                  "as_name": null,
                  "imported_items": [
                    "backup_manager"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "suppress_noisy_logging"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "core.scheduler",
                  "as_name": null,
                  "imported_items": [
                    "SchedulerManager"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "core.scheduler",
                  "as_name": null,
                  "imported_items": [
                    "SchedulerManager"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "core.scheduler",
                  "as_name": null,
                  "imported_items": [
                    "SchedulerManager"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "are_tasks_enabled"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "core.auto_cleanup",
                  "as_name": null,
                  "imported_items": [
                    "cleanup_data_directory",
                    "cleanup_tests_data_directory"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir",
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "load_active_tasks",
                    "are_tasks_enabled"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_schedule_time_periods"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_user_ids"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "compress_old_logs",
                    "cleanup_old_archives"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id",
                    "update_task"
                  ]
                }
              ]
            },
            "total_imports": 48
          },
          "core/schedule_management.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "calendar",
                  "as_name": null,
                  "imported_items": [
                    "calendar"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "Optional"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.service_utilities",
                  "as_name": null,
                  "imported_items": [
                    "create_reschedule_request"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIME_ONLY_MINUTE"
                  ]
                },
                {
                  "module": "user.user_context",
                  "as_name": null,
                  "imported_items": [
                    "UserContext"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors",
                    "ValidationError"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "update_user_schedules"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "update_user_schedules"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                }
              ]
            },
            "total_imports": 14
          },
          "core/schedule_utilities.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIME_ONLY_MINUTE"
                  ]
                }
              ]
            },
            "total_imports": 5
          },
          "core/schemas.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "__future__",
                  "as_name": null,
                  "imported_items": [
                    "annotations"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Any",
                    "Dict",
                    "List",
                    "Literal",
                    "Optional"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "pydantic",
                  "as_name": null,
                  "imported_items": [
                    "BaseModel",
                    "Field",
                    "ConfigDict",
                    "field_validator",
                    "model_validator",
                    "RootModel"
                  ]
                },
                {
                  "module": "pytz",
                  "as_name": null,
                  "imported_items": [
                    "pytz"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors",
                    "ValidationError"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "get_message_categories"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "is_valid_discord_id"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "core/service.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "signal",
                  "as_name": null,
                  "imported_items": [
                    "signal"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "atexit",
                  "as_name": null,
                  "imported_items": [
                    "atexit"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "logging",
                  "as_name": null,
                  "imported_items": [
                    "logging"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "random",
                  "as_name": null,
                  "imported_items": [
                    "random"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "psutil",
                  "as_name": null,
                  "imported_items": [
                    "psutil"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "validate_and_raise_if_invalid",
                    "print_configuration_report"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "LOG_MAIN_FILE",
                    "USER_INFO_DIR_PATH",
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.scheduler",
                  "as_name": null,
                  "imported_items": [
                    "SchedulerManager"
                  ]
                },
                {
                  "module": "core.service_utilities",
                  "as_name": null,
                  "imported_items": [
                    "get_flags_dir"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_FULL",
                    "now_timestamp_full"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "verify_file_access"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_user_ids"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors",
                    "FileOperationError"
                  ]
                },
                {
                  "module": "core.file_auditor",
                  "as_name": null,
                  "imported_items": [
                    "start_auditor"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "LOG_MAIN_FILE"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "LOG_MAIN_FILE"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "force_restart_logging"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_current_time_periods_with_validation",
                    "get_current_day_names"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_messages"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data"
                  ]
                },
                {
                  "module": "communication.message_processing.conversation_flow_manager",
                  "as_name": null,
                  "imported_items": [
                    "conversation_manager"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.file_auditor",
                  "as_name": null,
                  "imported_items": [
                    "stop_auditor"
                  ]
                },
                {
                  "module": "core.auto_cleanup",
                  "as_name": null,
                  "imported_items": [
                    "auto_cleanup_if_needed",
                    "cleanup_data_directory"
                  ]
                },
                {
                  "module": "core.auto_cleanup",
                  "as_name": null,
                  "imported_items": [
                    "cleanup_tests_data_directory"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                }
              ]
            },
            "total_imports": 44
          },
          "core/service_utilities.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "socket",
                  "as_name": null,
                  "imported_items": [
                    "socket"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "psutil",
                  "as_name": null,
                  "imported_items": [
                    "psutil"
                  ]
                },
                {
                  "module": "pytz",
                  "as_name": null,
                  "imported_items": [
                    "pytz"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_full",
                    "now_timestamp_filename",
                    "TIMESTAMP_MINUTE"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "SCHEDULER_INTERVAL"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.file_auditor",
                  "as_name": null,
                  "imported_items": [
                    "record_created"
                  ]
                }
              ]
            },
            "total_imports": 14
          },
          "core/tags.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Any"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors",
                    "ValidationError"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir",
                    "get_user_file_path"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data",
                    "save_json_data"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "core/time_utilities.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "__future__",
                  "as_name": null,
                  "imported_items": [
                    "annotations"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Iterable",
                    "Literal"
                  ]
                }
              ],
              "third_party": [],
              "local": []
            },
            "total_imports": 3
          },
          "core/ui_management.py": {
            "imports": {
              "standard_library": [],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "ui.widgets.period_row_widget",
                  "as_name": null,
                  "imported_items": [
                    "PeriodRowWidget"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_schedule_time_periods"
                  ]
                },
                {
                  "module": "ui.widgets.period_row_widget",
                  "as_name": null,
                  "imported_items": [
                    "PeriodRowWidget"
                  ]
                }
              ]
            },
            "total_imports": 5
          },
          "core/user_data_handlers.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "traceback",
                  "as_name": null,
                  "imported_items": [
                    "traceback"
                  ]
                },
                {
                  "module": "uuid",
                  "as_name": null,
                  "imported_items": [
                    "uuid"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List",
                    "Union",
                    "Optional"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "copy",
                  "as_name": null,
                  "imported_items": [
                    "copy"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "pytz",
                  "as_name": null,
                  "imported_items": [
                    "pytz"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "ensure_user_directory",
                    "get_user_file_path"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data",
                    "save_json_data",
                    "determine_file_path"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_full"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "validate_new_user_data",
                    "validate_user_update"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_account_dict",
                    "validate_preferences_dict",
                    "validate_schedules_dict"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir",
                    "get_user_file_path"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "USER_INFO_DIR_PATH"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "core.tags",
                  "as_name": null,
                  "imported_items": [
                    "load_user_tags"
                  ]
                },
                {
                  "module": "core.tags",
                  "as_name": null,
                  "imported_items": [
                    "save_user_tags"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "save_json_data"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_file_path"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "UserDataManager"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.file_locking",
                  "as_name": null,
                  "imported_items": [
                    "safe_json_read"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.file_locking",
                  "as_name": null,
                  "imported_items": [
                    "safe_json_read"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.file_locking",
                  "as_name": null,
                  "imported_items": [
                    "safe_json_read"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.file_locking",
                  "as_name": null,
                  "imported_items": [
                    "safe_json_read"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.file_locking",
                  "as_name": null,
                  "imported_items": [
                    "safe_json_read"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "UserDataManager"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "save_json_data"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_file_path",
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "ensure_user_message_files"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_preferences_dict"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_schedules_dict"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_preferences_dict"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_file_path"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data"
                  ]
                }
              ]
            },
            "total_imports": 56
          },
          "core/user_data_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "shutil",
                  "as_name": null,
                  "imported_items": [
                    "shutil"
                  ]
                },
                {
                  "module": "zipfile",
                  "as_name": null,
                  "imported_items": [
                    "zipfile"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional",
                    "Any"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_file_path",
                    "get_user_data_dir",
                    "BASE_DATA_DIR",
                    "get_backups_dir"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data",
                    "get_user_file_path",
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_user_ids"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_messages_file_dict"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_filename",
                    "now_timestamp_full"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_categories"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_categories"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "USER_DATA_LOADERS"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "ensure_user_message_files"
                  ]
                },
                {
                  "module": "core.file_locking",
                  "as_name": null,
                  "imported_items": [
                    "safe_json_read",
                    "safe_json_write"
                  ]
                },
                {
                  "module": "core.file_locking",
                  "as_name": null,
                  "imported_items": [
                    "safe_json_read",
                    "safe_json_write"
                  ]
                },
                {
                  "module": "core.file_locking",
                  "as_name": null,
                  "imported_items": [
                    "safe_json_write"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_checkins"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_responses"
                  ]
                }
              ]
            },
            "total_imports": 30
          },
          "core/user_data_validation.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "Tuple",
                    "List",
                    "Optional"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "DATE_ONLY",
                    "TIME_ONLY_MINUTE"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_account_dict"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "get_message_categories"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_preferences_dict"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_schedules_dict"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                }
              ]
            },
            "total_imports": 18
          },
          "core/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger",
                    "setup_logging",
                    "ComponentLogger",
                    "BackupDirectoryRotatingFileHandler",
                    "ExcludeLoggerNamesFilter",
                    "PytestContextLogFormatter",
                    "setup_third_party_error_logging",
                    "suppress_noisy_logging",
                    "set_console_log_level",
                    "toggle_verbose_logging",
                    "get_verbose_mode",
                    "set_verbose_mode",
                    "get_logger"
                  ]
                },
                {
                  "module": "error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors",
                    "MHMError",
                    "DataError",
                    "FileOperationError",
                    "ConfigurationError",
                    "CommunicationError",
                    "SchedulerError",
                    "UserInterfaceError",
                    "AIError",
                    "ValidationError",
                    "RecoveryError"
                  ]
                },
                {
                  "module": "user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "save_user_data",
                    "save_user_data_transaction",
                    "get_all_user_ids",
                    "update_user_account",
                    "update_user_preferences",
                    "update_user_schedules",
                    "update_user_context",
                    "update_channel_preferences",
                    "register_data_loader"
                  ]
                },
                {
                  "module": "file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data",
                    "save_json_data",
                    "determine_file_path",
                    "verify_file_access"
                  ]
                },
                {
                  "module": "message_management",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_messages",
                    "store_sent_message",
                    "get_message_categories",
                    "load_user_messages",
                    "add_message",
                    "archive_old_messages"
                  ]
                },
                {
                  "module": "response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_responses",
                    "store_chat_interaction",
                    "get_recent_checkins",
                    "is_user_checkins_enabled"
                  ]
                },
                {
                  "module": "user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "validate_schedule_periods",
                    "is_valid_email"
                  ]
                },
                {
                  "module": "error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_network_error"
                  ]
                },
                {
                  "module": "config",
                  "as_name": null,
                  "imported_items": [
                    "DISCORD_BOT_TOKEN",
                    "EMAIL_SMTP_SERVER",
                    "EMAIL_IMAP_SERVER",
                    "EMAIL_SMTP_USERNAME",
                    "LM_STUDIO_BASE_URL",
                    "LM_STUDIO_API_KEY",
                    "LM_STUDIO_MODEL",
                    "SCHEDULER_INTERVAL",
                    "get_available_channels"
                  ]
                },
                {
                  "module": "ui_management",
                  "as_name": null,
                  "imported_items": [
                    "collect_period_data_from_widgets",
                    "load_period_widgets_for_category"
                  ]
                },
                {
                  "module": "user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "ensure_all_categories_have_schedules",
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "schemas",
                  "as_name": null,
                  "imported_items": [
                    "validate_account_dict",
                    "validate_preferences_dict",
                    "validate_schedules_dict",
                    "validate_messages_file_dict"
                  ]
                },
                {
                  "module": "schemas",
                  "as_name": null,
                  "imported_items": [
                    "AccountModel",
                    "ChannelModel",
                    "PreferencesModel",
                    "CategoryScheduleModel",
                    "FeaturesModel",
                    "PeriodModel",
                    "MessagesFileModel",
                    "MessageModel",
                    "SchedulesModel"
                  ]
                },
                {
                  "module": "config",
                  "as_name": null,
                  "imported_items": [
                    "CONTEXT_CACHE_TTL",
                    "DISCORD_APPLICATION_ID",
                    "EMAIL_SMTP_PASSWORD"
                  ]
                },
                {
                  "module": "config",
                  "as_name": null,
                  "imported_items": [
                    "validate_all_configuration",
                    "validate_and_raise_if_invalid",
                    "get_backups_dir",
                    "ensure_user_directory",
                    "validate_email_config",
                    "validate_discord_config",
                    "validate_minimum_config",
                    "get_channel_class_mapping",
                    "validate_core_paths",
                    "validate_ai_configuration",
                    "validate_communication_channels",
                    "validate_logging_configuration",
                    "validate_scheduler_configuration",
                    "validate_file_organization_settings",
                    "validate_environment_variables",
                    "print_configuration_report"
                  ]
                },
                {
                  "module": "error_handling",
                  "as_name": null,
                  "imported_items": [
                    "ErrorHandler",
                    "ErrorRecoveryStrategy",
                    "ConfigurationRecovery"
                  ]
                },
                {
                  "module": "config",
                  "as_name": null,
                  "imported_items": [
                    "ConfigValidationError"
                  ]
                },
                {
                  "module": "service",
                  "as_name": null,
                  "imported_items": [
                    "MHMService",
                    "InitializationError"
                  ]
                },
                {
                  "module": "service_utilities",
                  "as_name": null,
                  "imported_items": [
                    "Throttler",
                    "InvalidTimeFormatError",
                    "create_reschedule_request",
                    "is_service_running",
                    "get_service_processes",
                    "is_headless_service_running",
                    "is_ui_service_running",
                    "wait_for_network",
                    "load_and_localize_datetime"
                  ]
                },
                {
                  "module": "headless_service",
                  "as_name": null,
                  "imported_items": [
                    "HeadlessServiceManager"
                  ]
                },
                {
                  "module": "file_operations",
                  "as_name": null,
                  "imported_items": [
                    "create_user_files"
                  ]
                },
                {
                  "module": "file_auditor",
                  "as_name": null,
                  "imported_items": [
                    "FileAuditor",
                    "start_auditor",
                    "stop_auditor",
                    "record_created"
                  ]
                },
                {
                  "module": "schedule_utilities",
                  "as_name": null,
                  "imported_items": [
                    "get_active_schedules",
                    "is_schedule_active",
                    "get_current_active_schedules"
                  ]
                },
                {
                  "module": "auto_cleanup",
                  "as_name": null,
                  "imported_items": [
                    "get_last_cleanup_timestamp",
                    "update_cleanup_timestamp",
                    "should_run_cleanup",
                    "perform_cleanup",
                    "auto_cleanup_if_needed",
                    "cleanup_data_directory",
                    "cleanup_tests_data_directory",
                    "archive_old_messages_for_all_users",
                    "get_cleanup_status"
                  ]
                },
                {
                  "module": "backup_manager",
                  "as_name": null,
                  "imported_items": [
                    "BackupManager"
                  ]
                },
                {
                  "module": "checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "DynamicCheckinManager"
                  ]
                },
                {
                  "module": "checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_ai_error"
                  ]
                },
                {
                  "module": "config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir",
                    "get_user_file_path"
                  ]
                },
                {
                  "module": "checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index",
                    "rebuild_user_index",
                    "UserDataManager",
                    "update_message_references",
                    "backup_user_data",
                    "export_user_data",
                    "delete_user_completely",
                    "get_user_data_summary",
                    "get_user_info_for_data_manager",
                    "build_user_index",
                    "get_user_summary",
                    "get_all_user_summaries",
                    "get_user_analytics_summary"
                  ]
                },
                {
                  "module": "user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_categories",
                    "clear_user_caches",
                    "register_default_loaders",
                    "get_available_data_types",
                    "get_data_type_info",
                    "create_default_schedule_periods",
                    "migrate_legacy_schedules_structure",
                    "ensure_category_has_default_schedule"
                  ]
                },
                {
                  "module": "scheduler",
                  "as_name": null,
                  "imported_items": [
                    "SchedulerManager"
                  ]
                },
                {
                  "module": "schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "add_schedule_period"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 34
          },
          "tasks/task_management.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "uuid",
                  "as_name": null,
                  "imported_items": [
                    "uuid"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime",
                    "timedelta"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Optional",
                    "Any"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "load_json_data",
                    "save_json_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_full",
                    "DATE_ONLY"
                  ]
                },
                {
                  "module": "core.service",
                  "as_name": null,
                  "imported_items": [
                    "get_scheduler_manager"
                  ]
                },
                {
                  "module": "core.service",
                  "as_name": null,
                  "imported_items": [
                    "get_scheduler_manager"
                  ]
                },
                {
                  "module": "core.tags",
                  "as_name": null,
                  "imported_items": [
                    "add_user_tag"
                  ]
                },
                {
                  "module": "core.tags",
                  "as_name": null,
                  "imported_items": [
                    "get_user_tags"
                  ]
                },
                {
                  "module": "core.tags",
                  "as_name": null,
                  "imported_items": [
                    "remove_user_tag"
                  ]
                }
              ]
            },
            "total_imports": 15
          },
          "tasks/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "task_management",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementError",
                    "create_task",
                    "update_task",
                    "complete_task",
                    "delete_task",
                    "load_active_tasks",
                    "save_active_tasks",
                    "get_task_by_id",
                    "get_tasks_due_soon",
                    "get_user_task_stats",
                    "ensure_task_directory",
                    "add_user_task_tag",
                    "remove_user_task_tag",
                    "setup_default_task_tags",
                    "are_tasks_enabled",
                    "load_completed_tasks",
                    "restore_task",
                    "save_completed_tasks",
                    "schedule_task_reminders",
                    "cleanup_task_reminders"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 1
          },
          "ui/generate_ui_files.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "subprocess",
                  "as_name": null,
                  "imported_items": [
                    "subprocess"
                  ]
                },
                {
                  "module": "sys",
                  "as_name": null,
                  "imported_items": [
                    "sys"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_full"
                  ]
                }
              ]
            },
            "total_imports": 7
          },
          "ui/ui_app_qt.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "sys",
                  "as_name": null,
                  "imported_items": [
                    "sys"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "subprocess",
                  "as_name": null,
                  "imported_items": [
                    "subprocess"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "sys",
                  "as_name": null,
                  "imported_items": [
                    "sys"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "webbrowser",
                  "as_name": null,
                  "imported_items": [
                    "webbrowser"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "psutil",
                  "as_name": null,
                  "imported_items": [
                    "psutil"
                  ]
                },
                {
                  "module": "run_mhm",
                  "as_name": null,
                  "imported_items": [
                    "resolve_python_interpreter",
                    "prepare_launch_environment"
                  ]
                },
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QApplication",
                    "QMainWindow",
                    "QMessageBox",
                    "QDialog",
                    "QVBoxLayout",
                    "QHBoxLayout",
                    "QLabel",
                    "QPushButton",
                    "QTextEdit",
                    "QWidget"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "QTimer"
                  ]
                },
                {
                  "module": "PySide6.QtGui",
                  "as_name": null,
                  "imported_items": [
                    "QFont"
                  ]
                },
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QTabWidget",
                    "QTextEdit",
                    "QScrollArea"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_FULL"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "validate_all_configuration"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.service_utilities",
                  "as_name": null,
                  "imported_items": [
                    "get_flags_dir"
                  ]
                },
                {
                  "module": "user.user_context",
                  "as_name": null,
                  "imported_items": [
                    "UserContext"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_all_user_ids"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "_shared__title_case"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "core.config"
                  ]
                },
                {
                  "module": "ui.generated.admin_panel_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_ui_app_mainwindow"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "rebuild_user_index"
                  ]
                },
                {
                  "module": "core.scheduler",
                  "as_name": null,
                  "imported_items": [
                    "run_full_scheduler_standalone"
                  ]
                },
                {
                  "module": "core.scheduler",
                  "as_name": null,
                  "imported_items": [
                    "run_user_scheduler_standalone"
                  ]
                },
                {
                  "module": "core.scheduler",
                  "as_name": null,
                  "imported_items": [
                    "run_category_scheduler_standalone"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "toggle_verbose_logging",
                    "get_verbose_mode"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "LOG_MAIN_FILE"
                  ]
                },
                {
                  "module": "core.auto_cleanup",
                  "as_name": null,
                  "imported_items": [
                    "get_cleanup_status",
                    "find_pycache_dirs",
                    "find_pyc_files",
                    "calculate_cache_size"
                  ]
                },
                {
                  "module": "core.auto_cleanup",
                  "as_name": null,
                  "imported_items": [
                    "perform_cleanup",
                    "update_cleanup_timestamp"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "validate_all_configuration"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "BASE_DATA_DIR",
                    "LOG_MAIN_FILE",
                    "LOG_LEVEL",
                    "LM_STUDIO_BASE_URL",
                    "AI_TIMEOUT_SECONDS",
                    "SCHEDULER_INTERVAL",
                    "EMAIL_SMTP_SERVER",
                    "EMAIL_IMAP_SERVER",
                    "EMAIL_SMTP_USERNAME",
                    "DISCORD_BOT_TOKEN"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "DISCORD_BOT_TOKEN"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "EMAIL_SMTP_SERVER",
                    "EMAIL_IMAP_SERVER",
                    "EMAIL_SMTP_USERNAME",
                    "EMAIL_SMTP_PASSWORD"
                  ]
                },
                {
                  "module": "ui.dialogs.account_creator_dialog",
                  "as_name": null,
                  "imported_items": [
                    "AccountCreatorDialog"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "ui.dialogs.channel_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "ChannelManagementDialog"
                  ]
                },
                {
                  "module": "ui.dialogs.category_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "CategoryManagementDialog"
                  ]
                },
                {
                  "module": "ui.dialogs.checkin_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "CheckinManagementDialog"
                  ]
                },
                {
                  "module": "ui.dialogs.task_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementDialog"
                  ]
                },
                {
                  "module": "ui.dialogs.task_crud_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskCrudDialog"
                  ]
                },
                {
                  "module": "ui.dialogs.user_profile_dialog",
                  "as_name": null,
                  "imported_items": [
                    "UserProfileDialog"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "update_user_context"
                  ]
                },
                {
                  "module": "ui.dialogs.user_analytics_dialog",
                  "as_name": null,
                  "imported_items": [
                    "open_user_analytics_dialog"
                  ]
                },
                {
                  "module": "ui.dialogs.message_editor_dialog",
                  "as_name": null,
                  "imported_items": [
                    "open_message_editor_dialog"
                  ]
                },
                {
                  "module": "ui.dialogs.schedule_editor_dialog",
                  "as_name": null,
                  "imported_items": [
                    "open_schedule_editor"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "are_tasks_enabled",
                    "load_active_tasks"
                  ]
                },
                {
                  "module": "core.scheduler",
                  "as_name": null,
                  "imported_items": [
                    "SchedulerManager"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                },
                {
                  "module": "ui.dialogs.process_watcher_dialog",
                  "as_name": null,
                  "imported_items": [
                    "ProcessWatcherDialog"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "save_user_data"
                  ]
                },
                {
                  "module": "communication.core.channel_orchestrator",
                  "as_name": null,
                  "imported_items": [
                    "CommunicationManager"
                  ]
                }
              ]
            },
            "total_imports": 71
          },
          "ui/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "ui_app_qt",
                  "as_name": null,
                  "imported_items": [
                    "MHMManagerUI",
                    "ServiceManager"
                  ]
                },
                {
                  "module": "dialogs.account_creator_dialog",
                  "as_name": null,
                  "imported_items": [
                    "AccountCreatorDialog"
                  ]
                },
                {
                  "module": "dialogs.user_profile_dialog",
                  "as_name": null,
                  "imported_items": [
                    "UserProfileDialog"
                  ]
                },
                {
                  "module": "dialogs.task_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementDialog"
                  ]
                },
                {
                  "module": "dialogs.checkin_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "CheckinManagementDialog"
                  ]
                },
                {
                  "module": "dialogs.category_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "CategoryManagementDialog"
                  ]
                },
                {
                  "module": "dialogs.channel_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "ChannelManagementDialog"
                  ]
                },
                {
                  "module": "dialogs.process_watcher_dialog",
                  "as_name": null,
                  "imported_items": [
                    "ProcessWatcherDialog"
                  ]
                },
                {
                  "module": "dialogs.task_completion_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskCompletionDialog"
                  ]
                },
                {
                  "module": "dialogs.task_crud_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskCrudDialog"
                  ]
                },
                {
                  "module": "dialogs.admin_panel",
                  "as_name": null,
                  "imported_items": [
                    "AdminPanelDialog"
                  ]
                },
                {
                  "module": "dialogs.message_editor_dialog",
                  "as_name": null,
                  "imported_items": [
                    "MessageEditDialog",
                    "MessageEditorDialog",
                    "open_message_editor_dialog"
                  ]
                },
                {
                  "module": "dialogs.schedule_editor_dialog",
                  "as_name": null,
                  "imported_items": [
                    "ScheduleEditorDialog",
                    "open_schedule_editor"
                  ]
                },
                {
                  "module": "dialogs.user_analytics_dialog",
                  "as_name": null,
                  "imported_items": [
                    "UserAnalyticsDialog",
                    "open_user_analytics_dialog"
                  ]
                },
                {
                  "module": "dialogs.user_profile_dialog",
                  "as_name": null,
                  "imported_items": [
                    "open_personalization_dialog"
                  ]
                },
                {
                  "module": "dialogs.account_creator_dialog",
                  "as_name": null,
                  "imported_items": [
                    "create_account_dialog"
                  ]
                },
                {
                  "module": "widgets.dynamic_list_container",
                  "as_name": null,
                  "imported_items": [
                    "DynamicListContainer"
                  ]
                },
                {
                  "module": "widgets.period_row_widget",
                  "as_name": null,
                  "imported_items": [
                    "PeriodRowWidget"
                  ]
                },
                {
                  "module": "widgets.category_selection_widget",
                  "as_name": null,
                  "imported_items": [
                    "CategorySelectionWidget"
                  ]
                },
                {
                  "module": "widgets.channel_selection_widget",
                  "as_name": null,
                  "imported_items": [
                    "ChannelSelectionWidget"
                  ]
                },
                {
                  "module": "widgets.task_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "TaskSettingsWidget"
                  ]
                },
                {
                  "module": "widgets.checkin_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "CheckinSettingsWidget"
                  ]
                },
                {
                  "module": "widgets.tag_widget",
                  "as_name": null,
                  "imported_items": [
                    "TagWidget"
                  ]
                },
                {
                  "module": "widgets.dynamic_list_field",
                  "as_name": null,
                  "imported_items": [
                    "DynamicListField"
                  ]
                },
                {
                  "module": "widgets.user_profile_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "UserProfileSettingsWidget"
                  ]
                },
                {
                  "module": "dialogs.task_edit_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskEditDialog"
                  ]
                },
                {
                  "module": "generate_ui_files",
                  "as_name": null,
                  "imported_items": [
                    "generate_ui_file",
                    "generate_all_ui_files"
                  ]
                },
                {
                  "module": "ui_app_qt",
                  "as_name": null,
                  "imported_items": [
                    "main"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 28
          },
          "ui/dialogs/account_creator_dialog.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "uuid",
                  "as_name": null,
                  "imported_items": [
                    "uuid"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any"
                  ]
                },
                {
                  "module": "warnings",
                  "as_name": null,
                  "imported_items": [
                    "warnings"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "time",
                  "as_name": null,
                  "imported_items": [
                    "time"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QMessageBox",
                    "QSizePolicy",
                    "QDialogButtonBox"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Signal",
                    "Qt"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "validate_schedule_periods"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "ui.widgets.category_selection_widget",
                  "as_name": null,
                  "imported_items": [
                    "CategorySelectionWidget"
                  ]
                },
                {
                  "module": "ui.widgets.channel_selection_widget",
                  "as_name": null,
                  "imported_items": [
                    "ChannelSelectionWidget"
                  ]
                },
                {
                  "module": "ui.widgets.task_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "TaskSettingsWidget"
                  ]
                },
                {
                  "module": "ui.widgets.checkin_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "CheckinSettingsWidget"
                  ]
                },
                {
                  "module": "ui.generated.account_creator_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_create_account"
                  ]
                },
                {
                  "module": "ui.dialogs.user_profile_dialog",
                  "as_name": null,
                  "imported_items": [
                    "open_personalization_dialog"
                  ]
                },
                {
                  "module": "core.file_operations",
                  "as_name": null,
                  "imported_items": [
                    "create_user_files"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data_dir"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "add_user_task_tag"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "setup_default_task_tags"
                  ]
                },
                {
                  "module": "core.user_data_manager",
                  "as_name": null,
                  "imported_items": [
                    "update_user_index"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_id_by_identifier"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.service",
                  "as_name": null,
                  "imported_items": [
                    "get_scheduler_manager"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "is_valid_email",
                    "is_valid_phone"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "is_valid_discord_id"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "is_valid_email"
                  ]
                }
              ]
            },
            "total_imports": 30
          },
          "ui/dialogs/admin_panel.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QVBoxLayout",
                    "QLabel",
                    "QWidget"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Qt"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors",
                    "DataError"
                  ]
                }
              ]
            },
            "total_imports": 4
          },
          "ui/dialogs/category_management_dialog.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QMessageBox"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Signal"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.category_management_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_category_management"
                  ]
                },
                {
                  "module": "ui.widgets.category_selection_widget",
                  "as_name": null,
                  "imported_items": [
                    "CategorySelectionWidget"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "update_user_preferences",
                    "update_user_account"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "clear_schedule_periods_cache"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "ui/dialogs/channel_management_dialog.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QMessageBox"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Signal"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.channel_management_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog"
                  ]
                },
                {
                  "module": "ui.widgets.channel_selection_widget",
                  "as_name": null,
                  "imported_items": [
                    "ChannelSelectionWidget"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "is_valid_email"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "update_channel_preferences",
                    "update_user_account"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 8
          },
          "ui/dialogs/checkin_management_dialog.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QMessageBox",
                    "QWidget"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Signal"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "set_schedule_periods",
                    "clear_schedule_periods_cache"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "update_user_preferences",
                    "update_user_account"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "validate_schedule_periods"
                  ]
                },
                {
                  "module": "ui.widgets.checkin_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "CheckinSettingsWidget"
                  ]
                },
                {
                  "module": "ui.generated.checkin_management_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_checkin_management"
                  ]
                }
              ]
            },
            "total_imports": 10
          },
          "ui/dialogs/message_editor_dialog.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "uuid",
                  "as_name": null,
                  "imported_items": [
                    "uuid"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QMessageBox",
                    "QVBoxLayout",
                    "QHBoxLayout",
                    "QLabel",
                    "QTextEdit",
                    "QPushButton",
                    "QTableWidgetItem",
                    "QCheckBox",
                    "QGroupBox",
                    "QFormLayout",
                    "QLineEdit",
                    "QDialogButtonBox",
                    "QWidget",
                    "QScrollArea"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Qt",
                    "Signal"
                  ]
                },
                {
                  "module": "PySide6.QtGui",
                  "as_name": null,
                  "imported_items": [
                    "QFont"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.message_editor_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_message_editor"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "load_user_messages",
                    "add_message",
                    "edit_message",
                    "delete_message"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "ui/dialogs/process_watcher_dialog.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QVBoxLayout",
                    "QHBoxLayout",
                    "QLabel",
                    "QPushButton",
                    "QTableWidget",
                    "QTableWidgetItem",
                    "QHeaderView",
                    "QTextEdit",
                    "QTabWidget",
                    "QWidget"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Qt",
                    "QTimer"
                  ]
                },
                {
                  "module": "PySide6.QtGui",
                  "as_name": null,
                  "imported_items": [
                    "QFont"
                  ]
                },
                {
                  "module": "psutil",
                  "as_name": null,
                  "imported_items": [
                    "psutil"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "TIMESTAMP_FULL"
                  ]
                },
                {
                  "module": "core.service_utilities",
                  "as_name": null,
                  "imported_items": [
                    "get_service_processes"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "ui/dialogs/schedule_editor_dialog.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "Optional",
                    "Callable"
                  ]
                },
                {
                  "module": "pathlib",
                  "as_name": null,
                  "imported_items": [
                    "Path"
                  ]
                },
                {
                  "module": "json",
                  "as_name": null,
                  "imported_items": [
                    "json"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QMessageBox"
                  ]
                },
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QMessageBox"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "clear_schedule_periods_cache",
                    "set_schedule_periods"
                  ]
                },
                {
                  "module": "core.ui_management",
                  "as_name": null,
                  "imported_items": [
                    "load_period_widgets_for_category",
                    "collect_period_data_from_widgets"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "_shared__title_case",
                    "validate_schedule_periods"
                  ]
                },
                {
                  "module": "core.time_utilities",
                  "as_name": null,
                  "imported_items": [
                    "now_timestamp_filename"
                  ]
                },
                {
                  "module": "ui.widgets.period_row_widget",
                  "as_name": null,
                  "imported_items": [
                    "PeriodRowWidget"
                  ]
                },
                {
                  "module": "ui.generated.schedule_editor_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_edit_schedule"
                  ]
                },
                {
                  "module": "core.config",
                  "as_name": null,
                  "imported_items": [
                    "core.config"
                  ]
                }
              ]
            },
            "total_imports": 17
          },
          "ui/dialogs/task_completion_dialog.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QButtonGroup"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "QDate",
                    "QTime"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.task_completion_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_task_completion"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                }
              ]
            },
            "total_imports": 5
          },
          "ui/dialogs/task_crud_dialog.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QMessageBox",
                    "QTableWidgetItem",
                    "QHeaderView"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Qt"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.task_crud_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_task_crud"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "load_active_tasks",
                    "load_completed_tasks",
                    "get_user_task_stats",
                    "get_tasks_due_soon",
                    "complete_task",
                    "delete_task"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "ui.dialogs.task_edit_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskEditDialog"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "ui.dialogs.task_edit_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskEditDialog"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "ui.dialogs.task_completion_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskCompletionDialog"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_task_by_id"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "restore_task"
                  ]
                }
              ]
            },
            "total_imports": 15
          },
          "ui/dialogs/task_edit_dialog.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QMessageBox",
                    "QWidget",
                    "QHBoxLayout",
                    "QLabel",
                    "QComboBox",
                    "QDateEdit",
                    "QPushButton",
                    "QButtonGroup"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "QDate",
                    "QTime"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.task_edit_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_task_edit"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "create_task",
                    "update_task"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "ui.widgets.tag_widget",
                  "as_name": null,
                  "imported_items": [
                    "TagWidget"
                  ]
                }
              ]
            },
            "total_imports": 7
          },
          "ui/dialogs/task_management_dialog.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QMessageBox",
                    "QWidget"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Signal"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.task_management_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_task_management"
                  ]
                },
                {
                  "module": "ui.widgets.task_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "TaskSettingsWidget"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "set_schedule_periods",
                    "clear_schedule_periods_cache"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "update_user_account"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "validate_schedule_periods"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "setup_default_task_tags"
                  ]
                }
              ]
            },
            "total_imports": 11
          },
          "ui/dialogs/user_analytics_dialog.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                },
                {
                  "module": "os",
                  "as_name": null,
                  "imported_items": [
                    "os"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QVBoxLayout",
                    "QHBoxLayout",
                    "QLabel",
                    "QTextEdit",
                    "QPushButton",
                    "QComboBox",
                    "QTabWidget",
                    "QWidget",
                    "QFrame"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Qt",
                    "QThread",
                    "Signal"
                  ]
                },
                {
                  "module": "PySide6.QtGui",
                  "as_name": null,
                  "imported_items": [
                    "QFont",
                    "QColor",
                    "QPalette",
                    "QPainter",
                    "QPen",
                    "QBrush"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.user_analytics_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_user_analytics"
                  ]
                },
                {
                  "module": "core.checkin_analytics",
                  "as_name": null,
                  "imported_items": [
                    "CheckinAnalytics"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_checkins_by_days"
                  ]
                }
              ]
            },
            "total_imports": 14
          },
          "ui/dialogs/user_profile_dialog.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                },
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "Optional",
                    "Callable"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QVBoxLayout",
                    "QHBoxLayout",
                    "QLabel",
                    "QPushButton",
                    "QLineEdit",
                    "QCheckBox",
                    "QComboBox",
                    "QTextEdit",
                    "QGroupBox",
                    "QGridLayout",
                    "QMessageBox",
                    "QFrame"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Qt",
                    "Signal"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.user_profile_management_dialog_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Dialog_user_profile"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_predefined_options"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "validate_personalization_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "ui.widgets.user_profile_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "UserProfileSettingsWidget"
                  ]
                },
                {
                  "module": "ui.widgets.dynamic_list_container",
                  "as_name": null,
                  "imported_items": [
                    "DynamicListContainer"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "update_user_context"
                  ]
                }
              ]
            },
            "total_imports": 13
          },
          "ui/dialogs/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "account_creator_dialog",
                  "as_name": null,
                  "imported_items": [
                    "AccountCreatorDialog"
                  ]
                },
                {
                  "module": "user_profile_dialog",
                  "as_name": null,
                  "imported_items": [
                    "UserProfileDialog"
                  ]
                },
                {
                  "module": "task_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "TaskManagementDialog"
                  ]
                },
                {
                  "module": "checkin_management_dialog",
                  "as_name": null,
                  "imported_items": [
                    "CheckinManagementDialog"
                  ]
                },
                {
                  "module": "schedule_editor_dialog",
                  "as_name": null,
                  "imported_items": [
                    "ScheduleEditorDialog"
                  ]
                },
                {
                  "module": "message_editor_dialog",
                  "as_name": null,
                  "imported_items": [
                    "MessageEditorDialog",
                    "MessageEditDialog"
                  ]
                },
                {
                  "module": "user_analytics_dialog",
                  "as_name": null,
                  "imported_items": [
                    "UserAnalyticsDialog"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 7
          },
          "ui/widgets/category_selection_widget.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QWidget"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.category_selection_widget_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Form_category_selection_widget"
                  ]
                },
                {
                  "module": "core.user_data_validation",
                  "as_name": null,
                  "imported_items": [
                    "_shared__title_case"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                }
              ]
            },
            "total_imports": 5
          },
          "ui/widgets/channel_selection_widget.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QWidget"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.channel_selection_widget_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Form_channel_selection"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_timezone_options"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 5
          },
          "ui/widgets/checkin_settings_widget.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "re",
                  "as_name": null,
                  "imported_items": [
                    "re"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QWidget",
                    "QMessageBox",
                    "QCheckBox",
                    "QHBoxLayout",
                    "QPushButton",
                    "QVBoxLayout",
                    "QGroupBox",
                    "QLabel",
                    "QSpinBox",
                    "QComboBox",
                    "QFormLayout"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Qt"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "QTimer"
                  ]
                },
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QDialog",
                    "QVBoxLayout",
                    "QHBoxLayout",
                    "QLabel",
                    "QLineEdit",
                    "QComboBox",
                    "QPushButton",
                    "QDialogButtonBox",
                    "QFormLayout",
                    "QGroupBox",
                    "QTextEdit"
                  ]
                },
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QMessageBox"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.checkin_settings_widget_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Form_checkin_settings"
                  ]
                },
                {
                  "module": "core.ui_management",
                  "as_name": null,
                  "imported_items": [
                    "load_period_widgets_for_category",
                    "collect_period_data_from_widgets"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "ui.widgets.period_row_widget",
                  "as_name": null,
                  "imported_items": [
                    "PeriodRowWidget"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                },
                {
                  "module": "core.checkin_dynamic_manager",
                  "as_name": null,
                  "imported_items": [
                    "dynamic_checkin_manager"
                  ]
                }
              ]
            },
            "total_imports": 20
          },
          "ui/widgets/dynamic_list_container.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QWidget",
                    "QVBoxLayout",
                    "QGridLayout"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Signal"
                  ]
                },
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QMessageBox"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.widgets.dynamic_list_field",
                  "as_name": null,
                  "imported_items": [
                    "DynamicListField"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_predefined_options"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                }
              ]
            },
            "total_imports": 13
          },
          "ui/widgets/dynamic_list_field.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QWidget"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Signal"
                  ]
                },
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QSizePolicy"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.dynamic_list_field_template_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Form_dynamic_list_field_template"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "ui.widgets.dynamic_list_container",
                  "as_name": null,
                  "imported_items": [
                    "DynamicListContainer"
                  ]
                },
                {
                  "module": "ui.widgets.dynamic_list_container",
                  "as_name": null,
                  "imported_items": [
                    "DynamicListContainer"
                  ]
                },
                {
                  "module": "ui.widgets.dynamic_list_container",
                  "as_name": null,
                  "imported_items": [
                    "DynamicListContainer"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "ui/widgets/period_row_widget.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "List",
                    "Optional"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QWidget",
                    "QButtonGroup"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Signal"
                  ]
                }
              ],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "get_period_data__time_24h_to_12h_display",
                    "get_period_data__time_12h_display_to_24h"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "ui.generated.period_row_template_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Form_period_row_template"
                  ]
                }
              ]
            },
            "total_imports": 7
          },
          "ui/widgets/tag_widget.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QWidget",
                    "QListWidgetItem",
                    "QInputDialog",
                    "QMessageBox"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Qt",
                    "Signal"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.tag_widget_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Widget_tag"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "add_user_task_tag",
                    "remove_user_task_tag"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                }
              ]
            },
            "total_imports": 7
          },
          "ui/widgets/task_settings_widget.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QWidget",
                    "QMessageBox"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.task_settings_widget_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Form_task_settings"
                  ]
                },
                {
                  "module": "tasks.task_management",
                  "as_name": null,
                  "imported_items": [
                    "get_user_task_stats"
                  ]
                },
                {
                  "module": "core.ui_management",
                  "as_name": null,
                  "imported_items": [
                    "load_period_widgets_for_category",
                    "collect_period_data_from_widgets"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "update_user_preferences"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "ui.widgets.period_row_widget",
                  "as_name": null,
                  "imported_items": [
                    "PeriodRowWidget"
                  ]
                },
                {
                  "module": "ui.widgets.tag_widget",
                  "as_name": null,
                  "imported_items": [
                    "TagWidget"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "ui/widgets/user_profile_settings_widget.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "Any",
                    "Optional"
                  ]
                }
              ],
              "third_party": [
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QWidget",
                    "QVBoxLayout"
                  ]
                },
                {
                  "module": "PySide6.QtCore",
                  "as_name": null,
                  "imported_items": [
                    "Qt",
                    "QDate"
                  ]
                },
                {
                  "module": "PySide6.QtWidgets",
                  "as_name": null,
                  "imported_items": [
                    "QLineEdit",
                    "QLabel"
                  ]
                }
              ],
              "local": [
                {
                  "module": "ui.generated.user_profile_settings_widget_pyqt",
                  "as_name": null,
                  "imported_items": [
                    "Ui_Form_user_profile_settings"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "ui.widgets.dynamic_list_container",
                  "as_name": null,
                  "imported_items": [
                    "DynamicListContainer"
                  ]
                }
              ]
            },
            "total_imports": 8
          },
          "ui/widgets/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "task_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "TaskSettingsWidget"
                  ]
                },
                {
                  "module": "checkin_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "CheckinSettingsWidget"
                  ]
                },
                {
                  "module": "user_profile_settings_widget",
                  "as_name": null,
                  "imported_items": [
                    "UserProfileSettingsWidget"
                  ]
                },
                {
                  "module": "channel_selection_widget",
                  "as_name": null,
                  "imported_items": [
                    "ChannelSelectionWidget"
                  ]
                },
                {
                  "module": "category_selection_widget",
                  "as_name": null,
                  "imported_items": [
                    "CategorySelectionWidget"
                  ]
                },
                {
                  "module": "tag_widget",
                  "as_name": null,
                  "imported_items": [
                    "TagWidget"
                  ]
                },
                {
                  "module": "period_row_widget",
                  "as_name": null,
                  "imported_items": [
                    "PeriodRowWidget"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 7
          },
          "user/context_manager.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "datetime",
                  "as_name": null,
                  "imported_items": [
                    "datetime"
                  ]
                },
                {
                  "module": "typing",
                  "as_name": null,
                  "imported_items": [
                    "Dict",
                    "List",
                    "Any"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.response_tracking",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_checkins",
                    "get_recent_chat_interactions"
                  ]
                },
                {
                  "module": "core.message_management",
                  "as_name": null,
                  "imported_items": [
                    "get_recent_messages"
                  ]
                },
                {
                  "module": "core.schedule_utilities",
                  "as_name": null,
                  "imported_items": [
                    "get_active_schedules"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "user.user_context",
                  "as_name": null,
                  "imported_items": [
                    "UserContext"
                  ]
                }
              ]
            },
            "total_imports": 9
          },
          "user/user_context.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "threading",
                  "as_name": null,
                  "imported_items": [
                    "threading"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                },
                {
                  "module": "core.schedule_utilities",
                  "as_name": null,
                  "imported_items": [
                    "get_active_schedules"
                  ]
                },
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "save_user_data"
                  ]
                }
              ]
            },
            "total_imports": 6
          },
          "user/user_preferences.py": {
            "imports": {
              "standard_library": [],
              "third_party": [],
              "local": [
                {
                  "module": "core.user_data_handlers",
                  "as_name": null,
                  "imported_items": [
                    "get_user_data",
                    "update_user_preferences"
                  ]
                },
                {
                  "module": "core.schedule_management",
                  "as_name": null,
                  "imported_items": [
                    "set_schedule_period_active",
                    "is_schedule_period_active"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 4
          },
          "user/__init__.py": {
            "imports": {
              "standard_library": [],
              "third_party": [
                {
                  "module": "context_manager",
                  "as_name": null,
                  "imported_items": [
                    "UserContextManager",
                    "user_context_manager"
                  ]
                },
                {
                  "module": "user_context",
                  "as_name": null,
                  "imported_items": [
                    "UserContext"
                  ]
                },
                {
                  "module": "user_preferences",
                  "as_name": null,
                  "imported_items": [
                    "UserPreferences"
                  ]
                }
              ],
              "local": []
            },
            "total_imports": 3
          },
          "create_backup.py": {
            "imports": {
              "standard_library": [],
              "third_party": [],
              "local": [
                {
                  "module": "core.backup_manager",
                  "as_name": null,
                  "imported_items": [
                    "BackupManager"
                  ]
                }
              ]
            },
            "total_imports": 1
          },
          "run_headless_service.py": {
            "imports": {
              "standard_library": [
                {
                  "module": "sys",
                  "as_name": null,
                  "imported_items": [
                    "sys"
                  ]
                }
              ],
              "third_party": [],
              "local": [
                {
                  "module": "core.headless_service",
                  "as_name": null,
                  "imported_items": [
                    "HeadlessServiceManager"
                  ]
                },
                {
                  "module": "core.logger",
                  "as_name": null,
                  "imported_items": [
                    "setup_logging",
                    "get_component_logger"
                  ]
                },
                {
                  "module": "core.error_handling",
                  "as_name": null,
                  "imported_items": [
                    "handle_errors"
                  ]
                }
              ]
            },
            "total_imports": 4
          }
        }
      },
      "timestamp": "2026-01-18T01:46:32"
    },
    "analyze_unused_imports": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 532,
          "files_affected": 185,
          "status": "CRITICAL"
        },
        "files": {
          "ai/cache_manager.py": 3,
          "ai/chatbot.py": 2,
          "ai/context_builder.py": 3,
          "ai/conversation_history.py": 3,
          "ai/prompt_manager.py": 2,
          "communication/command_handlers/account_handler.py": 3,
          "communication/command_handlers/analytics_handler.py": 2,
          "communication/command_handlers/base_handler.py": 2,
          "communication/command_handlers/checkin_handler.py": 2,
          "communication/command_handlers/interaction_handlers.py": 4,
          "communication/command_handlers/notebook_handler.py": 4,
          "communication/command_handlers/profile_handler.py": 2,
          "communication/command_handlers/schedule_handler.py": 2,
          "communication/command_handlers/shared_types.py": 3,
          "communication/command_handlers/task_handler.py": 3,
          "communication/communication_channels/base/base_channel.py": 3,
          "communication/communication_channels/base/command_registry.py": 3,
          "communication/communication_channels/base/message_formatter.py": 3,
          "communication/communication_channels/base/rich_formatter.py": 2,
          "communication/communication_channels/discord/api_client.py": 4,
          "communication/communication_channels/discord/bot.py": 3,
          "communication/communication_channels/discord/event_handler.py": 3,
          "communication/communication_channels/discord/webhook_handler.py": 2,
          "communication/communication_channels/discord/welcome_handler.py": 1,
          "communication/communication_channels/email/bot.py": 4,
          "communication/core/channel_monitor.py": 2,
          "communication/core/channel_orchestrator.py": 4,
          "communication/core/factory.py": 5,
          "communication/core/welcome_manager.py": 2,
          "communication/message_processing/command_parser.py": 3,
          "communication/message_processing/conversation_flow_manager.py": 3,
          "communication/message_processing/interaction_manager.py": 3,
          "communication/message_processing/message_router.py": 3,
          "core/checkin_analytics.py": 3,
          "core/checkin_dynamic_manager.py": 3,
          "core/config.py": 4,
          "core/error_handling.py": 3,
          "core/file_auditor.py": 3,
          "core/file_operations.py": 1,
          "core/headless_service.py": 1,
          "core/schedule_management.py": 2,
          "core/schedule_utilities.py": 3,
          "core/scheduler.py": 2,
          "core/schemas.py": 3,
          "core/user_data_handlers.py": 5,
          "core/user_data_manager.py": 3,
          "core/user_data_validation.py": 4,
          "development_tools/docs/generate_directory_tree.py": 1,
          "development_tools/functions/generate_function_docstrings.py": 1,
          "development_tools/imports/generate_unused_imports_report.py": 1,
          "development_tools/shared/common.py": 1,
          "notebook/notebook_data_handlers.py": 1,
          "notebook/notebook_data_manager.py": 3,
          "notebook/notebook_validation.py": 2,
          "notebook/schemas.py": 4,
          "tasks/task_management.py": 3,
          "tests/ai/ai_response_validator.py": 4,
          "tests/conftest.py": 3,
          "tests/ui/test_account_creation_ui.py": 17,
          "tests/unit/test_cleanup.py": 3,
          "ui/generate_ui_files.py": 1,
          "ui/widgets/period_row_widget.py": 3,
          "ui/widgets/user_profile_settings_widget.py": 2,
          "user/context_manager.py": 2,
          "communication/communication_channels/discord/account_flow_handler.py": 1,
          "core/message_management.py": 3,
          "tests/test_utilities.py": 10,
          "tests/ui/test_signal_handler_integration.py": 6,
          "tests/ai/test_ai_functionality_manual.py": 2,
          "tests/ai/test_cache_manager.py": 1,
          "tests/behavior/test_base_handler_behavior.py": 2,
          "tests/behavior/test_checkin_handler_behavior.py": 2,
          "tests/behavior/test_communication_manager_behavior.py": 2,
          "tests/behavior/test_interaction_handlers_behavior.py": 2,
          "tests/behavior/test_logger_coverage_expansion.py": 1,
          "tests/behavior/test_message_router_behavior.py": 2,
          "tests/behavior/test_notebook_handler_behavior.py": 3,
          "tests/behavior/test_profile_handler_behavior.py": 1,
          "tests/behavior/test_schedule_handler_behavior.py": 1,
          "tests/behavior/test_scheduler_coverage_expansion.py": 2,
          "tests/behavior/test_task_handler_behavior.py": 1,
          "tests/behavior/test_user_data_flow_architecture.py": 2,
          "tests/behavior/test_webhook_server_behavior.py": 2,
          "tests/behavior/test_welcome_handler_behavior.py": 1,
          "tests/core/test_file_auditor.py": 1,
          "tests/core/test_message_management.py": 4,
          "tests/core/test_schedule_utilities.py": 2,
          "tests/development_tools/test_analyze_ai_work.py": 5,
          "tests/development_tools/test_analyze_error_handling.py": 4,
          "tests/development_tools/test_analyze_function_registry.py": 5,
          "tests/development_tools/test_analyze_functions.py": 4,
          "tests/development_tools/test_analyze_module_dependencies.py": 5,
          "tests/development_tools/test_analyze_unused_imports.py": 4,
          "tests/development_tools/test_decision_support.py": 2,
          "tests/development_tools/test_false_negative_detection.py": 4,
          "tests/development_tools/test_fix_documentation.py": 2,
          "tests/development_tools/test_generate_error_handling_report.py": 2,
          "tests/development_tools/test_generate_function_docstrings.py": 4,
          "tests/development_tools/test_generate_unused_imports_report.py": 5,
          "tests/development_tools/test_output_storage_archiving.py": 3,
          "tests/development_tools/test_regenerate_coverage_metrics.py": 2,
          "tests/integration/test_task_cleanup_real_bug_verification.py": 1,
          "tests/integration/test_task_cleanup_silent_failure.py": 3,
          "tests/integration/test_user_creation.py": 6,
          "tests/test_error_handling_improvements.py": 5,
          "tests/ui/test_account_creator_dialog_validation.py": 1,
          "tests/ui/test_category_management_dialog.py": 5,
          "tests/ui/test_channel_management_dialog_coverage_expansion.py": 5,
          "tests/ui/test_dialog_behavior.py": 18,
          "tests/ui/test_dialog_coverage_expansion.py": 17,
          "tests/ui/test_message_editor_dialog.py": 4,
          "tests/ui/test_process_watcher_dialog.py": 4,
          "tests/ui/test_task_crud_dialog.py": 2,
          "tests/ui/test_task_management_dialog.py": 4,
          "tests/ui/test_task_settings_widget.py": 6,
          "tests/ui/test_ui_button_verification.py": 2,
          "tests/ui/test_ui_components_headless.py": 1,
          "tests/ui/test_ui_widgets_coverage_expansion.py": 8,
          "tests/ui/test_user_analytics_dialog.py": 4,
          "tests/ui/test_user_profile_dialog_coverage_expansion.py": 6,
          "tests/ui/test_widget_behavior.py": 17,
          "tests/ui/test_widget_behavior_simple.py": 5,
          "tests/unit/test_admin_panel.py": 4,
          "tests/unit/test_ai_chatbot_helpers.py": 2,
          "tests/unit/test_channel_orchestrator.py": 1,
          "tests/unit/test_checkin_management_dialog.py": 4,
          "tests/unit/test_checkin_view.py": 2,
          "tests/unit/test_command_parser_helpers.py": 1,
          "tests/unit/test_communication_core_init.py": 2,
          "tests/unit/test_communication_init.py": 2,
          "tests/unit/test_email_bot_body_extraction.py": 1,
          "tests/unit/test_enhanced_checkin_responses.py": 1,
          "tests/unit/test_file_operations.py": 2,
          "tests/unit/test_interaction_handlers_helpers.py": 2,
          "tests/unit/test_logger_unit.py": 3,
          "tests/unit/test_message_formatter.py": 2,
          "tests/unit/test_prompt_manager.py": 5,
          "tests/unit/test_recurring_tasks.py": 1,
          "tests/unit/test_ui_management.py": 1,
          "tests/unit/test_user_context.py": 2,
          "tests/unit/test_user_data_handlers.py": 1,
          "tests/unit/test_user_data_manager.py": 5,
          "tests/unit/test_user_management.py": 3,
          "tests/unit/test_user_preferences.py": 4,
          "tests/ai/test_ai_core.py": 1,
          "tests/ai/test_context_includes_recent_messages.py": 1,
          "tests/behavior/test_checkin_expiry_semantics.py": 1,
          "tests/behavior/test_conversation_flow_manager_behavior.py": 3,
          "tests/behavior/test_discord_checkin_retry_behavior.py": 1,
          "tests/behavior/test_discord_task_reminder_followup.py": 1,
          "tests/behavior/test_headless_service_behavior.py": 4,
          "tests/behavior/test_task_error_handling.py": 2,
          "tests/behavior/test_welcome_manager_behavior.py": 1,
          "tests/communication/test_retry_manager.py": 2,
          "tests/debug_file_paths.py": 1,
          "tests/development_tools/test_analysis_tool_validation.py": 2,
          "tests/development_tools/test_analyze_ascii_compliance.py": 2,
          "tests/development_tools/test_analyze_documentation.py": 2,
          "tests/development_tools/test_analyze_heading_numbering.py": 2,
          "tests/development_tools/test_analyze_missing_addresses.py": 4,
          "tests/development_tools/test_analyze_unconverted_links.py": 2,
          "tests/development_tools/test_audit_tier_e2e_verification.py": 1,
          "tests/development_tools/test_documentation_sync_checker.py": 1,
          "tests/development_tools/test_error_scenarios.py": 1,
          "tests/development_tools/test_exclusion_utilities.py": 1,
          "tests/development_tools/test_fix_documentation_addresses.py": 2,
          "tests/development_tools/test_fix_documentation_ascii.py": 2,
          "tests/development_tools/test_fix_documentation_headings.py": 1,
          "tests/development_tools/test_fix_documentation_links.py": 1,
          "tests/development_tools/test_fix_project_cleanup.py": 2,
          "tests/development_tools/test_generate_consolidated_report.py": 1,
          "tests/development_tools/test_generate_directory_tree.py": 2,
          "tests/development_tools/test_generate_function_registry.py": 1,
          "tests/development_tools/test_generate_module_dependencies.py": 1,
          "tests/development_tools/test_legacy_reference_cleanup.py": 1,
          "tests/development_tools/test_path_drift_detection.py": 3,
          "tests/development_tools/test_path_drift_integration.py": 3,
          "tests/development_tools/test_path_drift_verification_comprehensive.py": 2,
          "tests/development_tools/test_status_file_timing.py": 2,
          "tests/integration/test_account_management.py": 2,
          "tests/integration/test_task_cleanup_real.py": 1,
          "tests/integration/test_task_reminder_integration.py": 1,
          "tests/ui/test_dialogs.py": 1,
          "tests/unit/test_file_locking.py": 1,
          "ui/widgets/checkin_settings_widget.py": 1
        },
        "details": {
          "findings": {
            "obvious_unused": [
              {
                "file": "ai/cache_manager.py",
                "line": 6,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/cache_manager.py",
                "line": 6,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/cache_manager.py",
                "line": 6,
                "column": 0,
                "message": "Unused Tuple imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/chatbot.py",
                "line": 16,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/chatbot.py",
                "line": 44,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "ai/context_builder.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/context_builder.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/conversation_history.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/conversation_history.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/prompt_manager.py",
                "line": 4,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/prompt_manager.py",
                "line": 4,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/account_handler.py",
                "line": 7,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/account_handler.py",
                "line": 7,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/account_handler.py",
                "line": 7,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/analytics_handler.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/analytics_handler.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/base_handler.py",
                "line": 11,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/checkin_handler.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/checkin_handler.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/interaction_handlers.py",
                "line": 11,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/interaction_handlers.py",
                "line": 11,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/interaction_handlers.py",
                "line": 11,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/notebook_handler.py",
                "line": 10,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/notebook_handler.py",
                "line": 10,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/notebook_handler.py",
                "line": 10,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/notebook_handler.py",
                "line": 14,
                "column": 0,
                "message": "Unused TIMESTAMP_FULL imported from core.time_utilities",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/profile_handler.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/profile_handler.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/schedule_handler.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/schedule_handler.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/shared_types.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/shared_types.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/shared_types.py",
                "line": 3,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/task_handler.py",
                "line": 10,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/task_handler.py",
                "line": 10,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/base_channel.py",
                "line": 3,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/base_channel.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/base_channel.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/command_registry.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/command_registry.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/command_registry.py",
                "line": 3,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/message_formatter.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/message_formatter.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/message_formatter.py",
                "line": 3,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/rich_formatter.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/base/rich_formatter.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/api_client.py",
                "line": 6,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/api_client.py",
                "line": 6,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/api_client.py",
                "line": 6,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/api_client.py",
                "line": 6,
                "column": 0,
                "message": "Unused Union imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/bot.py",
                "line": 8,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/bot.py",
                "line": 8,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/bot.py",
                "line": 8,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/event_handler.py",
                "line": 5,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/event_handler.py",
                "line": 5,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/event_handler.py",
                "line": 5,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/webhook_handler.py",
                "line": 9,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/webhook_handler.py",
                "line": 9,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/welcome_handler.py",
                "line": 8,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/email/bot.py",
                "line": 10,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/email/bot.py",
                "line": 10,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/email/bot.py",
                "line": 10,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/email/bot.py",
                "line": 10,
                "column": 0,
                "message": "Unused Tuple imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/channel_monitor.py",
                "line": 5,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/channel_monitor.py",
                "line": 5,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/channel_orchestrator.py",
                "line": 8,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/channel_orchestrator.py",
                "line": 8,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/factory.py",
                "line": 1,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/factory.py",
                "line": 1,
                "column": 0,
                "message": "Unused Type imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/factory.py",
                "line": 1,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/welcome_manager.py",
                "line": 9,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/command_parser.py",
                "line": 14,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/command_parser.py",
                "line": 14,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/command_parser.py",
                "line": 14,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/conversation_flow_manager.py",
                "line": 19,
                "column": 0,
                "message": "Unused import os",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/conversation_flow_manager.py",
                "line": 23,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/conversation_flow_manager.py",
                "line": 32,
                "column": 0,
                "message": "Unused TIMESTAMP_FULL imported from core.time_utilities",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/interaction_manager.py",
                "line": 13,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/interaction_manager.py",
                "line": 13,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/interaction_manager.py",
                "line": 13,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/message_router.py",
                "line": 3,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/message_router.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/checkin_analytics.py",
                "line": 12,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/checkin_analytics.py",
                "line": 12,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/checkin_analytics.py",
                "line": 12,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/checkin_dynamic_manager.py",
                "line": 7,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/checkin_dynamic_manager.py",
                "line": 7,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/checkin_dynamic_manager.py",
                "line": 7,
                "column": 0,
                "message": "Unused Tuple imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/config.py",
                "line": 9,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/config.py",
                "line": 9,
                "column": 0,
                "message": "Unused Tuple imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/config.py",
                "line": 9,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/error_handling.py",
                "line": 14,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/error_handling.py",
                "line": 14,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/file_auditor.py",
                "line": 24,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/file_auditor.py",
                "line": 24,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/file_auditor.py",
                "line": 24,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/file_operations.py",
                "line": 10,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "core/headless_service.py",
                "line": 13,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/schedule_management.py",
                "line": 10,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/schedule_utilities.py",
                "line": 9,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/scheduler.py",
                "line": 12,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/schemas.py",
                "line": 12,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/schemas.py",
                "line": 12,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_handlers.py",
                "line": 12,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_handlers.py",
                "line": 14,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_handlers.py",
                "line": 14,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_handlers.py",
                "line": 14,
                "column": 0,
                "message": "Unused Union imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_handlers.py",
                "line": 14,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_manager.py",
                "line": 12,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_validation.py",
                "line": 9,
                "column": 0,
                "message": "Unused Tuple imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_validation.py",
                "line": 9,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_validation.py",
                "line": 9,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "development_tools/docs/generate_directory_tree.py",
                "line": 20,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "development_tools/functions/generate_function_docstrings.py",
                "line": 20,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "development_tools/imports/generate_unused_imports_report.py",
                "line": 25,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "development_tools/shared/common.py",
                "line": 13,
                "column": 0,
                "message": "Unused get_exclusions imported from development_tools.shared.standard_exclusions",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/notebook_data_handlers.py",
                "line": 8,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/notebook_data_manager.py",
                "line": 9,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/notebook_data_manager.py",
                "line": 9,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/notebook_data_manager.py",
                "line": 9,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/notebook_validation.py",
                "line": 10,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/notebook_validation.py",
                "line": 10,
                "column": 0,
                "message": "Unused Tuple imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/schemas.py",
                "line": 8,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/schemas.py",
                "line": 8,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/schemas.py",
                "line": 10,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "notebook/schemas.py",
                "line": 15,
                "column": 0,
                "message": "Unused TIMESTAMP_FULL imported from core.time_utilities",
                "symbol": "unused-import"
              },
              {
                "file": "tasks/task_management.py",
                "line": 10,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ai/ai_response_validator.py",
                "line": 9,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ai/ai_response_validator.py",
                "line": 9,
                "column": 0,
                "message": "Unused Tuple imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/conftest.py",
                "line": 27,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/conftest.py",
                "line": 27,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/conftest.py",
                "line": 27,
                "column": 0,
                "message": "Unused Type imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 22,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_cleanup.py",
                "line": 18,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_cleanup.py",
                "line": 18,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ui/generate_ui_files.py",
                "line": 13,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "ui/widgets/period_row_widget.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ui/widgets/period_row_widget.py",
                "line": 3,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ui/widgets/period_row_widget.py",
                "line": 3,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ui/widgets/user_profile_settings_widget.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ui/widgets/user_profile_settings_widget.py",
                "line": 3,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "user/context_manager.py",
                "line": 14,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "user/context_manager.py",
                "line": 14,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              }
            ],
            "type_hints_only": [
              {
                "file": "ai/context_builder.py",
                "line": 3,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "ai/conversation_history.py",
                "line": 3,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/base_handler.py",
                "line": 11,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/command_handlers/task_handler.py",
                "line": 10,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/communication_channels/discord/account_flow_handler.py",
                "line": 9,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/channel_orchestrator.py",
                "line": 8,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/welcome_manager.py",
                "line": 9,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "communication/message_processing/message_router.py",
                "line": 3,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/config.py",
                "line": 9,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/error_handling.py",
                "line": 14,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/message_management.py",
                "line": 12,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/message_management.py",
                "line": 12,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/message_management.py",
                "line": 12,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/schedule_management.py",
                "line": 10,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/schedule_utilities.py",
                "line": 9,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/schedule_utilities.py",
                "line": 9,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/scheduler.py",
                "line": 12,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/schemas.py",
                "line": 12,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_manager.py",
                "line": 12,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_manager.py",
                "line": 12,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "core/user_data_validation.py",
                "line": 9,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tasks/task_management.py",
                "line": 10,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tasks/task_management.py",
                "line": 10,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ai/ai_response_validator.py",
                "line": 9,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ai/ai_response_validator.py",
                "line": 9,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 16,
                "column": 0,
                "message": "Unused Dict imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 16,
                "column": 0,
                "message": "Unused Optional imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 16,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_cleanup.py",
                "line": 18,
                "column": 0,
                "message": "Unused List imported from typing",
                "symbol": "unused-import"
              }
            ],
            "re_exports": [],
            "conditional_imports": [
              {
                "file": "tests/ui/test_signal_handler_integration.py",
                "line": 35,
                "column": 12,
                "message": "Unused QtWidgets imported from PySide6",
                "symbol": "unused-import"
              }
            ],
            "star_imports": [],
            "test_mocking": [
              {
                "file": "tests/ai/test_ai_functionality_manual.py",
                "line": 20,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ai/test_ai_functionality_manual.py",
                "line": 27,
                "column": 0,
                "message": "Unused get_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ai/test_cache_manager.py",
                "line": 7,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_base_handler_behavior.py",
                "line": 11,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_base_handler_behavior.py",
                "line": 11,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_checkin_handler_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_communication_manager_behavior.py",
                "line": 8,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_interaction_handlers_behavior.py",
                "line": 21,
                "column": 0,
                "message": "Unused get_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_interaction_handlers_behavior.py",
                "line": 21,
                "column": 0,
                "message": "Unused save_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_logger_coverage_expansion.py",
                "line": 19,
                "column": 0,
                "message": "Unused mock_open imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_message_router_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_message_router_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_notebook_handler_behavior.py",
                "line": 9,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_notebook_handler_behavior.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_profile_handler_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_schedule_handler_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_scheduler_coverage_expansion.py",
                "line": 14,
                "column": 0,
                "message": "Unused get_user_categories imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_scheduler_coverage_expansion.py",
                "line": 14,
                "column": 0,
                "message": "Unused get_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_task_handler_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_user_data_flow_architecture.py",
                "line": 11,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_webhook_server_behavior.py",
                "line": 13,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_welcome_handler_behavior.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/core/test_file_auditor.py",
                "line": 8,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/core/test_message_management.py",
                "line": 8,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/core/test_message_management.py",
                "line": 10,
                "column": 0,
                "message": "Unused load_default_messages imported from core.message_management",
                "symbol": "unused-import"
              },
              {
                "file": "tests/core/test_schedule_utilities.py",
                "line": 8,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_ai_work.py",
                "line": 11,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_ai_work.py",
                "line": 11,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_error_handling.py",
                "line": 10,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_error_handling.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_function_registry.py",
                "line": 11,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_function_registry.py",
                "line": 11,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_functions.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_module_dependencies.py",
                "line": 11,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_module_dependencies.py",
                "line": 11,
                "column": 0,
                "message": "Unused mock_open imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_unused_imports.py",
                "line": 12,
                "column": 0,
                "message": "Unused mock_open imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_decision_support.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_false_negative_detection.py",
                "line": 15,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_documentation.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_error_handling_report.py",
                "line": 11,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_function_docstrings.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_unused_imports_report.py",
                "line": 10,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_unused_imports_report.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_output_storage_archiving.py",
                "line": 11,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_regenerate_coverage_metrics.py",
                "line": 10,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_task_cleanup_real_bug_verification.py",
                "line": 13,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_task_cleanup_silent_failure.py",
                "line": 9,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_task_cleanup_silent_failure.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_user_creation.py",
                "line": 19,
                "column": 0,
                "message": "Unused update_user_account imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_user_creation.py",
                "line": 19,
                "column": 0,
                "message": "Unused update_user_schedules imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_user_creation.py",
                "line": 28,
                "column": 0,
                "message": "Unused validate_schedule_periods__validate_time_format imported from core.user_data_validation",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_error_handling_improvements.py",
                "line": 13,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_error_handling_improvements.py",
                "line": 13,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_error_handling_improvements.py",
                "line": 16,
                "column": 0,
                "message": "Unused DataError imported from core.error_handling",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_error_handling_improvements.py",
                "line": 16,
                "column": 0,
                "message": "Unused FileOperationError imported from core.error_handling",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 21,
                "column": 0,
                "message": "Unused create_new_user imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 21,
                "column": 0,
                "message": "Unused save_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 21,
                "column": 0,
                "message": "Unused get_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 22,
                "column": 0,
                "message": "Unused ensure_user_directory imported from core.file_operations",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 23,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 37,
                "column": 0,
                "message": "Unused create_user_files imported from core.file_operations",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 37,
                "column": 0,
                "message": "Unused get_user_file_path imported from core.file_operations",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 38,
                "column": 0,
                "message": "Unused is_valid_email imported from core.user_data_validation",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 38,
                "column": 0,
                "message": "Unused validate_schedule_periods__validate_time_format imported from core.user_data_validation",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creator_dialog_validation.py",
                "line": 13,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_category_management_dialog.py",
                "line": 16,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_category_management_dialog.py",
                "line": 16,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_channel_management_dialog_coverage_expansion.py",
                "line": 16,
                "column": 0,
                "message": "Unused get_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_channel_management_dialog_coverage_expansion.py",
                "line": 16,
                "column": 0,
                "message": "Unused update_channel_preferences imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_channel_management_dialog_coverage_expansion.py",
                "line": 16,
                "column": 0,
                "message": "Unused update_user_account imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_channel_management_dialog_coverage_expansion.py",
                "line": 17,
                "column": 0,
                "message": "Unused is_valid_email imported from core.user_data_validation",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_channel_management_dialog_coverage_expansion.py",
                "line": 17,
                "column": 0,
                "message": "Unused is_valid_phone imported from core.user_data_validation",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 23,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 23,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 23,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 32,
                "column": 0,
                "message": "Unused save_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 33,
                "column": 0,
                "message": "Unused create_user_files imported from core.file_operations",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 33,
                "column": 0,
                "message": "Unused get_user_file_path imported from core.file_operations",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 26,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 26,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 35,
                "column": 0,
                "message": "Unused save_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 35,
                "column": 0,
                "message": "Unused get_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 36,
                "column": 0,
                "message": "Unused create_user_files imported from core.file_operations",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 37,
                "column": 0,
                "message": "Unused get_schedule_time_periods imported from core.schedule_management",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 37,
                "column": 0,
                "message": "Unused set_schedule_periods imported from core.schedule_management",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_message_editor_dialog.py",
                "line": 16,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_process_watcher_dialog.py",
                "line": 16,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_signal_handler_integration.py",
                "line": 40,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_signal_handler_integration.py",
                "line": 40,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_crud_dialog.py",
                "line": 13,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_management_dialog.py",
                "line": 18,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_settings_widget.py",
                "line": 18,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_settings_widget.py",
                "line": 18,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_button_verification.py",
                "line": 14,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_button_verification.py",
                "line": 14,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_components_headless.py",
                "line": 14,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_widgets_coverage_expansion.py",
                "line": 22,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_widgets_coverage_expansion.py",
                "line": 22,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_analytics_dialog.py",
                "line": 16,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_profile_dialog_coverage_expansion.py",
                "line": 13,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 24,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 24,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 24,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 33,
                "column": 0,
                "message": "Unused save_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 33,
                "column": 0,
                "message": "Unused get_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 34,
                "column": 0,
                "message": "Unused create_user_files imported from core.file_operations",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 34,
                "column": 0,
                "message": "Unused get_user_file_path imported from core.file_operations",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior_simple.py",
                "line": 15,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior_simple.py",
                "line": 15,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_admin_panel.py",
                "line": 17,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_admin_panel.py",
                "line": 17,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_ai_chatbot_helpers.py",
                "line": 8,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_ai_chatbot_helpers.py",
                "line": 8,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_channel_orchestrator.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_checkin_management_dialog.py",
                "line": 19,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_checkin_view.py",
                "line": 9,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_checkin_view.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_command_parser_helpers.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_communication_core_init.py",
                "line": 12,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_communication_core_init.py",
                "line": 12,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_communication_init.py",
                "line": 12,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_communication_init.py",
                "line": 12,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_email_bot_body_extraction.py",
                "line": 12,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_enhanced_checkin_responses.py",
                "line": 12,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_file_operations.py",
                "line": 20,
                "column": 0,
                "message": "Unused FileOperationError imported from core.error_handling",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_interaction_handlers_helpers.py",
                "line": 9,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_interaction_handlers_helpers.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_logger_unit.py",
                "line": 13,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_logger_unit.py",
                "line": 13,
                "column": 0,
                "message": "Unused mock_open imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_message_formatter.py",
                "line": 12,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_message_formatter.py",
                "line": 19,
                "column": 0,
                "message": "Unused DataError imported from core.error_handling",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_prompt_manager.py",
                "line": 19,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_prompt_manager.py",
                "line": 19,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_recurring_tasks.py",
                "line": 11,
                "column": 0,
                "message": "Unused _create_next_recurring_task_instance imported from tasks.task_management",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_ui_management.py",
                "line": 16,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_context.py",
                "line": 16,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_context.py",
                "line": 16,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_data_handlers.py",
                "line": 9,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_data_manager.py",
                "line": 14,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_data_manager.py",
                "line": 14,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_data_manager.py",
                "line": 14,
                "column": 0,
                "message": "Unused mock_open imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_management.py",
                "line": 10,
                "column": 0,
                "message": "Unused patch imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_management.py",
                "line": 12,
                "column": 0,
                "message": "Unused update_user_account imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_management.py",
                "line": 12,
                "column": 0,
                "message": "Unused update_user_context imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_preferences.py",
                "line": 16,
                "column": 0,
                "message": "Unused Mock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_preferences.py",
                "line": 16,
                "column": 0,
                "message": "Unused MagicMock imported from unittest.mock",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_preferences.py",
                "line": 20,
                "column": 0,
                "message": "Unused get_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              }
            ],
            "qt_testing": [
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 26,
                "column": 0,
                "message": "Unused QWidget imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 26,
                "column": 0,
                "message": "Unused QMessageBox imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 27,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 27,
                "column": 0,
                "message": "Unused QTimer imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_category_management_dialog.py",
                "line": 17,
                "column": 0,
                "message": "Unused QMessageBox imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_category_management_dialog.py",
                "line": 18,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_category_management_dialog.py",
                "line": 19,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 26,
                "column": 0,
                "message": "Unused QWidget imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 26,
                "column": 0,
                "message": "Unused QMessageBox imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 26,
                "column": 0,
                "message": "Unused QDialog imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 27,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 27,
                "column": 0,
                "message": "Unused QTimer imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 29,
                "column": 0,
                "message": "Unused QWidget imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 30,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 30,
                "column": 0,
                "message": "Unused QTimer imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 31,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_message_editor_dialog.py",
                "line": 17,
                "column": 0,
                "message": "Unused QTableWidgetItem imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_message_editor_dialog.py",
                "line": 18,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_message_editor_dialog.py",
                "line": 19,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_process_watcher_dialog.py",
                "line": 19,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_process_watcher_dialog.py",
                "line": 19,
                "column": 0,
                "message": "Unused QTimer imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_process_watcher_dialog.py",
                "line": 20,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_signal_handler_integration.py",
                "line": 41,
                "column": 0,
                "message": "Unused QLineEdit imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_signal_handler_integration.py",
                "line": 42,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_signal_handler_integration.py",
                "line": 43,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_crud_dialog.py",
                "line": 14,
                "column": 0,
                "message": "Unused QTableWidgetItem imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_management_dialog.py",
                "line": 19,
                "column": 0,
                "message": "Unused QMessageBox imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_management_dialog.py",
                "line": 20,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_management_dialog.py",
                "line": 21,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_settings_widget.py",
                "line": 19,
                "column": 0,
                "message": "Unused QWidget imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_settings_widget.py",
                "line": 19,
                "column": 0,
                "message": "Unused QMessageBox imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_settings_widget.py",
                "line": 20,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_task_settings_widget.py",
                "line": 21,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_widgets_coverage_expansion.py",
                "line": 26,
                "column": 0,
                "message": "Unused QWidget imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_widgets_coverage_expansion.py",
                "line": 26,
                "column": 0,
                "message": "Unused QListWidgetItem imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_widgets_coverage_expansion.py",
                "line": 26,
                "column": 0,
                "message": "Unused QInputDialog imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_widgets_coverage_expansion.py",
                "line": 27,
                "column": 0,
                "message": "Unused QTimer imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_widgets_coverage_expansion.py",
                "line": 28,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_analytics_dialog.py",
                "line": 19,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_analytics_dialog.py",
                "line": 20,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_profile_dialog_coverage_expansion.py",
                "line": 18,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 27,
                "column": 0,
                "message": "Unused QMessageBox imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 27,
                "column": 0,
                "message": "Unused QDialog imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 28,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 28,
                "column": 0,
                "message": "Unused QTimer imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 29,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior_simple.py",
                "line": 16,
                "column": 0,
                "message": "Unused QWidget imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior_simple.py",
                "line": 17,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior_simple.py",
                "line": 18,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_admin_panel.py",
                "line": 19,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_admin_panel.py",
                "line": 20,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_checkin_management_dialog.py",
                "line": 20,
                "column": 0,
                "message": "Unused QMessageBox imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_checkin_management_dialog.py",
                "line": 21,
                "column": 0,
                "message": "Unused Qt imported from PySide6.QtCore",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_checkin_management_dialog.py",
                "line": 22,
                "column": 0,
                "message": "Unused QTest imported from PySide6.QtTest",
                "symbol": "unused-import"
              }
            ],
            "test_infrastructure": [
              {
                "file": "tests/ai/test_ai_core.py",
                "line": 8,
                "column": 0,
                "message": "Unused import time",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ai/test_context_includes_recent_messages.py",
                "line": 1,
                "column": 0,
                "message": "Unused import os",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_checkin_expiry_semantics.py",
                "line": 1,
                "column": 0,
                "message": "Unused import os",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_checkin_handler_behavior.py",
                "line": 11,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_communication_manager_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused import time",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_conversation_flow_manager_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused import json",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_conversation_flow_manager_behavior.py",
                "line": 11,
                "column": 0,
                "message": "Unused import os",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_conversation_flow_manager_behavior.py",
                "line": 13,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_discord_checkin_retry_behavior.py",
                "line": 12,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_discord_task_reminder_followup.py",
                "line": 9,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_headless_service_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused import os",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_headless_service_behavior.py",
                "line": 11,
                "column": 0,
                "message": "Unused import json",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_headless_service_behavior.py",
                "line": 14,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_headless_service_behavior.py",
                "line": 18,
                "column": 0,
                "message": "Unused TestUserFactory imported from tests.test_utilities",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_notebook_handler_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_task_error_handling.py",
                "line": 10,
                "column": 0,
                "message": "Unused import json",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_task_error_handling.py",
                "line": 11,
                "column": 0,
                "message": "Unused import os",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_user_data_flow_architecture.py",
                "line": 10,
                "column": 0,
                "message": "Unused import json",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_webhook_server_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused import threading",
                "symbol": "unused-import"
              },
              {
                "file": "tests/behavior/test_welcome_manager_behavior.py",
                "line": 10,
                "column": 0,
                "message": "Unused import os",
                "symbol": "unused-import"
              },
              {
                "file": "tests/communication/test_retry_manager.py",
                "line": 6,
                "column": 0,
                "message": "Unused import time",
                "symbol": "unused-import"
              },
              {
                "file": "tests/communication/test_retry_manager.py",
                "line": 7,
                "column": 0,
                "message": "Unused import threading",
                "symbol": "unused-import"
              },
              {
                "file": "tests/core/test_message_management.py",
                "line": 7,
                "column": 0,
                "message": "Unused import json",
                "symbol": "unused-import"
              },
              {
                "file": "tests/core/test_message_management.py",
                "line": 9,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/core/test_schedule_utilities.py",
                "line": 7,
                "column": 0,
                "message": "Unused time imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/debug_file_paths.py",
                "line": 1,
                "column": 0,
                "message": "Unused import pytest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analysis_tool_validation.py",
                "line": 12,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analysis_tool_validation.py",
                "line": 12,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_ai_work.py",
                "line": 9,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_ai_work.py",
                "line": 10,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_ai_work.py",
                "line": 13,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_ascii_compliance.py",
                "line": 10,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_ascii_compliance.py",
                "line": 10,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_documentation.py",
                "line": 9,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_documentation.py",
                "line": 13,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_error_handling.py",
                "line": 12,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_error_handling.py",
                "line": 12,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_function_registry.py",
                "line": 10,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_function_registry.py",
                "line": 13,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_function_registry.py",
                "line": 13,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_functions.py",
                "line": 8,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_functions.py",
                "line": 11,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_functions.py",
                "line": 11,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_heading_numbering.py",
                "line": 10,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_heading_numbering.py",
                "line": 10,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_missing_addresses.py",
                "line": 8,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_missing_addresses.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_missing_addresses.py",
                "line": 11,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_missing_addresses.py",
                "line": 11,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_module_dependencies.py",
                "line": 9,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_module_dependencies.py",
                "line": 10,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_module_dependencies.py",
                "line": 13,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_unconverted_links.py",
                "line": 10,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_unconverted_links.py",
                "line": 10,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_unused_imports.py",
                "line": 10,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_unused_imports.py",
                "line": 11,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_analyze_unused_imports.py",
                "line": 14,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_audit_tier_e2e_verification.py",
                "line": 45,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_decision_support.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_documentation_sync_checker.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_error_scenarios.py",
                "line": 10,
                "column": 0,
                "message": "Unused import shutil",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_exclusion_utilities.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_false_negative_detection.py",
                "line": 14,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_false_negative_detection.py",
                "line": 17,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_false_negative_detection.py",
                "line": 17,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_documentation.py",
                "line": 8,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_documentation_addresses.py",
                "line": 8,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_documentation_addresses.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_documentation_ascii.py",
                "line": 8,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_documentation_ascii.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_documentation_headings.py",
                "line": 11,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_documentation_links.py",
                "line": 11,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_project_cleanup.py",
                "line": 9,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_fix_project_cleanup.py",
                "line": 13,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_consolidated_report.py",
                "line": 12,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_directory_tree.py",
                "line": 12,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_directory_tree.py",
                "line": 12,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_error_handling_report.py",
                "line": 10,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_function_docstrings.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_function_docstrings.py",
                "line": 12,
                "column": 0,
                "message": "Unused demo_project_root imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_function_docstrings.py",
                "line": 12,
                "column": 0,
                "message": "Unused test_config_path imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_function_registry.py",
                "line": 8,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_module_dependencies.py",
                "line": 8,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_unused_imports_report.py",
                "line": 8,
                "column": 0,
                "message": "Unused import json",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_unused_imports_report.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_generate_unused_imports_report.py",
                "line": 12,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_legacy_reference_cleanup.py",
                "line": 11,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_output_storage_archiving.py",
                "line": 10,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_output_storage_archiving.py",
                "line": 13,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_path_drift_detection.py",
                "line": 13,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_path_drift_detection.py",
                "line": 14,
                "column": 0,
                "message": "Unused import shutil",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_path_drift_detection.py",
                "line": 15,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_path_drift_integration.py",
                "line": 9,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_path_drift_integration.py",
                "line": 10,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_path_drift_integration.py",
                "line": 13,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_path_drift_verification_comprehensive.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_path_drift_verification_comprehensive.py",
                "line": 11,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_regenerate_coverage_metrics.py",
                "line": 9,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_status_file_timing.py",
                "line": 10,
                "column": 0,
                "message": "Unused import time",
                "symbol": "unused-import"
              },
              {
                "file": "tests/development_tools/test_status_file_timing.py",
                "line": 19,
                "column": 0,
                "message": "Unused temp_project_copy imported from tests.development_tools.conftest",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_account_management.py",
                "line": 11,
                "column": 0,
                "message": "Unused import time",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_account_management.py",
                "line": 12,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_task_cleanup_real.py",
                "line": 12,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_task_cleanup_silent_failure.py",
                "line": 10,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_task_reminder_integration.py",
                "line": 10,
                "column": 0,
                "message": "Unused import time",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_user_creation.py",
                "line": 14,
                "column": 0,
                "message": "Unused import json",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_user_creation.py",
                "line": 15,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/integration/test_user_creation.py",
                "line": 17,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_error_handling_improvements.py",
                "line": 14,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 8,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 15,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/test_utilities.py",
                "line": 17,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 21,
                "column": 0,
                "message": "Unused import shutil",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 24,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 25,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 43,
                "column": 0,
                "message": "Unused CategorySelectionWidget imported from ui.widgets.category_selection_widget",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 44,
                "column": 0,
                "message": "Unused ChannelSelectionWidget imported from ui.widgets.channel_selection_widget",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 45,
                "column": 0,
                "message": "Unused TaskSettingsWidget imported from ui.widgets.task_settings_widget",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_account_creation_ui.py",
                "line": 46,
                "column": 0,
                "message": "Unused CheckinSettingsWidget imported from ui.widgets.checkin_settings_widget",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 22,
                "column": 0,
                "message": "Unused import shutil",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 24,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 24,
                "column": 0,
                "message": "Unused time imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 39,
                "column": 0,
                "message": "Unused open_schedule_editor imported from ui.dialogs.schedule_editor_dialog",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 40,
                "column": 0,
                "message": "Unused TaskEditDialog imported from ui.dialogs.task_edit_dialog",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 41,
                "column": 0,
                "message": "Unused TaskCrudDialog imported from ui.dialogs.task_crud_dialog",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_behavior.py",
                "line": 42,
                "column": 0,
                "message": "Unused TaskCompletionDialog imported from ui.dialogs.task_completion_dialog",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 25,
                "column": 0,
                "message": "Unused import shutil",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 27,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 27,
                "column": 0,
                "message": "Unused time imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 28,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 40,
                "column": 0,
                "message": "Unused TaskCrudDialog imported from ui.dialogs.task_crud_dialog",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialog_coverage_expansion.py",
                "line": 43,
                "column": 0,
                "message": "Unused TestDataFactory imported from tests.test_utilities",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_dialogs.py",
                "line": 15,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_ui_widgets_coverage_expansion.py",
                "line": 23,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_analytics_dialog.py",
                "line": 17,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_profile_dialog_coverage_expansion.py",
                "line": 11,
                "column": 0,
                "message": "Unused import os",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_profile_dialog_coverage_expansion.py",
                "line": 12,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_profile_dialog_coverage_expansion.py",
                "line": 14,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_user_profile_dialog_coverage_expansion.py",
                "line": 21,
                "column": 0,
                "message": "Unused TestDataFactory imported from tests.test_utilities",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 22,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 23,
                "column": 0,
                "message": "Unused import shutil",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 25,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 25,
                "column": 0,
                "message": "Unused time imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/ui/test_widget_behavior.py",
                "line": 35,
                "column": 0,
                "message": "Unused TestUserDataFactory imported from tests.test_utilities",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_file_locking.py",
                "line": 14,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_file_operations.py",
                "line": 10,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_logger_unit.py",
                "line": 12,
                "column": 0,
                "message": "Unused import json",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_prompt_manager.py",
                "line": 17,
                "column": 0,
                "message": "Unused import os",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_prompt_manager.py",
                "line": 18,
                "column": 0,
                "message": "Unused import tempfile",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_prompt_manager.py",
                "line": 20,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_data_manager.py",
                "line": 19,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_data_manager.py",
                "line": 20,
                "column": 0,
                "message": "Unused datetime imported from datetime",
                "symbol": "unused-import"
              },
              {
                "file": "tests/unit/test_user_preferences.py",
                "line": 17,
                "column": 0,
                "message": "Unused Path imported from pathlib",
                "symbol": "unused-import"
              }
            ],
            "production_test_mocking": [
              {
                "file": "communication/command_handlers/interaction_handlers.py",
                "line": 18,
                "column": 0,
                "message": "Unused save_user_data imported from core.user_data_handlers",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/channel_orchestrator.py",
                "line": 19,
                "column": 0,
                "message": "Unused determine_file_path imported from core.file_operations",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/factory.py",
                "line": 5,
                "column": 0,
                "message": "Unused get_available_channels imported from core.config",
                "symbol": "unused-import"
              },
              {
                "file": "communication/core/factory.py",
                "line": 5,
                "column": 0,
                "message": "Unused get_channel_class_mapping imported from core.config",
                "symbol": "unused-import"
              }
            ],
            "ui_imports": [
              {
                "file": "ui/widgets/checkin_settings_widget.py",
                "line": 3,
                "column": 0,
                "message": "Unused QComboBox imported from PySide6.QtWidgets",
                "symbol": "unused-import"
              }
            ]
          },
          "stats": {
            "files_scanned": 412,
            "files_with_issues": 185,
            "total_unused": 532,
            "cache_hits": 0,
            "cache_misses": 0
          },
          "by_category": {
            "obvious_unused": 143,
            "type_hints_only": 29,
            "re_exports": 0,
            "conditional_imports": 1,
            "star_imports": 0,
            "test_mocking": 147,
            "qt_testing": 54,
            "test_infrastructure": 153,
            "production_test_mocking": 4,
            "ui_imports": 1
          },
          "files_scanned": 412
        }
      },
      "timestamp": "2026-01-18T01:49:20"
    },
    "analyze_legacy_references": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 73,
          "files_affected": 41
        },
        "details": {
          "findings": {
            "legacy_inventory_tracking": [
              [
                "run_tests.py",
                "#!/usr/bin/env python3\n\"\"\"\nTest Runner for MHM Project\n\nProvides different test execution modes for faster development workflow.\n\"\"\"\n\nimport sys\nimport subprocess\nimport argparse\nimport os\nimport time\nimport xml.etree.ElementTree as ET\nimport threading\nimport queue\nimport re\nimport logging\nimport signal\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Optional, List\nfrom core.error_handling import handle_errors\nfrom core.time_utilities import now_timestamp_filename, now_timestamp_full\n\n# Try to import psutil for resource monitoring (optional dependency)\ntry:\n    import psutil\n\n    PSUTIL_AVAILABLE = True\nexcept ImportError:\n    PSUTIL_AVAILABLE = False\n    psutil = None\n\n# Simple ANSI color codes for terminal output (works in modern PowerShell)\n# Enable ANSI color support in Windows\nif sys.platform == \"win32\":\n    import ctypes\n\n    kernel32 = ctypes.windll.kernel32\n    kernel32.SetConsoleMode(\n        kernel32.GetStdHandle(-11), 7\n    )  # Enable ANSI escape sequences\n\nGREEN = \"\\033[32m\"\nRED = \"\\033[31m\"\nYELLOW = \"\\033[33m\"\nRESET = \"\\033[0m\"\n\nANSI_ESCAPE_RE = re.compile(r\"\\x1b\\[[0-9;]*m\")\n\n# Global state for interrupt handling\n_interrupt_requested = False\n_current_process = None\n_current_junit_xml = None\n_partial_results_file = Path(\"tests/logs/partial_results.json\")\n_partial_results_file.parent.mkdir(parents=True, exist_ok=True)\n# Use backups directory (consolidated from results_backup)\n_backups_dir = Path(\"tests/logs/backups\")\n_backups_dir.mkdir(parents=True, exist_ok=True)\n_last_output_time = None\n_resource_warnings_enabled = True\n_critical_memory_limit = 98.0\n_auto_terminate_enabled = True\n_captured_output_lines = []  # Store captured output for partial results extraction\n\n# Global variable to store current test context\n_current_test_context = None\n\n\n@handle_errors(\n    \"killing process tree on Windows\", user_friendly=False, default_return=False\n)\ndef kill_process_tree_windows(pid):\n    \"\"\"Kill a process and all its children on Windows.\n\n    Returns:\n        bool: True if taskkill succeeded, False otherwise\n    \"\"\"\n    try:\n        # Use taskkill to kill the process tree\n        # /T = kill child processes, /F = force kill\n        result = subprocess.run(\n            [\"taskkill\", \"/F\", \"/T\", \"/PID\", str(pid)],\n            capture_output=True,\n            text=True,\n            timeout=5,\n            check=False,\n        )\n        if result.returncode == 0:\n            print(\n                f\"{GREEN}[CLEANUP]{RESET} Successfully killed process tree for PID {pid}\"\n            )\n            return True\n        elif (\n            \"not found\" in result.stdout.lower() or \"not found\" in result.stderr.lower()\n        ):\n            # Process already gone - that's fine\n            print(\n                f\"{YELLOW}[CLEANUP]{RESET} Process {pid} not found (already terminated)\"\n            )\n            return True\n        else:\n            print(\n                f\"{YELLOW}[CLEANUP]{RESET} taskkill returned code {result.returncode} for PID {pid}: {result.stderr}\"\n            )\n            return False\n    except Exception as e:\n        print(f\"{YELLOW}[CLEANUP]{RESET} Error killing process tree for PID {pid}: {e}\")\n        # Fallback to regular kill if taskkill fails\n        try:\n            import signal\n\n            os.kill(pid, signal.SIGTERM)\n            return True\n        except Exception:\n            return False\n\n\n@handle_errors(\n    \"cleaning up orphaned pytest processes\", user_friendly=False, default_return=0\n)\ndef cleanup_orphaned_pytest_processes():\n    \"\"\"Find and kill any orphaned pytest worker processes on Windows.\n\n    Returns:\n        int: Number of orphaned processes found and killed\n    \"\"\"\n    if sys.platform != \"win32\":\n        return 0  # Only needed on Windows\n\n    killed_count = 0\n    try:\n        # Find all Python processes that look like pytest workers\n        # Look for processes with \"pytest\" or \"gw\" (pytest-xdist worker) in command line\n        result = subprocess.run(\n            [\"tasklist\", \"/FO\", \"CSV\", \"/FI\", \"IMAGENAME eq python.exe\"],\n            capture_output=True,\n            text=True,\n            timeout=5,\n            check=False,\n        )\n\n        if result.returncode != 0:\n            return 0\n\n        # Parse output to find pytest processes\n        lines = result.stdout.strip().split(\"\\n\")\n        if len(lines) < 2:  # Header + at least one process\n            return 0\n\n        # Skip header line\n        for line in lines[1:]:\n            if not line.strip():\n                continue\n            try:\n                # CSV format: \"Image Name\",\"PID\",\"Session Name\",\"Session#\",\"Mem Usage\"\n                parts = line.split('\",\"')\n                if len(parts) >= 2:\n                    pid_str = parts[1].strip('\"')\n                    pid = int(pid_str)\n\n                    # Skip our own process\n                    if pid == os.getpid():\n                        continue\n\n                    # Get command line for this process to check if it's pytest\n                    cmd_result = subprocess.run(\n                        [\n                            \"wmic\",\n                            \"process\",\n                            \"where\",\n                            f\"ProcessId={pid}\",\n                            \"get\",\n                            \"CommandLine\",\n                        ],\n                        capture_output=True,\n                        text=True,\n                        timeout=3,\n                        check=False,\n                    )\n\n                    if cmd_result.returncode == 0 and cmd_result.stdout:\n                        cmdline = cmd_result.stdout.lower()\n                        # Check if it's a pytest process (but not our current process)\n                        if (\n                            \"pytest\" in cmdline or \"gw\" in cmdline\n                        ) and \"xdist\" in cmdline:\n                            # This is likely an orphaned pytest worker\n                            try:\n                                kill_result = subprocess.run(\n                                    [\"taskkill\", \"/F\", \"/PID\", str(pid)],\n                                    capture_output=True,\n                                    text=True,\n                                    timeout=2,\n                                    check=False,\n                                )\n                                if kill_result.returncode == 0:\n                                    killed_count += 1\n                                    print(\n                                        f\"{GREEN}[CLEANUP]{RESET} Killed orphaned pytest worker PID {pid}\"\n                                    )\n                            except Exception as e:\n                                print(\n                                    f\"{YELLOW}[CLEANUP]{RESET} Failed to kill orphaned process {pid}: {e}\"\n                                )\n            except (ValueError, IndexError):\n                continue\n\n        if killed_count > 0:\n            print(\n                f\"{GREEN}[CLEANUP]{RESET} Cleaned up {killed_count} orphaned pytest worker process(es)\"\n            )\n        elif killed_count == 0:\n            # Only print if we actually scanned (not if we returned early)\n            pass  # Don't print \"no orphaned processes\" here - let caller decide\n        return killed_count\n    except Exception as e:\n        print(f\"{YELLOW}[CLEANUP]{RESET} Error during orphaned process cleanup: {e}\")\n        return killed_count\n\n\n@handle_errors(\"handling interrupt signal\", user_friendly=False, default_return=None)\ndef interrupt_handler(signum, frame):\n    \"\"\"Handle interrupt signals (Ctrl+C) gracefully.\"\"\"\n    global _interrupt_requested, _captured_output_lines, _current_test_context\n    _interrupt_requested = True\n    print(\n        f\"\\n\\n{YELLOW}[INTERRUPT]{RESET} Received interrupt signal (Ctrl+C) - saving partial results...\"\n    )\n\n    # Collect any available output before saving\n    output_text = \"\".join(_captured_output_lines) if _captured_output_lines else None\n\n    # Save partial results if available (will try XML first, then output text)\n    if _current_junit_xml:\n        save_partial_results(\n            _current_junit_xml,\n            interrupted=True,\n            output_text=output_text,\n            test_context=_current_test_context,\n        )\n\n    # Also save captured output for worker test assignment extraction (even on interrupt)\n    if _captured_output_lines:\n        output_file = Path(\"tests/logs/pytest_console_output.txt\")\n        output_file.parent.mkdir(parents=True, exist_ok=True)\n        try:\n            with open(output_file, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n                f.write(\"\".join(_captured_output_lines))\n        except Exception:\n            pass  # Non-critical\n\n    # Terminate current process and all child processes (especially pytest-xdist workers)\n    if _current_process and _current_process.poll() is None:\n        print(\n            f\"{YELLOW}[INTERRUPT]{RESET} Terminating test process and all worker processes...\"\n        )\n        try:\n            # On Windows, we need to kill the entire process tree to get pytest-xdist workers\n            if sys.platform == \"win32\":\n                kill_process_tree_windows(_current_process.pid)\n            else:\n                # On Unix, terminate sends signal to process group\n                _current_process.terminate()\n        except Exception as e:\n            print(f\"{YELLOW}[WARNING]{RESET} Error during process termination: {e}\")\n            # Fallback to regular terminate\n            try:\n                _current_process.terminate()\n            except Exception:\n                pass\n\n        # Wait up to 5 seconds for graceful shutdown\n        shutdown_start = time.time()\n        while _current_process.poll() is None and (time.time() - shutdown_start) < 5:\n            time.sleep(0.1)\n        if _current_process.poll() is None:\n            print(\n                f\"{YELLOW}[INTERRUPT]{RESET} Process did not terminate, force killing process tree...\"\n            )\n            # Force kill the entire process tree\n            if sys.platform == \"win32\":\n                kill_process_tree_windows(_current_process.pid)\n            else:\n                _current_process.kill()\n\n        # Always check for orphaned processes after termination\n        if sys.platform == \"win32\":\n            # Small delay to allow processes to fully terminate\n            time.sleep(0.5)\n            orphaned_count = cleanup_orphaned_pytest_processes()\n            if orphaned_count == 0:\n                print(\n                    f\"{GREEN}[CLEANUP]{RESET} Verified: No orphaned processes remaining\"\n                )\n\n        # CRITICAL: Consolidate worker logs even after interrupt\n        # When process is forcefully terminated, pytest_sessionfinish may not run\n        # So we need to consolidate worker logs manually\n        try:\n            # Import consolidation function\n            sys.path.insert(0, str(Path(__file__).parent))\n            from tests.conftest import _consolidate_worker_logs\n\n            print(f\"{GREEN}[CLEANUP]{RESET} Consolidating worker log files...\")\n            _consolidate_worker_logs()\n            print(f\"{GREEN}[CLEANUP]{RESET} Worker log consolidation complete\")\n        except Exception as e:\n            print(\n                f\"{YELLOW}[CLEANUP]{RESET} Could not consolidate worker logs automatically: {e}\"\n            )\n            print(\n                f\"{YELLOW}[CLEANUP]{RESET} Run manually: python development_tools/consolidate_worker_logs.py\"\n            )\n\n\n@handle_errors(\"monitoring system resources\", user_friendly=False, default_return={})\ndef monitor_resources():\n    \"\"\"Monitor system resource usage and return metrics.\"\"\"\n    if not PSUTIL_AVAILABLE:\n        return {}\n\n    try:\n        process = psutil.Process()\n        mem_info = process.memory_info()\n        num_threads = process.num_threads()\n        num_fds = process.num_fds() if hasattr(process, \"num_fds\") else None\n\n        # Get system-wide memory info\n        sys_mem = psutil.virtual_memory()\n\n        return {\n            \"memory_rss_mb\": mem_info.rss / (1024 * 1024),\n            \"memory_vms_mb\": mem_info.vms / (1024 * 1024),\n            \"threads\": num_threads,\n            \"file_descriptors\": num_fds,\n            \"system_memory_percent\": sys_mem.percent,\n            \"system_memory_available_mb\": sys_mem.available / (1024 * 1024),\n        }\n    except Exception:\n        return {}\n\n\n@handle_errors(\"checking resource warnings\", user_friendly=False, default_return=False)\ndef check_resource_warnings(resources: dict) -> bool:\n    \"\"\"Check if resources exceed warning thresholds.\"\"\"\n    if not resources or not _resource_warnings_enabled:\n        return False\n\n    warnings = []\n    critical = False\n\n    # Memory warning (80% of system memory)\n    if \"system_memory_percent\" in resources:\n        mem_percent = resources[\"system_memory_percent\"]\n        if mem_percent > 98:\n            warnings.append(\n                f\"System memory usage: {mem_percent:.1f}% (CRITICAL - terminating to prevent system crash)\"\n            )\n            critical = True\n        elif mem_percent > 95:\n            warnings.append(\n                f\"System memory usage: {mem_percent:.1f}% (CRITICAL - consider terminating)\"\n            )\n            critical = True\n        elif mem_percent > 80:\n            warnings.append(f\"System memory usage: {mem_percent:.1f}%\")\n\n    # Process memory warning (2GB)\n    if \"memory_rss_mb\" in resources:\n        if resources[\"memory_rss_mb\"] > 2048:\n            warnings.append(f\"Process memory: {resources['memory_rss_mb']:.0f}MB\")\n\n    # Thread count warning (500 threads)\n    if \"threads\" in resources:\n        if resources[\"threads\"] > 500:\n            warnings.append(f\"Thread count: {resources['threads']}\")\n\n    # File descriptor warning (1000 handles)\n    if \"file_descriptors\" in resources and resources[\"file_descriptors\"]:\n        if resources[\"file_descriptors\"] > 1000:\n            warnings.append(f\"File descriptors: {resources['file_descriptors']}\")\n\n    if warnings:\n        if critical:\n            print(f\"\\n{RED}[RESOURCE CRITICAL]{RESET} {'; '.join(warnings)}\")\n        else:\n            print(f\"\\n{YELLOW}[RESOURCE WARNING]{RESET} {'; '.join(warnings)}\")\n        return True\n\n    return False\n\n\n@handle_errors(\n    \"checking critical resource limits\", user_friendly=False, default_return=False\n)\ndef check_critical_resources(resources: dict) -> bool:\n    \"\"\"Check if resources exceed critical thresholds requiring termination.\"\"\"\n    global _critical_memory_limit, _auto_terminate_enabled\n\n    if not resources or not _auto_terminate_enabled:\n        return False\n\n    # Critical memory threshold: configurable (default 98%)\n    if \"system_memory_percent\" in resources:\n        if resources[\"system_memory_percent\"] > _critical_memory_limit:\n            return True\n\n    return False\n\n\n@handle_errors(\n    \"extracting results from output text\",\n    user_friendly=False,\n    default_return={\n        \"passed\": 0,\n        \"failed\": 0,\n        \"skipped\": 0,\n        \"warnings\": 0,\n        \"errors\": 0,\n        \"total\": 0,\n        \"deselected\": 0,\n    },\n)\ndef extract_results_from_output(output_text: str) -> dict[str, int]:\n    \"\"\"Extract test results from pytest output text when JUnit XML is unavailable.\"\"\"\n    results = {\n        \"passed\": 0,\n        \"failed\": 0,\n        \"skipped\": 0,\n        \"warnings\": 0,\n        \"errors\": 0,\n        \"total\": 0,\n        \"deselected\": 0,\n    }\n\n    if not output_text:\n        return results\n\n    # Remove ANSI escape codes\n    output_plain = ANSI_ESCAPE_RE.sub(\"\", output_text)\n\n    # Try to find summary line like \"3819 passed, 1 skipped in 243.16s\" or \"4 failed, 2276 passed, 1 skipped, 4 warnings\"\n    # Pattern 1: Full summary with all counts\n    full_pattern = (\n        r\"(\\d+)\\s+failed[,\\s]+(\\d+)\\s+passed[,\\s]+(\\d+)\\s+skipped[,\\s]+(\\d+)\\s+warnings\"\n    )\n    match = re.search(full_pattern, output_plain)\n    if match:\n        results[\"failed\"] = int(match.group(1))\n        results[\"passed\"] = int(match.group(2))\n        results[\"skipped\"] = int(match.group(3))\n        results[\"warnings\"] = int(match.group(4))\n        results[\"total\"] = results[\"passed\"] + results[\"failed\"] + results[\"skipped\"]\n        return results\n\n    # Pattern 2: Just passed/skipped (no failures)\n    simple_pattern = r\"(\\d+)\\s+passed[,\\s]+(\\d+)\\s+skipped\"\n    match = re.search(simple_pattern, output_plain)\n    if match:\n        results[\"passed\"] = int(match.group(1))\n        results[\"skipped\"] = int(match.group(2))\n        results[\"total\"] = results[\"passed\"] + results[\"skipped\"]\n        return results\n\n    # Pattern 3: Count dots in progress output (rough estimate)\n    # Count dots (.) which represent passed tests\n    dot_count = output_plain.count(\".\")\n    if dot_count > 0:\n        results[\"passed\"] = dot_count\n        results[\"total\"] = dot_count\n\n    return results\n\n\n@handle_errors(\"extracting pytest session info\", user_friendly=False, default_return={})\ndef extract_pytest_session_info(output_text: str) -> dict:\n    \"\"\"Extract pytest session information from output text.\"\"\"\n    session_info = {}\n    if not output_text:\n        return session_info\n\n    output_plain = ANSI_ESCAPE_RE.sub(\"\", output_text)\n\n    # Extract number of workers\n    workers_match = re.search(\n        r\"created:\\s*(\\d+)/(\\d+)\\s+workers\", output_plain, re.IGNORECASE\n    )\n    if workers_match:\n        session_info[\"workers_created\"] = int(workers_match.group(1))\n        session_info[\"workers_total\"] = int(workers_match.group(2))\n\n    # Extract number of test items\n    items_match = re.search(\n        r\"(\\d+)\\s+workers\\s+\\[(\\d+)\\s+items?\\]\", output_plain, re.IGNORECASE\n    )\n    if items_match:\n        session_info[\"test_items\"] = int(items_match.group(2))\n\n    # Extract collected/deselected counts\n    collected_match = re.search(\n        r\"collected\\s+(\\d+)\\s+items?(?:\\s+/\\s+(\\d+)\\s+deselected)?\",\n        output_plain,\n        re.IGNORECASE,\n    )\n    if collected_match:\n        session_info[\"collected\"] = int(collected_match.group(1))\n        if collected_match.group(2):\n            session_info[\"deselected\"] = int(collected_match.group(2))\n\n    # Extract pytest plugins\n    plugins_match = re.search(r\"plugins:\\s+(.+?)(?:\\n|$)\", output_plain, re.IGNORECASE)\n    if plugins_match:\n        plugins_str = plugins_match.group(1)\n        # Parse plugins (format: \"plugin1-version, plugin2-version\")\n        plugins = [p.strip() for p in plugins_str.split(\",\")]\n        session_info[\"plugins\"] = plugins\n\n    return session_info\n\n\n@handle_errors(\"saving partial test results\", user_friendly=False, default_return=None)\ndef save_partial_results(\n    junit_xml_path: str,\n    interrupted: bool = False,\n    output_text: str = None,\n    test_context: dict = None,\n):\n    \"\"\"Save partial test results from JUnit XML, falling back to output text parsing.\"\"\"\n    global _captured_output_lines\n\n    results = {\n        \"passed\": 0,\n        \"failed\": 0,\n        \"skipped\": 0,\n        \"warnings\": 0,\n        \"errors\": 0,\n        \"total\": 0,\n        \"deselected\": 0,\n    }\n    failure_details = []  # List of failure dicts with test names and error messages\n\n    # First try to parse JUnit XML\n    if junit_xml_path and os.path.exists(junit_xml_path):\n        # Wait a moment for file to be written (especially important on interrupt)\n        if interrupted:\n            time.sleep(0.5)  # Give pytest time to write what it can\n        results = parse_junit_xml(junit_xml_path)\n        # Extract failure details even when interrupted\n        failure_details = extract_failures_from_junit_xml(junit_xml_path)\n\n    # If XML parsing returned zeros and we have output text, try extracting from output\n    if results.get(\"total\", 0) == 0 and (output_text or _captured_output_lines):\n        output_to_parse = output_text or \"\".join(_captured_output_lines)\n        if output_to_parse:\n            output_results = extract_results_from_output(output_to_parse)\n            # Merge results, preferring non-zero values\n            for key in results:\n                if output_results.get(key, 0) > 0 or results.get(key, 0) == 0:\n                    results[key] = output_results.get(key, 0)\n\n    # Extract pytest session info from output\n    output_to_parse = output_text or \"\".join(_captured_output_lines)\n    session_info = {}\n    if output_to_parse:\n        session_info = extract_pytest_session_info(output_to_parse)\n\n    # Extract failure details from output text if JUnit XML didn't provide them\n    if not failure_details and output_to_parse:\n        # Try to extract from \"short test summary info\" section\n        output_plain = ANSI_ESCAPE_RE.sub(\"\", output_to_parse)\n        # Look for \"FAILED tests/path.py::TestClass::test_method\" lines\n        failed_test_pattern = r\"FAILED\\s+([^\\s]+::[^\\s]+)\"\n        failed_tests = re.findall(failed_test_pattern, output_plain)\n        for test_name in failed_tests:\n            failure_details.append(\n                {\n                    \"test\": test_name,\n                    \"type\": \"failure\",\n                    \"message\": \"Extracted from output (JUnit XML unavailable)\",\n                    \"details\": \"\",\n                }\n            )\n\n    # If interrupted, use test_items from session_info as the actual total\n    # (JUnit XML might only contain completed tests, not all tests that were supposed to run)\n    if interrupted and session_info and \"test_items\" in session_info:\n        actual_total = session_info[\"test_items\"]\n        # Only update total if it's less than actual_total (meaning some tests didn't complete)\n        if results.get(\"total\", 0) < actual_total:\n            # Update total to reflect all tests that were supposed to run\n            results[\"total\"] = actual_total\n            # Note: we don't know which of the incomplete tests would have passed/failed,\n            # so we leave passed/failed/skipped as-is (they only reflect completed tests)\n\n    partial_data = {\n        \"timestamp\": now_timestamp_full(),\n        \"interrupted\": interrupted,\n        \"results\": results,\n        \"junit_xml_path\": junit_xml_path,\n        \"failures\": failure_details,  # Include failure details even when interrupted\n    }\n\n    # Add test run context if provided\n    if test_context:\n        partial_data[\"test_run_context\"] = test_context\n\n    # Add pytest session info if available\n    if session_info:\n        partial_data[\"pytest_session\"] = session_info\n\n    # Save to partial results file (both main file and timestamped backup)\n    try:\n        # Save to main partial_results.json for easy access\n        with open(_partial_results_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(partial_data, f, indent=2)\n\n        # Also save timestamped copy to backups directory so it doesn't get overwritten\n        timestamp = now_timestamp_filename()\n        phase = test_context.get(\"phase\", \"unknown\") if test_context else \"unknown\"\n        mode = test_context.get(\"mode\", \"unknown\") if test_context else \"unknown\"\n        partial_filename = f\"partial_results_{phase}_{mode}_{timestamp}.json\"\n        timestamped_partial_file = _backups_dir / partial_filename\n\n        with open(timestamped_partial_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(partial_data, f, indent=2)\n\n        if interrupted:\n            print(f\"{YELLOW}[PARTIAL RESULTS]{RESET} Saved to {_partial_results_file}\")\n            print(\n                f\"{YELLOW}[PARTIAL RESULTS]{RESET} Timestamped copy: {timestamped_partial_file.name}\"\n            )\n            print(\n                f\"  Passed: {results.get('passed', 0)}, Failed: {results.get('failed', 0)}, Skipped: {results.get('skipped', 0)}\"\n            )\n            if test_context:\n                print(f\"  Phase: {phase}, Mode: {mode}\")\n    except Exception as e:\n        # Don't fail if partial results can't be saved\n        print(f\"{YELLOW}[WARNING]{RESET} Could not save partial results: {e}\")\n\n\n@handle_errors(\"detecting stuck process\", user_friendly=False, default_return=False)\ndef detect_stuck_process(\n    last_output_time: float, current_time: float, threshold: float = 600\n) -> bool:\n    \"\"\"Detect if process appears stuck (no output for extended period).\"\"\"\n    if last_output_time is None:\n        return False\n\n    time_since_output = current_time - last_output_time\n    return time_since_output > threshold\n\n\n@handle_errors(\n    \"extracting failure details from JUnit XML\", user_friendly=False, default_return=[]\n)\ndef extract_failures_from_junit_xml(xml_path: str) -> list[dict[str, str]]:\n    \"\"\"\n    Extract detailed failure information from JUnit XML.\n\n    Returns a list of dicts with 'test', 'message', and 'type' keys.\n    \"\"\"\n    failures = []\n\n    if not os.path.exists(xml_path):\n        return failures\n\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n\n        # Find all testcase elements with failure or error children\n        for testcase in root.findall(\".//testcase\"):\n            # Check for failure element\n            failure = testcase.find(\"failure\")\n            if failure is not None:\n                test_name = (\n                    f\"{testcase.get('classname', '')}::{testcase.get('name', '')}\"\n                )\n                failures.append(\n                    {\n                        \"test\": test_name,\n                        \"type\": \"failure\",\n                        \"message\": failure.get(\"message\", \"\"),\n                        \"details\": failure.text or \"\",\n                    }\n                )\n\n            # Check for error element\n            error = testcase.find(\"error\")\n            if error is not None:\n                test_name = (\n                    f\"{testcase.get('classname', '')}::{testcase.get('name', '')}\"\n                )\n                failures.append(\n                    {\n                        \"test\": test_name,\n                        \"type\": \"error\",\n                        \"message\": error.get(\"message\", \"\"),\n                        \"details\": error.text or \"\",\n                    }\n                )\n    except (ET.ParseError, ValueError, AttributeError):\n        pass\n\n    return failures\n\n\n@handle_errors(\n    \"parsing JUnit XML report\",\n    user_friendly=False,\n    default_return={\n        \"passed\": 0,\n        \"failed\": 0,\n        \"skipped\": 0,\n        \"warnings\": 0,\n        \"errors\": 0,\n        \"total\": 0,\n        \"deselected\": 0,\n    },\n)\ndef parse_junit_xml(xml_path: str) -> dict[str, int]:\n    \"\"\"\n    Parse JUnit XML report to extract test statistics.\n\n    Returns a dictionary with: passed, failed, skipped, warnings, errors, total\n    \"\"\"\n    results = {\n        \"passed\": 0,\n        \"failed\": 0,\n        \"skipped\": 0,\n        \"warnings\": 0,\n        \"errors\": 0,\n        \"total\": 0,\n        \"deselected\": 0,\n    }\n\n    if not os.path.exists(xml_path):\n        return results\n\n    # Parse XML - errors are handled by decorator, but we catch parsing errors\n    # to return empty results gracefully\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n\n        # JUnit XML structure: <testsuites> contains <testsuite> elements\n        # Each testsuite has attributes: tests, failures, errors, skipped\n        for testsuite in root.findall(\".//testsuite\"):\n            tests = int(testsuite.get(\"tests\", 0))\n            failures = int(testsuite.get(\"failures\", 0))\n            errors = int(testsuite.get(\"errors\", 0))\n            skipped = int(testsuite.get(\"skipped\", 0))\n\n            results[\"total\"] += tests\n            results[\"failed\"] += failures\n            results[\"errors\"] += errors\n            results[\"skipped\"] += skipped\n            results[\"passed\"] += tests - failures - errors - skipped\n\n        # If no testsuites found, try direct attributes on root\n        if results[\"total\"] == 0:\n            results[\"total\"] = int(root.get(\"tests\", 0))\n            results[\"failed\"] = int(root.get(\"failures\", 0))\n            results[\"errors\"] = int(root.get(\"errors\", 0))\n            results[\"skipped\"] = int(root.get(\"skipped\", 0))\n            results[\"passed\"] = (\n                results[\"total\"]\n                - results[\"failed\"]\n                - results[\"errors\"]\n                - results[\"skipped\"]\n            )\n    except (ET.ParseError, ValueError, AttributeError):\n        # If parsing fails (malformed XML, invalid values, etc.), return empty results\n        # Other exceptions are handled by decorator\n        pass\n\n    return results\n\n\n@handle_errors(\n    \"running test command\",\n    user_friendly=False,\n    default_return={\n        \"success\": False,\n        \"output\": \"\",\n        \"results\": {},\n        \"duration\": 0,\n        \"warnings\": \"\",\n        \"failures\": \"\",\n    },\n)\ndef run_command(\n    cmd,\n    description,\n    progress_interval: int = 30,\n    capture_output: bool = True,\n    test_context: dict = None,\n):\n    \"\"\"\n    Run a command and return results with periodic progress logs.\n\n    Args:\n        cmd: Command to run\n        description: Description for progress messages\n        progress_interval: Seconds between progress updates\n        capture_output: If True, capture results via JUnit XML (always True in practice)\n        test_context: Optional dict with test run context (mode, phase, config, etc.)\n\n    Returns:\n        dict with 'success', 'output', 'results', 'duration', 'warnings', 'failures' keys\n    \"\"\"\n    global _interrupt_requested, _current_process, _current_junit_xml, _last_output_time, _captured_output_lines, _current_test_context\n\n    # Store test context globally for interrupt handler\n    _current_test_context = test_context\n\n    # Reset interrupt flag and captured output\n    _interrupt_requested = False\n    _last_output_time = None\n    _captured_output_lines = []\n\n    # Register signal handlers for graceful shutdown\n    @handle_errors(\"signal handler\", default_return=None)\n    def signal_handler(signum, frame):\n        interrupt_handler(signum, frame)\n\n    # Register handlers (Windows supports SIGINT, SIGTERM may not be available)\n    if hasattr(signal, \"SIGINT\"):\n        signal.signal(signal.SIGINT, signal_handler)\n    if hasattr(signal, \"SIGTERM\"):\n        try:\n            signal.signal(signal.SIGTERM, signal_handler)\n        except (ValueError, OSError):\n            pass  # SIGTERM not available on Windows\n\n    print(f\"\\nRunning: {description}...\")\n\n    start_time = time.time()\n\n    # To preserve pytest's colors, let it write directly to the terminal (no pipe)\n    # Use JUnit XML report to get structured results without capturing output\n    import tempfile\n\n    with tempfile.NamedTemporaryFile(\n        mode=\"w+\", delete=False, encoding=\"utf-8\", suffix=\".xml\"\n    ) as tmp_file:\n        junit_xml_path = tmp_file.name\n        _current_junit_xml = junit_xml_path\n\n    try:\n        # Generate JUnit XML report for parsing (doesn't affect colors)\n        cmd_with_junit = cmd + [\"--junit-xml\", junit_xml_path]\n\n        # Ensure pytest keeps ANSI colors even though we're piping output\n        contains_color_flag = any(opt.startswith(\"--color\") for opt in cmd_with_junit)\n        if not contains_color_flag:\n            cmd_with_junit.append(\"--color=yes\")\n\n        # Capture output to parse warnings, but also write to terminal to preserve colors\n        # Use a pipe that we can read from AND write to terminal\n        output_queue = queue.Queue()\n        output_lines = []\n\n        @handle_errors(\n            \"reading output from pipe\", user_friendly=False, default_return=None\n        )\n        def read_output(pipe, queue_obj):\n            \"\"\"Read from pipe and put lines in queue, also write to terminal.\"\"\"\n            global _last_output_time, _captured_output_lines\n            try:\n                for line in iter(pipe.readline, \"\"):\n                    if line:\n                        _last_output_time = time.time()  # Track last output time\n                        queue_obj.put(line)\n                        # Store in global list for partial results extraction\n                        _captured_output_lines.append(line)\n                        # Also write to terminal to preserve colors\n                        sys.stdout.write(line)\n                        sys.stdout.flush()\n            finally:\n                try:\n                    pipe.close()\n                except Exception:\n                    pass  # Ignore errors closing pipe\n\n        # Let pytest write directly to terminal - this preserves ALL colors\n        # We'll parse warnings from the output if we can capture it\n        # On Windows, create a new process group so we can kill the entire process tree\n        creation_flags = 0\n        if sys.platform == \"win32\":\n            # CREATE_NEW_PROCESS_GROUP allows us to kill the entire process tree\n            import subprocess as sp\n\n            creation_flags = sp.CREATE_NEW_PROCESS_GROUP\n            print(\n                f\"{GREEN}[PROCESS]{RESET} Creating process with CREATE_NEW_PROCESS_GROUP flag for tree termination\"\n            )\n\n        process = subprocess.Popen(\n            cmd_with_junit,\n            stdout=subprocess.PIPE,  # Capture for parsing\n            stderr=subprocess.STDOUT,  # Merge stderr into stdout\n            universal_newlines=True,\n            bufsize=1,\n            creationflags=creation_flags if sys.platform == \"win32\" else 0,\n        )\n\n        if sys.platform == \"win32\":\n            print(\n                f\"{GREEN}[PROCESS]{RESET} Started pytest process PID {process.pid} (can kill tree)\"\n            )\n\n        _current_process = process\n\n        # Start thread to read output and display it\n        output_thread = threading.Thread(\n            target=read_output, args=(process.stdout, output_queue)\n        )\n        output_thread.daemon = True\n        output_thread.start()\n\n        # Monitor progress while process runs\n        # Add maximum timeout to prevent infinite hanging (30 minutes max)\n        max_duration = 30 * 60  # 30 minutes\n        next_tick = start_time + max(1, progress_interval)\n        next_resource_check = (\n            start_time + 60\n        )  # Check resources every minute (more frequent if memory high)\n        next_partial_save = start_time + 120  # Save partial results every 2 minutes\n        stuck_warning_shown = False\n        last_memory_percent = 0  # Track last memory reading to adjust check frequency\n\n        while process.poll() is None:\n            now = time.time()\n            elapsed = now - start_time\n\n            # Check for interrupt request\n            if _interrupt_requested:\n                print(\n                    f\"\\n{YELLOW}[INTERRUPT]{RESET} Interrupt requested - terminating gracefully...\"\n                )\n                # Collect output before saving\n                output_text = (\n                    \"\".join(_captured_output_lines) if _captured_output_lines else None\n                )\n                save_partial_results(\n                    junit_xml_path,\n                    interrupted=True,\n                    output_text=output_text,\n                    test_context=_current_test_context,\n                )\n                # Kill entire process tree (especially important for pytest-xdist workers on Windows)\n                if sys.platform == \"win32\":\n                    kill_process_tree_windows(process.pid)\n                else:\n                    process.terminate()\n                shutdown_start = time.time()\n                while process.poll() is None and (time.time() - shutdown_start) < 5:\n                    time.sleep(0.1)\n                if process.poll() is None:\n                    # Force kill if still running\n                    if sys.platform == \"win32\":\n                        kill_process_tree_windows(process.pid)\n                    else:\n                        process.kill()\n                break\n\n            # Check for timeout\n            if elapsed > max_duration:\n                print(\n                    f\"\\n{RED}[ERROR]{RESET} {description} exceeded maximum duration ({max_duration}s) - terminating\"\n                )\n                save_partial_results(\n                    junit_xml_path, interrupted=True, test_context=_current_test_context\n                )\n                # Kill entire process tree (especially important for pytest-xdist workers on Windows)\n                if sys.platform == \"win32\":\n                    kill_process_tree_windows(process.pid)\n                else:\n                    process.terminate()\n                # Wait a bit for graceful shutdown (use polling since wait() timeout may not be available)\n                shutdown_start = time.time()\n                while process.poll() is None and (time.time() - shutdown_start) < 5:\n                    time.sleep(0.1)\n                if process.poll() is None:\n                    # Force kill if still running\n                    if sys.platform == \"win32\":\n                        kill_process_tree_windows(process.pid)\n                    else:\n                        process.kill()\n                break\n\n            # Check for stuck process (no output for 10 minutes)\n            if (\n                detect_stuck_process(_last_output_time, now, threshold=600)\n                and not stuck_warning_shown\n            ):\n                print(\n                    f\"\\n{YELLOW}[WARNING]{RESET} Process appears stuck (no output for 10 minutes)\"\n                )\n                print(f\"{YELLOW}[WARNING]{RESET} Saving partial results...\")\n                save_partial_results(\n                    junit_xml_path, interrupted=True, test_context=_current_test_context\n                )\n                stuck_warning_shown = True\n\n            # Monitor resources periodically (more frequently if memory is high)\n            if now >= next_resource_check:\n                resources = monitor_resources()\n                if resources:\n                    check_resource_warnings(resources)\n                    # Check for critical resource exhaustion - terminate if memory is critically high\n                    if check_critical_resources(resources):\n                        print(\n                            f\"\\n{RED}[CRITICAL]{RESET} System memory critically high (>98%) - terminating tests to prevent system crash\"\n                        )\n                        print(\n                            f\"{RED}[CRITICAL]{RESET} Saving partial results before termination...\"\n                        )\n                        # Collect output before saving\n                        output_text = (\n                            \"\".join(_captured_output_lines)\n                            if _captured_output_lines\n                            else None\n                        )\n                        save_partial_results(\n                            junit_xml_path,\n                            interrupted=True,\n                            output_text=output_text,\n                            test_context=_current_test_context,\n                        )\n                        # Kill entire process tree (especially important for pytest-xdist workers on Windows)\n                        if sys.platform == \"win32\":\n                            kill_process_tree_windows(process.pid)\n                        else:\n                            process.terminate()\n                        shutdown_start = time.time()\n                        while (\n                            process.poll() is None\n                            and (time.time() - shutdown_start) < 5\n                        ):\n                            time.sleep(0.1)\n                        if process.poll() is None:\n                            # Force kill if still running\n                            if sys.platform == \"win32\":\n                                kill_process_tree_windows(process.pid)\n                            else:\n                                process.kill()\n                        break\n\n                    # Adjust check frequency based on memory usage\n                    current_memory = resources.get(\"system_memory_percent\", 0)\n                    if current_memory > 95:\n                        # Check every 15 seconds if memory is critically high\n                        next_resource_check = now + 15\n                    elif current_memory > 90:\n                        # Check every 30 seconds if memory is very high\n                        next_resource_check = now + 30\n                    else:\n                        # Normal check every minute\n                        next_resource_check = now + 60\n                    last_memory_percent = current_memory\n                else:\n                    next_resource_check = (\n                        now + 60\n                    )  # Default check every minute if monitoring unavailable\n\n            # Save partial results periodically (less frequently to reduce file I/O)\n            # Only save every 5 minutes during normal runs (not on interrupt - those are saved immediately)\n            if now >= next_partial_save:\n                save_partial_results(junit_xml_path, interrupted=False)\n                next_partial_save = (\n                    now + 300\n                )  # Save every 5 minutes (reduced from 2 minutes)\n\n                # Also save captured output for worker test assignment extraction\n                # This helps identify which tests each worker ran (for memory leak debugging)\n                if _captured_output_lines:\n                    output_file = Path(\"tests/logs/pytest_console_output.txt\")\n                    output_file.parent.mkdir(parents=True, exist_ok=True)\n                    try:\n                        with open(\n                            output_file, \"w\", encoding=\"utf-8\", errors=\"replace\"\n                        ) as f:\n                            f.write(\"\".join(_captured_output_lines))\n                    except Exception:\n                        pass  # Non-critical, don't fail on this\n\n            if now >= next_tick:\n                elapsed_int = int(elapsed)\n                print(f\"[PROGRESS] {description} running for {elapsed_int}s...\")\n                next_tick += max(1, progress_interval)\n            time.sleep(0.5)\n\n        # Wait for process to complete\n        process.wait()\n\n        # Wait for output thread to finish\n        output_thread.join(timeout=1)\n\n        # Save final captured output for worker test assignment extraction\n        # This helps identify which tests each worker ran (for memory leak debugging)\n        if _captured_output_lines:\n            output_file = Path(\"tests/logs/pytest_console_output.txt\")\n            output_file.parent.mkdir(parents=True, exist_ok=True)\n            try:\n                with open(output_file, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n                    f.write(\"\".join(_captured_output_lines))\n            except Exception:\n                pass  # Non-critical, don't fail on this\n\n        # CRITICAL: On Windows, ensure all child processes (pytest-xdist workers) are terminated\n        # Even if the main process exited, workers might still be running\n        if sys.platform == \"win32\":\n            # Kill process tree to ensure workers are terminated\n            try:\n                if process.poll() is None:\n                    # Process still running - kill it and its children\n                    kill_process_tree_windows(process.pid)\n                else:\n                    # Process exited but workers might be orphaned - kill tree anyway\n                    kill_process_tree_windows(process.pid)\n            except Exception:\n                pass  # Process might already be gone, that's fine\n\n            # Also check for any orphaned pytest worker processes\n            cleanup_orphaned_pytest_processes()\n\n            # CRITICAL: Consolidate worker logs even if pytest didn't finish normally\n            # When process is forcefully terminated, pytest_sessionfinish may not run\n            try:\n                # Small delay to ensure workers have closed their file handles\n                time.sleep(1.0)  # Longer delay for normal completion\n                from tests.conftest import _consolidate_worker_logs\n\n                print(f\"{GREEN}[CLEANUP]{RESET} Consolidating worker log files...\")\n                _consolidate_worker_logs()\n                print(f\"{GREEN}[CLEANUP]{RESET} Worker log consolidation complete\")\n            except Exception as e:\n                print(\n                    f\"{YELLOW}[CLEANUP]{RESET} Could not consolidate worker logs: {e}\"\n                )\n                print(\n                    f\"{YELLOW}[CLEANUP]{RESET} Run manually: python scripts/testing/consolidate_worker_logs.py\"\n                )\n\n        # Clear global process reference\n        _current_process = None\n        _current_junit_xml = None\n\n        # Collect all output lines\n        while not output_queue.empty():\n            try:\n                output_lines.append(output_queue.get_nowait())\n            except queue.Empty:\n                break\n\n        output = \"\".join(output_lines)\n        output_plain = ANSI_ESCAPE_RE.sub(\"\", output)\n\n        summary_counts = {}\n        summary_line_matches = re.findall(\n            r\"={5,}\\s*(.+?)\\s*={5,}\", output_plain, re.MULTILINE\n        )\n        if summary_line_matches:\n            summary_content = summary_line_matches[-1]\n            count_matches = re.findall(\n                r\"(\\d+)\\s+(failed|passed|skipped|warnings?|deselected|errors?)\",\n                summary_content,\n                re.IGNORECASE,\n            )\n            for count_str, label in count_matches:\n                key = label.lower()\n                if key in (\"warning\", \"warnings\"):\n                    key = \"warnings\"\n                elif key in (\"error\", \"errors\"):\n                    key = \"errors\"\n                summary_counts[key] = int(count_str)\n\n        # Parse results from JUnit XML (with atomic read to ensure consistency)\n        # Use atomic file operations: read from temp location first\n        results = {}\n        try:\n            # Ensure file is flushed before reading\n            if os.path.exists(junit_xml_path):\n                # On Windows, ensure file handle is closed\n                time.sleep(0.1)  # Small delay to ensure file is written\n                results = parse_junit_xml(junit_xml_path)\n        except Exception as e:\n            print(f\"{YELLOW}[WARNING]{RESET} Could not parse JUnit XML: {e}\")\n            # Try to save partial results anyway\n            if os.path.exists(junit_xml_path):\n                save_partial_results(junit_xml_path, interrupted=_interrupt_requested)\n\n        # Ensure expected keys exist\n        results.setdefault(\"warnings\", 0)\n        results.setdefault(\"deselected\", 0)\n\n        # Update results with parsed summary counts\n        for key, value in summary_counts.items():\n            results[key] = value\n\n        # Parse warnings/failures details from pytest output\n        warnings_text = \"\"\n        failures_text = \"\"\n\n        # Extract warnings count from pytest output (e.g., \"10 warnings in 143.99s\")\n        # Look for pattern like \"1 failed, 3023 passed, 1 skipped, 10 warnings in 143.99s\"\n        warnings_match = re.search(r\"(\\d+)\\s+warnings?\\s+in\\s+[\\d.]+s\", output_plain)\n        if not warnings_match:\n            # Also try pattern without \"in\" (some pytest versions)\n            warnings_match = re.search(r\",\\s*(\\d+)\\s+warnings?\\s+in\", output_plain)\n        if warnings_match:\n            results[\"warnings\"] = int(warnings_match.group(1))\n\n        # Extract failures from short test summary\n        # Try multiple patterns to catch different pytest output formats\n        failures_text = \"\"\n\n        # Pattern 1: Standard pytest format \"FAILED tests/path.py::TestClass::test_method\"\n        failures_section = re.search(\n            r\"FAILED\\s+(.+?)(?=\\n\\n|\\n===|$)\", output_plain, re.DOTALL\n        )\n        if failures_section:\n            failures_text = failures_section.group(1).strip()\n        else:\n            # Pattern 2: Look for \"FAILURES\" section header followed by test names\n            failures_header = re.search(\n                r\"FAILURES\\s*=\\s*={3,}\\s*\\n(.+?)(?=\\n={3,}|\\n\\n|$)\",\n                output_plain,\n                re.DOTALL,\n            )\n            if failures_header:\n                failures_text = failures_header.group(1).strip()\n            else:\n                # Pattern 3: Extract individual FAILED lines (for parallel execution)\n                failed_lines = re.findall(r\"FAILED\\s+([^\\s]+::[^\\s]+)\", output_plain)\n                if failed_lines:\n                    failures_text = \"\\n\".join(failed_lines)\n\n        # Also extract failure details from JUnit XML if available (even when interrupted)\n        failure_details = []\n        if os.path.exists(junit_xml_path):\n            try:\n                failure_details = extract_failures_from_junit_xml(junit_xml_path)\n            except Exception:\n                pass  # Ignore errors extracting failures\n\n        duration = time.time() - start_time\n\n        # Print status message\n        if _interrupt_requested:\n            print(\n                f\"\\n{YELLOW}[INTERRUPTED]{RESET} {description} was interrupted after {duration}s\"\n            )\n            print(\n                f\"{YELLOW}[INTERRUPTED]{RESET} Partial results saved to {_partial_results_file}\"\n            )\n        elif process.returncode == 0:\n            print(\n                f\"\\n{GREEN}[SUCCESS]{RESET} {description} completed successfully in {duration}s\"\n            )\n        else:\n            print(\n                f\"\\n{RED}[FAILED]{RESET} {description} failed with exit code {process.returncode} after {duration}s\"\n            )\n            # Save partial results on failure\n            save_partial_results(\n                junit_xml_path, interrupted=False, test_context=_current_test_context\n            )\n\n        # Return dict with all information\n        return {\n            \"success\": process.returncode == 0 and not _interrupt_requested,\n            \"output\": output,\n            \"results\": results,\n            \"duration\": duration,\n            \"warnings\": warnings_text,\n            \"failures\": failures_text,\n            \"failure_details\": failure_details,  # Include structured failure details\n            \"interrupted\": _interrupt_requested,\n        }\n    finally:\n        # Save final results before cleanup (atomic write)\n        if os.path.exists(junit_xml_path):\n            try:\n                # Save to backup location with timestamp (using consolidated backups directory)\n                timestamp = now_timestamp_filename()\n                backup_path = _backups_dir / f\"junit_results_{timestamp}.xml\"\n\n                # Atomic copy: write to temp first, then rename\n                import shutil\n\n                temp_backup = str(backup_path) + \".tmp\"\n                shutil.copy2(junit_xml_path, temp_backup)\n                os.rename(temp_backup, str(backup_path))\n\n                # Keep only last 5 backups\n                backups = sorted(\n                    _backups_dir.glob(\"junit_results_*.xml\"), key=os.path.getmtime\n                )\n                if len(backups) > 5:\n                    for old_backup in backups[:-5]:\n                        try:\n                            old_backup.unlink()\n                        except Exception:\n                            pass\n            except Exception as e:\n                print(f\"{YELLOW}[WARNING]{RESET} Could not save result backup: {e}\")\n\n        # Clean up temp file (but keep it if interrupted for recovery)\n        if not _interrupt_requested:\n            try:\n                if os.path.exists(junit_xml_path):\n                    os.unlink(junit_xml_path)\n            except:\n                pass\n        # CRITICAL: On Windows, ensure all processes are killed even if there was an exception\n        if sys.platform == \"win32\" and _current_process:\n            try:\n                if _current_process.poll() is None:\n                    # Process still running - kill it and its children\n                    kill_process_tree_windows(_current_process.pid)\n                else:\n                    # Process exited but workers might be orphaned - kill tree anyway\n                    kill_process_tree_windows(_current_process.pid)\n            except Exception:\n                pass\n\n            # Also check for any orphaned pytest worker processes\n            cleanup_orphaned_pytest_processes()\n\n            # CRITICAL: Try to consolidate worker logs even on exception\n            # This ensures logs are consolidated even if something went wrong\n            try:\n                time.sleep(1.0)  # Delay to allow workers to close file handles\n                from tests.conftest import _consolidate_worker_logs\n\n                _consolidate_worker_logs()\n            except Exception:\n                pass  # Don't fail if consolidation doesn't work\n\n        # Clear global references\n        _current_process = None\n        _current_junit_xml = None\n\n\n@handle_errors(\"setting up test logger\", default_return=None)\ndef setup_test_logger():\n    \"\"\"\n    Set up logger for test duration logging.\n\n    Creates a logger for test run duration logging and ensures the tests/logs\n    directory exists. Returns a configured logger instance.\n\n    Returns:\n        logging.Logger: Configured logger instance for test runs\n    \"\"\"\n    logger = logging.getLogger(\"mhm_tests.run_tests\")\n    logger.setLevel(logging.INFO)\n\n    # Only add handler if one doesn't exist\n    if not logger.handlers:\n        # Ensure tests/logs directory exists\n        log_dir = Path(\"tests/logs\")\n        log_dir.mkdir(parents=True, exist_ok=True)\n\n        # Add file handler for test_run.log\n        log_file = log_dir / \"test_run.log\"\n        file_handler = logging.FileHandler(log_file, mode=\"a\", encoding=\"utf-8\")\n        file_handler.setLevel(logging.INFO)\n        formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n        )\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n        # Prevent propagation to root logger\n        logger.propagate = False\n\n    return logger\n\n\n@handle_errors(\"printing combined test summary\", default_return=None)\ndef print_combined_summary(\n    parallel_results: dict | None,\n    no_parallel_results: dict | None,\n    description: str,\n):\n    \"\"\"\n    Print a combined summary of test results from both parallel and serial runs.\n\n    Args:\n        parallel_results: Results dict from parallel test run (or None if not run)\n        no_parallel_results: Results dict from serial test run (or None if not run)\n        description: Test mode description\n    \"\"\"\n    # Extract results from both runs\n    parallel_res = (\n        parallel_results.get(\"results\", {})\n        if parallel_results and isinstance(parallel_results, dict)\n        else {}\n    )\n    no_parallel_res = (\n        no_parallel_results.get(\"results\", {})\n        if no_parallel_results and isinstance(no_parallel_results, dict)\n        else {}\n    )\n\n    # Combine results\n    combined = {\n        \"passed\": parallel_res.get(\"passed\", 0) + no_parallel_res.get(\"passed\", 0),\n        \"failed\": parallel_res.get(\"failed\", 0) + no_parallel_res.get(\"failed\", 0),\n        \"skipped\": parallel_res.get(\"skipped\", 0) + no_parallel_res.get(\"skipped\", 0),\n        \"warnings\": parallel_res.get(\"warnings\", 0)\n        + no_parallel_res.get(\"warnings\", 0),\n        \"errors\": parallel_res.get(\"errors\", 0) + no_parallel_res.get(\"errors\", 0),\n        \"deselected\": parallel_res.get(\"deselected\", 0)\n        + no_parallel_res.get(\"deselected\", 0),\n    }\n    combined[\"total\"] = (\n        combined[\"passed\"]\n        + combined[\"failed\"]\n        + combined[\"skipped\"]\n        + combined[\"errors\"]\n    )\n\n    # Calculate total duration (ensure floats for decimal precision)\n    parallel_duration = (\n        float(parallel_results.get(\"duration\", 0))\n        if parallel_results and isinstance(parallel_results, dict)\n        else 0.0\n    )\n    no_parallel_duration = (\n        float(no_parallel_results.get(\"duration\", 0))\n        if no_parallel_results and isinstance(no_parallel_results, dict)\n        else 0.0\n    )\n    total_duration = parallel_duration + no_parallel_duration\n\n    # Collect all warnings and failures\n    all_warnings = []\n    all_failures = []\n\n    if parallel_results and isinstance(parallel_results, dict):\n        if parallel_results.get(\"warnings\"):\n            all_warnings.append((\"Parallel Tests\", parallel_results[\"warnings\"]))\n        if parallel_results.get(\"failures\"):\n            all_failures.append((\"Parallel Tests\", parallel_results[\"failures\"]))\n\n    if no_parallel_results and isinstance(no_parallel_results, dict):\n        if no_parallel_results.get(\"warnings\"):\n            all_warnings.append((\"Serial Tests\", no_parallel_results[\"warnings\"]))\n        if no_parallel_results.get(\"failures\"):\n            all_failures.append((\"Serial Tests\", no_parallel_results[\"failures\"]))\n\n    # Print combined summary\n    print(f\"\\n{'='*80}\")\n    print(f\"COMBINED TEST RESULTS SUMMARY\")\n    print(f\"{'='*80}\")\n    print(f\"Mode: {description}\")\n    print(f\"\\nTest Statistics:\")\n    print(f\"  Total Tests:  {combined['total']}\")\n    print(f\"  Passed:       {combined['passed']}\")\n    print(f\"  Failed:       {combined['failed']}\")\n    print(f\"  Skipped:      {combined['skipped']}\")\n    print(f\"  Deselected:   {combined['deselected']}\")\n    print(f\"  Warnings:     {combined['warnings']}\")\n\n    # Set up logger for duration logging\n    test_logger = setup_test_logger()\n\n    # Show breakdown if we ran tests in two phases (parallel + serial)\n    if no_parallel_results and isinstance(no_parallel_results, dict):\n        print(f\"\\nBreakdown:\")\n        p_passed = parallel_res.get(\"passed\", 0)\n        p_failed = parallel_res.get(\"failed\", 0)\n        p_skipped = parallel_res.get(\"skipped\", 0)\n        p_warnings = parallel_res.get(\"warnings\", 0)\n        p_deselected = parallel_res.get(\"deselected\", 0)\n        p_interrupted = (\n            parallel_results.get(\"interrupted\", False)\n            if parallel_results and isinstance(parallel_results, dict)\n            else False\n        )\n        s_passed = no_parallel_res.get(\"passed\", 0)\n        s_failed = no_parallel_res.get(\"failed\", 0)\n        s_skipped = no_parallel_res.get(\"skipped\", 0)\n        s_warnings = no_parallel_res.get(\"warnings\", 0)\n        s_deselected = no_parallel_res.get(\"deselected\", 0)\n\n        # Format parallel tests line with interruption indicator\n        parallel_status = \"\"\n        if p_interrupted:\n            parallel_status = f\" {YELLOW}[INTERRUPTED]{RESET}\"\n        parallel_line = f\"  Parallel Tests:    {p_passed} passed, {p_failed} failed, {p_skipped} skipped, {p_deselected} deselected, {p_warnings} warnings ({parallel_duration}s){parallel_status}\"\n        print(parallel_line)\n        print(\n            f\"  Serial Tests:      {s_passed} passed, {s_failed} failed, {s_skipped} skipped, {s_deselected} deselected, {s_warnings} warnings ({no_parallel_duration}s)\"\n        )\n\n        print(f\"\\nTotal Duration: {total_duration}s\")\n\n        # Log durations and test counts to test log file\n        p_total = p_passed + p_failed + p_skipped\n        s_total = s_passed + s_failed + s_skipped\n        total_tests = p_total + s_total\n        test_logger.info(\n            f\"TEST SUITE DURATION - Parallel: {parallel_duration:.2f}s, Serial: {no_parallel_duration:.2f}s, Total: {total_duration:.2f}s\"\n        )\n        test_logger.info(\n            f\"TEST SUITE COUNTS - Parallel: {p_total} tests ({p_passed} passed, {p_failed} failed, {p_skipped} skipped), Serial: {s_total} tests ({s_passed} passed, {s_failed} failed, {s_skipped} skipped), Total: {total_tests} tests ({combined['passed']} passed, {combined['failed']} failed, {combined['skipped']} skipped)\"\n        )\n    elif parallel_duration > 0:\n        # Single run (--no-parallel was used)\n        print(f\"\\nDuration: {parallel_duration}s\")\n\n        # Log duration and test counts to test log file\n        p_total = (\n            parallel_res.get(\"passed\", 0)\n            + parallel_res.get(\"failed\", 0)\n            + parallel_res.get(\"skipped\", 0)\n        )\n        test_logger.info(\n            f\"TEST SUITE DURATION - Total: {parallel_duration:.2f}s (single run mode)\"\n        )\n        test_logger.info(\n            f\"TEST SUITE COUNTS - Total: {p_total} tests ({parallel_res.get('passed', 0)} passed, {parallel_res.get('failed', 0)} failed, {parallel_res.get('skipped', 0)} skipped)\"\n        )\n\n    # Show failures details\n    # Collect failure details from both sources: failures_text and failure_details\n    # Priority: Use failure_details from JUnit XML (most reliable), fall back to failures_text\n    failure_details_to_print = []\n    seen_tests = set()  # Track which tests we've already printed to avoid duplicates\n\n    # First, add structured failure details from JUnit XML (most reliable source)\n    if parallel_results and isinstance(parallel_results, dict):\n        parallel_failure_details = parallel_results.get(\"failure_details\", [])\n        if parallel_failure_details:\n            for failure_detail in parallel_failure_details:\n                test_name = failure_detail.get(\"test\", \"Unknown test\")\n                if test_name not in seen_tests:\n                    seen_tests.add(test_name)\n                    message = failure_detail.get(\"message\", \"\")\n                    details = failure_detail.get(\"details\", \"\")\n                    failure_info = f\"{test_name}\"\n                    if message:\n                        failure_info += (\n                            f\"\\n  Message: {message[:200]}\"  # Truncate long messages\n                        )\n                    if details:\n                        # Truncate very long details but keep first few lines\n                        details_lines = details.split(\"\\n\")[:10]  # First 10 lines\n                        details_preview = \"\\n\".join(details_lines)\n                        if len(details) > len(details_preview):\n                            details_preview += \"\\n  ... (truncated)\"\n                        failure_info += f\"\\n  Details:\\n{details_preview}\"\n                    failure_details_to_print.append((\"Parallel Tests\", failure_info))\n\n    if no_parallel_results and isinstance(no_parallel_results, dict):\n        serial_failure_details = no_parallel_results.get(\"failure_details\", [])\n        if serial_failure_details:\n            for failure_detail in serial_failure_details:\n                test_name = failure_detail.get(\"test\", \"Unknown test\")\n                if test_name not in seen_tests:\n                    seen_tests.add(test_name)\n                    message = failure_detail.get(\"message\", \"\")\n                    details = failure_detail.get(\"details\", \"\")\n                    failure_info = f\"{test_name}\"\n                    if message:\n                        failure_info += (\n                            f\"\\n  Message: {message[:200]}\"  # Truncate long messages\n                        )\n                    if details:\n                        # Truncate very long details but keep first few lines\n                        details_lines = details.split(\"\\n\")[:10]  # First 10 lines\n                        details_preview = \"\\n\".join(details_lines)\n                        if len(details) > len(details_preview):\n                            details_preview += \"\\n  ... (truncated)\"\n                        failure_info += f\"\\n  Details:\\n{details_preview}\"\n                    failure_details_to_print.append((\"Serial Tests\", failure_info))\n\n    # Also add failures from failures_text (parsed from output) if not already covered\n    if all_failures:\n        for source, failure_text in all_failures:\n            if failure_text:\n                # Extract test names from failure_text to check for duplicates\n                test_names_in_text = re.findall(r\"([^\\s]+::[^\\s]+)\", failure_text)\n                # Only add if we haven't seen these tests already\n                if not any(test_name in seen_tests for test_name in test_names_in_text):\n                    failure_details_to_print.append((source, failure_text))\n\n    # Print all failures\n    if failure_details_to_print:\n        print(f\"\\n{RED}FAILURES:{RESET}\")\n        for source, failure_info in failure_details_to_print:\n            print(f\"\\n{RED}From {source}:{RESET}\")\n            print(failure_info)\n    elif combined[\"failed\"] > 0:\n        # If we have failures but couldn't extract details, at least show the count\n        print(f\"\\n{RED}FAILURES:{RESET}\")\n        print(\n            f\"  {combined['failed']} test(s) failed, but failure details could not be extracted.\"\n        )\n        print(\n            f\"  Check {_partial_results_file} or {_backups_dir.name}/ for detailed failure information.\"\n        )\n\n    # Show warnings details\n    if all_warnings:\n        print(f\"\\nWARNINGS:\")\n        for source, warning_text in all_warnings:\n            if warning_text:\n                print(f\"\\nFrom {source}:\")\n                print(warning_text)\n\n    # Show interruption notice if parallel tests were interrupted (before final summary)\n    if (\n        parallel_results\n        and isinstance(parallel_results, dict)\n        and parallel_results.get(\"interrupted\", False)\n    ):\n        print(f\"\\n{YELLOW}\u26a0\ufe0f  INTERRUPTION DETECTED:{RESET}\")\n        print(f\"  Parallel tests were interrupted before completion.\")\n        print(f\"  Partial results saved to: {_partial_results_file}\")\n        print(f\"  Check {_backups_dir.name}/ directory for timestamped copies.\")\n\n    # Final summary line in pytest format - match pytest's exact format\n    print()\n    parallel_deselected = parallel_res.get(\"deselected\", 0)\n    serial_deselected = (\n        no_parallel_res.get(\"deselected\", 0)\n        if no_parallel_results and isinstance(no_parallel_results, dict)\n        else None\n    )\n    if serial_deselected is None:\n        include_deselected_in_summary = parallel_deselected > 0\n    else:\n        include_deselected_in_summary = (\n            parallel_deselected > 0 and serial_deselected > 0\n        )\n\n    summary_parts = [\n        f\"{RED}{combined['failed']} failed{RESET}\",\n        f\"{GREEN}{combined['passed']} passed{RESET}\",\n    ]\n    if include_deselected_in_summary:\n        summary_parts.append(f\"{YELLOW}{combined['deselected']} deselected{RESET}\")\n    if combined[\"skipped\"] > 0:\n        summary_parts.append(f\"{YELLOW}{combined['skipped']} skipped{RESET}\")\n    summary_parts.append(f\"{YELLOW}{combined['warnings']} warnings{RESET}\")\n\n    summary_text = \", \".join(summary_parts)\n\n    # Format duration like pytest: \"143.99s (0:02:23)\"\n    duration_decimal = f\"{total_duration:.2f}s\"\n    # Calculate human-readable format (hours:minutes:seconds)\n    hours = int(total_duration // 3600)\n    minutes = int((total_duration % 3600) // 60)\n    seconds = int(total_duration % 60)\n    duration_human = f\"({hours}:{minutes:02d}:{seconds:02d})\"\n    duration_str = f\"{duration_decimal} {duration_human}\"\n\n    # Color-code border based on result state\n    if combined[\"failed\"] > 0:\n        border_color = RED\n    elif combined[\"warnings\"] > 0:\n        border_color = YELLOW\n    else:\n        border_color = GREEN\n\n    border = f\"{border_color}{'='*10}{RESET}\"\n    duration_segment = f\"{border_color}in {duration_str}{RESET}\"\n\n    # Match pytest's format with colored components\n    summary_line = f\"{border} {summary_text}, {duration_segment} {border}\"\n    print(summary_line)\n    print()\n\n\n@handle_errors(\"printing test mode information\", default_return=None)\ndef print_test_mode_info():\n    \"\"\"Print helpful information about test modes.\"\"\"\n    print(\"\\n\" + \"=\" * 60)\n    print(\"MHM TEST RUNNER - Available Modes\")\n    print(\"=\" * 60)\n    print(\"Test Modes:\")\n    print(\"  all         - Run ALL tests (default)\")\n    print(\"  fast        - Unit tests only (excluding slow tests)\")\n    print(\"  unit        - Unit tests only\")\n    print(\"  integration - Integration tests only\")\n    print(\"  behavior    - Behavior tests (excluding slow tests)\")\n    print(\"  ui          - UI tests (excluding slow tests)\")\n    print(\"  slow        - Slow tests only\")\n    print(\"\\nOptions:\")\n    print(\"  --verbose   - Verbose output\")\n    print(\"  --no-parallel - Disable parallel execution (parallel enabled by default)\")\n    print(\n        \"  --workers N - Number of parallel workers (default: 2, or 'auto' for optimal)\"\n    )\n    print(\"  --coverage  - Run with coverage reporting\")\n    print(\"  --durations-all - Show timing for all tests\")\n    print(\"  --no-shim   - Disable test data shim (for burn-in validation)\")\n    print(\n        \"  --random-order - Use random test order (for order independence validation)\"\n    )\n    print(\n        \"  --burnin-mode - Enable burn-in mode (combines --no-shim and --random-order)\"\n    )\n    print(\"\\nExamples:\")\n    print(\"  python run_tests.py                    # Run all tests in parallel\")\n    print(\"  python run_tests.py --mode fast        # Quick unit tests only\")\n    print(\"  python run_tests.py --mode all --verbose # All tests with verbose output\")\n    print(\"  python run_tests.py --no-parallel      # Disable parallel execution\")\n    print(\"  python run_tests.py --workers 4       # Use 4 parallel workers\")\n    print(\n        \"  python run_tests.py --burnin-mode     # Validation run (no shim, random order)\"\n    )\n    print(\"=\" * 60)\n\n\n@handle_errors(\"running static logging check\", default_return=False)\ndef run_static_logging_check() -> bool:\n    \"\"\"Run the static logging enforcement script before executing tests.\"\"\"\n    script_path = (\n        Path(__file__).parent / \"scripts\" / \"static_checks\" / \"check_channel_loggers.py\"\n    )\n    if not script_path.exists():\n        print(f\"[STATIC CHECK] Missing script: {script_path}\")\n        return False\n\n    result = subprocess.run(\n        [sys.executable, str(script_path)], capture_output=True, text=True\n    )\n    if result.returncode != 0:\n        print(\"[STATIC CHECK] Logging style violations detected:\")\n        if result.stdout:\n            print(result.stdout)\n        if result.stderr:\n            print(result.stderr)\n        return False\n\n    if result.stdout:\n        print(result.stdout.strip())\n    return True\n\n\ndef main():\n    \"\"\"\n    Main entry point for MHM test runner.\n\n    Parses command-line arguments and executes pytest with appropriate configuration\n    based on the selected test mode (all, fast, unit, integration, behavior, ui, slow).\n\n    Returns:\n        int: Exit code (0 for success, 1 for failure)\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"MHM Test Runner\")\n    parser.add_argument(\n        \"--mode\",\n        choices=[\"fast\", \"unit\", \"integration\", \"behavior\", \"ui\", \"all\", \"slow\"],\n        default=\"all\",\n        help=\"Test execution mode (default: all)\",\n    )\n    parser.add_argument(\n        \"--no-parallel\",\n        action=\"store_true\",\n        help=\"Disable parallel test execution (parallel is enabled by default)\",\n    )\n    parser.add_argument(\n        \"--workers\",\n        type=str,\n        default=\"auto\",\n        help=\"Number of parallel workers (default: auto to let pytest-xdist decide, or specify a number like 2, 4, etc.)\",\n    )\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output\")\n    parser.add_argument(\n        \"--coverage\", action=\"store_true\", help=\"Run with coverage reporting\"\n    )\n    parser.add_argument(\n        \"--progress-interval\",\n        type=int,\n        default=30,\n        help=\"Print progress every N seconds (default: 30)\",\n    )\n    parser.add_argument(\n        \"--durations-all\",\n        action=\"store_true\",\n        help=\"Ask pytest to report durations for all tests at the end\",\n    )\n    parser.add_argument(\n        \"--no-shim\",\n        action=\"store_true\",\n        help=\"Disable test data shim (ENABLE_TEST_DATA_SHIM=0) for burn-in validation\",\n    )\n    parser.add_argument(\n        \"--random-order\",\n        action=\"store_true\",\n        help=\"Use truly random test order (not fixed seed) for order independence validation\",\n    )\n    parser.add_argument(\n        \"--burnin-mode\",\n        action=\"store_true\",\n        help=\"Enable burn-in validation mode (combines --no-shim and --random-order)\",\n    )\n    parser.add_argument(\n        \"--help-modes\",\n        action=\"store_true\",\n        help=\"Show detailed information about test modes\",\n    )\n    parser.add_argument(\n        \"--skip-static-logging-check\",\n        action=\"store_true\",\n        help=\"Skip the static logging enforcement preflight (not recommended)\",\n    )\n    parser.add_argument(\n        \"--resource-warnings\",\n        action=\"store_true\",\n        default=True,\n        help=\"Enable resource usage warnings (default: enabled)\",\n    )\n    parser.add_argument(\n        \"--no-resource-warnings\",\n        dest=\"resource_warnings\",\n        action=\"store_false\",\n        help=\"Disable resource usage warnings\",\n    )\n    parser.add_argument(\n        \"--critical-memory-limit\",\n        type=float,\n        default=98.0,\n        help=\"System memory percentage threshold for automatic termination (default: 98.0%%)\",\n    )\n    parser.add_argument(\n        \"--no-auto-terminate\",\n        dest=\"auto_terminate\",\n        action=\"store_false\",\n        default=True,\n        help=\"Disable automatic termination on critical memory usage\",\n    )\n\n    args = parser.parse_args()\n\n    # Set resource warnings flag\n    global _resource_warnings_enabled\n    # args.resource_warnings is True by default, False if --no-resource-warnings is used\n    _resource_warnings_enabled = args.resource_warnings\n\n    # Set critical memory handling\n    global _critical_memory_limit, _auto_terminate_enabled\n    _critical_memory_limit = args.critical_memory_limit\n    _auto_terminate_enabled = args.auto_terminate\n\n    if not _auto_terminate_enabled:\n        print(\n            f\"[INFO] Automatic termination on critical memory disabled (use --critical-memory-limit to set threshold)\"\n        )\n    else:\n        print(\n            f\"[INFO] Automatic termination enabled at {_critical_memory_limit}% system memory\"\n        )\n\n    # Show help modes if requested\n    if args.help_modes:\n        print_test_mode_info()\n        return 0\n\n    if args.skip_static_logging_check:\n        print(\"[STATIC CHECK] Skipped by user request\")\n    elif not run_static_logging_check():\n        return 1\n\n    # Enforce safe defaults for Windows console\n    os.environ.setdefault(\"PYTHONUTF8\", \"1\")\n\n    # Handle burn-in mode (combines --no-shim and --random-order)\n    if args.burnin_mode:\n        args.no_shim = True\n        args.random_order = True\n        print(\"[BURN-IN MODE] Enabled: --no-shim and --random-order for validation\")\n\n    # Ensure test data shim is enabled by default for CI/local runs unless explicitly disabled\n    if args.no_shim:\n        os.environ[\"ENABLE_TEST_DATA_SHIM\"] = \"0\"\n        print(\"[BURN-IN] Test data shim disabled (ENABLE_TEST_DATA_SHIM=0)\")\n    else:\n        os.environ.setdefault(\"ENABLE_TEST_DATA_SHIM\", \"1\")\n\n    # Base pytest command\n    cmd = [sys.executable, \"-m\", \"pytest\"]\n\n    # Track if we need to exclude no_parallel tests from parallel execution\n    exclude_no_parallel = False\n\n    # Add parallel execution (enabled by default, can be disabled with --no-parallel)\n    # When parallel is enabled, exclude no_parallel tests from parallel execution\n    # They will be run separately in serial mode\n    if not args.no_parallel:\n        exclude_no_parallel = True\n\n        # Use 'auto' to let pytest-xdist determine optimal worker count, or use specified number\n        # CRITICAL: Limit auto workers to prevent OOM errors - use CPU count but cap at 6\n        # Default to 2 workers for safety (some tests may have race conditions with more workers)\n        # User reported 6 workers worked fine until recently, so allow up to 6\n        if args.workers == \"auto\":\n            import multiprocessing\n\n            cpu_count = multiprocessing.cpu_count()\n            # Reduce workers to prevent memory issues - cap at 4 workers instead of 6\n            # For systems with 12+ CPUs, use 4 workers; for 8 CPUs use 3; for 4 CPUs use 2\n            max_workers = min(4, max(2, cpu_count // 3))\n            cmd.extend([\"-n\", str(max_workers)])\n            print(\n                f\"[INFO] Auto-detected {cpu_count} CPUs, using {max_workers} workers to prevent OOM\"\n            )\n        else:\n            try:\n                # Validate that workers is a valid number if not \"auto\"\n                num_workers = int(args.workers)\n                if num_workers < 1:\n                    print(\n                        f\"[WARNING] Invalid worker count: {num_workers}. Using 2 instead.\"\n                    )\n                    cmd.extend([\"-n\", \"2\"])\n                else:\n                    cmd.extend([\"-n\", str(num_workers)])\n            except ValueError:\n                print(\n                    f\"[WARNING] Invalid worker value: {args.workers}. Using 2 instead.\"\n                )\n                cmd.extend([\"-n\", \"2\"])\n\n    # Add verbose output\n    if args.verbose:\n        cmd.append(\"-v\")\n\n    # Set test environment variables\n    os.environ[\"DISABLE_LOG_ROTATION\"] = \"1\"  # Prevent log rotation issues during tests\n\n    # Handle test order randomization\n    addopts = os.environ.get(\"PYTEST_ADDOPTS\", \"\")\n    has_seed = \"--randomly-seed\" in addopts or any(\n        arg.startswith(\"--randomly-seed\") for arg in sys.argv\n    )\n\n    if args.random_order:\n        # Use truly random order (pytest-randomly will generate a random seed)\n        # Don't add --randomly-seed, let pytest-randomly use a random seed\n        print(\"[BURN-IN] Using random test order (no fixed seed)\")\n    elif not has_seed:\n        # Default: use fixed seed for deterministic runs\n        cmd.extend(\n            [\"--randomly-seed=12345\"]\n        )  # default stable seed for order independence verification\n\n    # Add coverage if requested\n    if args.coverage:\n        cmd.extend(\n            [\n                \"--cov=core\",\n                \"--cov=communication\",\n                \"--cov=ui\",\n                \"--cov=tasks\",\n                \"--cov=user\",\n                \"--cov=ai\",\n                \"--cov-report=html:tests/coverage_html\",\n                \"--cov-report=term\",\n            ]\n        )\n        # Set environment variable to control coverage data file location\n        os.environ[\"COVERAGE_FILE\"] = \"tests/.coverage\"\n\n    # Optional per-test durations report\n    if args.durations_all:\n        cmd.append(\"--durations=0\")\n\n    # Track marker filters for later use in no_parallel test run\n    mode_marker_filter = None\n\n    # Add test selection based on mode\n    if args.mode == \"fast\":\n        # Fast tests: unit tests only\n        cmd.extend([\"tests/unit/\"])\n        mode_marker_filter = \"not slow\"\n        description = \"Fast Tests (Unit tests only, excluding slow tests)\"\n\n    elif args.mode == \"unit\":\n        # Unit tests\n        cmd.extend([\"tests/unit/\"])\n        description = \"Unit Tests\"\n\n    elif args.mode == \"integration\":\n        # Integration tests\n        cmd.extend([\"tests/integration/\"])\n        description = \"Integration Tests\"\n\n    elif args.mode == \"behavior\":\n        # Behavior tests\n        cmd.extend([\"tests/behavior/\"])\n        mode_marker_filter = \"not slow\"\n        description = \"Behavior Tests (excluding slow tests)\"\n\n    elif args.mode == \"ui\":\n        # UI tests\n        cmd.extend([\"tests/ui/\"])\n        mode_marker_filter = \"not slow\"\n        description = \"UI Tests (excluding slow tests)\"\n\n    elif args.mode == \"slow\":\n        # Slow tests only\n        cmd.extend([\"tests/\"])\n        mode_marker_filter = \"slow\"\n        description = \"Slow Tests Only\"\n\n    elif args.mode == \"all\":\n        # All tests\n        cmd.extend([\"tests/\"])\n        description = \"All Tests (Unit, Integration, Behavior, UI)\"\n\n    # Add marker filters (combine mode filter with no_parallel and e2e exclusion if needed)\n    marker_parts = []\n    if mode_marker_filter:\n        marker_parts.append(mode_marker_filter)\n    if exclude_no_parallel:\n        marker_parts.append(\"not no_parallel\")\n    # Always exclude e2e tests from regular runs (they are slow and should only run explicitly)\n    marker_parts.append(\"not e2e\")\n\n    if marker_parts:\n        # Combine all marker filters with \"and\"\n        combined_filter = \" and \".join(marker_parts)\n        cmd.extend([\"-m\", combined_filter])\n\n    # Print clear information about what we're running\n    print(f\"\\nMHM Test Runner\")\n    print(f\"Mode: {args.mode}\")\n    print(f\"Description: {description}\")\n    if not args.no_parallel:\n        print(f\"Parallel: Yes ({args.workers} workers)\")\n        print(\n            f\"Note: Tests marked with @pytest.mark.no_parallel will run separately in serial mode\"\n        )\n    else:\n        print(f\"Parallel: No (disabled)\")\n    if args.verbose:\n        print(f\"Verbose: Yes\")\n    if args.coverage:\n        print(f\"Coverage: Yes\")\n    if args.no_shim:\n        print(f\"Test Data Shim: Disabled (burn-in validation)\")\n    if args.random_order:\n        print(f\"Test Order: Random (burn-in validation)\")\n\n    print(f\"\\nPytest Configuration:\")\n    print(f\"  Platform: {sys.platform}\")\n    print(f\"  Python: {sys.version.split()[0]}\")\n    if not args.random_order and not has_seed:\n        print(f\"  Random Seed: 12345 (fixed for reproducibility)\")\n\n    # Build test context for partial results\n    try:\n        import pytest\n\n        pytest_version = pytest.__version__\n    except (ImportError, AttributeError):\n        pytest_version = \"unknown\"\n\n    base_test_context = {\n        \"mode\": args.mode,\n        \"description\": description,\n        \"parallel\": not args.no_parallel,\n        \"workers\": args.workers if not args.no_parallel else None,\n        \"platform\": sys.platform,\n        \"python_version\": sys.version.split()[0],\n        \"pytest_version\": pytest_version,\n        \"random_seed\": \"12345\" if not args.random_order and not has_seed else None,\n        \"random_order\": args.random_order,\n    }\n\n    # Run the tests\n    parallel_test_context = {**base_test_context, \"phase\": \"parallel\"}\n    parallel_results = run_command(\n        cmd,\n        description,\n        progress_interval=args.progress_interval,\n        test_context=parallel_test_context,\n    )\n    success = parallel_results[\"success\"]\n\n    # If parallel execution was enabled, also run no_parallel tests separately in serial mode\n    no_parallel_results = None\n    if not args.no_parallel:\n        # Create a separate command for no_parallel tests (serial execution)\n        no_parallel_cmd = [sys.executable, \"-m\", \"pytest\"]\n\n        # Build marker filter: combine no_parallel with mode filter if present\n        # Always exclude e2e tests (they are slow and should only run explicitly)\n        if mode_marker_filter:\n            # Combine markers: e.g., \"no_parallel and not slow and not e2e\" or \"no_parallel and slow and not e2e\"\n            no_parallel_cmd.extend(\n                [\"-m\", f\"no_parallel and {mode_marker_filter} and not e2e\"]\n            )\n        else:\n            no_parallel_cmd.extend(\n                [\"-m\", \"no_parallel and not e2e\"]\n            )  # Only run no_parallel tests, exclude e2e\n\n        # Copy test selection from main command (directory paths)\n        if args.mode == \"fast\":\n            no_parallel_cmd.extend([\"tests/unit/\"])\n        elif args.mode == \"unit\":\n            no_parallel_cmd.extend([\"tests/unit/\"])\n        elif args.mode == \"integration\":\n            no_parallel_cmd.extend([\"tests/integration/\"])\n        elif args.mode == \"behavior\":\n            no_parallel_cmd.extend([\"tests/behavior/\"])\n        elif args.mode == \"ui\":\n            no_parallel_cmd.extend([\"tests/ui/\"])\n        elif args.mode == \"slow\":\n            no_parallel_cmd.extend([\"tests/\"])\n        elif args.mode == \"all\":\n            no_parallel_cmd.extend([\"tests/\"])\n\n        # Copy other options\n        if args.verbose:\n            no_parallel_cmd.append(\"-v\")\n        if args.coverage:\n            no_parallel_cmd.extend(\n                [\n                    \"--cov=core\",\n                    \"--cov=communication\",\n                    \"--cov=ui\",\n                    \"--cov=tasks\",\n                    \"--cov=user\",\n                    \"--cov=ai\",\n                    \"--cov-report=html:tests/coverage_html\",\n                    \"--cov-report=term\",\n                ]\n            )\n        if args.durations_all:\n            no_parallel_cmd.append(\"--durations=0\")\n\n        # Copy randomization settings\n        if args.random_order:\n            pass  # Don't add seed for random order\n        elif not has_seed:\n            no_parallel_cmd.extend([\"--randomly-seed=12345\"])\n\n        print(\n            f\"\\n[NO_PARALLEL] Running tests marked with @pytest.mark.no_parallel in serial mode...\"\n        )\n        no_parallel_test_context = {\n            **base_test_context,\n            \"phase\": \"serial\",\n            \"parallel\": False,\n        }\n        no_parallel_results = run_command(\n            no_parallel_cmd,\n            \"No-Parallel Tests (Serial)\",\n            progress_interval=args.progress_interval,\n            test_context=no_parallel_test_context,\n        )\n        no_parallel_success = no_parallel_results[\"success\"]\n\n        if not no_parallel_success:\n            success = False\n\n    # Print combined summary (always show, even if tests failed)\n    # Handle case where parallel_results might be a bool (backward compatibility)\n    if not isinstance(parallel_results, dict):\n        # Convert bool to dict format for summary\n        parallel_results = {\n            \"success\": parallel_results,\n            \"results\": {},\n            \"duration\": 0,\n            \"output\": \"\",\n        }\n    print_combined_summary(parallel_results, no_parallel_results, description)\n\n    # Final status message (summary already printed above)\n    if success:\n        return 0\n    else:\n        return 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 2172,
                    "line_content": "# Handle case where parallel_results might be a bool (backward compatibility)",
                    "start": 85421,
                    "end": 85443
                  }
                ]
              ],
              [
                "core\\config.py",
                "# config.py\n\"\"\"\nConfiguration management for MHM.\nHandles environment variables, validation, and system settings.\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\nfrom typing import Dict, List, Tuple, Optional\nfrom pathlib import Path\n\nfrom core.error_handling import (\n    ConfigurationError, handle_configuration_error,\n    handle_errors\n)\n\nclass ConfigValidationError(Exception):\n    \"\"\"Custom exception for configuration validation errors with detailed information.\"\"\"\n    @handle_errors(\"initializing config error\", default_return=None)\n    def __init__(self, message: str, missing_configs: list[str] | None = None, warnings: list[str] | None = None):\n        \"\"\"Initialize the object.\"\"\"\n        super().__init__(message)\n        self.missing_configs = missing_configs or []\n        self.warnings = warnings or []\n\n# Load environment variables (tolerant mode)\ntry:\n    # do not raise on parsing issues; warnings are logged by python-dotenv itself\n    load_dotenv(override=False, verbose=False)\nexcept Exception as _e:\n    # keep startup resilient; config validation will report issues later\n    pass\n\n@handle_errors(\"normalizing path\", default_return=None)\ndef _normalize_path(value: str) -> str:\n    \"\"\"Normalize path strings from environment to avoid Windows escape issues.\n    - Removes CR/LF control chars\n    - Strips surrounding quotes\n    - Normalizes separators to OS-specific\n    \"\"\"\n    try:\n        if value is None:\n            return value\n        cleaned = value.replace('\\r', '').replace('\\n', '').strip().strip('\"').strip(\"'\")\n        cleaned = cleaned.replace('/', os.sep).replace('\\\\', os.sep)\n        return os.path.normpath(cleaned)\n    except Exception as e:\n        logger.error(f\"Error normalizing path '{value}': {e}\")\n        return value  # Return original value on error\n\n# Base data directory (preserve env value formatting to satisfy tests)\n# During tests, redirect to TEST_DATA_DIR (defaults to tests/data) for\n# complete isolation from real user data. This ensures all test\n# artifacts are created under the project tree instead of system temp\n# directories.\nif os.getenv('MHM_TESTING') == '1':\n    BASE_DATA_DIR = os.getenv('TEST_DATA_DIR', 'tests/data')\nelse:\n    BASE_DATA_DIR = os.getenv('BASE_DATA_DIR', 'data')\n\n# Paths - Updated for better organization\nUSER_INFO_DIR_PATH = _normalize_path(os.getenv('USER_INFO_DIR_PATH', str(Path(BASE_DATA_DIR) / 'users')))\n# Fix: Use forward slashes for cross-platform compatibility and avoid path normalization issues\nDEFAULT_MESSAGES_DIR_PATH = os.getenv('DEFAULT_MESSAGES_DIR_PATH', 'resources/default_messages')\nif os.getenv('MHM_TESTING') == '1':\n    # Force default messages dir during tests to avoid absolute path/env variance\n    DEFAULT_MESSAGES_DIR_PATH = 'resources/default_messages'\n\n# LM Studio Configuration\n# SETUP INSTRUCTIONS:\n# 1. Download and install LM Studio from https://lmstudio.ai/\n# 2. In LM Studio, load your downloaded Phi 2 Chat model\n# 3. Start the local server (usually localhost:1234)\n# 4. The server will be available at http://localhost:1234/v1 with OpenAI-compatible API\nLM_STUDIO_BASE_URL = os.getenv('LM_STUDIO_BASE_URL', 'http://localhost:1234/v1')\nLM_STUDIO_API_KEY = os.getenv('LM_STUDIO_API_KEY', 'lm-studio')  # LM Studio uses any key\nLM_STUDIO_MODEL = os.getenv('LM_STUDIO_MODEL', 'phi-2')  # Model name for API calls\n\n# LM Studio Auto-Management Configuration\nLM_STUDIO_AUTO_START = os.getenv('LM_STUDIO_AUTO_START', 'true').lower() == 'true'  # Auto-start LM Studio if not running\nLM_STUDIO_AUTO_LOAD_MODEL = os.getenv('LM_STUDIO_AUTO_LOAD_MODEL', 'true').lower() == 'true'  # Auto-load model if not loaded\nLM_STUDIO_STARTUP_TIMEOUT = int(os.getenv('LM_STUDIO_STARTUP_TIMEOUT', '60'))  # Timeout for LM Studio startup (seconds)\nLM_STUDIO_MODEL_LOAD_TIMEOUT = int(os.getenv('LM_STUDIO_MODEL_LOAD_TIMEOUT', '30'))  # Timeout for model loading (seconds)\n\n# AI System Prompt Configuration\nAI_SYSTEM_PROMPT_PATH = os.getenv('AI_SYSTEM_PROMPT_PATH', 'resources/assistant_system_prompt.txt')\nAI_USE_CUSTOM_PROMPT = os.getenv('AI_USE_CUSTOM_PROMPT', 'true').lower() == 'true'\n\n\n\n# AI Performance Configuration\nAI_TIMEOUT_SECONDS = int(os.getenv('AI_TIMEOUT_SECONDS', '30'))  # Increased timeout for resource-constrained systems\nAI_BATCH_SIZE = int(os.getenv('AI_BATCH_SIZE', '4'))  # For batch processing\nAI_CUDA_WARMUP = os.getenv('AI_CUDA_WARMUP', 'false').lower() == 'true'  # Disabled for LM Studio\nAI_CACHE_RESPONSES = os.getenv('AI_CACHE_RESPONSES', 'true').lower() == 'true'\n\n# LM Studio AI Model Configuration - Centralized Timeouts and Confidence Thresholds\nAI_CONNECTION_TEST_TIMEOUT = int(os.getenv('AI_CONNECTION_TEST_TIMEOUT', '15'))  # Timeout for testing LM Studio connection\nAI_API_CALL_TIMEOUT = int(os.getenv('AI_API_CALL_TIMEOUT', '15'))  # Default timeout for LM Studio API calls\nAI_COMMAND_PARSING_TIMEOUT = int(os.getenv('AI_COMMAND_PARSING_TIMEOUT', '15'))  # Timeout for AI-enhanced command parsing\nAI_PERSONALIZED_MESSAGE_TIMEOUT = int(os.getenv('AI_PERSONALIZED_MESSAGE_TIMEOUT', '40'))  # Longer timeout for personalized messages\nAI_CONTEXTUAL_RESPONSE_TIMEOUT = int(os.getenv('AI_CONTEXTUAL_RESPONSE_TIMEOUT', '35'))  # Timeout for contextual responses\nAI_QUICK_RESPONSE_TIMEOUT = int(os.getenv('AI_QUICK_RESPONSE_TIMEOUT', '8'))  # Shorter timeout for real-time interactions\n\n# Command Parsing Confidence Thresholds\nAI_RULE_BASED_HIGH_CONFIDENCE_THRESHOLD = float(os.getenv('AI_RULE_BASED_HIGH_CONFIDENCE_THRESHOLD', '0.8'))  # Threshold for high-confidence rule-based parsing\nAI_AI_ENHANCED_CONFIDENCE_THRESHOLD = float(os.getenv('AI_AI_ENHANCED_CONFIDENCE_THRESHOLD', '0.6'))  # Threshold for using AI-enhanced parsing results\nAI_RULE_BASED_FALLBACK_THRESHOLD = float(os.getenv('AI_RULE_BASED_FALLBACK_THRESHOLD', '0.3'))  # Threshold for fallback to rule-based parsing\nAI_AI_PARSING_BASE_CONFIDENCE = float(os.getenv('AI_AI_PARSING_BASE_CONFIDENCE', '0.7'))  # Base confidence for successful AI parsing\nAI_AI_PARSING_PARTIAL_CONFIDENCE = float(os.getenv('AI_AI_PARSING_PARTIAL_CONFIDENCE', '0.6'))  # Confidence for partial AI parsing results\n\n# AI Response Length Configuration - Centralized limits for AI chatbot responses only\n# Note: These limits apply ONLY to AI chatbot responses, not system-generated messages\nAI_MAX_RESPONSE_LENGTH = int(os.getenv('AI_MAX_RESPONSE_LENGTH', '1200'))  # Maximum character length for AI-generated responses\nAI_MAX_RESPONSE_WORDS = int(os.getenv('AI_MAX_RESPONSE_WORDS', '0')) or None  # Optional word limit (0 = disabled)\nAI_MAX_RESPONSE_TOKENS = int(os.getenv('AI_MAX_RESPONSE_TOKENS', '300'))  # Maximum tokens for AI responses\nAI_MIN_RESPONSE_LENGTH = int(os.getenv('AI_MIN_RESPONSE_LENGTH', '50'))  # Minimum character length to avoid too-short responses\n\n# AI Temperature Configuration - Controls response randomness for different modes\nAI_CHAT_TEMPERATURE = float(os.getenv('AI_CHAT_TEMPERATURE', '0.7'))  # Chat mode: conversational and varied\nAI_COMMAND_TEMPERATURE = float(os.getenv('AI_COMMAND_TEMPERATURE', '0.0'))  # Command parsing: deterministic classification\nAI_CLARIFICATION_TEMPERATURE = float(os.getenv('AI_CLARIFICATION_TEMPERATURE', '0.1'))  # Clarification: consistent clarification requests\n\n# User Context Caching\nCONTEXT_CACHE_TTL = int(os.getenv('CONTEXT_CACHE_TTL', '300'))  # 5 minutes\nCONTEXT_CACHE_MAX_SIZE = int(os.getenv('CONTEXT_CACHE_MAX_SIZE', '100'))\n\n# AI Response Cache Configuration\nAI_RESPONSE_CACHE_TTL = int(os.getenv('AI_RESPONSE_CACHE_TTL', '300'))  # 5 minutes\n\n# File Organization Settings\nAUTO_CREATE_USER_DIRS = os.getenv('AUTO_CREATE_USER_DIRS', 'true').lower() == 'true'\n\n# Service and Flag Files Configuration\nMHM_FLAGS_DIR = os.getenv('MHM_FLAGS_DIR')  # Directory for service flag files (defaults to project root)\n\n# Test Environment Configuration\nTEST_LOGS_DIR = os.getenv('TEST_LOGS_DIR')  # Test logs directory (defaults to tests/logs)\nTEST_DATA_DIR = os.getenv('TEST_DATA_DIR')  # Test data directory (defaults to tests/data)\n\n# Logging Configuration\nLOG_LEVEL = os.getenv('LOG_LEVEL', 'WARNING').upper()  # Default to WARNING for quiet operation\nLOG_MAX_BYTES = int(os.getenv('LOG_MAX_BYTES', '5242880'))  # 5MB default\nLOG_BACKUP_COUNT = int(os.getenv('LOG_BACKUP_COUNT', '7'))  # Keep 7 backup files (standardized for consistency)\nLOG_COMPRESS_BACKUPS = os.getenv('LOG_COMPRESS_BACKUPS', 'false').lower() == 'true'  # Compress old logs\n\n# New organized logging structure\n# In test mode, route all logs to tests/logs/ instead of logs/\nif os.getenv('MHM_TESTING') == '1':\n    _default_logs_dir = os.getenv('LOGS_DIR') or str(Path('tests') / 'logs')\nelse:\n    _default_logs_dir = 'logs'\nLOGS_DIR = _normalize_path(os.getenv('LOGS_DIR', _default_logs_dir))  # Main logs directory\nLOG_BACKUP_DIR = _normalize_path(os.getenv('LOG_BACKUP_DIR', str(Path(LOGS_DIR) / 'backups')))  # Backup directory for rotated logs\nLOG_ARCHIVE_DIR = _normalize_path(os.getenv('LOG_ARCHIVE_DIR', str(Path(LOGS_DIR) / 'archive')))  # Archive directory for old logs\n\n# Component-specific log files\nLOG_MAIN_FILE = _normalize_path(os.getenv('LOG_MAIN_FILE', str(Path(LOGS_DIR) / 'app.log')))  # Main application log\nLOG_DISCORD_FILE = _normalize_path(os.getenv('LOG_DISCORD_FILE', str(Path(LOGS_DIR) / 'discord.log')))  # Discord bot log\nLOG_AI_FILE = _normalize_path(os.getenv('LOG_AI_FILE', str(Path(LOGS_DIR) / 'ai.log')))  # AI interactions log\nLOG_USER_ACTIVITY_FILE = _normalize_path(os.getenv('LOG_USER_ACTIVITY_FILE', str(Path(LOGS_DIR) / 'user_activity.log')))  # User activity log\nLOG_ERRORS_FILE = _normalize_path(os.getenv('LOG_ERRORS_FILE', str(Path(LOGS_DIR) / 'errors.log')))  # Errors only log\nLOG_COMMUNICATION_MANAGER_FILE = _normalize_path(os.getenv('LOG_COMMUNICATION_MANAGER_FILE', str(Path(LOGS_DIR) / 'communication_manager.log')))  # Communication manager log\nLOG_EMAIL_FILE = _normalize_path(os.getenv('LOG_EMAIL_FILE', str(Path(LOGS_DIR) / 'email.log')))  # Email channel log\nLOG_UI_FILE = _normalize_path(os.getenv('LOG_UI_FILE', str(Path(LOGS_DIR) / 'ui.log')))  # UI interactions log\nLOG_FILE_OPS_FILE = _normalize_path(os.getenv('LOG_FILE_OPS_FILE', str(Path(LOGS_DIR) / 'file_ops.log')))  # File operations log\nLOG_SCHEDULER_FILE = _normalize_path(os.getenv('LOG_SCHEDULER_FILE', str(Path(LOGS_DIR) / 'scheduler.log')))  # Scheduler log\nLOG_AI_DEV_TOOLS_FILE = _normalize_path(os.getenv('LOG_AI_DEV_TOOLS_FILE', str(Path(LOGS_DIR) / 'ai_dev_tools.log')))  # AI development tools log\n\n# Set up logger after path definitions to avoid circular imports\nfrom core.logger import get_component_logger\nlogger = get_component_logger('main')\n\n# Communication Channel Configurations (non-blocking)\n\n\nEMAIL_SMTP_SERVER = os.getenv('EMAIL_SMTP_SERVER')\nEMAIL_IMAP_SERVER = os.getenv('EMAIL_IMAP_SERVER')\nEMAIL_SMTP_USERNAME = os.getenv('EMAIL_SMTP_USERNAME')\nEMAIL_SMTP_PASSWORD = os.getenv('EMAIL_SMTP_PASSWORD')\nif not all([EMAIL_SMTP_SERVER, EMAIL_IMAP_SERVER, EMAIL_SMTP_USERNAME, EMAIL_SMTP_PASSWORD]):\n    logger.warning(\"Email configuration incomplete - Email channel will be disabled\")\n\nDISCORD_BOT_TOKEN = os.getenv('DISCORD_BOT_TOKEN')\n# Optional: Discord application ID for slash command sync\n_app_id_env = os.getenv('DISCORD_APPLICATION_ID')\nDISCORD_APPLICATION_ID = int(_app_id_env) if _app_id_env and _app_id_env.isdigit() else None\n# Discord webhook configuration for installation events\nDISCORD_PUBLIC_KEY = os.getenv('DISCORD_PUBLIC_KEY', '')  # Public key for webhook signature verification\nDISCORD_WEBHOOK_PORT = int(os.getenv('DISCORD_WEBHOOK_PORT', '8080'))  # Port for webhook server\n# Auto-launch ngrok for webhook tunneling (development only)\nDISCORD_AUTO_NGROK = os.getenv('DISCORD_AUTO_NGROK', 'false').lower() in ('true', '1', 'yes')\nif not DISCORD_BOT_TOKEN:\n    logger.warning(\"DISCORD_BOT_TOKEN not found - Discord channel will be disabled\")\n\n@handle_errors(\"getting available channels\", default_return=[])\ndef get_available_channels() -> list[str]:\n    \"\"\"\n    Get list of available communication channels based on configuration.\n    \n    Returns:\n        List[str]: List of available channel names that can be used with ChannelFactory\n    \"\"\"\n    try:\n        available_channels = []\n        \n        # Check email configuration\n        if all([EMAIL_SMTP_SERVER, EMAIL_IMAP_SERVER, EMAIL_SMTP_USERNAME, EMAIL_SMTP_PASSWORD]):\n            available_channels.append('email')\n        \n        # Check Discord configuration\n        if DISCORD_BOT_TOKEN:\n            available_channels.append('discord')\n        \n        return available_channels\n    except Exception as e:\n        logger.error(f\"Error getting available channels: {e}\")\n        return []\n\n@handle_errors(\"getting channel class mapping\", default_return={})\ndef get_channel_class_mapping() -> dict[str, str]:\n    \"\"\"\n    Get mapping of channel names to their class names for dynamic imports.\n    \n    Returns:\n        Dict[str, str]: Mapping of channel name to fully qualified class name\n    \"\"\"\n    try:\n        return {\n            'email': 'communication.communication_channels.email.bot.EmailBot',\n            'discord': 'communication.communication_channels.discord.bot.DiscordBot',\n            \n        }\n    except Exception as e:\n        logger.error(f\"Error getting channel class mapping: {e}\")\n        return {}\n\n# Scheduler Configuration\nSCHEDULER_INTERVAL = int(os.getenv('SCHEDULER_INTERVAL', '60'))\n\n# Configuration Validation Functions\n@handle_errors(\"validating core paths\", user_friendly=False)\ndef validate_core_paths() -> tuple[bool, list[str], list[str]]:\n    \"\"\"Validate that all core paths are accessible and can be created if needed.\"\"\"\n    errors = []\n    warnings = []\n    \n    paths_to_check = [\n        ('BASE_DATA_DIR', BASE_DATA_DIR),\n        ('USER_INFO_DIR_PATH', USER_INFO_DIR_PATH),\n        ('DEFAULT_MESSAGES_DIR_PATH', DEFAULT_MESSAGES_DIR_PATH),\n    ]\n    \n    for name, path in paths_to_check:\n        try:\n            path_obj = Path(path)\n            if not path_obj.exists():\n                # Try to create the directory\n                try:\n                    # Create directory\n                    path_obj.mkdir(parents=True, exist_ok=True)\n                    warnings.append(f\"Created missing directory: {name} ({path})\")\n                except Exception as e:\n                    error_msg = f\"Cannot create directory {name} ({path}): {e}\"\n                    errors.append(error_msg)\n                    handle_configuration_error(e, name, f\"creating directory {path}\")\n            elif not path_obj.is_dir():\n                error_msg = f\"Path {name} exists but is not a directory: {path}\"\n                errors.append(error_msg)\n            elif not os.access(path, os.W_OK):\n                error_msg = f\"No write access to directory {name}: {path}\"\n                errors.append(error_msg)\n        except Exception as e:\n            error_msg = f\"Error validating path {name} ({path}): {e}\"\n            errors.append(error_msg)\n            # Use our error handling for this specific error\n            handle_configuration_error(e, name, f\"validating path {path}\")\n    \n    return len(errors) == 0, errors, warnings\n\n@handle_errors(\"validating AI configuration\", default_return=(False, [\"Validation failed\"], []))\ndef validate_ai_configuration() -> tuple[bool, list[str], list[str]]:\n    \"\"\"Validate AI-related configuration settings.\"\"\"\n    try:\n        errors = []\n        warnings = []\n        \n        # Check LM Studio configuration\n        if not LM_STUDIO_BASE_URL:\n            errors.append(\"LM_STUDIO_BASE_URL is not configured\")\n        elif not LM_STUDIO_BASE_URL.startswith(('http://', 'https://')):\n            errors.append(\"LM_STUDIO_BASE_URL must be a valid URL starting with http:// or https://\")\n        \n        # LM_STUDIO_API_KEY has a default value, so we only warn if it's explicitly set to empty\n        if LM_STUDIO_API_KEY == '':\n            warnings.append(\"LM_STUDIO_API_KEY is explicitly set to empty (using default)\")\n        \n        # LM_STUDIO_MODEL has a default value, so we only warn if it's explicitly set to empty\n        if LM_STUDIO_MODEL == '':\n            warnings.append(\"LM_STUDIO_MODEL is explicitly set to empty (using default)\")\n        \n        # Check AI performance settings\n        if AI_TIMEOUT_SECONDS < 5:\n            warnings.append(\"AI_TIMEOUT_SECONDS is very low (< 5 seconds), may cause timeouts\")\n        elif AI_TIMEOUT_SECONDS > 300:\n            warnings.append(\"AI_TIMEOUT_SECONDS is very high (> 5 minutes), may cause long waits\")\n        \n        if AI_BATCH_SIZE < 1:\n            errors.append(\"AI_BATCH_SIZE must be at least 1\")\n        elif AI_BATCH_SIZE > 20:\n            warnings.append(\"AI_BATCH_SIZE is very high (> 20), may cause memory issues\")\n        \n        # Check cache settings\n        if CONTEXT_CACHE_TTL < 60:\n            warnings.append(\"CONTEXT_CACHE_TTL is very low (< 1 minute), may cause frequent cache misses\")\n        elif CONTEXT_CACHE_TTL > 3600:\n            warnings.append(\"CONTEXT_CACHE_TTL is very high (> 1 hour), may use excessive memory\")\n        \n        if CONTEXT_CACHE_MAX_SIZE < 10:\n            warnings.append(\"CONTEXT_CACHE_MAX_SIZE is very low (< 10), may cause frequent cache evictions\")\n        elif CONTEXT_CACHE_MAX_SIZE > 1000:\n            warnings.append(\"CONTEXT_CACHE_MAX_SIZE is very high (> 1000), may use excessive memory\")\n        \n        return len(errors) == 0, errors, warnings\n    except Exception as e:\n        logger.error(f\"Error validating AI configuration: {e}\")\n        return False, [f\"AI configuration validation failed: {e}\"], []\n\n@handle_errors(\"validating communication channels\", default_return=(False, [\"Validation failed\"], []))\ndef validate_communication_channels() -> tuple[bool, list[str], list[str]]:\n    \"\"\"Validate communication channel configurations.\"\"\"\n    try:\n        errors = []\n        warnings = []\n        available_channels = []\n        \n        # Check email configuration dynamically from environment\n        email_config = {\n            'EMAIL_SMTP_SERVER': os.getenv('EMAIL_SMTP_SERVER'),\n            'EMAIL_IMAP_SERVER': os.getenv('EMAIL_IMAP_SERVER'),\n            'EMAIL_SMTP_USERNAME': os.getenv('EMAIL_SMTP_USERNAME'),\n            'EMAIL_SMTP_PASSWORD': os.getenv('EMAIL_SMTP_PASSWORD')\n        }\n\n        email_missing = [key for key, value in email_config.items() if not value]\n        if email_missing:\n            warnings.append(f\"Email channel disabled - missing: {', '.join(email_missing)}\")\n        else:\n            available_channels.append('email')\n            # Validate email format\n            email_user = email_config['EMAIL_SMTP_USERNAME']\n            if email_user and '@' not in email_user:\n                warnings.append(\"EMAIL_SMTP_USERNAME doesn't appear to be a valid email address\")\n\n        # Check Discord configuration dynamically from environment\n        discord_token = os.getenv('DISCORD_BOT_TOKEN')\n        if not discord_token:\n            warnings.append(\"Discord channel disabled - DISCORD_BOT_TOKEN not configured\")\n        else:\n            available_channels.append('discord')\n            # Basic Discord token validation (should start with specific pattern)\n            if not discord_token.startswith(('MT', 'OT', 'NT')):\n                warnings.append(\"DISCORD_BOT_TOKEN doesn't match expected Discord bot token format\")\n        \n        # If no channels are configured, treat as warning for flexibility\n        if not available_channels:\n            warnings.append(\"No communication channels are properly configured. At least one channel (Email or Discord) should be configured.\")\n\n        return len(errors) == 0, errors, warnings\n    except Exception as e:\n        logger.error(f\"Error validating communication channels: {e}\")\n        return False, [f\"Communication channels validation failed: {e}\"], []\n\n@handle_errors(\"validating logging configuration\", default_return=(False, [\"Validation failed\"], []))\ndef validate_logging_configuration() -> tuple[bool, list[str], list[str]]:\n    \"\"\"Validate logging configuration.\"\"\"\n    try:\n        errors = []\n        warnings = []\n        \n        # Check log level\n        valid_log_levels = ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']\n        if LOG_LEVEL not in valid_log_levels:\n            errors.append(f\"Invalid LOG_LEVEL '{LOG_LEVEL}'. Must be one of: {', '.join(valid_log_levels)}\")\n        \n        # Check log file path\n        try:\n            log_path = Path(LOG_MAIN_FILE)\n            log_dir = log_path.parent\n            if log_dir.exists() and not os.access(log_dir, os.W_OK):\n                errors.append(f\"Cannot write to log directory: {log_dir}\")\n            elif not log_dir.exists():\n                try:\n                    log_dir.mkdir(parents=True, exist_ok=True)\n                    warnings.append(f\"Created log directory: {log_dir}\")\n                except Exception as e:\n                    errors.append(f\"Cannot create log directory {log_dir}: {e}\")\n        except Exception as e:\n            errors.append(f\"Error validating log file path {LOG_MAIN_FILE}: {e}\")\n        \n        # Validate log rotation settings\n        if LOG_MAX_BYTES < 1024 * 1024:  # Less than 1MB\n            warnings.append(f\"LOG_MAX_BYTES ({LOG_MAX_BYTES}) is very small, consider increasing\")\n        \n        if LOG_BACKUP_COUNT < 1:\n            errors.append(\"LOG_BACKUP_COUNT must be at least 1\")\n        elif LOG_BACKUP_COUNT > 20:\n            warnings.append(f\"LOG_BACKUP_COUNT ({LOG_BACKUP_COUNT}) is very high, consider reducing\")\n        \n        # Check current log file size if it exists\n        if os.path.exists(LOG_MAIN_FILE):\n            current_size = os.path.getsize(LOG_MAIN_FILE)\n            current_size_mb = current_size / (1024 * 1024)\n            if current_size_mb > 10:  # More than 10MB\n                warnings.append(f\"Current log file is {current_size_mb:.1f}MB, consider reducing LOG_LEVEL or increasing LOG_MAX_BYTES\")\n        \n        return len(errors) == 0, errors, warnings\n    except Exception as e:\n        logger.error(f\"Error validating logging configuration: {e}\")\n        return False, [f\"Logging configuration validation failed: {e}\"], []\n\n@handle_errors(\"validating scheduler configuration\", default_return=(False, [\"Validation failed\"], []))\ndef validate_scheduler_configuration() -> tuple[bool, list[str], list[str]]:\n    \"\"\"Validate scheduler configuration.\"\"\"\n    try:\n        errors = []\n        warnings = []\n        \n        if SCHEDULER_INTERVAL < 10:\n            errors.append(\"SCHEDULER_INTERVAL must be at least 10 seconds\")\n        elif SCHEDULER_INTERVAL < 30:\n            warnings.append(\"SCHEDULER_INTERVAL is very low (< 30 seconds), may cause high CPU usage\")\n        elif SCHEDULER_INTERVAL > 3600:\n            warnings.append(\"SCHEDULER_INTERVAL is very high (> 1 hour), may cause delayed responses\")\n        \n        return len(errors) == 0, errors, warnings\n    except Exception as e:\n        logger.error(f\"Error validating scheduler configuration: {e}\")\n        return False, [f\"Scheduler configuration validation failed: {e}\"], []\n\n@handle_errors(\"validating file organization settings\", default_return=(False, [\"Validation failed\"], []))\ndef validate_file_organization_settings() -> tuple[bool, list[str], list[str]]:\n    \"\"\"Validate file organization settings.\"\"\"\n    try:\n        errors = []\n        warnings = []\n        \n        if not isinstance(AUTO_CREATE_USER_DIRS, bool):\n            errors.append(\"AUTO_CREATE_USER_DIRS must be a boolean value\")\n        \n        # Check for potential conflicts\n        if AUTO_CREATE_USER_DIRS:\n            warnings.append(\"AUTO_CREATE_USER_DIRS is enabled - user directories will be created automatically\")\n        \n        return len(errors) == 0, errors, warnings\n    except Exception as e:\n        logger.error(f\"Error validating file organization settings: {e}\")\n        return False, [f\"File organization settings validation failed: {e}\"], []\n\n@handle_errors(\"validating environment variables\", default_return=(False, [\"Validation failed\"], []))\ndef validate_environment_variables() -> tuple[bool, list[str], list[str]]:\n    \"\"\"Check for common environment variable issues.\"\"\"\n    try:\n        errors = []\n        warnings = []\n        \n        # Check if .env file exists\n        env_file = Path('.env')\n        if not env_file.exists():\n            warnings.append(\"No .env file found - using default configuration values\")\n        \n        # Check for potentially problematic environment variables\n        problematic_vars = []\n        for key, value in os.environ.items():\n            if key.startswith('MHM_') and not value:\n                problematic_vars.append(key)\n        \n        if problematic_vars:\n            warnings.append(f\"Empty environment variables found: {', '.join(problematic_vars)}\")\n        \n        return len(errors) == 0, errors, warnings\n    except Exception as e:\n        logger.error(f\"Error validating environment variables: {e}\")\n        return False, [f\"Environment variables validation failed: {e}\"], []\n\n@handle_errors(\"validating all configuration\", default_return={'valid': False, 'errors': ['Validation failed'], 'warnings': [], 'available_channels': [], 'summary': 'Configuration validation failed'})\ndef validate_all_configuration() -> dict[str, any]:\n    \"\"\"Comprehensive configuration validation that checks all aspects of the configuration.\n    \n    Returns:\n        Dict containing validation results with the following structure:\n        {\n            'valid': bool,\n            'errors': List[str],\n            'warnings': List[str],\n            'available_channels': List[str],\n            'summary': str\n        }\n    \"\"\"\n    try:\n        all_errors = []\n        all_warnings = []\n        \n        # Run all validation functions\n        validators = [\n            ('Core Paths', validate_core_paths),\n            ('AI Configuration', validate_ai_configuration),\n            ('Communication Channels', validate_communication_channels),\n            ('Logging Configuration', validate_logging_configuration),\n            ('Scheduler Configuration', validate_scheduler_configuration),\n            ('File Organization Settings', validate_file_organization_settings),\n            ('Environment Variables', validate_environment_variables),\n        ]\n        \n        for name, validator in validators:\n            try:\n                is_valid, errors, warnings = validator()\n                if errors:\n                    all_errors.extend([f\"{name}: {error}\" for error in errors])\n                if warnings:\n                    all_warnings.extend([f\"{name}: {warning}\" for warning in warnings])\n            except Exception as e:\n                all_errors.append(f\"{name}: Validation failed with exception: {e}\")\n        \n        # Get available channels\n        available_channels = get_available_channels()\n        \n        # Create summary\n        if all_errors:\n            summary = f\"Configuration validation failed with {len(all_errors)} error(s) and {len(all_warnings)} warning(s)\"\n        elif all_warnings:\n            summary = f\"Configuration validation passed with {len(all_warnings)} warning(s)\"\n        else:\n            summary = \"Configuration validation passed successfully\"\n        \n        if available_channels:\n            summary += f\" - Available channels: {', '.join(available_channels)}\"\n        else:\n            summary += \" - No communication channels available\"\n        \n        return {\n            'valid': len(all_errors) == 0,\n            'errors': all_errors,\n            'warnings': all_warnings,\n            'available_channels': available_channels,\n            'summary': summary\n        }\n    except Exception as e:\n        logger.error(f\"Error validating all configuration: {e}\")\n        return {\n            'valid': False,\n            'errors': [f\"Configuration validation failed: {e}\"],\n            'warnings': [],\n            'available_channels': [],\n            'summary': 'Configuration validation failed'\n        }\n\n# ERROR_HANDLING_EXCLUDE: This function intentionally raises ConfigValidationError exceptions\n# that should propagate to callers for proper error handling, not be caught by decorator\ndef validate_and_raise_if_invalid() -> list[str]:\n    \"\"\"Validate configuration and raise ConfigValidationError if invalid.\n    \n    Note: This function intentionally does not use @handle_errors decorator\n    because it is designed to raise ConfigValidationError exceptions, which\n    should propagate to the caller for proper error handling.\n    \n    Returns:\n        List of available communication channels if validation passes.\n    \n    Raises:\n        ConfigValidationError: If configuration is invalid with detailed error information.\n    \"\"\"\n    try:\n        result = validate_all_configuration()\n        \n        if not result['valid']:\n            raise ConfigValidationError(\n                f\"Configuration validation failed: {result['summary']}\",\n                missing_configs=result['errors'],\n                warnings=result['warnings']\n            )\n        \n        if result['warnings']:\n            logger.warning(f\"Configuration warnings: {'; '.join(result['warnings'])}\")\n        \n        logger.info(f\"Configuration validation passed: {result['summary']}\")\n        return result['available_channels']\n    except ConfigValidationError:\n        # Re-raise configuration errors as-is\n        raise\n    except Exception as e:\n        logger.error(f\"Error validating and raising if invalid: {e}\")\n        raise ConfigValidationError(\n            f\"Configuration validation failed: {e}\",\n            missing_configs=[str(e)],\n            warnings=[]\n        )\n\n@handle_errors(\"printing configuration report\", default_return=False)\ndef print_configuration_report():\n    \"\"\"Print a detailed configuration report to the console.\"\"\"\n    try:\n        result = validate_all_configuration()\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"MHM CONFIGURATION VALIDATION REPORT\")\n        print(\"=\"*60)\n        \n        print(f\"\\nSUMMARY: {result['summary']}\")\n        \n        if result['available_channels']:\n            print(f\"\\nAVAILABLE CHANNELS: {', '.join(result['available_channels'])}\")\n        else:\n            print(\"\\nAVAILABLE CHANNELS: None\")\n        \n        if result['errors']:\n            print(f\"\\nERRORS ({len(result['errors'])}):\")\n            for i, error in enumerate(result['errors'], 1):\n                print(f\"  {i}. {error}\")\n        \n        if result['warnings']:\n            print(f\"\\nWARNINGS ({len(result['warnings'])}):\")\n            for i, warning in enumerate(result['warnings'], 1):\n                print(f\"  {i}. {warning}\")\n        \n        # Print current configuration values\n        print(f\"\\nCURRENT CONFIGURATION:\")\n        print(f\"  Base Data Directory: {BASE_DATA_DIR}\")\n        print(f\"  Log File: {LOG_MAIN_FILE}\")\n        print(f\"  Log Level: {LOG_LEVEL}\")\n        print(f\"  LM Studio URL: {LM_STUDIO_BASE_URL}\")\n        print(f\"  AI Timeout: {AI_TIMEOUT_SECONDS}s\")\n        print(f\"  Scheduler Interval: {SCHEDULER_INTERVAL}s\")\n        print(f\"  Auto Create User Dirs: {AUTO_CREATE_USER_DIRS}\")\n        \n        print(\"\\n\" + \"=\"*60)\n        \n        return result['valid']\n    except Exception as e:\n        logger.error(f\"Error printing configuration report: {e}\")\n        print(f\"\\nError generating configuration report: {e}\")\n        return False\n\n# Data Management Functions\n@handle_errors(\"getting user data directory\", default_return=\"\")\ndef get_user_data_dir(user_id: str) -> str:\n    \"\"\"Get the data directory for a specific user.\"\"\"\n    try:\n        if not user_id:\n            logger.error(f\"Invalid user_id: {user_id}\")\n            # Return empty string to prevent creating files in root\n            return \"\"\n        return str(Path(BASE_DATA_DIR) / 'users' / user_id)\n    except Exception as e:\n        logger.error(f\"Error getting user data directory for user {user_id}: {e}\")\n        return \"\"\n\n@handle_errors(\"getting backups directory\", default_return=\"data/backups\")\ndef get_backups_dir() -> str:\n    \"\"\"Get the backups directory, redirected under tests when MHM_TESTING=1.\n    Returns tests/data/backups if testing, otherwise BASE_DATA_DIR/backups.\n    \"\"\"\n    if os.getenv('MHM_TESTING') == '1':\n        # Use configurable test data directory\n        test_data_dir = os.getenv('TEST_DATA_DIR', str(Path('tests') / 'data'))\n        return str(Path(test_data_dir) / 'backups')\n    return str(Path(BASE_DATA_DIR) / 'backups')\n\n@handle_errors(\"getting user file path\", default_return=\"\")\ndef get_user_file_path(user_id: str, file_type: str) -> str:\n    \"\"\"Get the file path for a specific user file type.\"\"\"\n    user_dir = get_user_data_dir(user_id)\n    \n    # Prevent creating files in root directory\n    if not user_dir or not user_dir.strip():\n        logger.error(f\"Invalid user_id: {user_id} - cannot create file path\")\n        return \"\"\n    \n    file_mapping = {\n        # New structure\n        'account': 'account.json',\n        'preferences': 'preferences.json',\n        'context': 'user_context.json',\n        'schedules': 'schedules.json',\n        # Other files\n        'checkins': 'checkins.json',\n        'chat_interactions': 'chat_interactions.json',\n        'sent_messages': 'messages/sent_messages.json',\n        'conversation_history': 'conversation_history.json',\n        'tags': 'tags.json'\n    }\n    return str(Path(user_dir) / file_mapping.get(file_type, f'{file_type}.json'))\n\n@handle_errors(\"ensuring user directory exists\", default_return=False)\ndef ensure_user_directory(user_id: str) -> bool:\n    \"\"\"Ensure user directory exists if using subdirectories.\"\"\"\n    if not AUTO_CREATE_USER_DIRS:\n        return True\n    \n    user_dir = get_user_data_dir(user_id)\n    try:\n        os.makedirs(user_dir, exist_ok=True)\n        return True\n    except Exception as e:\n        logger.error(f\"Failed to create user directory {user_dir}: {e}\")\n        return False\n\n# Legacy validation functions (kept for backward compatibility)\n\n\n@handle_errors(\"validating email configuration\", user_friendly=False)\ndef validate_email_config():\n    \"\"\"Validate email configuration settings.\n    \n    Returns:\n        bool: True if email configuration is valid\n        \n    Raises:\n        ConfigurationError: If required email configuration variables are missing\n    \"\"\"\n    required_vars = [EMAIL_SMTP_SERVER, EMAIL_IMAP_SERVER, EMAIL_SMTP_USERNAME, EMAIL_SMTP_PASSWORD]\n    if not all(required_vars):\n        missing = [var for var in ['EMAIL_SMTP_SERVER', 'EMAIL_IMAP_SERVER', 'EMAIL_SMTP_USERNAME', 'EMAIL_SMTP_PASSWORD'] \n                  if not globals()[var]]\n        raise ConfigurationError(f\"Missing email configuration variables: {', '.join(missing)}\")\n    return True\n\n@handle_errors(\"validating Discord configuration\", user_friendly=False)\ndef validate_discord_config():\n    \"\"\"Validate Discord configuration settings.\n    \n    Returns:\n        bool: True if Discord configuration is valid\n        \n    Raises:\n        ConfigurationError: If DISCORD_BOT_TOKEN is missing\n    \"\"\"\n    if not DISCORD_BOT_TOKEN:\n        raise ConfigurationError(\"DISCORD_BOT_TOKEN is missing from environment configuration.\")\n    return True\n\n@handle_errors(\"validating minimum configuration\", user_friendly=False)\ndef validate_minimum_config():\n    \"\"\"Ensure at least one communication channel is configured\"\"\"\n    available = get_available_channels()\n    if not available:\n        raise ConfigurationError(\"No communication channels are properly configured. Please set up at least one channel (Email or Discord).\")\n    return available\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 708,
                    "line_content": "# Legacy validation functions (kept for backward compatibility)",
                    "start": 33961,
                    "end": 33983
                  }
                ]
              ],
              [
                "core\\message_management.py",
                "# message_management.py\n\"\"\"\nMessage management utilities for MHM.\nContains functions for message categories, loading, adding, editing, deleting, and storing messages.\n\"\"\"\n\nimport os\nfrom pathlib import Path\nimport json\nimport uuid\nfrom datetime import datetime, timezone, timedelta\nfrom typing import List, Dict, Any, Optional\nfrom core.logger import get_component_logger\nfrom core.config import DEFAULT_MESSAGES_DIR_PATH, get_user_data_dir\nfrom core.file_operations import load_json_data, save_json_data, determine_file_path\nfrom core.schemas import validate_messages_file_dict\nfrom core.error_handling import ValidationError, handle_errors\nfrom core.time_utilities import (\n    now_timestamp_filename,\n    now_timestamp_full,\n    TIMESTAMP_FULL,\n)\n\n\nlogger = get_component_logger(\"message\")\n\n\n@handle_errors(\"getting message categories\", default_return=[])\ndef get_message_categories():\n    \"\"\"\n    Retrieves message categories from the environment variable CATEGORIES.\n    Allows for either a comma-separated string or a JSON array.\n\n    Returns:\n        List[str]: List of message categories\n    \"\"\"\n    raw_categories = os.getenv(\"CATEGORIES\")\n    if not raw_categories:\n        logger.error(\"No CATEGORIES found in environment. Returning empty list.\")\n        return []\n\n    raw_categories = raw_categories.strip()\n\n    # If it looks like JSON (starts with '['), try parsing it\n    if raw_categories.startswith(\"[\") and raw_categories.endswith(\"]\"):\n        try:\n            parsed = json.loads(raw_categories)\n            if isinstance(parsed, list):\n                category_list = [\n                    cat.strip()\n                    for cat in parsed\n                    if isinstance(cat, str) and cat.strip()\n                ]\n                logger.debug(\n                    f\"Retrieved message categories from JSON list: {category_list}\"\n                )\n                return category_list\n            else:\n                # If JSON parsed but it's not a list, treat it as a fallback\n                logger.warning(\n                    \"CATEGORIES JSON is not a list. Falling back to comma-split logic.\"\n                )\n        except json.JSONDecodeError:\n            logger.warning(\n                \"Failed to parse CATEGORIES as JSON. Falling back to comma-split logic.\"\n            )\n\n    # Fallback: treat it as a comma-separated string\n    category_list = [cat.strip() for cat in raw_categories.split(\",\") if cat.strip()]\n    logger.debug(\n        f\"Retrieved message categories from comma-separated string: {category_list}\"\n    )\n    return category_list\n\n\n@handle_errors(\"loading user messages\", default_return=[])\ndef load_user_messages(user_id, category):\n    \"\"\"\n    Load user's message templates for a specific category.\n\n    Args:\n        user_id: The user ID\n        category: The message category\n\n    Returns:\n        List[dict]: List of message templates for the category\n    \"\"\"\n    if user_id is None:\n        logger.error(\"load_user_messages called with None user_id\")\n        return []\n\n    try:\n        # Use new user-specific message file structure\n        user_messages_dir = Path(get_user_data_dir(user_id)) / \"messages\"\n        file_path = user_messages_dir / f\"{category}.json\"\n\n        if not file_path.exists():\n            logger.debug(\n                f\"No message file found for user {user_id}, category {category}\"\n            )\n            return []\n\n        data = load_json_data(str(file_path))\n\n        if data is None or \"messages\" not in data:\n            logger.debug(\n                f\"No messages found in file for user {user_id}, category {category}\"\n            )\n            return []\n\n        messages = data[\"messages\"]\n        logger.debug(\n            f\"Loaded {len(messages)} messages for user {user_id}, category {category}\"\n        )\n        return messages\n\n    except Exception as e:\n        logger.error(\n            f\"Error loading user messages for user {user_id}, category {category}: {e}\"\n        )\n        return []\n\n\n@handle_errors(\"loading default messages\", default_return=[])\ndef load_default_messages(category):\n    \"\"\"Load default messages for a specific category.\"\"\"\n    try:\n        # Add debug logging to see what path is being used\n        logger.info(f\"Loading default messages for category: {category}\")\n        logger.info(f\"DEFAULT_MESSAGES_DIR_PATH: {DEFAULT_MESSAGES_DIR_PATH}\")\n        logger.info(f\"Current working directory: {os.getcwd()}\")\n\n        default_messages_file = Path(DEFAULT_MESSAGES_DIR_PATH) / f\"{category}.json\"\n\n        # Add debug logging to see what path is being used\n        logger.info(f\"Looking for default messages file: {default_messages_file}\")\n        logger.info(f\"File exists: {default_messages_file.exists()}\")\n        logger.info(f\"Absolute path: {default_messages_file.absolute()}\")\n\n        try:\n            with open(default_messages_file, encoding=\"utf-8\") as f:\n                data = json.load(f)\n                messages = data.get(\"messages\", [])\n                logger.info(\n                    f\"Loaded {len(messages)} default messages for category {category}\"\n                )\n                return messages\n        except FileNotFoundError:\n            logger.error(f\"Default messages file not found for category: {category}\")\n            logger.error(f\"Attempted path: {default_messages_file}\")\n            logger.error(f\"Absolute path: {default_messages_file.absolute()}\")\n            return []\n        except json.JSONDecodeError as e:\n            logger.error(\n                f\"Invalid JSON in default messages file for category {category}: {e}\"\n            )\n            return []\n    except Exception as e:\n        logger.error(f\"Error loading default messages for category {category}: {e}\")\n        return []\n\n\n@handle_errors(\"adding message\")\ndef add_message(user_id, category, message_data, index=None):\n    \"\"\"\n    Add a new message to a user's category.\n\n    Args:\n        user_id: The user ID\n        category: The message category\n        message_data: Dictionary containing message data\n        index: Optional position to insert the message (None for append)\n    \"\"\"\n    if user_id is None:\n        logger.error(\"add_message called with None user_id\")\n        return\n\n    # Use new user-specific message file structure\n    user_messages_dir = Path(get_user_data_dir(user_id)) / \"messages\"\n    # Ensure the messages directory exists\n    user_messages_dir.mkdir(parents=True, exist_ok=True)\n    file_path = user_messages_dir / f\"{category}.json\"\n\n    data = load_json_data(str(file_path))\n\n    if data is None:\n        data = {\"messages\": []}\n\n    if \"message_id\" not in message_data:\n        message_data[\"message_id\"] = str(uuid.uuid4())\n\n    if index is not None and 0 <= index < len(data[\"messages\"]):\n        data[\"messages\"].insert(index, message_data)\n    else:\n        data[\"messages\"].append(message_data)\n\n    # Validate/normalize via Pydantic schema (non-blocking)\n    try:\n        data, _errs = validate_messages_file_dict(data)\n    except Exception:\n        pass\n    save_json_data(data, str(file_path))\n\n    try:\n        from core.user_data_manager import update_user_index\n\n        update_user_index(user_id)\n    except Exception as e:\n        logger.warning(\n            f\"Failed to update user index after message addition for user {user_id}: {e}\"\n        )\n\n    logger.info(\n        f\"Added message to category {category} for user {user_id}: {message_data}\"\n    )\n\n\n@handle_errors(\"editing message\")\ndef edit_message(user_id, category, message_id, updated_data):\n    \"\"\"\n    Edit an existing message in a user's category.\n\n    Args:\n        user_id: The user ID\n        category: The message category\n        message_id: The ID of the message to edit\n        updated_data: Dictionary containing updated message data\n\n    Raises:\n        ValidationError: If message ID is not found or category is invalid\n    \"\"\"\n    if user_id is None:\n        logger.error(\"edit_message called with None user_id\")\n        return\n\n    # Use new user-specific message file structure\n    user_messages_dir = Path(get_user_data_dir(user_id)) / \"messages\"\n    user_messages_dir.mkdir(parents=True, exist_ok=True)\n    file_path = user_messages_dir / f\"{category}.json\"\n\n    data = load_json_data(str(file_path))\n\n    if data is None or \"messages\" not in data:\n        raise ValidationError(\"Invalid category or data file.\")\n\n    message_index = next(\n        (\n            i\n            for i, msg in enumerate(data[\"messages\"])\n            if msg[\"message_id\"] == message_id\n        ),\n        None,\n    )\n\n    if message_index is None:\n        raise ValidationError(\"Message ID not found.\")\n\n    # Update the message\n    data[\"messages\"][message_index].update(updated_data)\n    try:\n        data, _errs = validate_messages_file_dict(data)\n    except Exception:\n        pass\n    save_json_data(data, str(file_path))\n\n    try:\n        from core.user_data_manager import update_user_index\n\n        update_user_index(user_id)\n    except Exception as e:\n        logger.warning(\n            f\"Failed to update user index after message edit for user {user_id}: {e}\"\n        )\n\n    logger.info(\n        f\"Edited message with ID {message_id} in category {category} for user {user_id}.\"\n    )\n\n\n@handle_errors(\"updating message by ID\")\ndef update_message(user_id, category, message_id, new_message_data):\n    \"\"\"\n    Update a message by its message_id.\n\n    Args:\n        user_id: The user ID\n        category: The message category\n        message_id: The ID of the message to update\n        new_message_data: Complete new message data to replace the existing message\n\n    Raises:\n        ValidationError: If message ID is not found or category is invalid\n    \"\"\"\n    if user_id is None:\n        logger.error(\"update_message called with None user_id\")\n        return\n\n    file_path = determine_file_path(\"messages\", f\"{category}/{user_id}\")\n    Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n    data = load_json_data(file_path)\n\n    if data is None or \"messages\" not in data:\n        raise ValidationError(\"Invalid category or data file.\")\n\n    # Find the message by ID\n    for i, msg in enumerate(data[\"messages\"]):\n        if msg.get(\"message_id\") == message_id:\n            data[\"messages\"][i] = new_message_data\n            save_json_data(data, file_path)\n            logger.info(\n                f\"Updated message with ID {message_id} in category {category} for user {user_id}\"\n            )\n            return\n\n    raise ValidationError(\"Message ID not found.\")\n\n\n@handle_errors(\"deleting message\")\ndef delete_message(user_id, category, message_id):\n    \"\"\"\n    Delete a specific message from a user's category.\n\n    Args:\n        user_id: The user ID\n        category: The message category\n        message_id: The ID of the message to delete\n\n    Raises:\n        ValidationError: If the message ID is not found or the category is invalid\n    \"\"\"\n    if user_id is None:\n        logger.error(\"delete_message called with None user_id\")\n        return\n\n    # Use new user-specific message file structure\n    user_messages_dir = Path(get_user_data_dir(user_id)) / \"messages\"\n    # Ensure the messages directory exists\n    user_messages_dir.mkdir(parents=True, exist_ok=True)\n    file_path = user_messages_dir / f\"{category}.json\"\n\n    data = load_json_data(str(file_path))\n\n    if data is None or \"messages\" not in data:\n        raise ValidationError(\"Invalid category or data file.\")\n\n    message_to_delete = next(\n        (msg for msg in data[\"messages\"] if msg[\"message_id\"] == message_id), None\n    )\n\n    if not message_to_delete:\n        raise ValidationError(\"Message ID not found.\")\n\n    data[\"messages\"].remove(message_to_delete)\n    # If no messages remain, keep an empty file (tests expect file to exist post-delete)\n    if not data.get(\"messages\"):\n        data = {\"messages\": []}\n    save_json_data(data, str(file_path))\n\n    try:\n        from core.user_data_manager import update_user_index\n\n        update_user_index(user_id)\n    except Exception as e:\n        logger.warning(\n            f\"Failed to update user index after message deletion for user {user_id}: {e}\"\n        )\n\n    logger.info(\n        f\"Deleted message with ID {message_id} in category {category} for user {user_id}.\"\n    )\n\n\n@handle_errors(\"getting recent messages\", default_return=[])\ndef get_recent_messages(\n    user_id: str,\n    category: str | None = None,\n    limit: int = 10,\n    days_back: int | None = None,\n) -> list[dict[str, Any]]:\n    \"\"\"\n    Get recent messages with flexible filtering.\n\n    This function replaces get_last_10_messages() with enhanced functionality\n    that supports both category-specific and cross-category queries.\n\n    Args:\n        user_id: The user ID\n        category: Optional category filter (None = all categories)\n        limit: Maximum number of messages to return\n        days_back: Only include messages from last N days\n\n    Returns:\n        List[dict]: List of recent messages, sorted by timestamp descending\n    \"\"\"\n    if user_id is None:\n        logger.error(\"get_recent_messages called with None user_id\")\n        return []\n\n    try:\n        file_path = determine_file_path(\"sent_messages\", user_id)\n        data = load_json_data(file_path)\n\n        if not data:\n            logger.debug(f\"No sent messages found for user {user_id}\")\n            return []\n\n        # Normalize/validate to drop malformed entries and apply defaults\n        normalized_data, errors = validate_messages_file_dict(data)\n        if errors:\n            logger.warning(\n                f\"Validation issues in sent messages for user {user_id}: {'; '.join(errors)}\"\n            )\n\n        # Preserve categories from the source data for filtering\n        source_messages = data.get(\"messages\", [])\n        if isinstance(source_messages, list):\n            id_to_category = {\n                msg.get(\"message_id\"): msg.get(\"category\")\n                for msg in source_messages\n                if isinstance(msg, dict) and msg.get(\"message_id\")\n            }\n            for message in normalized_data.get(\"messages\", []):\n                if \"category\" not in message:\n                    category_value = id_to_category.get(message.get(\"message_id\"))\n                    if category_value:\n                        message[\"category\"] = category_value\n\n        if \"messages\" in normalized_data:\n            messages = normalized_data[\"messages\"]\n        else:\n            logger.debug(\n                f\"No 'messages' key found in normalized data for user {user_id}, using empty list\"\n            )\n            messages = []\n\n        if not messages:\n            logger.debug(f\"No messages found for user {user_id}\")\n            return []\n\n        # Apply filters\n        filtered_messages = messages\n\n        # Filter by category if specified\n        if category:\n            filtered_messages = [\n                msg for msg in filtered_messages if msg.get(\"category\") == category\n            ]\n\n        # Filter by days_back if specified\n        if days_back:\n            cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_back)\n            filtered_messages = [\n                msg\n                for msg in filtered_messages\n                if _parse_timestamp(msg.get(\"timestamp\", \"\")) >= cutoff_date\n            ]\n\n        # Sort by timestamp descending (newest first)\n        filtered_messages.sort(\n            key=lambda msg: _parse_timestamp(msg.get(\"timestamp\", \"\")), reverse=True\n        )\n\n        # Apply limit\n        result = filtered_messages[:limit]\n\n        logger.debug(\n            f\"Retrieved {len(result)} recent messages for user {user_id}, category={category}, limit={limit}, days_back={days_back}\"\n        )\n        return result\n\n    except Exception as e:\n        logger.error(f\"Error getting recent messages for user {user_id}: {e}\")\n        return []\n\n\n@handle_errors(\"storing sent message\", default_return=False)\ndef store_sent_message(\n    user_id: str,\n    category: str,\n    message_id: str,\n    message: str,\n    delivery_status: str = \"sent\",\n    time_period: str | None = None,\n) -> bool:\n    \"\"\"\n    Store sent message in chronological order.\n\n    This function maintains the chronological structure by inserting new messages\n    in the correct position based on timestamp.\n\n    Args:\n        user_id: The user ID\n        category: The message category\n        message_id: The message ID\n        message: The message content\n        delivery_status: Delivery status (default: \"sent\")\n        time_period: The time period when the message was sent (e.g., \"morning\", \"evening\")\n\n    Returns:\n        bool: True if message stored successfully\n    \"\"\"\n    if user_id is None:\n        logger.error(\"store_sent_message called with None user_id\")\n        return False\n\n    try:\n        file_path = determine_file_path(\"sent_messages\", user_id)\n        data = load_json_data(file_path) or {}\n\n        # Create new message entry (without redundant user_id field)\n        new_message = {\n            \"message_id\": message_id,\n            \"message\": message,\n            \"category\": category,\n            # Canonical readable timestamp for metadata/log display fields\n            \"timestamp\": now_timestamp_full(),\n            \"delivery_status\": delivery_status,\n        }\n\n        # Add time_period if provided\n        if time_period:\n            new_message[\"time_period\"] = time_period\n\n        # Insert message in chronological order (newest first)\n        messages = data.get(\"messages\", [])\n\n        # Find insertion point\n        insert_index = 0\n        new_timestamp = _parse_timestamp(new_message[\"timestamp\"])\n\n        for i, existing_msg in enumerate(messages):\n            existing_timestamp = _parse_timestamp(existing_msg.get(\"timestamp\", \"\"))\n            if new_timestamp > existing_timestamp:\n                insert_index = i\n                break\n            insert_index = i + 1\n\n        # Insert message\n        messages.insert(insert_index, new_message)\n        data[\"messages\"] = messages\n\n        # Update metadata\n        if \"metadata\" not in data:\n            data[\"metadata\"] = {}\n\n        data[\"metadata\"][\"total_messages\"] = len(messages)\n        # Canonical readable timestamp for metadata/log display fields\n        data[\"metadata\"][\"last_updated\"] = now_timestamp_full()\n\n        save_json_data(data, file_path)\n\n        logger.debug(f\"Stored sent message for user {user_id}, category {category}\")\n        return True\n\n    except Exception as e:\n        logger.error(f\"Error storing sent message for user {user_id}: {e}\")\n        return False\n\n\n@handle_errors(\"archiving old messages\", default_return=False)\ndef archive_old_messages(user_id: str, days_to_keep: int = 365) -> bool:\n    \"\"\"\n    Archive messages older than specified days.\n\n    This function implements file rotation by moving old messages to archive files,\n    keeping the active sent_messages.json file manageable in size.\n\n    Args:\n        user_id: The user ID\n        days_to_keep: Number of days to keep in active file\n\n    Returns:\n        bool: True if archiving successful\n    \"\"\"\n    if user_id is None:\n        logger.error(\"archive_old_messages called with None user_id\")\n        return False\n\n    try:\n        file_path = determine_file_path(\"sent_messages\", user_id)\n        data = load_json_data(file_path)\n\n        if not data or \"messages\" not in data:\n            logger.debug(f\"No messages to archive for user {user_id}\")\n            return True\n\n        # Calculate cutoff date\n        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_to_keep)\n\n        messages = data[\"messages\"]\n        active_messages = []\n        archived_messages = []\n\n        for message in messages:\n            message_timestamp = _parse_timestamp(message.get(\"timestamp\", \"\"))\n            if message_timestamp >= cutoff_date:\n                active_messages.append(message)\n            else:\n                archived_messages.append(message)\n\n        if not archived_messages:\n            logger.debug(f\"No messages to archive for user {user_id}\")\n            return True\n\n        # Create archive file\n        archive_dir = Path(file_path).parent / \"archives\"\n        archive_dir.mkdir(exist_ok=True)\n\n        archive_filename = f\"sent_messages_archive_{now_timestamp_filename()}.json\"\n        archive_path = archive_dir / archive_filename\n\n        # Save archived messages\n        archive_data = {\n            \"metadata\": {\n                \"version\": \"2.0\",\n                # Canonical readable timestamp for metadata/log display fields\n                \"archived_date\": now_timestamp_full(),\n                \"original_file\": str(file_path),\n                \"total_messages\": len(archived_messages),\n                \"date_range\": {\n                    \"oldest\": min(\n                        msg.get(\"timestamp\", \"\") for msg in archived_messages\n                    ),\n                    \"newest\": max(\n                        msg.get(\"timestamp\", \"\") for msg in archived_messages\n                    ),\n                },\n            },\n            \"messages\": archived_messages,\n        }\n\n        save_json_data(archive_data, archive_path)\n\n        # Update active file\n        data[\"messages\"] = active_messages\n        data[\"metadata\"][\"total_messages\"] = len(active_messages)\n        # Canonical readable timestamp for metadata/log display fields\n        data[\"metadata\"][\"last_archived\"] = now_timestamp_full()\n        data[\"metadata\"][\"archived_count\"] = len(archived_messages)\n\n        save_json_data(data, file_path)\n\n        logger.info(\n            f\"Archived {len(archived_messages)} old messages for user {user_id} to {archive_path}\"\n        )\n        return True\n\n    except Exception as e:\n        logger.error(f\"Error archiving old messages for user {user_id}: {e}\")\n        return False\n\n\n@handle_errors(\n    \"parsing timestamp\",\n    # IMPORTANT: avoid datetime.now() here (it would be evaluated at import time).\n    # Use the same \"invalid timestamp\" sentinel the function already returns.\n    default_return=datetime.min.replace(tzinfo=timezone.utc),\n)\ndef _parse_timestamp(timestamp_str: str) -> datetime:\n    \"\"\"\n    Parse timestamp string to datetime object.\n\n    Handles multiple timestamp formats for backward compatibility.\n\n    Args:\n        timestamp_str: Timestamp string to parse\n\n    Returns:\n        datetime: Parsed datetime object\n    \"\"\"\n    if not timestamp_str:\n        return datetime.min.replace(tzinfo=timezone.utc)\n\n    # Try different timestamp formats\n    formats = [\n        TIMESTAMP_FULL,\n        \"%Y-%m-%dT%H:%M:%S\",\n        \"%Y-%m-%dT%H:%M:%SZ\",\n        \"%Y-%m-%dT%H:%M:%S.%fZ\",\n    ]\n\n    for fmt in formats:\n        try:\n            dt = datetime.strptime(timestamp_str, fmt)\n            if dt.tzinfo is None:\n                dt = dt.replace(tzinfo=timezone.utc)\n            return dt\n        except ValueError:\n            continue\n\n    return datetime.min.replace(tzinfo=timezone.utc)\n\n\n@handle_errors(\"creating message file from defaults\")\ndef create_message_file_from_defaults(user_id: str, category: str) -> bool:\n    \"\"\"\n    Create a user's message file for a specific category from default messages.\n    This is the actual worker function that creates the file.\n\n    Args:\n        user_id: The user ID\n        category: The specific category to create a message file for\n\n    Returns:\n        bool: True if file was created successfully\n    \"\"\"\n    if not user_id or not category:\n        logger.error(f\"Invalid parameters: user_id={user_id}, category={category}\")\n        return False\n\n    try:\n        # Load default messages for the category\n        default_messages = load_default_messages(category)\n        if not default_messages:\n            logger.warning(f\"No default messages found for category {category}\")\n            return False\n\n        # Ensure default messages are in the correct format\n        formatted_messages = []\n        for message in default_messages:\n            if isinstance(message, dict):\n                if \"message\" not in message:\n                    logger.warning(\n                        f\"Default message missing 'message' field: {message}\"\n                    )\n                    continue\n                if \"message_id\" not in message:\n                    message[\"message_id\"] = str(uuid.uuid4())\n                if \"days\" not in message:\n                    message[\"days\"] = [\"ALL\"]\n                if \"time_periods\" not in message:\n                    message[\"time_periods\"] = [\"ALL\"]\n                formatted_messages.append(message)\n            else:\n                logger.warning(f\"Invalid message format in defaults: {message}\")\n                continue\n\n        if not formatted_messages:\n            logger.error(f\"No valid messages found in defaults for category {category}\")\n            return False\n\n        # Create user message file with proper format\n        # Note: messages directory should be created by create_user_files() during account creation\n        user_messages_dir = Path(get_user_data_dir(user_id)) / \"messages\"\n        # Ensure the messages directory exists\n        user_messages_dir.mkdir(parents=True, exist_ok=True)\n        category_message_file = user_messages_dir / f\"{category}.json\"\n        message_data = {\"messages\": formatted_messages}\n        try:\n            message_data, _errs = validate_messages_file_dict(message_data)\n        except Exception:\n            pass\n        # Convert Path to string for save_json_data\n        success = save_json_data(message_data, str(category_message_file))\n        if not success:\n            logger.error(\n                f\"Failed to save message file for user {user_id}, category {category}\"\n            )\n            return False\n        logger.info(\n            f\"Created message file for user {user_id}, category {category} from defaults ({len(formatted_messages)} messages)\"\n        )\n        return True\n\n    except Exception as e:\n        logger.error(\n            f\"Error creating message file for user {user_id}, category {category}: {e}\"\n        )\n        return False\n\n\n@handle_errors(\"ensuring user message files exist\")\ndef ensure_user_message_files(user_id: str, categories: list[str]) -> dict:\n    \"\"\"\n    Ensure user has message files for specified categories.\n    Creates messages directory if missing, checks which files are missing, and creates them.\n\n    Args:\n        user_id: The user ID\n        categories: List of categories to check/create message files for (can be subset of user's categories)\n\n    Returns:\n        dict: Summary of the operation with keys:\n            - success: bool - True if all files were created/validated successfully\n            - directory_created: bool - True if messages directory was created\n            - files_checked: int - Number of categories checked\n            - files_created: int - Number of new files created\n            - files_existing: int - Number of files that already existed\n    \"\"\"\n    if not user_id or not categories:\n        logger.error(f\"Invalid parameters: user_id={user_id}, categories={categories}\")\n        return {\n            \"success\": False,\n            \"directory_created\": False,\n            \"files_checked\": 0,\n            \"files_created\": 0,\n            \"files_existing\": 0,\n        }\n\n    try:\n        # Create messages directory for user if it doesn't exist\n        user_messages_dir = Path(get_user_data_dir(user_id)) / \"messages\"\n        directory_created = not user_messages_dir.exists()\n        user_messages_dir.mkdir(parents=True, exist_ok=True)\n\n        # Check which categories are missing message files\n        missing_categories = []\n        for category in categories:\n            category_message_file = user_messages_dir / f\"{category}.json\"\n            if not category_message_file.exists():\n                missing_categories.append(category)\n\n        # Create missing files\n        success_count = 0\n        for category in missing_categories:\n            if create_message_file_from_defaults(user_id, category):\n                success_count += 1\n                logger.debug(\n                    f\"Created missing message file for category {category} for user {user_id}\"\n                )\n            else:\n                logger.warning(\n                    f\"Failed to create missing message file for category {category} for user {user_id}\"\n                )\n\n        # Count existing files as successes\n        existing_count = len(categories) - len(missing_categories)\n        total_success = success_count + existing_count\n\n        result = {\n            \"success\": total_success == len(categories),\n            \"directory_created\": directory_created,\n            \"files_checked\": len(categories),\n            \"files_created\": success_count,\n            \"files_existing\": existing_count,\n        }\n\n        # Only log if files were actually created or if there were issues\n        if success_count > 0 or not result[\"success\"]:\n            logger.info(\n                f\"Ensured message files for user {user_id}: {total_success}/{len(categories)} categories (created {success_count} new files, directory_created={directory_created})\"\n            )\n        else:\n            logger.debug(\n                f\"Verified message files for user {user_id}: {total_success}/{len(categories)} categories (all files already exist)\"\n            )\n        return result\n\n    except Exception as e:\n        logger.error(f\"Error ensuring user message files for user {user_id}: {e}\")\n        return {\n            \"success\": False,\n            \"directory_created\": False,\n            \"files_checked\": len(categories),\n            \"files_created\": 0,\n            \"files_existing\": 0,\n        }\n\n\n@handle_errors(\"getting timestamp for sorting\", default_return=0.0)\ndef get_timestamp_for_sorting(item):\n    \"\"\"\n    Convert timestamp to float for consistent sorting.\n\n    Args:\n        item: Dictionary containing a timestamp field or other data type\n\n    Returns:\n        float: Timestamp as float for sorting, or 0.0 for invalid items\n    \"\"\"\n    if isinstance(item, str):\n        return 0.0\n    elif not isinstance(item, dict):\n        return 0.0\n    timestamp = item.get(\"timestamp\", \"1970-01-01 00:00:00\")\n    try:\n        dt = datetime.strptime(timestamp, TIMESTAMP_FULL)\n        return dt.timestamp()\n    except (ValueError, TypeError):\n        return 0.0\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 663,
                    "line_content": "Handles multiple timestamp formats for backward compatibility.",
                    "start": 22243,
                    "end": 22265
                  }
                ]
              ],
              [
                "core\\scheduler.py",
                "# scheduler.py\n\nimport schedule\nimport time\nimport calendar\nimport pytz\nimport threading\nimport random\nimport subprocess\nimport os  # Needed for test mocking (os.path.exists)\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict, Any\n\nfrom core.user_data_handlers import get_all_user_ids\nfrom core.schedule_management import get_schedule_time_periods\nfrom core.service_utilities import load_and_localize_datetime\nfrom core.time_utilities import (\n    now_timestamp_filename,\n    TIMESTAMP_FULL,\n    DATE_ONLY,\n    TIME_ONLY_MINUTE,\n    TIMESTAMP_MINUTE,\n)\nfrom core.logger import get_component_logger\nfrom user.user_context import UserContext\nfrom core.error_handling import handle_errors\nfrom core.user_data_handlers import get_user_data\nfrom core.backup_manager import backup_manager\n\n# Suppress debug logging from the schedule library to reduce log spam\nfrom core.logger import suppress_noisy_logging\n\nsuppress_noisy_logging()\n\nlogger = get_component_logger(\"scheduler\")\nscheduler_logger = logger\n\n\nclass SchedulerManager:\n    @handle_errors(\"initializing scheduler manager\")\n    def __init__(self, communication_manager):\n        \"\"\"\n        Initialize the SchedulerManager with communication manager.\n\n        Args:\n            communication_manager: The communication manager for sending messages\n        \"\"\"\n        self.communication_manager = communication_manager\n        self.scheduler_thread = None\n        self.running = False\n        self._stop_event = (\n            threading.Event()\n        )  # Add stop event for proper thread management\n        # Track reminder selection state to provide smooth weighted scheduling across calls\n        self._reminder_selection_state: dict[str, float] = {}\n        logger.info(\"SchedulerManager ready\")\n\n    @handle_errors(\"running daily scheduler\")\n    def run_daily_scheduler(self):\n        \"\"\"\n        Starts the daily scheduler in a separate thread that handles all users.\n        \"\"\"\n\n        @handle_errors(\"scheduler loop\", default_return=None)\n        def scheduler_loop():\n            # Clear all accumulated jobs first to prevent job accumulation\n            self.clear_all_accumulated_jobs()\n\n            # Schedule daily log archival at 02:00\n            schedule.every().day.at(\"02:00\").do(self.perform_daily_log_archival)\n            logger.info(\"Scheduled new daily job for log archival at 02:00\")\n\n            # Schedule a single full daily scheduler job at 01:00 to handle complete system initialization\n            # This ensures checkins, task reminders, full cleanup, and weekly backups (if needed) happen daily\n            schedule.every().day.at(\"01:00\").do(self.run_full_daily_scheduler)\n            logger.info(\n                \"Scheduled full daily scheduler job at 01:00 (includes checkins, task reminders, cleanup, and backup check)\"\n            )\n\n            # Schedule messages for all users immediately on startup (one-time only)\n            self.schedule_all_users_immediately()\n\n            # Log job count after daily job scheduling\n            active_jobs = len(schedule.jobs)\n            logger.info(\n                f\"Daily job scheduling complete: {active_jobs} total active jobs scheduled\"\n            )\n\n            loop_count = 0\n            while not self._stop_event.is_set():  # Check for stop signal\n                schedule.run_pending()\n                loop_count += 1\n\n                # Log every 60 iterations (60 minutes) for diagnostic purposes\n                if loop_count % 60 == 0:\n                    active_jobs = len(schedule.jobs)\n                    # Only log if there are actually jobs scheduled - don't log 0 jobs\n                    if active_jobs > 0:\n                        # Count different types of jobs for more meaningful logging\n                        system_jobs = 0\n                        user_message_jobs = 0\n                        task_jobs = 0\n\n                        for job in schedule.jobs:\n                            job_func = job.job_func\n                            if not job_func:\n                                continue\n                            if hasattr(job_func, \"func\"):\n                                if job_func.func == self.perform_daily_log_archival:\n                                    system_jobs += 1\n                                elif job_func.func == self.run_full_daily_scheduler:\n                                    system_jobs += 1\n                                elif (\n                                    job_func.func\n                                    == self.handle_sending_scheduled_message\n                                ):\n                                    user_message_jobs += 1\n                                elif job_func.func == self.handle_task_reminder:\n                                    task_jobs += 1\n\n                        logger.info(\n                            f\"Scheduler running: {active_jobs} total jobs ({system_jobs} system, {user_message_jobs} message, {task_jobs} task)\"\n                        )\n\n                # Use wait instead of sleep to allow immediate shutdown\n                # Use shorter timeout to allow responsive shutdown\n                if self._stop_event.wait(\n                    timeout=10\n                ):  # Wait 10 seconds or until stop signal\n                    break\n            logger.info(\"Scheduler loop stopped gracefully.\")\n\n        self._stop_event.clear()  # Ensure stop event is reset\n        self.scheduler_thread = threading.Thread(target=scheduler_loop)\n        self.scheduler_thread.daemon = True\n        self.scheduler_thread.start()\n        logger.info(\"Scheduler thread started\")\n\n        # Update last run time after successful scheduling\n        self.last_run_time = time.time()\n\n    @handle_errors(\"stopping scheduler\", default_return=None)\n    def stop_scheduler(self):\n        \"\"\"Stops the scheduler thread.\"\"\"\n        if self.scheduler_thread is not None and self.scheduler_thread.is_alive():\n            logger.info(\"Stopping scheduler thread...\")\n            self._stop_event.set()  # Signal the thread to stop\n            self.scheduler_thread.join(\n                timeout=10\n            )  # Wait up to 10 seconds for clean shutdown\n            if self.scheduler_thread.is_alive():\n                logger.warning(\"Scheduler thread didn't stop within timeout period\")\n            else:\n                logger.info(\"Scheduler thread stopped successfully.\")\n            self.scheduler_thread = None\n        else:\n            logger.warning(\"No active scheduler thread to stop.\")\n\n    @handle_errors(\"resetting and rescheduling daily messages\")\n    def reset_and_reschedule_daily_messages(self, category, user_id=None):\n        \"\"\"\n        Resets scheduled tasks for a specific category and reschedules daily messages for that category.\n        \"\"\"\n        # Get the active user ID - either from parameter or UserContext\n        if user_id is None:\n            active_user_id = UserContext().get_user_id()\n        else:\n            active_user_id = user_id\n\n        if not active_user_id:\n            logger.error(\"No active user found during reset and reschedule.\")\n            return\n\n        # Remove only the scheduled jobs for the active user and the specific category\n        schedule.jobs = [\n            job\n            for job in schedule.jobs\n            if not self.is_job_for_category(job, active_user_id, category)\n        ]\n\n        # Handle different categories appropriately\n        if category == \"tasks\":\n            # For tasks, check if task management is enabled and schedule task reminders\n            try:\n                # Get user account data\n                user_data_result = get_user_data(active_user_id, \"account\")\n                user_account = user_data_result.get(\"account\")\n                if (\n                    user_account\n                    and user_account.get(\"features\", {}).get(\"task_management\")\n                    == \"enabled\"\n                ):\n                    self.schedule_all_task_reminders(active_user_id)\n                    logger.info(f\"Rescheduled task reminders for user {active_user_id}\")\n                else:\n                    logger.info(\n                        f\"Task management disabled for user {active_user_id}, skipping task reminder scheduling\"\n                    )\n            except Exception as e:\n                logger.error(\n                    f\"Error rescheduling task reminders for user {active_user_id}: {e}\"\n                )\n        elif category == \"checkin\":\n            # For check-ins, use the standard scheduling\n            self.schedule_daily_message_job(user_id=active_user_id, category=category)\n        else:\n            # For regular message categories, use the standard scheduling\n            self.schedule_daily_message_job(user_id=active_user_id, category=category)\n\n        logger.info(\n            f\"Scheduler reset and rescheduled daily messages for active user: {active_user_id}, category: {category}.\"\n        )\n\n    @handle_errors(\"checking if job exists for category\", default_return=False)\n    def is_job_for_category(self, job, user_id, category):\n        \"\"\"Determines if a job is scheduled for a specific user and category.\"\"\"\n        if job is None:\n            # Check all jobs for this user and category\n            for existing_job in schedule.jobs:\n                job_func = existing_job.job_func\n                if not job_func:\n                    continue\n                # Check if this is a daily scheduler job for this user/category\n                if (\n                    hasattr(job_func, \"func\")\n                    and job_func.func == self.schedule_daily_message_job\n                    and hasattr(job_func, \"keywords\")\n                    and job_func.keywords.get(\"user_id\") == user_id\n                    and job_func.keywords.get(\"category\") == category\n                ):\n                    logger.debug(\n                        f\"Found existing daily job for user {user_id}, category {category}\"\n                    )\n                    return True\n            logger.debug(\n                f\"No existing daily job found for user {user_id}, category {category}\"\n            )\n            return False\n        else:\n            # Check specific job\n            job_func = job.job_func\n            if not job_func:\n                return False\n            if (\n                hasattr(job_func, \"func\")\n                and job_func.func == self.schedule_daily_message_job\n                and hasattr(job_func, \"keywords\")\n                and job_func.keywords.get(\"user_id\") == user_id\n                and job_func.keywords.get(\"category\") == category\n            ):\n                return True\n            return False\n\n    @handle_errors(\"scheduling all users immediately\", default_return=None)\n    def schedule_all_users_immediately(self):\n        \"\"\"Schedule daily messages immediately for all users\"\"\"\n        user_ids = get_all_user_ids()\n        if not user_ids:\n            logger.warning(\"No users found for scheduling\")\n            return\n\n        total_scheduled = 0\n        logger.info(f\"Starting immediate scheduling for {len(user_ids)} users\")\n\n        # Log current time once at the start\n        tz = pytz.timezone(\"America/Regina\")\n        now_datetime = datetime.now(tz)\n        logger.info(\n            f\"Current time for scheduling: {now_datetime.strftime(TIMESTAMP_MINUTE)}\"\n        )\n\n        for user_id in user_ids:\n            try:\n                # Schedule regular message categories\n                prefs_result = get_user_data(user_id, \"preferences\")\n                categories = prefs_result.get(\"preferences\", {}).get(\"categories\", [])\n                if isinstance(categories, list):\n                    if categories:  # Only process if list is not empty\n                        for category in categories:\n                            try:\n                                self.schedule_daily_message_job(user_id, category)\n                                total_scheduled += 1\n                                logger.debug(\n                                    f\"Scheduled messages for user {user_id}, category {category}\"\n                                )\n                            except Exception as e:\n                                logger.error(\n                                    f\"Failed to schedule for user {user_id}, category {category}: {e}\"\n                                )\n                    # Empty list is fine - no warning needed\n                else:\n                    logger.warning(\n                        f\"Expected list for categories, got {type(categories)} for user '{user_id}'\"\n                    )\n\n                # Schedule check-ins if enabled\n                try:\n                    # Get user account data\n                    user_data_result = get_user_data(user_id, \"account\")\n                    user_account = user_data_result.get(\"account\")\n                    if (\n                        user_account\n                        and user_account.get(\"features\", {}).get(\"checkins\")\n                        == \"enabled\"\n                    ):\n                        # Check if check-in category exists in schedules\n                        time_periods = get_schedule_time_periods(user_id, \"checkin\")\n                        if time_periods:\n                            self.schedule_daily_message_job(user_id, \"checkin\")\n                            total_scheduled += 1\n                            logger.debug(f\"Scheduled check-ins for user {user_id}\")\n                        else:\n                            logger.debug(\n                                f\"No check-in schedule found for user {user_id}\"\n                            )\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to schedule check-ins for user {user_id}: {e}\"\n                    )\n\n                # Schedule task reminders if tasks are enabled\n                try:\n                    self.schedule_all_task_reminders(user_id)\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to schedule task reminders for user {user_id}: {e}\"\n                    )\n\n            except Exception as e:\n                logger.error(f\"Failed to get categories for user {user_id}: {e}\")\n\n        logger.info(\n            f\"Scheduling complete: {total_scheduled} user/category combinations scheduled (includes checkins if enabled)\"\n        )\n\n        # Log consolidated scheduler status\n        active_jobs = len(schedule.jobs)\n        if active_jobs > 0:\n            # Count job types for diagnostic purposes\n            job_types = {}\n            for job in schedule.jobs:\n                job_func = job.job_func\n                if not job_func:\n                    continue\n\n                # schedule often wraps callables (functools.partial) which may not have __name__\n                func_obj = job_func.func if hasattr(job_func, \"func\") else job_func\n                job_func_name = getattr(func_obj, \"__name__\", \"UnknownJob\")\n\n                job_types[job_func_name] = job_types.get(job_func_name, 0) + 1\n\n            job_type_summary = \", \".join(\n                [f\"{count} {name}\" for name, count in job_types.items()]\n            )\n            logger.info(\n                f\"Scheduler status: {active_jobs} total jobs ({job_type_summary})\"\n            )\n        else:\n            logger.info(\"Scheduler status: No active jobs\")\n\n    @handle_errors(\"scheduling new user\", default_return=None)\n    def schedule_new_user(self, user_id: str):\n        \"\"\"\n        Schedule a newly created user immediately.\n        This method should be called after a new user is created to add them to the scheduler.\n\n        Args:\n            user_id: The ID of the newly created user\n        \"\"\"\n        logger.info(f\"Scheduling new user: {user_id}\")\n\n        try:\n            # Schedule regular message categories\n            prefs_result = get_user_data(user_id, \"preferences\")\n            categories = prefs_result.get(\"preferences\", {}).get(\"categories\", [])\n            if isinstance(categories, list) and categories:\n                for category in categories:\n                    try:\n                        self.schedule_daily_message_job(user_id, category)\n                        logger.info(\n                            f\"Scheduled messages for new user {user_id}, category {category}\"\n                        )\n                    except Exception as e:\n                        logger.error(\n                            f\"Failed to schedule for new user {user_id}, category {category}: {e}\"\n                        )\n\n            # Schedule check-ins if enabled\n            try:\n                # Get user account data\n                user_data_result = get_user_data(user_id, \"account\")\n                user_account = user_data_result.get(\"account\")\n                if (\n                    user_account\n                    and user_account.get(\"features\", {}).get(\"checkins\") == \"enabled\"\n                ):\n                    # Check if check-in category exists in schedules\n                    time_periods = get_schedule_time_periods(user_id, \"checkin\")\n                    if time_periods:\n                        self.schedule_daily_message_job(user_id, \"checkin\")\n                        logger.info(f\"Scheduled check-ins for new user {user_id}\")\n                    else:\n                        logger.debug(\n                            f\"No check-in schedule found for new user {user_id}\"\n                        )\n            except Exception as e:\n                logger.error(\n                    f\"Failed to schedule check-ins for new user {user_id}: {e}\"\n                )\n\n            # Schedule task reminders if tasks are enabled\n            try:\n                self.schedule_all_task_reminders(user_id)\n                logger.info(f\"Scheduled task reminders for new user {user_id}\")\n            except Exception as e:\n                logger.error(\n                    f\"Failed to schedule task reminders for new user {user_id}: {e}\"\n                )\n\n            logger.info(f\"Successfully scheduled new user: {user_id}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to schedule new user {user_id}: {e}\")\n            raise\n\n    @handle_errors(\"running full daily scheduler\")\n    def run_full_daily_scheduler(self):\n        \"\"\"\n        Runs the full daily scheduler process - same as system startup.\n        This includes clearing accumulated jobs, scheduling all users, checkins, task reminders, and checking for weekly backups.\n        \"\"\"\n        logger.info(\"Running full daily scheduler process (01:00 daily job)\")\n\n        # Check if weekly backup is needed (before everything else, so it runs before archival at 02:00)\n        self.check_and_perform_weekly_backup()\n\n        # Clear all accumulated jobs first to prevent job accumulation\n        self.clear_all_accumulated_jobs()\n\n        # Immediately schedule messages for all users (includes checkins and task reminders)\n        self.schedule_all_users_immediately()\n\n        # Schedule daily log archival at 02:00 (after user scheduling)\n        schedule.every().day.at(\"02:00\").do(self.perform_daily_log_archival)\n        logger.info(\"Scheduled new daily job for log archival at 02:00\")\n\n        # Schedule the next day's full daily scheduler job at 01:00\n        schedule.every().day.at(\"01:00\").do(self.run_full_daily_scheduler)\n        logger.info(\n            \"Scheduled full daily scheduler job at 01:00 (includes checkins, task reminders, cleanup, and backup check)\"\n        )\n\n        # Schedule periodic orphaned reminder cleanup at 03:00 daily\n        schedule.every().day.at(\"03:00\").do(self.cleanup_orphaned_task_reminders)\n        logger.info(\"Scheduled daily orphaned task reminder cleanup at 03:00\")\n\n        # Schedule data directory cleanup at 04:00 daily (after backups and reminders)\n        try:\n            from core.auto_cleanup import (\n                cleanup_data_directory,\n                cleanup_tests_data_directory,\n            )\n\n            schedule.every().day.at(\"04:00\").do(cleanup_data_directory)\n            logger.info(\"Scheduled daily data directory cleanup at 04:00\")\n            # Also clean up tests/data directory\n            schedule.every().day.at(\"04:05\").do(cleanup_tests_data_directory)\n            logger.info(\"Scheduled daily tests data directory cleanup at 04:05\")\n        except Exception as e:\n            logger.warning(f\"Failed to schedule data directory cleanup: {e}\")\n\n        # Log job count after daily job scheduling\n        active_jobs = len(schedule.jobs)\n        logger.info(\n            f\"Full daily scheduler complete: {active_jobs} total active jobs scheduled\"\n        )\n\n    @handle_errors(\"scheduling daily message job\")\n    def schedule_daily_message_job(self, user_id, category):\n        \"\"\"\n        Schedules daily messages immediately for the specified user and category.\n        Schedules one message per active period in the category.\n        \"\"\"\n        logger.info(\n            f\"Scheduling daily messages immediately for user {user_id}, category {category}.\"\n        )\n\n        # Clean up old jobs for this user and category\n        self.cleanup_old_tasks(user_id, category)\n\n        # Get all time periods for this user and category\n        time_periods = get_schedule_time_periods(user_id, category)\n        if not time_periods:\n            logger.error(\n                f\"No time periods found for user {user_id}, category {category}.\"\n            )\n            return\n\n        # Schedule a message for each active period\n        scheduled_count = 0\n\n        # Avoid hardcoded strftime(\"%A\") format strings.\n        # Use weekday index + calendar for a stable day-name.\n        today_name = calendar.day_name[datetime.now().weekday()]\n\n        for period_name, period_data in time_periods.items():\n            # Skip the \"ALL\" period - it should not be scheduled, only used as fallback\n            if period_name == \"ALL\":\n                logger.debug(\n                    f\"Skipping ALL period scheduling for user {user_id}, category {category} - ALL is fallback only\"\n                )\n                continue\n\n            # Check if this period is active (default to active if not specified)\n            if period_data.get(\"active\", True):\n                # If 'days' is present, only schedule if today is in days or if days contains \"ALL\"\n                if \"days\" in period_data:\n                    days = period_data[\"days\"]\n                    if \"ALL\" in days:\n                        # Schedule for all days\n                        logger.debug(\n                            f\"Scheduling period {period_name} for user {user_id}, category {category} (ALL days)\"\n                        )\n                    elif today_name in days:\n                        # Schedule for today\n                        logger.debug(\n                            f\"Scheduling period {period_name} for user {user_id}, category {category} (today: {today_name})\"\n                        )\n                    else:\n                        logger.debug(\n                            f\"Skipping period {period_name} for user {user_id}, category {category} (not scheduled for today: {today_name})\"\n                        )\n                        continue\n\n                try:\n                    self.schedule_message_for_period(user_id, category, period_name)\n                    scheduled_count += 1\n                    logger.debug(\n                        f\"Scheduled message for user {user_id}, category {category}, period {period_name}\"\n                    )\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to schedule message for user {user_id}, category {category}, period {period_name}: {e}\"\n                    )\n            else:\n                logger.debug(\n                    f\"Skipping inactive period {period_name} for user {user_id}, category {category}\"\n                )\n\n        logger.info(\n            f\"Scheduled {scheduled_count} messages for user {user_id}, category {category}\"\n        )\n\n    @handle_errors(\"scheduling message for specific period\")\n    def schedule_message_for_period(self, user_id, category, period_name):\n        \"\"\"\n        Schedules a message at a random time within a specific period for a user and category.\n        \"\"\"\n        logger.info(\n            f\"Scheduling message for period '{period_name}' for user {user_id}, category {category}.\"\n        )\n\n        max_retries = 10\n        retry_count = 0\n\n        while retry_count < max_retries:\n            # Get a random time within the specified period\n            random_time_str = self.get_random_time_within_period(\n                user_id, category, period_name\n            )\n            if not random_time_str:\n                logger.error(\n                    f\"Could not generate random time for user {user_id}, category {category}, period {period_name}.\"\n                )\n                return\n\n            # Try to schedule the message\n            try:\n                datetime_str = random_time_str\n                schedule_datetime = load_and_localize_datetime(\n                    datetime_str, \"America/Regina\"\n                )\n                now = datetime.now(pytz.timezone(\"America/Regina\"))\n\n                logger.info(\n                    f\"Attempting to schedule message for user {user_id}, category {category}, period {period_name} at {schedule_datetime} (now is {now})\"\n                )\n\n                if not self.is_time_conflict(user_id, schedule_datetime):\n                    if schedule_datetime <= now:\n                        schedule_datetime += timedelta(days=1)\n                        logger.info(\n                            f\"Adjusted scheduling time to future for user {user_id}: {schedule_datetime}\"\n                        )\n\n                    time_part = schedule_datetime.strftime(TIME_ONLY_MINUTE)\n                    # Schedule as one-time job that will remove itself after execution\n                    schedule.every().day.at(time_part).do(\n                        self.handle_sending_scheduled_message,\n                        user_id=user_id,\n                        category=category,\n                    )\n                    logger.info(\n                        f\"Successfully scheduled {category} message for user {user_id}, period {period_name} \"\n                        f\"at {time_part} on {schedule_datetime.strftime(DATE_ONLY)}.\"\n                    )\n\n                    # Set the wake timer for the scheduled time\n                    self.set_wake_timer(\n                        schedule_datetime, user_id, category, period_name\n                    )\n                    break  # Exit the loop after successfully scheduling the message\n                else:\n                    logger.info(\n                        f\"Conflict detected for user {user_id}, category {category}, period {period_name} at {schedule_datetime}. Retrying...\"\n                    )\n                    retry_count += 1\n\n            except Exception as e:\n                logger.error(\n                    f\"Error while scheduling {category} message for user {user_id}, period {period_name}: {str(e)}\"\n                )\n                retry_count += 1\n\n        if retry_count == max_retries:\n            logger.warning(\n                f\"Max retries reached. Could not find a suitable time for user {user_id}, category {category}, period {period_name}.\"\n            )\n\n    @handle_errors(\"scheduling check-in at exact time\")\n    def schedule_checkin_at_exact_time(self, user_id, period_name):\n        \"\"\"\n        Schedule a check-in at the exact time specified in the period.\n        \"\"\"\n        logger.info(f\"Scheduling check-in for user {user_id}, period {period_name}\")\n\n        try:\n            # Get the time periods for check-in category\n            time_periods = get_schedule_time_periods(user_id, \"checkin\")\n            if not time_periods or period_name not in time_periods:\n                logger.error(\n                    f\"No time period '{period_name}' found for check-in scheduling for user {user_id}\"\n                )\n                return\n\n            period_data = time_periods[period_name]\n            # Use canonical keys with fallback to legacy keys\n            checkin_time = period_data.get(\"start_time\") or period_data.get(\"start\")\n            if not checkin_time:\n                logger.error(\n                    f\"Missing start time for check-in period {period_name} in user {user_id}\"\n                )\n                return\n\n            # Create datetime for today at the specified time\n            tz = pytz.timezone(\"America/Regina\")\n            now = datetime.now(tz)\n            today = now.date()\n\n            # Parse the check-in time\n            hour, minute = map(int, checkin_time.split(\":\"))\n            schedule_datetime = datetime.combine(\n                today, datetime.min.time().replace(hour=hour, minute=minute), tzinfo=tz\n            )\n\n            # If the time has already passed today, schedule for tomorrow\n            if schedule_datetime <= now:\n                schedule_datetime += timedelta(days=1)\n                logger.info(\n                    f\"Check-in time has passed today, scheduling for tomorrow: {schedule_datetime}\"\n                )\n\n            # Schedule the check-in\n            time_part = schedule_datetime.strftime(TIME_ONLY_MINUTE)\n            schedule.every().day.at(time_part).do(\n                self.handle_sending_scheduled_message,\n                user_id=user_id,\n                category=\"checkin\",\n            )\n            logger.info(\n                f\"Successfully scheduled check-in for user {user_id} at {time_part} \"\n                f\"on {schedule_datetime.strftime(DATE_ONLY)}.\"\n            )\n\n            # Set the wake timer for the scheduled time\n            self.set_wake_timer(schedule_datetime, user_id, \"checkin\", period_name)\n\n        except Exception as e:\n            logger.error(f\"Error scheduling check-in for user {user_id}: {e}\")\n\n    @handle_errors(\"scheduling message at random time\")\n    def schedule_message_at_random_time(self, user_id, category):\n        \"\"\"\n        Schedules a message at a random time within the user's preferred time periods.\n        \"\"\"\n        logger.info(\n            f\"Scheduling message at random time for user {user_id}, category {category}.\"\n        )\n\n        time_periods = get_schedule_time_periods(user_id, category)\n        if not time_periods:\n            logger.error(\n                f\"No time periods found for user {user_id}, category {category}.\"\n            )\n            return\n\n        available_periods = list(time_periods.keys())\n        if not available_periods:\n            logger.error(\n                f\"No available periods for user {user_id}, category {category}.\"\n            )\n            return\n\n        selected_period = available_periods[0]\n        logger.info(\n            f\"Using period '{selected_period}' for user {user_id}, category {category}\"\n        )\n\n        max_retries = 10\n        retry_count = 0\n\n        while retry_count < max_retries:\n            random_time_str = self.get_random_time_within_period(\n                user_id, category, selected_period\n            )\n            if not random_time_str:\n                logger.error(\n                    f\"Could not generate random time for user {user_id}, category {category}.\"\n                )\n                return\n\n            try:\n                schedule_datetime = load_and_localize_datetime(\n                    random_time_str, \"America/Regina\"\n                )\n                now = datetime.now(pytz.timezone(\"America/Regina\"))\n\n                logger.info(\n                    f\"Attempting to schedule message for user {user_id}, category {category} at {schedule_datetime} (now is {now})\"\n                )\n\n                if not self.is_time_conflict(user_id, schedule_datetime):\n                    if schedule_datetime <= now:\n                        schedule_datetime += timedelta(days=1)\n                        logger.info(\n                            f\"Adjusted scheduling time to future for user {user_id}: {schedule_datetime}\"\n                        )\n\n                    time_part = schedule_datetime.strftime(TIME_ONLY_MINUTE)\n                    schedule.every().day.at(time_part).do(\n                        self.handle_sending_scheduled_message,\n                        user_id=user_id,\n                        category=category,\n                    )\n                    logger.info(\n                        f\"Successfully scheduled {category} message for user {user_id} at {time_part} \"\n                        f\"on {schedule_datetime.strftime(DATE_ONLY)}.\"\n                    )\n\n                    self.set_wake_timer(\n                        schedule_datetime, user_id, category, selected_period\n                    )\n                    break\n                else:\n                    logger.info(\n                        f\"Conflict detected for user {user_id}, category {category} at {schedule_datetime}. Retrying...\"\n                    )\n                    retry_count += 1\n\n            except Exception as e:\n                logger.error(\n                    f\"Error while scheduling {category} message for user {user_id}: {str(e)}\"\n                )\n                retry_count += 1\n\n        if retry_count == max_retries:\n            logger.warning(\n                f\"Max retries reached. Could not find a suitable time for user {user_id}, category {category}.\"\n            )\n\n    @handle_errors(\"checking time conflict\", default_return=False)\n    def is_time_conflict(self, user_id, schedule_datetime):\n        \"\"\"\n        Checks if there is a time conflict with any existing scheduled jobs for the user.\n\n        NOTE:\n        The `schedule` library commonly uses naive datetimes for `job.next_run`.\n        Our schedule_datetime is often tz-aware (pytz). Subtracting naive/aware raises TypeError.\n        \"\"\"\n        # Normalize schedule_datetime to naive for comparison against schedule's job.next_run.\n        # We intentionally drop tzinfo here because schedule's next_run is not reliably tz-aware.\n        if (\n            isinstance(schedule_datetime, datetime)\n            and schedule_datetime.tzinfo is not None\n        ):\n            schedule_dt_for_compare = schedule_datetime.replace(tzinfo=None)\n        else:\n            schedule_dt_for_compare = schedule_datetime\n\n        for job in schedule.jobs:\n            job_func = job.job_func\n            if not job_func:\n                continue\n\n            job_args = getattr(job_func, \"args\", None)\n            if job_args and job_args[0] == user_id:\n                job_time = job.next_run\n                if not job_time:\n                    continue\n\n                # Normalize job_time too (defensive)\n                if isinstance(job_time, datetime) and job_time.tzinfo is not None:\n                    job_time_for_compare = job_time.replace(tzinfo=None)\n                else:\n                    job_time_for_compare = job_time\n\n                # Increase conflict window to 2 hours to prevent multiple messages at similar times\n                try:\n                    if (\n                        abs(\n                            (\n                                job_time_for_compare - schedule_dt_for_compare\n                            ).total_seconds()\n                        )\n                        < 7200\n                    ):\n                        return True\n                except TypeError as e:\n                    # Extremely defensive: if something weird slips through, log and treat as no conflict\n                    logger.debug(\n                        f\"TypeError comparing job_time and schedule_datetime for conflict check: {e}\"\n                    )\n                    continue\n\n        return False\n\n    @handle_errors(\"getting random time within period\", default_return=None)\n    def get_random_time_within_period(\n        self, user_id, category, period, timezone_str=\"America/Regina\"\n    ):\n        \"\"\"Get a random time within a specified period for a given category.\"\"\"\n        tz = pytz.timezone(timezone_str)\n        now_datetime = datetime.now(tz)\n\n        time_periods = get_schedule_time_periods(user_id, category)\n\n        # Add validation for period existence\n        if period not in time_periods:\n            logger.error(\n                f\"Period '{period}' not found in time periods for user {user_id}, category {category}. Available periods: {list(time_periods.keys())}\"\n            )\n            return None\n\n        # Use canonical keys with fallback to legacy keys\n        start_time = time_periods[period].get(\"start_time\") or time_periods[period].get(\n            \"start\"\n        )\n        end_time = time_periods[period].get(\"end_time\") or time_periods[period].get(\n            \"end\"\n        )\n\n        if not start_time or not end_time:\n            logger.error(\n                f\"Missing start/end time for period {period} in user {user_id}, category {category}\"\n            )\n            return None\n\n        period_start_time = datetime.strptime(start_time, TIME_ONLY_MINUTE).time()\n        period_end_time = datetime.strptime(end_time, TIME_ONLY_MINUTE).time()\n\n        # Create datetime objects for today\n        start_datetime = datetime.combine(\n            now_datetime.date(), period_start_time, tzinfo=tz\n        )\n        end_datetime = datetime.combine(now_datetime.date(), period_end_time, tzinfo=tz)\n\n        # If the period has already ended today, schedule for tomorrow\n        if end_datetime <= now_datetime:\n            start_datetime += timedelta(days=1)\n            end_datetime += timedelta(days=1)\n        # If the period is currently active or about to start within the next 30 minutes, schedule for tomorrow\n        elif start_datetime <= now_datetime + timedelta(minutes=30):\n            start_datetime += timedelta(days=1)\n            end_datetime += timedelta(days=1)\n\n        total_seconds = (end_datetime - start_datetime).total_seconds()\n        if total_seconds <= 0:\n            logger.error(\"Invalid time range calculated.\")\n            return None\n\n        random_seconds = random.randint(0, int(total_seconds))\n        random_datetime = start_datetime + timedelta(seconds=random_seconds)\n\n        # Final safety check - ensure the time is in the future\n        if random_datetime <= now_datetime:\n            random_datetime += timedelta(days=1)\n\n        random_time_str = random_datetime.strftime(TIMESTAMP_MINUTE)\n        logger.info(f\"Scheduled random time: {random_time_str}\")\n        return random_time_str\n\n    @handle_errors(\"logging scheduled tasks\")\n    def log_scheduled_tasks(self):\n        \"\"\"Logs all current and upcoming scheduled tasks in a user-friendly manner.\"\"\"\n        for job in schedule.jobs:\n            job_func = job.job_func\n            if not job_func:\n                continue\n\n            next_run = job.next_run.strftime(TIMESTAMP_FULL) if job.next_run else \"None\"\n\n            # Safely resolve function name for schedule-wrapped callables.\n            # schedule often wraps functions in functools.partial, which may not have __name__.\n            func_obj = job_func.func if hasattr(job_func, \"func\") else job_func\n            task_name = getattr(func_obj, \"__name__\", \"Scheduled Task\")\n\n            task_description = (\n                str(job_func).split(\"function \")[-1].split(\" at \")[0]\n                if \"function\" in str(job_func)\n                else task_name\n            )\n\n            category = \"Generic Task\"\n            job_args = getattr(job_func, \"args\", None)\n            if hasattr(job_func, \"func\") and job_args:\n                category = job_args[0]\n\n            logger.info(\n                f\"Task: {task_description}, Category: {category}, Scheduled at: {job.at_time}, Next run: {next_run}\"\n            )\n\n    @handle_errors(\"handling sending scheduled message\")\n    def handle_sending_scheduled_message(\n        self, user_id, category, retry_attempts=3, retry_delay=30\n    ):\n        \"\"\"\n        Handles the sending of scheduled messages with retries.\n        This is a one-time job that removes itself after execution.\n        \"\"\"\n        if self.communication_manager is None:\n            logger.error(\"Communication manager is not initialized.\")\n            return\n\n        attempt = 0\n        while attempt < retry_attempts:\n            try:\n                # Try to send the message\n                self.communication_manager.handle_message_sending(user_id, category)\n                logger.info(\n                    f\"Message sent successfully for user {user_id}, category {category}.\"\n                )\n\n                # Remove this job after successful execution to make it a one-time job\n                self._remove_user_message_job(user_id, category)\n                return  # Exit after successful execution\n            except Exception as e:\n                logger.error(\n                    f\"Error sending message for user {user_id}, category {category}: {e}\"\n                )\n                attempt += 1\n                logger.info(\n                    f\"Retrying in {retry_delay} seconds... ({attempt}/{retry_attempts})\"\n                )\n                time.sleep(retry_delay)  # Wait before retrying\n\n        # Remove job even if it failed after all retries\n        self._remove_user_message_job(user_id, category)\n\n    @handle_errors(\"removing user message job\")\n    def _remove_user_message_job(self, user_id, category):\n        \"\"\"\n        Removes user message jobs from the scheduler after execution.\n        This makes user message jobs effectively one-time jobs.\n        \"\"\"\n        try:\n            # Find and remove jobs for this user and category\n            jobs_to_remove = []\n            for job in schedule.jobs:\n                job_func = job.job_func\n                if not job_func:\n                    continue\n                job_args = getattr(job_func, \"args\", None)\n                if (\n                    hasattr(job_func, \"func\")\n                    and job_func.func == self.handle_sending_scheduled_message\n                    and job_args\n                    and len(job_args) >= 2\n                    and job_args[0] == user_id\n                    and job_args[1] == category\n                ):\n                    jobs_to_remove.append(job)\n\n            # Remove the jobs\n            for job in jobs_to_remove:\n                schedule.jobs.remove(job)\n                logger.debug(\n                    f\"Removed one-time job for user {user_id}, category {category}\"\n                )\n\n            if jobs_to_remove:\n                logger.info(\n                    f\"Removed {len(jobs_to_remove)} completed message job(s) for user {user_id}, category {category}\"\n                )\n\n        except Exception as e:\n            logger.error(\n                f\"Error removing user message job for user {user_id}, category {category}: {e}\"\n            )\n\n    @handle_errors(\"handling task reminder\")\n    def handle_task_reminder(self, user_id, task_id, retry_attempts=3, retry_delay=30):\n        \"\"\"\n        Handles sending task reminders with retries.\n        \"\"\"\n        if self.communication_manager is None:\n            logger.error(\"Communication manager is not initialized.\")\n            return\n\n        attempt = 0\n        while attempt < retry_attempts:\n            try:\n                # Import task management functions\n                from tasks.task_management import get_task_by_id, update_task\n\n                # Get the task details\n                task = get_task_by_id(user_id, task_id)\n                if not task:\n                    logger.error(f\"Task {task_id} not found for user {user_id}\")\n                    return\n\n                # Check if task is still active and not already completed\n                if task.get(\"completed\", False):\n                    logger.info(\n                        f\"Task {task_id} is already completed, skipping reminder\"\n                    )\n                    return\n\n                # Send the task reminder via communication manager\n                self.communication_manager.handle_task_reminder(user_id, task_id)\n\n                # Mark reminder as sent\n                update_task(user_id, task_id, {\"reminder_sent\": True})\n\n                logger.info(\n                    f\"Task reminder sent successfully for user {user_id}, task {task_id}\"\n                )\n                return  # Exit after successful execution\n\n            except Exception as e:\n                logger.error(\n                    f\"Error sending task reminder for user {user_id}, task {task_id}: {e}\"\n                )\n                attempt += 1\n                logger.info(\n                    f\"Retrying in {retry_delay} seconds... ({attempt}/{retry_attempts})\"\n                )\n                time.sleep(retry_delay)  # Wait before retrying\n\n    @handle_errors(\"setting wake timer\")\n    def set_wake_timer(\n        self, schedule_time, user_id, category, period, wake_ahead_minutes=4\n    ):\n        \"\"\"\n        Set a Windows scheduled task to wake the computer before a scheduled message.\n\n        Args:\n            schedule_time: The datetime when the message is scheduled\n            user_id: The user ID\n            category: The message category\n            period: The time period name\n            wake_ahead_minutes: Minutes before schedule_time to wake the computer (default: 4)\n        \"\"\"\n        # CRITICAL: Never create real Windows tasks for test users\n        # Check if we're in test mode or if user data is in test directory\n        if os.getenv(\"MHM_TESTING\") == \"1\":\n            logger.debug(\n                f\"Skipping wake timer creation for test user {user_id} (MHM_TESTING=1)\"\n            )\n            return\n\n        # Additional safety check: verify user data directory is not in tests directory\n        try:\n            from core.config import get_user_data_dir, BASE_DATA_DIR\n\n            user_data_dir = get_user_data_dir(user_id)\n            if user_data_dir and (\n                \"tests\" in user_data_dir or \"test\" in BASE_DATA_DIR.lower()\n            ):\n                logger.debug(\n                    f\"Skipping wake timer creation for test user {user_id} (test data directory detected)\"\n                )\n                return\n        except Exception as e:\n            logger.debug(f\"Could not verify user data directory for {user_id}: {e}\")\n            # If we can't verify, err on the side of caution and skip task creation\n            if os.getenv(\"MHM_TESTING\") == \"1\":\n                return\n\n        # Adjust the schedule_time to wake the computer a few minutes earlier\n        wake_time = schedule_time - timedelta(minutes=wake_ahead_minutes)\n        task_name = (\n            f\"Wake_{user_id}_{category}_{period}_{wake_time.strftime(TIME_ONLY_MINUTE)}\"\n        )\n        task_time = wake_time.strftime(TIME_ONLY_MINUTE)\n        task_date = wake_time.strftime(DATE_ONLY)\n\n        # PowerShell script to create the task with Wake computer enabled\n        ps_command = f\"\"\"\n        $action = New-ScheduledTaskAction -Execute 'cmd.exe' -Argument '/c exit'\n        $trigger = New-ScheduledTaskTrigger -Once -At \"{task_date}T{task_time}:00\"\n        $settings = New-ScheduledTaskSettingsSet -AllowStartIfOnBatteries -DontStopIfGoingOnBatteries -StartWhenAvailable -WakeToRun\n        Register-ScheduledTask -TaskName \"{task_name}\" -Action $action -Trigger $trigger -Settings $settings -Force\n        \"\"\"\n\n        # Execute the PowerShell command and capture the output\n        result = subprocess.run(\n            [\"powershell\", \"-Command\", ps_command], capture_output=True, text=True\n        )\n\n        if result.returncode == 0:\n            logger.info(\n                f\"Wake timer set for {wake_time.strftime(TIMESTAMP_MINUTE)} with task {task_name}\"\n            )\n        else:\n            logger.error(f\"Failed to set wake timer with error: {result.stderr}\")\n            # Only log detailed info on errors\n            logger.debug(f\"PowerShell command: {ps_command.strip()}\")\n            logger.debug(f\"PowerShell stdout: {result.stdout}\")\n            logger.debug(f\"PowerShell return code: {result.returncode}\")\n\n    @handle_errors(\"cleaning up old tasks\")\n    def cleanup_old_tasks(self, user_id, category):\n        \"\"\"Cleans up all tasks (scheduled jobs and system tasks) associated with a given user and category.\"\"\"\n        # Count jobs before cleanup\n        initial_job_count = len(schedule.jobs)\n        jobs_removed = 0\n\n        # Remove only jobs for this specific user and category\n        jobs_to_remove = []\n        for job in schedule.jobs:\n            if self.is_job_for_category(job, user_id, category):\n                jobs_to_remove.append(job)\n\n        # Remove the identified jobs\n        for job in jobs_to_remove:\n            try:\n                schedule.jobs.remove(job)\n                jobs_removed += 1\n                logger.debug(f\"Removed job for user {user_id}, category {category}\")\n            except ValueError:\n                # Job was already removed\n                pass\n\n        final_job_count = len(schedule.jobs)\n        logger.info(\n            f\"In-memory scheduler cleanup completed for user {user_id}, category {category}. Removed {jobs_removed} jobs (from {initial_job_count} to {final_job_count} total jobs).\"\n        )\n\n    @handle_errors(\"clearing all accumulated jobs\")\n    def clear_all_accumulated_jobs(self):\n        \"\"\"Clears all accumulated scheduler jobs and reschedules only the necessary ones.\"\"\"\n        logger.info(\"Clearing all accumulated scheduler jobs...\")\n\n        # Count jobs before cleanup\n        initial_job_count = len(schedule.jobs)\n        logger.info(\n            f\"Initial job count: {initial_job_count} (accumulated jobs to be cleared)\"\n        )\n\n        # Clear all jobs\n        schedule.clear()\n        logger.info(\"All scheduler jobs cleared\")\n\n        # Don't reschedule daily jobs here - the main scheduler loop will handle that\n        # This function is only for clearing accumulated jobs, not scheduling new ones\n\n        final_job_count = len(schedule.jobs)\n        jobs_cleared = initial_job_count - final_job_count\n\n        # Log cleanup results in a single message\n        if jobs_cleared > 0:\n            logger.info(\n                f\"Job cleanup complete: {jobs_cleared} accumulated jobs cleared\"\n            )\n        elif jobs_cleared == 0:\n            logger.debug(\n                \"Job cleanup complete: No accumulated jobs to clear (system was already clean)\"\n            )\n        else:\n            logger.info(\n                f\"Job cleanup complete: Added {abs(jobs_cleared)} scheduler jobs (system had fewer jobs than expected)\"\n            )\n\n        # Cleanup system tasks for all users/categories\n        logger.info(\"Starting system task cleanup for all users...\")\n        system_tasks_cleaned = 0\n\n        try:\n            result = subprocess.run(\n                [\"schtasks\", \"/query\", \"/fo\", \"LIST\", \"/v\"],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n            )\n            if result.returncode != 0:\n                logger.debug(f\"Could not query system tasks: {result.stderr}\")\n                logger.info(\"System task cleanup completed (no tasks to clean).\")\n                return\n\n            tasks = result.stdout.splitlines()\n            tasks_deleted = 0\n\n            # Get user IDs for task cleanup\n            user_ids = get_all_user_ids()\n\n            # Look for tasks with our user ID prefixes\n            for line in tasks:\n                if line.startswith(\"TaskName:\"):\n                    task_name = line.split(\":\")[1].strip()\n                    # Check if this is one of our MHM wake tasks\n                    if \"Wake_\" in task_name and any(\n                        user_id in task_name for user_id in user_ids\n                    ):\n                        logger.info(f\"Deleting old system task: {task_name}\")\n                        try:\n                            # Use check=False to prevent exceptions on missing tasks\n                            del_result = subprocess.run(\n                                [\"schtasks\", \"/delete\", \"/tn\", task_name, \"/f\"],\n                                stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE,\n                                text=True,\n                                check=False,\n                            )\n                            if del_result.returncode == 0:\n                                tasks_deleted += 1\n                                logger.debug(f\"Successfully deleted task: {task_name}\")\n                            else:\n                                # Task probably doesn't exist anymore - this is fine\n                                logger.debug(\n                                    f\"Task {task_name} may already be deleted: {del_result.stderr}\"\n                                )\n                        except Exception as del_error:\n                            logger.debug(\n                                f\"Error deleting task {task_name}: {del_error}\"\n                            )\n\n            logger.info(\n                f\"System task cleanup completed: {tasks_deleted} tasks deleted.\"\n            )\n            if tasks_deleted > 0:\n                logger.debug(f\"Deleted {tasks_deleted} old system tasks\")\n\n        except Exception as query_error:\n            logger.debug(f\"Error querying system tasks for cleanup: {query_error}\")\n            logger.info(f\"System task cleanup skipped (query failed).\")\n\n    @handle_errors(\"scheduling all task reminders\")\n    def schedule_all_task_reminders(self, user_id):\n        \"\"\"\n        Schedule reminders for all active tasks for a user.\n        For each reminder period, pick one random task and schedule it at a random time within the period.\n        \"\"\"\n        try:\n            from tasks.task_management import load_active_tasks, are_tasks_enabled\n            from core.schedule_management import get_schedule_time_periods\n            import random\n\n            # Check if tasks are enabled for this user\n            if not are_tasks_enabled(user_id):\n                logger.debug(f\"Tasks not enabled for user {user_id}\")\n                return\n\n            # Get user's configured task reminder periods from the periods structure\n            task_periods = get_schedule_time_periods(user_id, \"tasks\")\n            if not task_periods:\n                logger.debug(f\"No task reminder periods configured for user {user_id}\")\n                return\n\n            # Load active tasks\n            active_tasks = load_active_tasks(user_id)\n\n            # Filter to only incomplete tasks\n            incomplete_tasks = [\n                task for task in active_tasks if not task.get(\"completed\", False)\n            ]\n\n            if not incomplete_tasks:\n                logger.debug(f\"No incomplete tasks found for user {user_id}\")\n                return\n\n            scheduled_count = 0\n\n            # For each active task reminder period, pick one random task and schedule it\n            for period_name, period_data in task_periods.items():\n                if not period_data.get(\"active\", True):\n                    continue\n\n                # Pick one random task for this period\n                selected_task = self.select_task_for_reminder(incomplete_tasks)\n\n                if not selected_task:\n                    logger.debug(\n                        f\"No task selected for period {period_name} for user {user_id}\"\n                    )\n                    continue\n\n                # Use canonical keys with fallback to legacy keys for task reminder periods\n                start_time = period_data.get(\"start_time\") or period_data.get(\"start\")\n                end_time = period_data.get(\"end_time\") or period_data.get(\"end\")\n\n                if not start_time or not end_time:\n                    logger.warning(\n                        f\"Missing start/end time for task reminder period '{period_name}' in user {user_id}\"\n                    )\n                    continue\n\n                # Generate a random time within the period\n                random_time = self.get_random_time_within_task_period(\n                    start_time, end_time\n                )\n                if not random_time:\n                    logger.warning(\n                        f\"Could not generate random time for period {start_time}-{end_time}\"\n                    )\n                    continue\n\n                # Schedule the task reminder\n                if self.schedule_task_reminder_at_time(\n                    user_id, selected_task[\"task_id\"], random_time\n                ):\n                    scheduled_count += 1\n                    logger.info(\n                        f\"Scheduled reminder for task '{selected_task['title']}' at {random_time} (period: {period_name} {start_time}-{end_time})\"\n                    )\n\n            logger.info(\n                f\"Scheduled {scheduled_count} task reminders for user {user_id}\"\n            )\n\n        except Exception as e:\n            logger.error(f\"Error scheduling task reminders for user {user_id}: {e}\")\n\n    @handle_errors(\"selecting task for reminder with priority and due date weighting\")\n    def _select_task_for_reminder__handle_edge_cases(self, incomplete_tasks):\n        \"\"\"Handle edge cases for task selection.\"\"\"\n        if not incomplete_tasks:\n            return None\n\n        # If only one task, return it\n        if len(incomplete_tasks) == 1:\n            return incomplete_tasks[0]\n\n        return \"PROCEED\"  # Signal to continue with normal processing\n\n    @handle_errors(\"calculating priority weight for task reminder\")\n    def _select_task_for_reminder__calculate_priority_weight(self, task):\n        \"\"\"Calculate priority-based weight for a task.\"\"\"\n        priority = task.get(\"priority\", \"medium\").lower()\n        priority_multipliers = {\n            \"critical\": 3.0,  # Critical priority tasks 3x more likely\n            \"high\": 2.0,  # High priority tasks 2x more likely\n            \"medium\": 1.5,  # Medium priority tasks 1.5x more likely\n            \"low\": 1.0,  # Low priority tasks base weight\n            \"none\": 0.8,  # No priority tasks slightly less likely\n        }\n        return priority_multipliers.get(priority, 1.0)\n\n    def _select_task_for_reminder__calculate_due_date_weight(self, task, today):\n        \"\"\"Calculate due date proximity weight for a task.\"\"\"\n        due_date_str = task.get(\"due_date\")\n        if not due_date_str:\n            # No due date: slight reduction to encourage setting due dates\n            return 0.9\n\n        try:\n            from datetime import datetime\n\n            due_date = datetime.strptime(due_date_str, DATE_ONLY).date()\n            days_until_due = (due_date - today).days\n\n            # Sliding scale calculation\n            if days_until_due < 0:\n                # Overdue tasks: exponential increase based on how overdue\n                # Formula: 2.5 + (days_overdue * 0.1), max 4.0\n                days_overdue = abs(days_until_due)\n                return min(2.5 + (days_overdue * 0.1), 4.0)\n            elif days_until_due == 0:\n                # Due today: maximum weight\n                return 2.5\n            elif days_until_due <= 7:\n                # Due within a week: sliding scale from 2.5 to 1.0\n                # Formula: 2.5 - (days_until_due * 0.2)\n                return max(2.5 - (days_until_due * 0.2), 1.0)\n            elif days_until_due <= 30:\n                # Due within a month: sliding scale from 1.0 to 0.8\n                # Formula: 1.0 - (days_until_due - 7) * 0.01\n                return max(1.0 - (days_until_due - 7) * 0.01, 0.8)\n            else:\n                # Due later than a month: base weight\n                return 0.8\n        except ValueError:\n            # Invalid date format, use base weight\n            return 1.0\n\n    @handle_errors(\"calculating task weights for reminder selection\")\n    def _select_task_for_reminder__calculate_task_weights(\n        self, incomplete_tasks, today\n    ):\n        \"\"\"Calculate weights for all tasks.\"\"\"\n        task_weights = []\n\n        for task in incomplete_tasks:\n            weight = 1.0  # Base weight\n\n            # Apply priority weighting\n            priority_multiplier = (\n                self._select_task_for_reminder__calculate_priority_weight(task)\n            )\n            weight *= priority_multiplier\n\n            # Apply due date proximity weighting\n            due_date_multiplier = (\n                self._select_task_for_reminder__calculate_due_date_weight(task, today)\n            )\n            weight *= due_date_multiplier\n\n            task_weights.append((task, weight))\n\n        return task_weights\n\n    @handle_errors(\"building task key for reminder selection\", default_return=\"\")\n    def _select_task_for_reminder__task_key(\n        self, task: dict[str, Any], index: int\n    ) -> str:\n        \"\"\"Build a stable key for tracking reminder selection state.\"\"\"\n        candidate_keys = [\n            str(task.get(\"id\") or \"\").strip(),\n            str(task.get(\"task_id\") or \"\").strip(),\n            str(task.get(\"uuid\") or \"\").strip(),\n            str(task.get(\"title\") or \"\").strip(),\n        ]\n        # Fall back to index when no identifier data is present\n        base_key = next((key for key in candidate_keys if key), f\"idx-{index}\")\n        return f\"{base_key}|{index}\"\n\n    @handle_errors(\"selecting task by weight for reminder\", default_return=None)\n    def _select_task_for_reminder__select_task_by_weight(\n        self, task_weights, incomplete_tasks\n    ):\n        \"\"\"Select a task based on calculated weights using weighted random selection.\"\"\"\n        import random\n\n        if not task_weights:\n            return random.choice(incomplete_tasks) if incomplete_tasks else None\n\n        total_weight = sum(weight for _, weight in task_weights)\n        if total_weight <= 0:\n            # Fallback to random selection if all weights are zero or invalid\n            return random.choice(incomplete_tasks)\n\n        # Use weighted random selection for proper semi-random behavior\n        # This ensures higher-weighted tasks are more likely but not always selected\n        rand_value = random.uniform(0, total_weight)\n        cumulative_weight = 0.0\n\n        # Clean up stale state first\n        active_keys = {\n            self._select_task_for_reminder__task_key(t, i)\n            for i, (t, _) in enumerate(task_weights)\n        }\n        for key in list(self._reminder_selection_state.keys()):\n            if key not in active_keys:\n                self._reminder_selection_state.pop(key, None)\n\n        for index, (task, weight) in enumerate(task_weights):\n            cumulative_weight += weight\n            if rand_value <= cumulative_weight:\n                # Update accumulated state for smooth distribution over time\n                task_key = self._select_task_for_reminder__task_key(task, index)\n                previous_score = self._reminder_selection_state.get(task_key, 0.0)\n                self._reminder_selection_state[task_key] = previous_score + weight\n\n                return task\n\n        # Fallback to last task if something went wrong\n        return task_weights[-1][0] if task_weights else random.choice(incomplete_tasks)\n\n    @handle_errors(\"selecting task for reminder\", default_return=None)\n    def select_task_for_reminder(\n        self, incomplete_tasks: list[dict[str, Any]]\n    ) -> dict[str, Any]:\n        \"\"\"\n        Select a task for reminder using priority-based and due date proximity weighting.\n\n        Args:\n            incomplete_tasks: List of incomplete tasks to choose from\n\n        Returns:\n            Selected task dictionary\n        \"\"\"\n        from datetime import datetime\n\n        # Handle edge cases\n        edge_case_result = self._select_task_for_reminder__handle_edge_cases(\n            incomplete_tasks\n        )\n        if edge_case_result != \"PROCEED\":\n            return edge_case_result\n\n        # Calculate weights for each task\n        today = datetime.now().date()\n        task_weights = self._select_task_for_reminder__calculate_task_weights(\n            incomplete_tasks, today\n        )\n\n        # Select task based on weights\n        selected_task = self._select_task_for_reminder__select_task_by_weight(\n            task_weights, incomplete_tasks\n        )\n\n        logger.debug(\n            f\"Selected task '{selected_task.get('title', 'Unknown')}' with priority '{selected_task.get('priority', 'medium')}' and due date '{selected_task.get('due_date', 'None')}'\"\n        )\n\n        return selected_task\n\n    @handle_errors(\"getting random time within task period\")\n    def get_random_time_within_task_period(self, start_time, end_time):\n        \"\"\"\n        Generate a random time within a task reminder period.\n        Args:\n            start_time: Start time in HH:MM format (e.g., \"17:00\")\n            end_time: End time in HH:MM format (e.g., \"18:00\")\n        Returns:\n            Random time in HH:MM format\n        \"\"\"\n        try:\n            from datetime import datetime, timedelta\n            import random\n\n            # Parse start and end times\n            start_dt = datetime.strptime(start_time, TIME_ONLY_MINUTE)\n            end_dt = datetime.strptime(end_time, TIME_ONLY_MINUTE)\n\n            # If end time is before start time, it means the period spans midnight\n            # For now, we'll assume it's the same day\n            if end_dt < start_dt:\n                end_dt += timedelta(days=1)\n\n            # Calculate total seconds in the period\n            total_seconds = (end_dt - start_dt).total_seconds()\n\n            if total_seconds <= 0:\n                logger.error(f\"Invalid time range: {start_time} to {end_time}\")\n                return None\n\n            # Generate random seconds within the period\n            random_seconds = random.randint(0, int(total_seconds))\n            random_dt = start_dt + timedelta(seconds=random_seconds)\n\n            # Format as HH:MM\n            random_time = random_dt.strftime(TIME_ONLY_MINUTE)\n\n            logger.debug(\n                f\"Generated random time {random_time} within period {start_time}-{end_time}\"\n            )\n            return random_time\n\n        except Exception as e:\n            logger.error(\n                f\"Error generating random time within period {start_time}-{end_time}: {e}\"\n            )\n            return None\n\n    @handle_errors(\"scheduling task reminder at specific time\")\n    def schedule_task_reminder_at_time(self, user_id, task_id, reminder_time):\n        \"\"\"\n        Schedule a reminder for a specific task at the specified time (daily).\n        \"\"\"\n        try:\n            from tasks.task_management import get_task_by_id\n\n            # Get the task to verify it exists and is active\n            task = get_task_by_id(user_id, task_id)\n            if not task:\n                logger.error(f\"Task {task_id} not found for user {user_id}\")\n                return False\n\n            if task.get(\"completed\", False):\n                logger.info(\n                    f\"Task {task_id} is already completed, skipping reminder scheduling\"\n                )\n                return False\n\n            # Parse the reminder time\n            try:\n                hour, minute = map(int, reminder_time.split(\":\"))\n                time_str = f\"{hour:02d}:{minute:02d}\"\n            except ValueError:\n                logger.error(f\"Invalid reminder time format: {reminder_time}\")\n                return False\n\n            # Schedule the task reminder\n            schedule.every().day.at(time_str).do(\n                self.handle_task_reminder, user_id=user_id, task_id=task_id\n            )\n\n            # Set wake timer for the task reminder\n            from datetime import datetime, timedelta\n            import pytz\n\n            # Create datetime for today at the specified time\n            tz = pytz.timezone(\"America/Regina\")\n            now = datetime.now(tz)\n            today = now.date()\n\n            # Parse the reminder time\n            hour, minute = map(int, time_str.split(\":\"))\n            schedule_datetime = datetime.combine(\n                today, datetime.min.time().replace(hour=hour, minute=minute), tzinfo=tz\n            )\n\n            # If the time has already passed today, schedule for tomorrow\n            if schedule_datetime <= now:\n                schedule_datetime += timedelta(days=1)\n\n            # Set wake timer for task reminder\n            self.set_wake_timer(schedule_datetime, user_id, \"tasks\", \"task_reminder\")\n\n            logger.info(\n                f\"Scheduled daily task reminder for user {user_id}, task {task_id} at {time_str}\"\n            )\n            return True\n\n        except Exception as e:\n            logger.error(\n                f\"Error scheduling task reminder for user {user_id}, task {task_id}: {e}\"\n            )\n            return False\n\n    @handle_errors(\"scheduling task reminder\")\n    def schedule_task_reminder(self, user_id, task_id, reminder_time):\n        \"\"\"\n        Legacy function for backward compatibility.\n        Schedule a reminder for a specific task at the specified time.\n        \"\"\"\n        return self.schedule_task_reminder_at_time(user_id, task_id, reminder_time)\n\n    @handle_errors(\"scheduling task reminder at specific datetime\")\n    def schedule_task_reminder_at_datetime(self, user_id, task_id, date_str, time_str):\n        \"\"\"\n        Schedule a reminder for a specific task at a specific date and time.\n        \"\"\"\n        try:\n            from tasks.task_management import get_task_by_id\n            from datetime import datetime, timedelta\n\n            # Get the task to verify it exists and is active\n            task = get_task_by_id(user_id, task_id)\n            if not task:\n                logger.error(f\"Task {task_id} not found for user {user_id}\")\n                return False\n\n            if task.get(\"completed\", False):\n                logger.info(\n                    f\"Task {task_id} is already completed, skipping reminder scheduling\"\n                )\n                return False\n\n            # Parse the date and time\n            try:\n                reminder_datetime = datetime.strptime(\n                    f\"{date_str} {time_str}\", TIMESTAMP_MINUTE\n                )\n            except ValueError:\n                logger.error(f\"Invalid date/time format: {date_str} {time_str}\")\n                return False\n\n            # Check if the reminder time is in the past\n            if reminder_datetime < datetime.now():\n                logger.debug(\n                    f\"Reminder time {reminder_datetime} is in the past, skipping\"\n                )\n                return False\n\n            # Calculate delay until the reminder time\n            delay_seconds = (reminder_datetime - datetime.now()).total_seconds()\n\n            # Schedule the task reminder\n            delay_seconds_int = max(1, int(delay_seconds))\n            schedule.every(delay_seconds_int).seconds.do(\n                self.handle_task_reminder, user_id=user_id, task_id=task_id\n            )\n\n            logger.info(\n                f\"Scheduled one-time task reminder for user {user_id}, task {task_id} at {reminder_datetime}\"\n            )\n            return True\n\n        except Exception as e:\n            logger.error(\n                f\"Error scheduling task reminder for user {user_id}, task {task_id}: {e}\"\n            )\n            return False\n\n    @handle_errors(\"cleaning up task reminders\")\n    def cleanup_task_reminders(self, user_id, task_id):\n        \"\"\"\n        Clean up all reminders for a specific task.\n\n        Finds and removes all APScheduler jobs that call handle_task_reminder for the given task_id.\n        Handles both one-time reminders (schedule_task_reminder_at_datetime) and daily reminders (schedule_task_reminder_at_time).\n\n        Args:\n            user_id: The user's ID\n            task_id: The task ID to clean up reminders for\n\n        Returns:\n            bool: True if cleanup succeeded (or no reminders found), False on error\n        \"\"\"\n        try:\n            if not user_id or not task_id:\n                logger.error(\n                    f\"Invalid parameters for cleanup_task_reminders: user_id={user_id}, task_id={task_id}\"\n                )\n                return False\n\n            # Count jobs before cleanup\n            initial_job_count = len(schedule.jobs)\n            jobs_to_remove = []\n\n            # Find all jobs that call handle_task_reminder with this user_id and task_id\n            for job in schedule.jobs:\n                try:\n                    # Check if this is a task reminder job\n                    # Jobs created by schedule_task_reminder_at_datetime use: schedule.every(delay).seconds.do(handle_task_reminder, user_id=..., task_id=...)\n                    # Jobs created by schedule_task_reminder_at_time use: schedule.every().day.at(time).do(handle_task_reminder, user_id=..., task_id=...)\n\n                    # Check if job function is handle_task_reminder\n                    job_func = job.job_func\n                    if not job_func:\n                        continue\n                    if (\n                        hasattr(job_func, \"func\")\n                        and job_func.func == self.handle_task_reminder\n                    ):\n                        # Check keyword arguments for user_id and task_id\n                        if hasattr(job_func, \"keywords\"):\n                            kwargs = job_func.keywords\n                            if (\n                                kwargs.get(\"user_id\") == user_id\n                                and kwargs.get(\"task_id\") == task_id\n                            ):\n                                jobs_to_remove.append(job)\n                                logger.debug(\n                                    f\"Found reminder job for task {task_id}, user {user_id}\"\n                                )\n\n                    # Also check positional arguments (some job types may use args instead of keywords)\n                    else:\n                        args = getattr(job_func, \"args\", None)\n                        if args and len(args) >= 2:\n                            # handle_task_reminder signature: (self, user_id, task_id, ...)\n                            if (\n                                len(args) >= 2\n                                and args[0] == user_id\n                                and args[1] == task_id\n                            ):\n                                # Verify it's actually handle_task_reminder\n                                if (\n                                    hasattr(job_func, \"func\")\n                                    and job_func.func == self.handle_task_reminder\n                                ):\n                                    jobs_to_remove.append(job)\n                                    logger.debug(\n                                        f\"Found reminder job for task {task_id}, user {user_id} (positional args)\"\n                                    )\n\n                except Exception as e:\n                    # Skip jobs that can't be inspected\n                    logger.debug(f\"Could not inspect job for cleanup: {e}\")\n                    continue\n\n            # Remove the identified jobs\n            jobs_removed = 0\n            for job in jobs_to_remove:\n                try:\n                    schedule.jobs.remove(job)\n                    jobs_removed += 1\n                    logger.debug(\n                        f\"Removed reminder job for task {task_id}, user {user_id}\"\n                    )\n                except ValueError:\n                    # Job was already removed\n                    pass\n\n            final_job_count = len(schedule.jobs)\n            logger.info(\n                f\"Cleaned up {jobs_removed} reminder job(s) for task {task_id}, user {user_id} \"\n                f\"(from {initial_job_count} to {final_job_count} total jobs)\"\n            )\n\n            return True\n\n        except Exception as e:\n            logger.error(\n                f\"Error cleaning up task reminders for task {task_id}, user {user_id}: {e}\"\n            )\n            return False\n\n    @handle_errors(\"cleaning up orphaned task reminders\")\n    def cleanup_orphaned_task_reminders(self):\n        \"\"\"\n        Periodic cleanup job to remove reminders for tasks that no longer exist.\n\n        Scans all scheduled reminder jobs and verifies the associated tasks still exist.\n        Removes reminders for tasks that have been deleted or completed.\n        Runs daily at 03:00.\n        \"\"\"\n        try:\n            logger.info(\"Starting periodic cleanup of orphaned task reminders\")\n\n            # Get all active users (we'll need to scan their tasks)\n            from core.user_data_handlers import get_all_user_ids\n\n            user_ids = get_all_user_ids()\n\n            orphaned_count = 0\n            total_checked = 0\n\n            # Find all task reminder jobs\n            jobs_to_check = []\n            for job in schedule.jobs:\n                try:\n                    # Check if this is a task reminder job\n                    job_func = job.job_func\n                    if not job_func:\n                        continue\n                    if (\n                        hasattr(job_func, \"func\")\n                        and job_func.func == self.handle_task_reminder\n                    ):\n                        if hasattr(job_func, \"keywords\"):\n                            kwargs = job_func.keywords\n                            user_id = kwargs.get(\"user_id\")\n                            task_id = kwargs.get(\"task_id\")\n                            if user_id and task_id:\n                                jobs_to_check.append((job, user_id, task_id))\n                                total_checked += 1\n                except Exception as e:\n                    logger.debug(f\"Could not inspect job for orphaned cleanup: {e}\")\n                    continue\n\n            # Check each job's task still exists\n            from tasks.task_management import get_task_by_id\n\n            for job, user_id, task_id in jobs_to_check:\n                try:\n                    task = get_task_by_id(user_id, task_id)\n                    if not task:\n                        # Task doesn't exist - remove the reminder job\n                        try:\n                            schedule.jobs.remove(job)\n                            orphaned_count += 1\n                            logger.info(\n                                f\"Removed orphaned reminder for non-existent task {task_id}, user {user_id}\"\n                            )\n                        except ValueError:\n                            # Job was already removed\n                            pass\n                    elif task.get(\"completed\", False):\n                        # Task is completed - remove the reminder job\n                        try:\n                            schedule.jobs.remove(job)\n                            orphaned_count += 1\n                            logger.info(\n                                f\"Removed reminder for completed task {task_id}, user {user_id}\"\n                            )\n                        except ValueError:\n                            # Job was already removed\n                            pass\n                except Exception as e:\n                    logger.debug(\n                        f\"Error checking task {task_id} for user {user_id}: {e}\"\n                    )\n                    continue\n\n            logger.info(\n                f\"Orphaned reminder cleanup complete: checked {total_checked} reminders, removed {orphaned_count} orphaned reminders\"\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Error during orphaned task reminder cleanup: {e}\", exc_info=True\n            )\n\n    @handle_errors(\"performing daily log archival\")\n    def perform_daily_log_archival(self):\n        \"\"\"\n        Perform daily log archival to compress old logs and clean up archives.\n        This runs automatically at 02:00 daily via the scheduler.\n        \"\"\"\n        try:\n            from core.logger import compress_old_logs, cleanup_old_archives\n\n            logger.info(\"Starting daily log archival process\")\n\n            # Compress logs older than 7 days\n            compressed_count = compress_old_logs()\n            logger.info(\n                f\"Daily log archival: compressed {compressed_count} old log files\"\n            )\n\n            # Clean up archives older than 30 days\n            cleaned_count = cleanup_old_archives()\n            logger.info(\n                f\"Daily log archival: cleaned {cleaned_count} old archive files\"\n            )\n\n            logger.info(\"Daily log archival process completed successfully\")\n\n        except Exception as e:\n            logger.error(f\"Error during daily log archival: {e}\")\n\n    @handle_errors(\"checking and performing weekly backup\")\n    def check_and_perform_weekly_backup(self):\n        \"\"\"\n        Check if a weekly backup is needed and perform it if so.\n        Runs during the daily scheduler job at 01:00 (before log archival at 02:00).\n        Creates a backup if:\n        - No backups exist, OR\n        - Last backup is 7+ days old\n        Keeps last 10 backups with 30-day retention as configured in BackupManager.\n        \"\"\"\n        try:\n            # Get list of existing backups\n            backups = backup_manager.list_backups()\n\n            # Check if backup is needed\n            needs_backup = False\n\n            if not backups:\n                logger.info(\"No existing backups found - creating first backup\")\n                needs_backup = True\n            else:\n                # Get the most recent backup (list is sorted newest first)\n                last_backup = backups[0]\n                last_backup_time = datetime.fromisoformat(last_backup[\"created_at\"])\n                days_since_backup = (datetime.now() - last_backup_time).days\n\n                if days_since_backup >= 7:\n                    logger.info(\n                        f\"Last backup was {days_since_backup} days ago - creating new backup\"\n                    )\n                    needs_backup = True\n                else:\n                    logger.debug(\n                        f\"Backup not needed - last backup was {days_since_backup} days ago\"\n                    )\n\n            # Create backup if needed\n            if needs_backup:\n                logger.info(\"Starting weekly backup process\")\n\n                backup_path = backup_manager.create_backup(\n                    backup_name=f\"weekly_backup_{now_timestamp_filename()}\",\n                    include_users=True,\n                    include_config=True,\n                    include_logs=False,\n                )\n\n                if backup_path:\n                    logger.info(f\"Weekly backup completed successfully: {backup_path}\")\n\n                    # Check backup health\n                    backups = backup_manager.list_backups()\n                    if backups:\n                        latest_backup = backups[0]\n                        backup_time = datetime.fromisoformat(\n                            latest_backup[\"created_at\"]\n                        )\n                        days_old = (datetime.now() - backup_time).days\n                        backup_size_mb = latest_backup.get(\"file_size\", 0) / (\n                            1024 * 1024\n                        )\n                        logger.info(\n                            f\"Backup health: Latest backup is {days_old} days old, size: {backup_size_mb:.2f} MB\"\n                        )\n                    else:\n                        logger.warning(\n                            \"No backups found after creation - backup health check failed\"\n                        )\n                else:\n                    logger.error(\"Weekly backup failed - no backup path returned\")\n\n        except Exception as e:\n            logger.error(f\"Error during weekly backup check: {e}\")\n\n    # Task reminders are now managed consistently with other jobs\n    # No special cleanup function needed - they're handled by the main scheduler cleanup\n\n\n# Standalone functions for admin UI access\n@handle_errors(\"running full scheduler standalone\", default_return=False)\ndef run_full_scheduler_standalone():\n    \"\"\"\n    Standalone function to run the full scheduler for all users.\n    This can be called from the admin UI without needing a scheduler instance.\n    \"\"\"\n    from communication.core.channel_orchestrator import CommunicationManager\n    from core.scheduler import SchedulerManager\n\n    # Create communication manager and scheduler manager\n    communication_manager = CommunicationManager()\n    scheduler_manager = SchedulerManager(communication_manager)\n\n    # Run the full scheduler\n    logger.info(\"Standalone: Running full scheduler for all users\")\n    scheduler_manager.run_daily_scheduler()\n\n    logger.info(\"Standalone: Full scheduler started successfully\")\n    return True\n\n\n@handle_errors(\"running user scheduler standalone\", default_return=False)\ndef run_user_scheduler_standalone(user_id):\n    \"\"\"\n    Standalone function to run scheduler for a specific user.\n    This can be called from the admin UI without needing a scheduler instance.\n    \"\"\"\n    from communication.core.channel_orchestrator import CommunicationManager\n    from core.scheduler import SchedulerManager\n\n    # Create communication manager and scheduler manager\n    communication_manager = CommunicationManager()\n    scheduler_manager = SchedulerManager(communication_manager)\n\n    # Run scheduler for the specific user\n    logger.info(f\"Standalone: Running scheduler for user {user_id}\")\n    scheduler_manager.schedule_new_user(user_id)\n\n    logger.info(f\"Standalone: User scheduler started successfully for {user_id}\")\n    return True\n\n\n@handle_errors(\"running category scheduler standalone\", default_return=False)\ndef run_category_scheduler_standalone(user_id, category):\n    \"\"\"\n    Standalone function to run scheduler for a specific user and category.\n    This can be called from the admin UI without needing a scheduler instance.\n    \"\"\"\n    from communication.core.channel_orchestrator import CommunicationManager\n    from core.scheduler import SchedulerManager\n\n    # Create communication manager and scheduler manager\n    communication_manager = CommunicationManager()\n    scheduler_manager = SchedulerManager(communication_manager)\n\n    # Run scheduler for the specific user and category\n    logger.info(\n        f\"Standalone: Running category scheduler for user {user_id}, category {category}\"\n    )\n    scheduler_manager.schedule_daily_message_job(user_id, category)\n\n    logger.info(\n        f\"Standalone: Category scheduler started successfully for {user_id}, {category}\"\n    )\n    return True\n\n\n@handle_errors(\"scheduling all task reminders for user\", default_return=None)\ndef schedule_all_task_reminders(user_id):\n    \"\"\"\n    Standalone function to schedule all task reminders for a user.\n    This can be called from the admin UI without needing a scheduler instance.\n    \"\"\"\n    from tasks.task_management import are_tasks_enabled\n\n    # Check if tasks are enabled for this user\n    if not are_tasks_enabled(user_id):\n        logger.debug(f\"Tasks not enabled for user {user_id}\")\n        return\n\n    # For now, just log that scheduling was requested\n    # The actual scheduling will happen when the main scheduler starts\n    logger.info(f\"Task reminder scheduling requested for user {user_id}\")\n    logger.info(\"Task reminders will be scheduled when the main scheduler starts\")\n\n\n# Task reminders are now managed consistently with other jobs\n# No special cleanup function needed - they're handled by the main scheduler cleanup\n\n\n@handle_errors(\"clearing all accumulated jobs standalone\", default_return=False)\ndef clear_all_accumulated_jobs_standalone():\n    \"\"\"\n    Standalone function to clear all accumulated scheduler jobs.\n    This can be called from the admin UI or service to fix job accumulation issues.\n    \"\"\"\n    from communication.core.channel_orchestrator import CommunicationManager\n\n    # Create communication manager and scheduler manager\n    communication_manager = CommunicationManager()\n    scheduler_manager = SchedulerManager(communication_manager)\n\n    # Clear all accumulated jobs\n    scheduler_manager.clear_all_accumulated_jobs()\n\n    logger.info(\"Standalone: All accumulated scheduler jobs cleared successfully\")\n    return True\n\n\n@handle_errors(\"processing user schedules\", default_return=None)\ndef process_user_schedules(user_id: str):\n    \"\"\"Process schedules for a specific user.\"\"\"\n    # Get user's categories\n    prefs_result = get_user_data(user_id, \"preferences\")\n    categories = prefs_result.get(\"preferences\", {}).get(\"categories\", [])\n    if not categories:\n        logger.debug(f\"No categories found for user {user_id}\")\n        return\n\n    # Process each category\n    for category in categories:\n        process_category_schedule(user_id, category)\n\n\n@handle_errors(\"processing category schedule\", default_return=None)\ndef process_category_schedule(user_id: str, category: str):\n    \"\"\"Process schedule for a specific user and category.\"\"\"\n    # Create a scheduler instance to process this category\n    from communication.core.channel_orchestrator import CommunicationManager\n\n    communication_manager = CommunicationManager()\n    scheduler_manager = SchedulerManager(communication_manager)\n\n    # Schedule messages for this category\n    scheduler_manager.schedule_daily_message_job(user_id, category)\n\n    logger.info(f\"Processed schedule for user {user_id}, category {category}\")\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1644,
                    "line_content": "Legacy function for backward compatibility.",
                    "start": 69305,
                    "end": 69327
                  }
                ]
              ],
              [
                "core\\schemas.py",
                "\"\"\"\nPydantic models and helpers for user data: account, preferences, schedules, and messages.\n\nDesign goals:\n- Be tolerant of existing on-disk shapes (no breaking changes).\n- Normalize when safe; log validation issues but do not raise by default.\n- Keep string-based timestamps and enumerations used by current JSON files.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any, Dict, List, Literal, Optional\nfrom pydantic import (\n    BaseModel,\n    Field,\n    ConfigDict,\n    field_validator,\n    model_validator,\n    RootModel,\n)\nimport re\n\ntry:\n    import pytz  # Best-effort timezone validation\nexcept Exception:\n    pytz = None\n\n# Add logging support for schema validation\nfrom core.logger import get_component_logger\nfrom core.error_handling import handle_errors, ValidationError\n\nlogger = get_component_logger(\"main\")\n\n\n# ----------------------------- Common Validators -----------------------------\n\n_TIME_PATTERN = re.compile(r\"^([01]?[0-9]|2[0-3]):[0-5][0-9]$\")\n_VALID_DAYS = {\n    \"ALL\",\n    \"Monday\",\n    \"Tuesday\",\n    \"Wednesday\",\n    \"Thursday\",\n    \"Friday\",\n    \"Saturday\",\n    \"Sunday\",\n}\n\n\n# --------------------------------- Account ----------------------------------\n\n\nclass FeaturesModel(BaseModel):\n    model_config = ConfigDict(extra=\"ignore\")\n\n    automated_messages: Literal[\"enabled\", \"disabled\"] = \"disabled\"\n    checkins: Literal[\"enabled\", \"disabled\"] = \"disabled\"\n    task_management: Literal[\"enabled\", \"disabled\"] = \"disabled\"\n\n    @classmethod\n    @handle_errors(\"coercing boolean value\", default_return=\"disabled\")\n    def _coerce_bool(cls, v: Any) -> Literal[\"enabled\", \"disabled\"]:\n        try:\n            if isinstance(v, bool):\n                return \"enabled\" if v else \"disabled\"\n            if isinstance(v, str):\n                vv = v.strip().lower()\n                if vv in (\"enabled\", \"enable\", \"true\", \"yes\", \"1\"):\n                    return \"enabled\"\n                if vv in (\"disabled\", \"disable\", \"false\", \"no\", \"0\"):\n                    return \"disabled\"\n            return \"disabled\"\n        except Exception as e:\n            logger.error(f\"Error coercing boolean value '{v}': {e}\")\n            return \"disabled\"\n\n    @field_validator(\"automated_messages\", \"checkins\", \"task_management\", mode=\"before\")\n    @classmethod\n    def _normalize_flags(cls, v: Any) -> Literal[\"enabled\", \"disabled\"]:\n        \"\"\"\n        Normalize feature flag values to \"enabled\" or \"disabled\".\n\n        Converts various input formats (boolean, string variants) to the standard\n        \"enabled\"/\"disabled\" literal values using the _coerce_bool helper.\n\n        Args:\n            v: Input value (bool, str, or other) to normalize\n\n        Returns:\n            Literal[\"enabled\", \"disabled\"]: Normalized flag value\n        \"\"\"\n        # NOTE: Pydantic validators should not have try-except blocks.\n        # Pydantic handles exceptions internally and will raise ValidationError if needed.\n        # Adding try-except here would interfere with Pydantic's validation flow.\n        # Error handling is provided by _coerce_bool's @handle_errors decorator.\n        return cls._coerce_bool(v)\n\n\nclass AccountModel(BaseModel):\n    # Allow unknown fields (e.g., legacy/pass-through like 'channel', 'enabled_features')\n    model_config = ConfigDict(extra=\"allow\")\n\n    user_id: str\n    internal_username: str = \"\"\n    account_status: Literal[\"active\", \"inactive\", \"suspended\"] = \"active\"\n\n    chat_id: str = \"\"\n    phone: str = \"\"\n    email: str = \"\"\n    discord_user_id: str = \"\"\n    discord_username: str = \"\"\n    timezone: str = \"\"\n\n    created_at: str = \"\"\n    updated_at: str = \"\"\n\n    features: FeaturesModel = Field(default_factory=FeaturesModel)\n\n    @field_validator(\"email\")\n    @classmethod\n    def _validate_email(cls, v: str) -> str:\n        # NOTE: Pydantic validators should not have try-except blocks.\n        # Pydantic handles exceptions internally and will raise ValidationError if needed.\n        # This validator performs simple regex matching which cannot raise exceptions.\n        if not v:\n            return v\n        pattern = re.compile(r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n        if pattern.match(v):\n            logger.debug(f\"Email validation passed: {v}\")\n            return v\n        else:\n            logger.warning(\n                f\"Invalid email format provided: '{v}' - normalized to empty string\"\n            )\n            return \"\"\n\n    @field_validator(\"discord_user_id\")\n    @classmethod\n    def _validate_discord_id(cls, v: str) -> str:\n        \"\"\"\n        Validate and normalize Discord user ID.\n\n        Discord user IDs are snowflakes (numeric IDs) that are 17-19 digits long.\n        Empty strings are allowed (Discord ID is optional).\n        \"\"\"\n        try:\n            if not v:\n                return \"\"\n            if isinstance(v, str):\n                normalized = v.strip()\n                if normalized != v:\n                    logger.debug(\n                        f\"Discord ID normalized (whitespace trimmed): '{v}' -> '{normalized}'\"\n                    )\n\n                # Validate format using centralized validation function\n                from core.user_data_validation import is_valid_discord_id\n\n                if normalized and not is_valid_discord_id(normalized):\n                    logger.warning(\n                        f\"Invalid Discord ID format: '{normalized}' - normalized to empty string\"\n                    )\n                    return \"\"\n\n                return normalized\n            else:\n                logger.warning(\n                    f\"Discord ID validation failed: expected string, got {type(v).__name__}: {v}\"\n                )\n                return \"\"\n        except Exception as e:\n            logger.warning(\n                f\"Error validating Discord ID '{v}': {e} - normalized to empty string\"\n            )\n            return \"\"\n\n    @field_validator(\"discord_username\")\n    @classmethod\n    def _normalize_discord_username(cls, v: str) -> str:\n        \"\"\"Normalize Discord username while tolerating empty or legacy values.\"\"\"\n        if not v or not isinstance(v, str):\n            return \"\"\n\n        normalized = v.strip()\n        if normalized != v:\n            logger.debug(\n                f\"Discord username normalized (trimmed): '{v}' -> '{normalized}'\"\n            )\n\n        # Discord usernames can contain discriminator formats (legacy) or @handles; accept as-is\n        # but bound the length to keep files tidy.\n        if len(normalized) > 100:\n            logger.warning(\"Discord username too long; truncating to 100 characters\")\n            return normalized[:100]\n\n        return normalized\n\n    @field_validator(\"timezone\")\n    @classmethod\n    def _validate_timezone(cls, v: str) -> str:\n        # Best-effort: keep empty if invalid; tolerate unknowns\n        if not v or not isinstance(v, str):\n            return \"\"\n        vv = v.strip()\n        if not vv:\n            return \"\"\n        try:\n            if pytz and vv in pytz.all_timezones:\n                logger.debug(f\"Timezone validation passed: {vv}\")\n                return vv\n        except Exception as e:\n            logger.warning(f\"Timezone validation error for '{vv}': {e}\")\n            pass\n        # Unknown timezone \u2192 return empty to avoid misleading data\n        logger.warning(\n            f\"Unknown timezone provided: '{vv}' - normalized to empty string\"\n        )\n        return \"\"\n\n\n# -------------------------------- Preferences --------------------------------\n\n\nclass ChannelModel(BaseModel):\n    model_config = ConfigDict(extra=\"ignore\")\n\n    type: Literal[\"email\", \"discord\"]\n    contact: str | None = None\n\n    @model_validator(mode=\"after\")\n    def _normalize_contact(self):\n        # If contact is missing, leave as None. If present, lightly normalize.\n        if self.contact is None:\n            return self\n        try:\n            if isinstance(self.contact, str):\n                self.contact = self.contact.strip()\n            if self.type == \"email\":\n                pattern = re.compile(\n                    r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n                )\n                # If invalid, keep as-is for backward compatibility (tests may set placeholders)\n                # Future: optionally warn rather than drop\n            elif self.type == \"discord\":\n                # Accept any non-empty string, including username#discriminator or IDs\n                if not (isinstance(self.contact, str) and self.contact):\n                    self.contact = None\n        except Exception:\n            # On any failure, keep original contact to avoid data loss\n            pass\n        return self\n\n\nclass PreferencesModel(BaseModel):\n    # Allow unknown top-level fields so tests expecting passthrough extras succeed\n    model_config = ConfigDict(extra=\"allow\")\n\n    categories: list[str] = Field(default_factory=list)\n    channel: ChannelModel = Field(\n        default_factory=lambda: ChannelModel(type=\"email\", contact=None)\n    )\n    checkin_settings: dict[str, Any] | None = None\n    task_settings: dict[str, Any] | None = None\n\n    @field_validator(\"categories\")\n    @classmethod\n    def _validate_categories(cls, v: list[str]) -> list[str]:\n        \"\"\"Validate that all categories are in the allowed list.\"\"\"\n        if not v:\n            return v\n\n        try:\n            from core.message_management import get_message_categories\n\n            allowed_categories = get_message_categories()\n            invalid_categories = [c for c in v if c not in allowed_categories]\n\n            if invalid_categories:\n                logger.error(\n                    f\"Invalid categories provided: {invalid_categories}. Allowed categories: {allowed_categories}\"\n                )\n                raise ValidationError(\n                    f\"Invalid categories: {invalid_categories}. Allowed categories: {allowed_categories}\",\n                    details={\n                        \"invalid_categories\": invalid_categories,\n                        \"allowed_categories\": allowed_categories,\n                    },\n                )\n\n            logger.debug(f\"Categories validation passed: {v}\")\n            return v\n        except ImportError:\n            # If message_management is not available, allow all categories\n            logger.warning(\n                \"Message management module not available - allowing all categories without validation\"\n            )\n            return v\n        except ValueError:\n            # Re-raise ValueError (invalid categories) - don't catch this\n            raise\n        except Exception as e:\n            # If there's any other error (like missing env vars), use default categories\n            logger.warning(f\"Category validation error: {e}, using default categories\")\n\n            # Default categories that should always be valid\n            default_categories = [\n                \"motivational\",\n                \"health\",\n                \"fun_facts\",\n                \"quotes_to_ponder\",\n                \"word_of_the_day\",\n            ]\n            invalid_categories = [c for c in v if c not in default_categories]\n\n            if invalid_categories:\n                raise ValidationError(\n                    f\"Invalid categories: {invalid_categories}. Allowed categories: {default_categories}\",\n                    details={\n                        \"invalid_categories\": invalid_categories,\n                        \"allowed_categories\": default_categories,\n                    },\n                )\n\n            return v\n\n\n# --------------------------------- Schedules ---------------------------------\n\n\nclass PeriodModel(BaseModel):\n    model_config = ConfigDict(extra=\"ignore\")\n\n    active: bool = True\n    days: list[str] = Field(default_factory=lambda: [\"ALL\"])\n    start_time: str = \"00:00\"\n    end_time: str = \"23:59\"\n\n    @field_validator(\"start_time\", \"end_time\")\n    @classmethod\n    def _valid_time(cls, v: str) -> str:\n        # NOTE: Pydantic validators should not have try-except blocks.\n        # Pydantic handles exceptions internally and will raise ValidationError if needed.\n        # This validator performs simple regex matching which cannot raise exceptions.\n        if not v:\n            return \"00:00\"\n        if _TIME_PATTERN.match(v):\n            logger.debug(f\"Time validation passed: {v}\")\n            return v\n        else:\n            logger.warning(\n                f\"Invalid time format provided: '{v}' - normalized to '00:00'\"\n            )\n            return \"00:00\"\n\n    @field_validator(\"days\")\n    @classmethod\n    def _valid_days(cls, v: list[str]) -> list[str]:\n        # NOTE: Pydantic validators should not have try-except blocks.\n        # Pydantic handles exceptions internally and will raise ValidationError if needed.\n        # This validator performs simple list filtering which cannot raise exceptions.\n        if not v:\n            return [\"ALL\"]\n        filtered = [d for d in v if d in _VALID_DAYS]\n        invalid_days = [d for d in v if d not in _VALID_DAYS]\n        if invalid_days:\n            logger.warning(\n                f\"Invalid days provided: {invalid_days} - filtered out. Valid days: {_VALID_DAYS}\"\n            )\n        if not filtered:\n            logger.warning(f\"No valid days provided, defaulting to ['ALL']\")\n            return [\"ALL\"]\n        logger.debug(f\"Days validation passed: {filtered}\")\n        return filtered\n\n\nclass CategoryScheduleModel(BaseModel):\n    model_config = ConfigDict(extra=\"ignore\")\n\n    periods: dict[str, PeriodModel]\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def _accept_legacy_shape(cls, data: Any):\n        \"\"\"\n        Accept legacy schedule data format where periods are at top-level.\n\n        This validator converts legacy schedule data (where periods are directly\n        in the dict) to the new format (where periods are under a 'periods' key).\n\n        Args:\n            data: Schedule data dict that may be in legacy format\n\n        Returns:\n            dict: Data in the new format with 'periods' key\n        \"\"\"\n        # NOTE: Pydantic validators should not have try-except blocks.\n        # Pydantic handles exceptions internally and will raise ValidationError if needed.\n        # This validator performs simple dict checks which cannot raise exceptions.\n        # Accept legacy shape where periods are at top-level without 'periods' key\n        if isinstance(data, dict) and \"periods\" not in data:\n            return {\"periods\": data}\n        return data\n\n\nclass SchedulesModel(RootModel[dict[str, CategoryScheduleModel]]):\n    # RootModel does not support extra config; rely on inner models' tolerance\n    pass\n\n    @handle_errors(\"converting to dictionary\")\n    def to_dict(self) -> dict[str, Any]:\n        try:\n            return {k: v.model_dump() for k, v in self.root.items()}\n        except Exception as e:\n            logger.error(f\"Error converting to dictionary: {e}\")\n            return {}\n\n\n# --------------------------------- Messages ----------------------------------\n\n\nclass MessageModel(BaseModel):\n    model_config = ConfigDict(extra=\"ignore\")\n\n    message_id: str\n    message: str\n    days: list[str] = Field(default_factory=lambda: [\"ALL\"])\n    time_periods: list[str] = Field(default_factory=lambda: [\"ALL\"])\n    timestamp: str | None = None\n\n    @field_validator(\"days\")\n    @classmethod\n    def _normalize_days(cls, v: list[str]) -> list[str]:\n        \"\"\"\n        Normalize days list for message scheduling.\n\n        Ensures the days list is not empty by defaulting to [\"ALL\"] if the\n        input list is empty or None.\n\n        Args:\n            v: List of day strings (may be empty)\n\n        Returns:\n            List[str]: Normalized days list, defaults to [\"ALL\"] if empty\n        \"\"\"\n        # NOTE: Pydantic validators should not have try-except blocks.\n        # Pydantic handles exceptions internally and will raise ValidationError if needed.\n        # This validator performs simple list checks which cannot raise exceptions.\n        if not v:\n            return [\"ALL\"]\n        return v\n\n    @field_validator(\"time_periods\")\n    @classmethod\n    def _normalize_periods(cls, v: list[str]) -> list[str]:\n        \"\"\"\n        Normalize time periods list for message scheduling.\n\n        Ensures the time_periods list is not empty by defaulting to [\"ALL\"] if the\n        input list is empty or None.\n\n        Args:\n            v: List of time period strings (may be empty)\n\n        Returns:\n            List[str]: Normalized time periods list, defaults to [\"ALL\"] if empty\n        \"\"\"\n        # NOTE: Pydantic validators should not have try-except blocks.\n        # Pydantic handles exceptions internally and will raise ValidationError if needed.\n        # This validator performs simple list checks which cannot raise exceptions.\n        if not v:\n            return [\"ALL\"]\n        return v\n\n\nclass MessagesFileModel(BaseModel):\n    model_config = ConfigDict(extra=\"ignore\")\n\n    messages: list[MessageModel] = Field(default_factory=list)\n\n\n# ------------------------------ Helper functions -----------------------------\n\n\n@handle_errors(\n    \"validating account dictionary\", default_return=({}, [\"Validation failed\"])\n)\ndef validate_account_dict(data: dict[str, Any]) -> tuple[dict[str, Any], list[str]]:\n    try:\n        errors: list[str] = []\n        try:\n            model = AccountModel.model_validate(data)\n            return model.model_dump(), errors\n        except Exception as e:\n            errors.append(str(e))\n            # Best effort normalization: coerce features\n            fixed = data.copy()\n            feats = fixed.get(\"features\") or {}\n            for k in (\"automated_messages\", \"checkins\", \"task_management\"):\n                v = feats.get(k)\n                feats[k] = FeaturesModel._coerce_bool(v)\n            fixed[\"features\"] = feats\n        return fixed, errors\n    except Exception as e:\n        logger.error(f\"Error validating account dictionary: {e}\")\n        return data, [str(e)]\n\n\n@handle_errors(\"validating preferences dictionary\")\ndef validate_preferences_dict(data: dict[str, Any]) -> tuple[dict[str, Any], list[str]]:\n    try:\n        errors: list[str] = []\n        try:\n            model = PreferencesModel.model_validate(data)\n            # Exclude None so optional blocks like task_settings/checkin_settings vanish when absent\n            return model.model_dump(exclude_none=True), errors\n        except Exception as e:\n            errors.append(str(e))\n            # Return original data to avoid disruption\n            return data, errors\n    except Exception as e:\n        logger.error(f\"Error validating preferences dictionary: {e}\")\n        return data, [str(e)]\n\n\n@handle_errors(\"validating schedules dictionary\")\ndef validate_schedules_dict(data: dict[str, Any]) -> tuple[dict[str, Any], list[str]]:\n    try:\n        errors: list[str] = []\n        try:\n            model = SchedulesModel.model_validate(data)\n            return model.to_dict(), errors\n        except Exception as e:\n            errors.append(str(e))\n            return data, errors\n    except Exception as e:\n        logger.error(f\"Error validating schedules dictionary: {e}\")\n        return data, [str(e)]\n\n\n@handle_errors(\"validating messages file dictionary\")\ndef validate_messages_file_dict(\n    data: dict[str, Any],\n) -> tuple[dict[str, Any], list[str]]:\n    try:\n        errors: list[str] = []\n        try:\n            model = MessagesFileModel.model_validate(data)\n            return model.model_dump(), errors\n        except Exception as e:\n            errors.append(str(e))\n            # Try to coerce to messages list\n            msgs = data.get(\"messages\")\n            if isinstance(msgs, list):\n                normalized: list[dict[str, Any]] = []\n                for item in msgs:\n                    try:\n                        mi = MessageModel.model_validate(item)\n                        normalized.append(mi.model_dump())\n                    except Exception:\n                        # skip bad rows\n                        continue\n                return {\"messages\": normalized}, errors\n            return {\"messages\": []}, errors\n    except Exception as e:\n        logger.error(f\"Error validating messages file dictionary: {e}\")\n        return {\"messages\": []}, [str(e)]\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 242,
                    "line_content": "# If invalid, keep as-is for backward compatibility (tests may set placeholders)",
                    "start": 8181,
                    "end": 8203
                  }
                ]
              ],
              [
                "core\\user_data_handlers.py",
                "\"\"\"\nCentralized User Data Handlers for MHM.\n\nThis module provides a unified API for loading, saving, and managing user data\nacross all data types (account, preferences, context, schedules, etc.).\n\"\"\"\n\nimport os\nimport time\nimport traceback\nimport uuid\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, Any, List, Union, Optional\nfrom core.logger import get_component_logger\nfrom core.error_handling import handle_errors\nfrom core.config import ensure_user_directory, get_user_file_path\nfrom core.file_operations import load_json_data, save_json_data, determine_file_path\nfrom core.time_utilities import now_timestamp_full\nfrom core.user_data_validation import (\n    validate_new_user_data,\n    validate_user_update,\n)\nfrom core.schemas import (\n    validate_account_dict,\n    validate_preferences_dict,\n    validate_schedules_dict,\n)\n\nlogger = get_component_logger(\"main\")\nhandlers_logger = get_component_logger(\"user_activity\")\nlogger = get_component_logger(\"main\")\nhandlers_logger = get_component_logger(\"user_activity\")\n# Cache configuration\n_cache_timeout = 300  # 5 minutes\n_user_account_cache = {}\n_user_preferences_cache = {}\n_user_context_cache = {}\n_user_schedules_cache = {}\n\n# ---------------------------------------------------------------------------\n# DATA LOADER REGISTRY (centralized here)\n# ---------------------------------------------------------------------------\n\n# Single source of truth for user data loaders now lives here.\n\n_DEFAULT_USER_DATA_LOADERS = {\n    \"account\": {\n        \"loader\": None,  # Will be set after function definition\n        \"file_type\": \"account\",\n        \"default_fields\": [\"user_id\", \"internal_username\", \"account_status\"],\n        \"metadata_fields\": [\"created_at\", \"updated_at\"],\n        \"description\": \"User account information and settings\",\n    },\n    \"preferences\": {\n        \"loader\": None,  # Will be set after function definition\n        \"file_type\": \"preferences\",\n        \"default_fields\": [\"categories\", \"channel\"],\n        \"metadata_fields\": [\"last_updated\"],\n        \"description\": \"User preferences and configuration\",\n    },\n    \"context\": {\n        \"loader\": None,  # Will be set after function definition\n        \"file_type\": \"user_context\",\n        \"default_fields\": [\"preferred_name\", \"gender_identity\"],\n        \"metadata_fields\": [\"created_at\", \"last_updated\"],\n        \"description\": \"User context and personal information\",\n    },\n    \"schedules\": {\n        \"loader\": None,  # Will be set after function definition\n        \"file_type\": \"schedules\",\n        \"default_fields\": [],\n        \"metadata_fields\": [\"last_updated\"],\n        \"description\": \"User schedule and timing preferences\",\n    },\n    \"tags\": {\n        \"loader\": None,  # Will be set after function definition\n        \"file_type\": \"tags\",\n        \"default_fields\": [\"tags\"],\n        \"metadata_fields\": [\"created_at\", \"updated_at\"],\n        \"description\": \"User tags for tasks and notebook entries\",\n    },\n}\n\nif \"USER_DATA_LOADERS\" not in globals():\n    USER_DATA_LOADERS = {}\n\nif not USER_DATA_LOADERS:\n    USER_DATA_LOADERS.update(_DEFAULT_USER_DATA_LOADERS)\n\n\n@handle_errors(\"registering data loader\", default_return=False)\ndef register_data_loader(\n    data_type: str,\n    loader_func,\n    file_type: str,\n    default_fields: list[str] | None = None,\n    metadata_fields: list[str] | None = None,\n    description: str = \"\",\n    *,\n    force: bool = False,\n):\n    \"\"\"\n    Register a new data loader for the centralized system.\n\n    Args:\n        data_type: Unique identifier for the data type\n        loader_func: Function that loads the data\n        file_type: File type identifier\n        default_fields: Commonly accessed fields\n        metadata_fields: Fields that contain metadata\n        description: Human-readable description\n    \"\"\"\n    # Validate inputs\n    if not data_type or not isinstance(data_type, str):\n        logger.error(f\"Invalid data_type: {data_type}\")\n        return False\n\n    if not file_type or not isinstance(file_type, str):\n        logger.error(f\"Invalid file_type: {file_type}\")\n        return False\n\n    if not callable(loader_func):\n        logger.error(f\"Invalid loader_func: {loader_func}\")\n        return False\n\n    # Idempotent behavior: if an entry exists with a non-None loader and not forcing, do not overwrite\n    existing = USER_DATA_LOADERS.get(data_type)\n    if existing is not None and existing.get(\"loader\") is not None and not force:\n        logger.debug(\n            f\"register_data_loader no-op for '{data_type}' (loader already set and force=False)\"\n        )\n        return False\n\n    USER_DATA_LOADERS[data_type] = {\n        \"loader\": loader_func,\n        \"file_type\": file_type,\n        \"default_fields\": default_fields or [],\n        \"metadata_fields\": metadata_fields or [],\n        \"description\": description,\n    }\n    logger.info(f\"Registered data loader for type: {data_type}\")\n    return True\n\n\n_DEFAULT_LOADERS_REGISTERED = False\n\n\n@handle_errors(\"registering default data loaders\", default_return=None)\ndef register_default_loaders() -> None:\n    \"\"\"Ensure required loaders are registered (idempotent).\n\n    Mutates the shared USER_DATA_LOADERS in-place, setting any missing/None\n    loader entries for: account, preferences, context, schedules, tags.\n    \"\"\"\n    required = [\n        (\"account\", _get_user_data__load_account, \"account\"),\n        (\"preferences\", _get_user_data__load_preferences, \"preferences\"),\n        (\"context\", _get_user_data__load_context, \"user_context\"),\n        (\"schedules\", _get_user_data__load_schedules, \"schedules\"),\n        (\"tags\", _get_user_data__load_tags, \"tags\"),\n    ]\n\n    registered_loaders = []\n\n    for key, func, ftype in required:\n        entry = USER_DATA_LOADERS.get(key)\n        if entry is None or entry.get(\"loader\") is None:\n            USER_DATA_LOADERS[key] = {\n                \"loader\": func,\n                \"file_type\": ftype,\n                \"default_fields\": [],\n                \"metadata_fields\": [],\n                \"description\": f\"Default {key} data loader\",\n            }\n            registered_loaders.append(key)\n\n    if registered_loaders:\n        logger.info(\n            f\"Registered data loaders: {', '.join(registered_loaders)} ({len(registered_loaders)} total)\"\n        )\n\n\n@handle_errors(\"ensuring default loaders are registered once\", default_return=None)\ndef _ensure_default_loaders_once() -> None:\n    global _DEFAULT_LOADERS_REGISTERED\n    if not _DEFAULT_LOADERS_REGISTERED:\n        register_default_loaders()\n        _DEFAULT_LOADERS_REGISTERED = True\n\n\n@handle_errors(\"getting available data types\", default_return=[])\ndef get_available_data_types() -> list[str]:\n    \"\"\"Get list of available data types.\"\"\"\n    return list(USER_DATA_LOADERS.keys())\n\n\n@handle_errors(\"getting data type info\", default_return=None)\ndef get_data_type_info(data_type: str) -> dict[str, Any] | None:\n    \"\"\"Get information about a specific data type.\"\"\"\n    return USER_DATA_LOADERS.get(data_type)\n\n\n@handle_errors(\"loading user account data\", default_return=None)\ndef _get_user_data__load_account(\n    user_id: str, auto_create: bool = True\n) -> dict[str, Any] | None:\n    \"\"\"Load user account data from account.json.\"\"\"\n    if not user_id:\n        logger.error(\"_get_user_data__load_account called with None user_id\")\n        return None\n\n    # Check cache first\n    current_time = time.time()\n    cache_key = f\"account_{user_id}\"\n\n    if cache_key in _user_account_cache:\n        cached_data, cache_time = _user_account_cache[cache_key]\n        if current_time - cache_time < _cache_timeout:\n            return cached_data\n\n    # Check if user directory exists (indicates user was created before)\n    user_dir = os.path.dirname(get_user_file_path(user_id, \"account\"))\n    user_dir_exists = os.path.exists(user_dir)\n\n    # Check if file exists before loading\n    account_file = get_user_file_path(user_id, \"account\")\n    if not os.path.exists(account_file):\n        if not auto_create:\n            if not user_dir_exists:\n                return None\n            else:\n                return None\n\n        # Only auto-create if user directory exists (user was created before)\n        if not user_dir_exists:\n            logger.debug(\n                f\"User directory doesn't exist for {user_id}, not auto-creating account file\"\n            )\n            return None\n\n        # Auto-create the file with default data\n        logger.info(\n            f\"Auto-creating missing account file for user {user_id} (user directory exists)\"\n        )\n        ensure_user_directory(user_id)\n        # Create default account data\n        # Canonical readable timestamp for metadata fields\n        current_time_str = now_timestamp_full()\n        default_account = {\n            \"user_id\": user_id,\n            \"internal_username\": \"\",\n            \"account_status\": \"active\",\n            \"chat_id\": \"\",\n            \"phone\": \"\",\n            \"email\": \"\",\n            \"discord_user_id\": \"\",\n            \"discord_username\": \"\",\n            \"created_at\": current_time_str,\n            \"updated_at\": current_time_str,\n            \"features\": {\n                \"automated_messages\": \"disabled\",\n                \"checkins\": \"disabled\",\n                \"task_management\": \"disabled\",\n            },\n        }\n        save_json_data(default_account, account_file)\n        account_data = default_account\n    else:\n        # Load from file\n        ensure_user_directory(user_id)\n        account_data = load_json_data(account_file)\n        try:\n            if isinstance(account_data, dict) and not account_data.get(\"timezone\"):\n                account_data[\"timezone\"] = \"UTC\"\n        except Exception:\n            pass\n\n    if isinstance(account_data, dict):\n        normalized_account, errors = validate_account_dict(account_data)\n        if errors:\n            logger.warning(\n                f\"Validation issues in account data for user {user_id}: {'; '.join(errors)}\"\n            )\n        account_data = normalized_account or account_data\n\n    # Cache the data\n    _user_account_cache[cache_key] = (account_data, current_time)\n\n    return account_data\n\n\n@handle_errors(\"saving user account data\")\ndef _save_user_data__save_account(user_id: str, account_data: dict[str, Any]) -> bool:\n    \"\"\"Save user account data to account.json.\"\"\"\n    if not user_id:\n        logger.error(\"_save_user_data__save_account called with None user_id\")\n        return False\n\n    ensure_user_directory(user_id)\n    account_file = get_user_file_path(user_id, \"account\")\n\n    # Add metadata\n    account_data[\"updated_at\"] = now_timestamp_full()\n\n    # Validate/normalize via Pydantic schema (non-blocking)\n    try:\n        account_data, _errs = validate_account_dict(account_data)\n    except Exception:\n        pass\n    save_json_data(account_data, account_file)\n\n    # Update cache\n    cache_key = f\"account_{user_id}\"\n    _user_account_cache[cache_key] = (account_data, time.time())\n\n    # Update user index\n    try:\n        from core.user_data_manager import update_user_index\n\n        update_user_index(user_id)\n    except Exception as e:\n        logger.warning(\n            f\"Failed to update user index after account update for user {user_id}: {e}\"\n        )\n\n    logger.debug(f\"Account data saved for user {user_id}\")\n    return True\n\n\n@handle_errors(\"loading user preferences data\", default_return=None)\ndef _get_user_data__load_preferences(\n    user_id: str, auto_create: bool = True\n) -> dict[str, Any] | None:\n    \"\"\"Load user preferences data from preferences.json.\"\"\"\n    if not user_id:\n        logger.error(\"_get_user_data__load_preferences called with None user_id\")\n        return None\n\n    # Check cache first\n    current_time = time.time()\n    cache_key = f\"preferences_{user_id}\"\n\n    if cache_key in _user_preferences_cache:\n        cached_data, cache_time = _user_preferences_cache[cache_key]\n        if current_time - cache_time < _cache_timeout:\n            return cached_data\n\n    # Check if user directory exists (indicates user was created before)\n    user_dir = os.path.dirname(get_user_file_path(user_id, \"preferences\"))\n    user_dir_exists = os.path.exists(user_dir)\n\n    # Check if file exists before loading\n    preferences_file = get_user_file_path(user_id, \"preferences\")\n    if not os.path.exists(preferences_file):\n        if not auto_create:\n            if not user_dir_exists:\n                return None\n            else:\n                return None\n\n        # Only auto-create if user directory exists (user was created before)\n        if not user_dir_exists:\n            logger.debug(\n                f\"User directory doesn't exist for {user_id}, not auto-creating preferences file\"\n            )\n            return None\n\n        # Auto-create the file with default data\n        logger.info(\n            f\"Auto-creating missing preferences file for user {user_id} (user directory exists)\"\n        )\n        ensure_user_directory(user_id)\n        # Create default preferences data\n        default_preferences = {\n            \"categories\": [],\n            \"channel\": {\"type\": \"email\"},\n            \"checkin_settings\": {\"enabled\": False},\n            \"task_settings\": {\"enabled\": False},\n        }\n        save_json_data(default_preferences, preferences_file)\n        preferences_data = default_preferences\n    else:\n        # Load from file\n        ensure_user_directory(user_id)\n        preferences_data = load_json_data(preferences_file)\n\n        # If load_json_data returned None (corrupted file) and auto_create is False, return None\n        if preferences_data is None and not auto_create:\n            return None\n\n        # Use empty dict as fallback only when auto_create is True\n        preferences_data = preferences_data or {}\n\n    if isinstance(preferences_data, dict):\n        normalized_preferences, errors = validate_preferences_dict(preferences_data)\n        if errors:\n            logger.warning(\n                f\"Validation issues in preferences for user {user_id}: {'; '.join(errors)}\"\n            )\n        preferences_data = normalized_preferences or preferences_data\n\n    # Cache the data\n    _user_preferences_cache[cache_key] = (preferences_data, current_time)\n\n    return preferences_data\n\n\n@handle_errors(\"saving user preferences data\")\ndef _save_user_data__save_preferences(\n    user_id: str, preferences_data: dict[str, Any]\n) -> bool:\n    \"\"\"Save user preferences data to preferences.json.\"\"\"\n    if not user_id:\n        logger.error(\"_save_user_data__save_preferences called with None user_id\")\n        return False\n\n    ensure_user_directory(user_id)\n    preferences_file = get_user_file_path(user_id, \"preferences\")\n\n    # Validate/normalize via Pydantic schema (non-blocking)\n    try:\n        preferences_data, _perrs = validate_preferences_dict(preferences_data)\n    except Exception:\n        pass\n    save_json_data(preferences_data, preferences_file)\n\n    # Update cache\n    cache_key = f\"preferences_{user_id}\"\n    _user_preferences_cache[cache_key] = (preferences_data, time.time())\n\n    # Update user index\n    try:\n        from core.user_data_manager import update_user_index\n\n        update_user_index(user_id)\n    except Exception as e:\n        logger.warning(\n            f\"Failed to update user index after preferences update for user {user_id}: {e}\"\n        )\n\n    logger.debug(f\"Preferences data saved for user {user_id}\")\n    return True\n\n\n@handle_errors(\"loading user context data\", default_return=None)\ndef _get_user_data__load_context(\n    user_id: str, auto_create: bool = True\n) -> dict[str, Any] | None:\n    \"\"\"Load user context data from user_context.json.\"\"\"\n    if not user_id:\n        logger.error(\"_get_user_data__load_context called with None user_id\")\n        return None\n\n    # Check cache first\n    current_time = time.time()\n    cache_key = f\"context_{user_id}\"\n\n    if cache_key in _user_context_cache:\n        cached_data, cache_time = _user_context_cache[cache_key]\n        if current_time - cache_time < _cache_timeout:\n            return cached_data\n\n    # Check if user directory exists (indicates user was created before)\n    user_dir = os.path.dirname(get_user_file_path(user_id, \"context\"))\n    user_dir_exists = os.path.exists(user_dir)\n\n    # Check if file exists before loading\n    context_file = get_user_file_path(user_id, \"context\")\n    if not os.path.exists(context_file):\n        if not auto_create:\n            if not user_dir_exists:\n                return None\n            else:\n                return None\n\n        # Only auto-create if user directory exists (user was created before)\n        if not user_dir_exists:\n            logger.debug(\n                f\"User directory doesn't exist for {user_id}, not auto-creating user context file\"\n            )\n            return None\n\n        # Auto-create the file with default data\n        logger.info(\n            f\"Auto-creating missing user context file for user {user_id} (user directory exists)\"\n        )\n        ensure_user_directory(user_id)\n        # Create default context data\n        current_time_str = now_timestamp_full()\n        default_context = {\n            \"preferred_name\": \"\",\n            \"gender_identity\": [],\n            \"date_of_birth\": \"\",\n            \"custom_fields\": {\n                \"reminders_needed\": [],\n                \"health_conditions\": [],\n                \"medications_treatments\": [],\n                \"allergies_sensitivities\": [],\n            },\n            \"interests\": [],\n            \"goals\": [],\n            \"loved_ones\": [],\n            \"activities_for_encouragement\": [],\n            \"notes_for_ai\": [],\n            \"created_at\": current_time_str,\n            \"last_updated\": current_time_str,\n        }\n        save_json_data(default_context, context_file)\n        context_data = default_context\n    else:\n        # Load from file\n        ensure_user_directory(user_id)\n        context_data = load_json_data(context_file)\n\n        # If load_json_data returned None (corrupted file) and auto_create is False, return None\n        if context_data is None and not auto_create:\n            return None\n\n        # Use empty dict as fallback only when auto_create is True\n        context_data = context_data or {}\n\n    # Cache the data\n    _user_context_cache[cache_key] = (context_data, current_time)\n\n    return context_data\n\n\n@handle_errors(\"saving user context data\")\ndef _save_user_data__save_context(user_id: str, context_data: dict[str, Any]) -> bool:\n    \"\"\"Save user context data to user_context.json.\"\"\"\n    if not user_id:\n        logger.error(\"_save_user_data__save_context called with None user_id\")\n        return False\n\n    ensure_user_directory(user_id)\n    context_file = get_user_file_path(user_id, \"context\")\n\n    # Add metadata\n    context_data[\"last_updated\"] = now_timestamp_full()\n\n    save_json_data(context_data, context_file)\n\n    # Update cache\n    cache_key = f\"context_{user_id}\"\n    _user_context_cache[cache_key] = (context_data, time.time())\n\n    # Update user index\n    try:\n        from core.user_data_manager import update_user_index\n\n        update_user_index(user_id)\n    except Exception as e:\n        logger.warning(\n            f\"Failed to update user index after context update for user {user_id}: {e}\"\n        )\n\n    logger.debug(f\"User context data saved for user {user_id}\")\n    return True\n\n\n@handle_errors(\"loading user schedules data\", default_return=None)\ndef _get_user_data__load_schedules(\n    user_id: str, auto_create: bool = True\n) -> dict[str, Any] | None:\n    \"\"\"Load user schedules data from schedules.json.\"\"\"\n    if not user_id:\n        logger.error(\"_get_user_data__load_schedules called with None user_id\")\n        return None\n\n    # Check cache first\n    current_time = time.time()\n    cache_key = f\"schedules_{user_id}\"\n\n    if cache_key in _user_schedules_cache:\n        cached_data, cache_time = _user_schedules_cache[cache_key]\n        if current_time - cache_time < _cache_timeout:\n            return cached_data\n\n    user_dir = os.path.dirname(get_user_file_path(user_id, \"schedules\"))\n    user_dir_exists = os.path.exists(user_dir)\n    schedules_file = get_user_file_path(user_id, \"schedules\")\n    if not os.path.exists(schedules_file):\n        if not auto_create:\n            if not user_dir_exists:\n                return None\n            else:\n                return None\n        if not user_dir_exists:\n            logger.debug(\n                f\"User directory doesn't exist for {user_id}, not auto-creating schedules file\"\n            )\n            return None\n        # Auto-create the file with default data\n        logger.info(\n            f\"Auto-creating missing schedules file for user {user_id} (user directory exists)\"\n        )\n        ensure_user_directory(user_id)\n        default_schedules = {}\n        save_json_data(default_schedules, schedules_file)\n        schedules_data = default_schedules\n    else:\n        ensure_user_directory(user_id)\n        schedules_data = load_json_data(schedules_file)\n        if schedules_data is None and not auto_create:\n            return None\n        schedules_data = schedules_data or {}\n    if isinstance(schedules_data, dict):\n        normalized_schedules, errors = validate_schedules_dict(schedules_data)\n        if errors:\n            logger.warning(\n                f\"Validation issues in schedules for user {user_id}: {'; '.join(errors)}\"\n            )\n        schedules_data = normalized_schedules or schedules_data\n\n    # Cache the data\n    _user_schedules_cache[cache_key] = (schedules_data, current_time)\n\n    return schedules_data\n\n\n@handle_errors(\"saving user schedules data\")\ndef _save_user_data__save_schedules(\n    user_id: str, schedules_data: dict[str, Any]\n) -> bool:\n    \"\"\"Save user schedules data to schedules.json.\"\"\"\n    if not user_id:\n        logger.error(\"_save_user_data__save_schedules called with None user_id\")\n        return False\n\n    ensure_user_directory(user_id)\n    schedules_file = get_user_file_path(user_id, \"schedules\")\n\n    # Normalize using tolerant Pydantic schema to keep on-disk data consistent\n    normalized, errors = validate_schedules_dict(schedules_data)\n    if errors:\n        logger.warning(\n            f\"Schedules validation reported {len(errors)} issue(s); saving normalized data\"\n        )\n    schedules_data = normalized or {}\n\n    save_json_data(schedules_data, schedules_file)\n\n    # Update cache\n    cache_key = f\"schedules_{user_id}\"\n    _user_schedules_cache[cache_key] = (schedules_data, time.time())\n\n    logger.debug(f\"Schedules data saved for user {user_id}\")\n    return True\n\n\n@handle_errors(\"loading user tags data\", default_return=None)\ndef _get_user_data__load_tags(\n    user_id: str, auto_create: bool = True\n) -> dict[str, Any] | None:\n    \"\"\"Load user tags data from tags.json.\"\"\"\n    if not user_id:\n        logger.error(\"_get_user_data__load_tags called with None user_id\")\n        return None\n\n    try:\n        from core.tags import load_user_tags\n\n        tags_data = load_user_tags(user_id)\n        # load_user_tags returns {} on error or for new users (lazy init creates file)\n        # Return empty dict with default structure if file doesn't exist yet (for auto_create=True)\n        # Return None only if there was an actual error\n        if not tags_data and not auto_create:\n            return None\n        # Ensure we always return a dict with at least the expected structure\n        if not tags_data:\n            return {\"tags\": []}\n        return tags_data\n    except Exception as e:\n        logger.error(f\"Error loading tags for user {user_id}: {e}\")\n        return None\n\n\n@handle_errors(\"saving user tags data\")\ndef _save_user_data__save_tags(user_id: str, tags_data: dict[str, Any]) -> bool:\n    \"\"\"Save user tags data to tags.json.\"\"\"\n    if not user_id:\n        logger.error(\"_save_user_data__save_tags called with None user_id\")\n        return False\n\n    try:\n        from core.tags import save_user_tags\n\n        return save_user_tags(user_id, tags_data)\n    except Exception as e:\n        logger.error(f\"Error saving tags for user {user_id}: {e}\")\n        return False\n\n\n@handle_errors(\"creating default schedule periods\", default_return={})\ndef create_default_schedule_periods(category: str | None = None) -> dict[str, Any]:\n    \"\"\"Create default schedule periods for a new category.\"\"\"\n    if category:\n        # Use category-specific naming\n        if category in (\"tasks\", \"checkin\"):\n            # For tasks and check-ins, use the descriptive naming with title case\n            if category == \"tasks\":\n                default_period_name = \"Task Reminder Default\"\n            else:  # checkin\n                default_period_name = \"Check-in Reminder Default\"\n        else:\n            # For message categories, use category-specific naming\n            # Replace underscores with spaces for better readability and use title case\n            category_display = category.replace(\"_\", \" \").title()\n            default_period_name = f\"{category_display} Message Default\"\n    else:\n        # Fallback to generic naming\n        default_period_name = \"Default\"\n\n    return {\n        \"ALL\": {\n            \"active\": True,\n            \"days\": [\"ALL\"],\n            \"start_time\": \"00:00\",\n            \"end_time\": \"23:59\",\n        },\n        default_period_name: {\n            \"active\": True,\n            \"days\": [\"ALL\"],\n            \"start_time\": \"18:00\",\n            \"end_time\": \"20:00\",\n        },\n    }\n\n\n@handle_errors(\"migrating legacy schedules structure\", default_return={})\ndef migrate_legacy_schedules_structure(\n    schedules_data: dict[str, Any],\n) -> dict[str, Any]:\n    \"\"\"Migrate legacy schedules structure to new format.\"\"\"\n    migrated_data = {}\n\n    for category, category_data in schedules_data.items():\n        if isinstance(category_data, dict):\n            # Check if this is already in new format\n            if \"periods\" in category_data:\n                migrated_data[category] = category_data\n            else:\n                # This is legacy format - convert to new format\n                legacy_periods = {}\n                for period_name, period_data in category_data.items():\n                    if isinstance(period_data, dict) and (\n                        \"start_time\" in period_data or \"start\" in period_data\n                    ):\n                        legacy_periods[period_name] = period_data\n\n                # Add default periods if none exist\n                if not legacy_periods:\n                    legacy_periods = create_default_schedule_periods(category)\n\n                # Convert legacy periods to include days\n                for period_name, period_data in legacy_periods.items():\n                    if \"days\" not in period_data:\n                        period_data[\"days\"] = [\"ALL\"]\n\n                # All categories use the periods wrapper for consistency\n                migrated_data[category] = {\"periods\": legacy_periods}\n        else:\n            # Invalid data, create default structure\n            migrated_data[category] = {\n                \"periods\": create_default_schedule_periods(category)\n            }\n\n    return migrated_data\n\n\n@handle_errors(\"ensuring category has default schedule\", default_return=False)\ndef ensure_category_has_default_schedule(user_id: str, category: str) -> bool:\n    \"\"\"Ensure a category has default schedule periods if it doesn't exist.\"\"\"\n    if not user_id or not category:\n        logger.warning(f\"Invalid user_id or category: {user_id}, {category}\")\n        return False\n\n    try:\n        # Load current schedules data\n        schedules_data = _get_user_data__load_schedules(user_id) or {}\n        logger.debug(f\"Current schedules data for user {user_id}: {schedules_data}\")\n\n        # Migrate legacy structure if needed\n        if schedules_data and any(\n            isinstance(v, dict) and \"periods\" not in v for v in schedules_data.values()\n        ):\n            logger.info(f\"Migrating legacy schedules structure for user {user_id}\")\n            schedules_data = migrate_legacy_schedules_structure(schedules_data)\n            _save_user_data__save_schedules(user_id, schedules_data)\n\n        # Check if category exists and has periods\n        category_exists = category in schedules_data\n        has_periods = (\n            schedules_data.get(category, {}).get(\"periods\")\n            if category_exists\n            else False\n        )\n\n        logger.debug(\n            f\"Category '{category}' exists: {category_exists}, has periods: {has_periods}\"\n        )\n\n        if not category_exists or not has_periods:\n            # Create default periods for this category\n            default_periods = create_default_schedule_periods(category)\n            logger.debug(\n                f\"Creating default periods for category '{category}': {default_periods}\"\n            )\n\n            if not category_exists:\n                # All categories use the periods wrapper for consistency\n                schedules_data[category] = {\"periods\": default_periods}\n            else:\n                # Category exists but has no periods\n                schedules_data[category][\"periods\"] = default_periods\n\n            # Save the updated schedules\n            _save_user_data__save_schedules(user_id, schedules_data)\n            logger.info(\n                f\"Created default schedule periods for category '{category}' for user {user_id}\"\n            )\n            return True\n\n        logger.debug(\n            f\"Category '{category}' already has periods, skipping default creation\"\n        )\n        return True\n    except Exception as e:\n        logger.error(\n            f\"Error ensuring default schedule for category '{category}' for user {user_id}: {e}\"\n        )\n        return False\n\n\n@handle_errors(\"ensuring all categories have schedules\", default_return=False)\ndef ensure_all_categories_have_schedules(\n    user_id: str, suppress_logging: bool = False\n) -> bool:\n    \"\"\"Ensure all categories in user preferences have corresponding schedules.\"\"\"\n    if not user_id:\n        logger.warning(f\"Invalid user_id: {user_id}\")\n        return False\n\n    try:\n        user_data = get_user_data(user_id, \"preferences\")\n        if not user_data or \"preferences\" not in user_data:\n            logger.warning(f\"User preferences not found for user_id: {user_id}\")\n            return False\n\n        preferences_data = user_data[\"preferences\"]\n        categories = preferences_data.get(\"categories\", [])\n\n        if not categories:\n            logger.debug(f\"No categories found for user {user_id}\")\n            return True\n\n        # Track which schedules are actually created (not just verified)\n        created_schedules = []\n\n        # Ensure each category has a default schedule\n        for category in categories:\n            if ensure_category_has_default_schedule(user_id, category):\n                created_schedules.append(category)\n\n        # Only log when schedules are actually created, not when they already exist\n        # Suppress logging for internal calls to avoid duplicate messages\n        if created_schedules and not suppress_logging:\n            logger.info(f\"Created schedules for user {user_id}: {created_schedules}\")\n        elif not suppress_logging:\n            logger.debug(f\"Verified schedules exist for user {user_id}: {categories}\")\n\n        return True\n    except Exception as e:\n        logger.error(\n            f\"Error ensuring all categories have schedules for user {user_id}: {e}\"\n        )\n        return False\n\n\n@handle_errors(\"clearing user caches\")\ndef clear_user_caches(user_id: str | None = None):\n    \"\"\"Clear user data caches.\"\"\"\n    global _user_account_cache, _user_preferences_cache, _user_context_cache, _user_schedules_cache\n\n    if user_id:\n        # Clear specific user's cache\n        account_key = f\"account_{user_id}\"\n        preferences_key = f\"preferences_{user_id}\"\n        context_key = f\"context_{user_id}\"\n        schedules_key = f\"schedules_{user_id}\"\n\n        if account_key in _user_account_cache:\n            del _user_account_cache[account_key]\n        if preferences_key in _user_preferences_cache:\n            del _user_preferences_cache[preferences_key]\n        if context_key in _user_context_cache:\n            del _user_context_cache[context_key]\n        if schedules_key in _user_schedules_cache:\n            del _user_schedules_cache[schedules_key]\n\n        logger.debug(f\"Cleared cache for user {user_id}\")\n    else:\n        # Clear all caches\n        _user_account_cache.clear()\n        _user_preferences_cache.clear()\n        _user_context_cache.clear()\n        _user_schedules_cache.clear()\n        logger.debug(\"Cleared all user caches\")\n\n\n@handle_errors(\"ensuring unique ids\", default_return=None)\ndef ensure_unique_ids(data):\n    \"\"\"Ensure all messages have unique IDs.\"\"\"\n    if not data or \"messages\" not in data:\n        return data\n\n    existing_ids = set()\n    for message in data[\"messages\"]:\n        if \"message_id\" not in message or message[\"message_id\"] in existing_ids:\n            message[\"message_id\"] = str(uuid.uuid4())\n        existing_ids.add(message[\"message_id\"])\n\n    return data\n\n\n@handle_errors(\"loading and ensuring ids\", default_return=None)\ndef load_and_ensure_ids(user_id):\n    \"\"\"Load messages for all categories and ensure IDs are unique for a user.\"\"\"\n    user_data = get_user_data(user_id, \"preferences\")\n    if not user_data or \"preferences\" not in user_data:\n        logger.warning(f\"User preferences not found for user_id: {user_id}\")\n        return\n\n    preferences = user_data[\"preferences\"]\n    categories = preferences.get(\"categories\", [])\n    if not categories:\n        logger.debug(f\"No categories found for user {user_id}\")\n        return\n\n    for category in categories:\n        file_path = determine_file_path(\"messages\", f\"{category}/{user_id}\")\n        data = load_json_data(file_path)\n        if data:\n            data = ensure_unique_ids(data)\n            save_json_data(data, file_path)\n\n    logger.debug(f\"Ensured message ids are unique for user '{user_id}'\")\n\n\n@handle_errors(\"getting user data\", default_return={})\ndef get_user_data(\n    user_id: str,\n    data_types: str | list[str] = \"all\",\n    fields: str | list[str] | dict[str, str | list[str]] | None = None,\n    auto_create: bool = True,\n    include_metadata: bool = False,\n    normalize_on_read: bool = False,\n) -> dict[str, Any]:\n    \"\"\"\n    Get user data with comprehensive validation.\n\n    Returns:\n        Dict[str, Any]: User data dictionary, empty dict if failed\n    \"\"\"\n    # Validate user_id early\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id: {user_id}\")\n        return {}\n\n    if not user_id.strip():\n        logger.error(\"Empty user_id provided\")\n        return {}\n    \"\"\"Migrated implementation of get_user_data.\"\"\"\n    logger.debug(f\"get_user_data called: user_id={user_id}, data_types={data_types}\")\n    # Ensure default loaders are registered (idempotent). Under certain import\n    # orders in the test environment, callers may invoke this function before\n    # default registration runs. This guard safely fills any missing loader\n    # entries without overwriting existing ones.\n    try:\n        # Only invoke if any loader is missing to avoid unnecessary work\n        if any((not info.get(\"loader\")) for info in USER_DATA_LOADERS.values()):\n            register_default_loaders()\n    except Exception:\n        # If registration cannot be ensured, continue; downstream logic will\n        # gracefully warn about missing loaders and return an empty result.\n        pass\n\n    # TEST-GATED DIAGNOSTICS: capture loader registry state for debugging\n    try:\n        if os.getenv(\"MHM_TESTING\") == \"1\":\n            loader_state = {\n                k: bool(v.get(\"loader\")) for k, v in USER_DATA_LOADERS.items()\n            }\n            missing = [k for k, v in USER_DATA_LOADERS.items() if not v.get(\"loader\")]\n            logger.debug(f\"[TEST] USER_DATA_LOADERS state: {loader_state}\")\n            if missing:\n                logger.debug(f\"[TEST] Missing loaders detected: {missing}\")\n    except Exception:\n        # Never let diagnostics interfere with normal operation\n        pass\n\n    # Additional validation for user_id format\n    if len(user_id) < 1 or len(user_id) > 100:\n        logger.error(f\"Invalid user_id length: {len(user_id)}\")\n        return {}\n\n    # Early exit: for strict no-autocreate requests on truly nonexistent users, return empty\n    try:\n        if auto_create is False:\n            from core.config import get_user_data_dir as _get_user_data_dir\n\n            if not os.path.exists(_get_user_data_dir(user_id)):\n                logger.debug(\n                    f\"get_user_data: user directory missing for {user_id} with auto_create=False; returning empty\"\n                )\n                return {}\n            # Treat users not present in the index as nonexistent, even if stray files exist\n            try:\n                known_ids = set(get_all_user_ids())\n                if user_id not in known_ids:\n                    logger.debug(\n                        f\"get_user_data: user {user_id} not in index with auto_create=False; returning empty\"\n                    )\n                    return {}\n            except Exception as index_error:\n                logger.debug(f\"Index check failed for user {user_id}: {index_error}\")\n                # If index check fails, fall back to file-based checks below\n        # For auto_create=True, check if user directory exists\n        # If directory exists, always allow loaders to proceed\n        # If directory doesn't exist, only return empty if user is truly nonexistent (not in index)\n        elif auto_create is True:\n            from core.config import get_user_data_dir as _get_user_data_dir\n\n            user_dir = _get_user_data_dir(user_id)\n            if not os.path.exists(user_dir):\n                # Directory doesn't exist - check if user is in index\n                # If user is in index, allow loaders to proceed (they may create the directory)\n                # If user is not in index, return empty (truly nonexistent user)\n                try:\n                    known_ids = set(get_all_user_ids())\n                    if user_id not in known_ids:\n                        logger.debug(\n                            f\"get_user_data: user {user_id} not in index and directory missing with auto_create=True; returning empty\"\n                        )\n                        return {}\n                    # User is in index but directory missing - allow loaders to proceed\n                except Exception:\n                    # If index check fails, allow loaders to proceed (they may create the directory)\n                    pass\n            # If directory exists, always proceed (regardless of index status)\n    except Exception:\n        pass\n\n    # Normalize data_types\n    if data_types == \"all\":\n        data_types = list(USER_DATA_LOADERS.keys())\n    elif isinstance(data_types, str):\n        data_types = [data_types]\n\n    # Validate data types\n    available_types = get_available_data_types()\n    try:\n        if os.getenv(\"MHM_TESTING\") == \"1\":\n            logger.debug(\n                f\"[TEST] get_user_data request types={data_types}; available={available_types}\"\n            )\n    except Exception:\n        pass\n    invalid_types = [dt for dt in data_types if dt not in available_types]\n    if invalid_types:\n        logger.error(\n            f\"Invalid data types requested: {invalid_types}. Valid types: {available_types}\"\n        )\n        return {}\n\n    result: dict[str, Any] = {}\n\n    for data_type in data_types:\n        loader_info = USER_DATA_LOADERS.get(data_type)\n        if not loader_info or not loader_info[\"loader\"]:\n            logger.warning(f\"No loader registered for data type: {data_type}\")\n            continue\n\n        file_path = get_user_file_path(user_id, loader_info[\"file_type\"])\n        loader_name = getattr(\n            loader_info[\"loader\"], \"__name__\", repr(loader_info[\"loader\"])\n        )\n        try:\n            if os.getenv(\"MHM_TESTING\") == \"1\":\n                logger.debug(\n                    f\"[TEST] Loading {data_type} for {user_id} path={file_path} via={loader_name} auto_create={auto_create}\"\n                )\n        except Exception as test_error:\n            logger.debug(f\"Test logging failed: {test_error}\")\n        # Honor auto_create=False strictly: if target file does not exist, skip loading\n        try:\n            if auto_create is False and not os.path.exists(file_path):\n                data = None\n            else:\n                data = loader_info[\"loader\"](user_id, auto_create=auto_create)\n        except Exception as load_error:\n            logger.warning(\n                f\"Failed to load {data_type} for user {user_id}: {load_error}\"\n            )\n            data = None\n        # Enforce strict no-autocreate semantics for nonexistent users/files\n        if auto_create is False:\n            try:\n                from core.config import get_user_data_dir as _get_user_data_dir\n\n                user_dir_exists = os.path.exists(_get_user_data_dir(user_id))\n            except Exception as dir_error:\n                logger.debug(f\"Failed to check user directory existence: {dir_error}\")\n                user_dir_exists = False\n            # Exclude any data for users not present in the index (per-type guard)\n            try:\n                known_ids = set(get_all_user_ids())\n                if user_id not in known_ids:\n                    data = None\n            except Exception as known_ids_error:\n                logger.debug(f\"Failed to get known user IDs: {known_ids_error}\")\n            # If user dir doesn't exist or file doesn't exist, treat as no data\n            if not user_dir_exists or not os.path.exists(file_path):\n                data = None\n        if not data:\n            # In test mode, this is expected behavior (files may not exist yet), so use DEBUG\n            if os.getenv(\"MHM_TESTING\") == \"1\":\n                logger.debug(\n                    f\"No data returned for {data_type} (user={user_id}, path={file_path}, loader={loader_name})\"\n                )\n            else:\n                logger.warning(\n                    f\"No data returned for {data_type} (user={user_id}, path={file_path}, loader={loader_name})\"\n                )\n        elif isinstance(data, dict):\n            logger.debug(f\"Loaded {data_type} keys: {list(data.keys())}\")\n\n        # Field extraction logic\n        if fields is not None:\n            if isinstance(fields, str):\n                data = data.get(fields) if data else None\n            elif isinstance(fields, list):\n                if data:\n                    extracted = {f: data[f] for f in fields if f in data}\n                    data = extracted if extracted else None\n                else:\n                    data = None\n            elif isinstance(fields, dict) and data_type in fields:\n                type_fields = fields[data_type]\n                if isinstance(type_fields, str):\n                    data = data.get(type_fields) if data else None\n                elif isinstance(type_fields, list):\n                    if data:\n                        extracted = {f: data[f] for f in type_fields if f in data}\n                        data = extracted if extracted else None\n                    else:\n                        data = None\n\n        # Optional normalization on read (only when returning full objects; skip if fields selection applied)\n        if normalize_on_read and fields is None and isinstance(data, dict):\n            try:\n                if data_type == \"account\":\n                    normalized, _errs = validate_account_dict(data)\n                    if normalized:\n                        # Ensure a default timezone when missing\n                        if not normalized.get(\"timezone\"):\n                            normalized[\"timezone\"] = \"UTC\"\n                        data = normalized\n                elif data_type == \"preferences\":\n                    normalized, _errs = validate_preferences_dict(data)\n                    if normalized:\n                        # Preserve caller-provided category order; append any normalized uniques preserving order\n                        try:\n                            caller_categories = (\n                                data.get(\"categories\", [])\n                                if isinstance(data.get(\"categories\"), list)\n                                else []\n                            )\n                            normalized_categories = (\n                                normalized.get(\"categories\", [])\n                                if isinstance(normalized.get(\"categories\"), list)\n                                else []\n                            )\n                            seen = set()\n                            merged = []\n                            for cat in caller_categories + normalized_categories:\n                                if isinstance(cat, str) and cat and cat not in seen:\n                                    seen.add(cat)\n                                    merged.append(cat)\n                            normalized[\"categories\"] = merged\n                        except Exception:\n                            pass\n                        data = normalized\n                elif data_type == \"schedules\":\n                    normalized, _errs = validate_schedules_dict(data)\n                    if normalized:\n                        data = normalized\n                        # Ensure message categories in preferences have default schedule blocks\n                        try:\n                            prefs = get_user_data(user_id, \"preferences\").get(\n                                \"preferences\", {}\n                            )\n                            categories = (\n                                prefs.get(\"categories\", [])\n                                if isinstance(prefs, dict)\n                                else []\n                            )\n                            if categories:\n                                ensure_all_categories_have_schedules(\n                                    user_id, suppress_logging=True\n                                )\n                                # reload after potential creation\n                                normalized_after, _e2 = validate_schedules_dict(\n                                    get_user_data(user_id, \"schedules\").get(\n                                        \"schedules\", {}\n                                    )\n                                )\n                                if normalized_after:\n                                    data = normalized_after\n                        except Exception:\n                            pass\n            except Exception:\n                # Best-effort normalization; ignore failures\n                pass\n        # Ensure schedules are returned unwrapped as a category map at result['schedules']\n        if data_type == \"schedules\" and isinstance(data, dict):\n            try:\n                if \"schedules\" in data and isinstance(data[\"schedules\"], dict):\n                    data = data[\"schedules\"]\n            except Exception:\n                pass\n\n        # Metadata section\n        if include_metadata and data is not None:\n            file_path = get_user_file_path(user_id, loader_info[\"file_type\"])\n            if os.path.exists(file_path):\n                stat = os.stat(file_path)\n                metadata = {\n                    \"file_size\": stat.st_size,\n                    \"modified_time\": stat.st_mtime,\n                    \"file_path\": file_path,\n                    \"data_type\": data_type,\n                    \"description\": loader_info[\"description\"],\n                }\n                if isinstance(data, dict):\n                    data[\"_metadata\"] = metadata\n                else:\n                    data = {\"data\": data, \"_metadata\": metadata}\n\n        if data is not None:\n            result[data_type] = data\n\n    # TEST-ONLY STRUCTURE ASSEMBLY: ensure callers receive structured dicts\n    # in the test environment even if individual loaders returned empty.\n    # IMPORTANT: Respect auto_create flag \u2013 when auto_create=False, tests expect\n    # empty results for nonexistent users or corrupted files. So only assemble\n    # when auto_create=True.\n    try:\n        if os.getenv(\"MHM_TESTING\") == \"1\" and auto_create:\n            # Only assemble when the user directory actually exists; otherwise\n            # unit tests expect empty results for truly nonexistent users.\n            try:\n                from core.config import get_user_data_dir as _get_user_data_dir\n\n                if not os.path.exists(_get_user_data_dir(user_id)):\n                    raise RuntimeError(\"skip_assembly_nonexistent_user\")\n            except Exception:\n                # If path resolution fails, be conservative and skip assembly\n                raise\n            requested_types = set(data_types) if isinstance(data_types, list) else set()\n            # If 'all' was requested earlier we normalized to full list\n            expected_keys = [\"account\", \"preferences\", \"context\", \"schedules\"]\n            for key in expected_keys:\n                needs_key = (not result.get(key)) and (\n                    (not requested_types) or (key in requested_types)\n                )\n                if needs_key:\n                    try:\n                        entry = USER_DATA_LOADERS.get(key)\n                        loader = (\n                            entry.get(\"loader\") if isinstance(entry, dict) else None\n                        )\n                        if loader is None:\n                            # Attempt self-heal registration\n                            healing = {\n                                \"account\": (_get_user_data__load_account, \"account\"),\n                                \"preferences\": (\n                                    _get_user_data__load_preferences,\n                                    \"preferences\",\n                                ),\n                                \"context\": (\n                                    _get_user_data__load_context,\n                                    \"user_context\",\n                                ),\n                                \"schedules\": (\n                                    _get_user_data__load_schedules,\n                                    \"schedules\",\n                                ),\n                            }.get(key)\n                            if healing is not None:\n                                func, ftype = healing\n                                try:\n                                    register_data_loader(key, func, ftype)\n                                    entry = USER_DATA_LOADERS.get(key)\n                                    loader = (\n                                        entry.get(\"loader\")\n                                        if isinstance(entry, dict)\n                                        else func\n                                    )\n                                except Exception:\n                                    loader = func\n                        if loader is not None:\n                            loaded_val = loader(user_id, True)\n                            if loaded_val is not None:\n                                result[key] = loaded_val\n                                continue\n                    except Exception:\n                        pass\n                    # Fallback: read file directly\n                    try:\n                        from core.config import (\n                            get_user_file_path as _get_user_file_path,\n                        )\n                        from core.file_operations import load_json_data as _load_json\n\n                        file_map = {\n                            \"account\": \"account\",\n                            \"preferences\": \"preferences\",\n                            \"context\": \"context\",\n                            \"schedules\": \"schedules\",\n                        }\n                        ftype = file_map.get(key)\n                        if ftype is not None:\n                            fpath = _get_user_file_path(user_id, ftype)\n                            data_from_file = _load_json(fpath)\n                            if data_from_file is not None:\n                                result[key] = data_from_file\n                    except Exception:\n                        pass\n    except Exception:\n        # Never let test-only assembly interfere with normal operation\n        pass\n\n    logger.debug(f\"get_user_data returning: {result}\")\n    # Final safeguard: when auto_create=False, enforce strict non-existence semantics\n    try:\n        if auto_create is False and isinstance(result, dict):\n            # If user is not present in the index, treat as nonexistent regardless of stray files\n            try:\n                known_ids = set(get_all_user_ids())\n                if user_id not in known_ids:\n                    logger.debug(\n                        f\"get_user_data final-guard: {user_id} not in index; returning empty under auto_create=False\"\n                    )\n                    return {}\n            except Exception:\n                # If we cannot determine, fall back to per-type filtering below\n                pass\n            # Include only types whose files exist\n            filtered: dict[str, Any] = {}\n            for dt_key, dt_val in result.items():\n                try:\n                    loader_info = USER_DATA_LOADERS.get(dt_key, {})\n                    ftype = loader_info.get(\"file_type\")\n                    if not ftype:\n                        continue\n                    fpath = get_user_file_path(user_id, ftype)\n                    if os.path.exists(fpath):\n                        filtered[dt_key] = dt_val\n                except Exception:\n                    # If any resolution fails, err on the side of exclusion under strict no-autocreate\n                    continue\n            result = filtered\n    except Exception:\n        pass\n    return result\n\n\n@handle_errors(\"validating input parameters\", default_return=(False, {}, []))\ndef _save_user_data__validate_input(\n    user_id: str, data_updates: dict[str, dict[str, Any]]\n) -> tuple[bool, dict[str, bool], list[str]]:\n    \"\"\"\n    Validate input parameters with enhanced validation.\n\n    Returns:\n        tuple: (is_valid, valid_types, error_messages)\n    \"\"\"\n    # Validate user_id\n    if not user_id or not isinstance(user_id, str):\n        return False, {}, [f\"Invalid user_id: {user_id}\"]\n\n    if not user_id.strip():\n        return False, {}, [\"Empty user_id provided\"]\n\n    # Validate data_updates\n    if not data_updates or not isinstance(data_updates, dict):\n        return False, {}, [f\"Invalid data_updates: {type(data_updates)}\"]\n    \"\"\"Validate input parameters and initialize result structure.\"\"\"\n    if not user_id:\n        logger.error(\"save_user_data called with None user_id\")\n        return False, {}, []\n\n    if not data_updates:\n        logger.warning(\"save_user_data called with empty data_updates\")\n        return False, {}, []\n\n    # Every requested data_type gets an entry that defaults to False\n    result: dict[str, bool] = {dt: False for dt in data_updates}\n\n    # Validate data types\n    available_types = get_available_data_types()\n    invalid_types = [dt for dt in data_updates if dt not in available_types]\n    if invalid_types:\n        logger.error(\n            f\"Invalid data types in save_user_data: {invalid_types}. Valid types: {available_types}\"\n        )\n\n    return True, result, invalid_types\n\n\n@handle_errors(\"creating user data backup\", default_return=False)\ndef _save_user_data__create_backup(\n    user_id: str, valid_types: list[str], create_backup: bool\n) -> bool:\n    \"\"\"\n    Create backup with validation.\n\n    Returns:\n        bool: True if successful or not needed, False if failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id for backup: {user_id}\")\n        return False\n\n    if not isinstance(valid_types, list):\n        logger.error(f\"Invalid valid_types: {valid_types}\")\n        return False\n    \"\"\"Create backup if needed for major data updates.\"\"\"\n    if create_backup and len(valid_types) > 1:\n        try:\n            from core.user_data_manager import UserDataManager\n\n            backup_path = UserDataManager().backup_user_data(user_id)\n            logger.info(f\"Created backup before major data update: {backup_path}\")\n        except Exception as e:\n            logger.warning(f\"Failed to create backup before data update: {e}\")\n\n\n@handle_errors(\"validating user data\", default_return=([], {}))\ndef _save_user_data__validate_data(\n    user_id: str,\n    data_updates: dict[str, dict[str, Any]],\n    valid_types: list[str],\n    validate_data: bool,\n    is_new_user: bool,\n) -> tuple[list[str], dict[str, bool]]:\n    \"\"\"\n    Validate user data with enhanced validation.\n\n    Returns:\n        tuple: (error_messages, validation_results)\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        return [f\"Invalid user_id: {user_id}\"], {}\n\n    if not isinstance(data_updates, dict):\n        return [f\"Invalid data_updates: {type(data_updates)}\"], {}\n    \"\"\"Validate data for new and existing users.\"\"\"\n    result: dict[str, bool] = {dt: False for dt in data_updates}\n    invalid_types = []\n\n    if not validate_data:\n        return invalid_types, result\n\n    # Validate new user data\n    if is_new_user and valid_types:\n        ok, errors = validate_new_user_data(user_id, data_updates)\n        if not ok:\n            logger.error(f\"New-user validation failed: {errors}\")\n            return invalid_types, result\n\n    # Validate existing user data\n    if not is_new_user:\n        for dt in valid_types:\n            logger.debug(f\"Validating {dt} for existing user {user_id}\")\n            ok, errors = validate_user_update(user_id, dt, data_updates[dt])\n            # Graceful allowance: feature-flag-only updates to account are safe\n            if not ok and dt == \"account\":\n                try:\n                    upd = data_updates.get(dt, {})\n                    feats = upd.get(\"features\", {}) if isinstance(upd, dict) else {}\n                    if (\n                        isinstance(feats, dict)\n                        and feats\n                        and all(v in (\"enabled\", \"disabled\") for v in feats.values())\n                    ):\n                        logger.debug(\n                            \"Bypassing strict validation for account feature-only update\"\n                        )\n                        ok = True\n                        errors = []\n                except Exception:\n                    pass\n            # For preferences, be more lenient - Pydantic validation errors might be non-critical\n            if not ok and dt == \"preferences\":\n                # Check if errors are warnings (like invalid categories that get filtered)\n                # If the merged data will still be valid after normalization, allow it\n                try:\n                    from core.schemas import validate_preferences_dict\n\n                    # Try validating the merged data (from Phase 1) to see if it's actually invalid\n                    # This is a bit of a hack, but we need to check if the data will be valid after merge\n                    logger.debug(\n                        f\"Preferences validation returned errors: {errors}, but checking if data is still usable\"\n                    )\n                    # Don't fail validation if Pydantic can still normalize it\n                    # The merge function will handle normalization\n                except Exception:\n                    pass\n            if not ok:\n                logger.error(f\"Validation failed for {dt}: {errors}\")\n                result[dt] = False\n                invalid_types.append(dt)\n            else:\n                logger.debug(f\"Validation passed for {dt}\")\n\n    return invalid_types, result\n\n\n@handle_errors(\"preserving preference settings\", default_return=False)\ndef _save_user_data__preserve_preference_settings(\n    updated: dict[str, Any], updates: dict[str, Any], user_id: str\n) -> bool:\n    \"\"\"\n    Preserve preference settings blocks when saving preferences.\n\n    Note: task_settings and checkin_settings blocks are preserved even when features are disabled.\n    This allows users to re-enable features later and restore their previous settings.\n    Feature enablement is controlled by account.features, not by the presence of settings blocks.\n\n    Settings preservation happens automatically through the merge process (current.copy() + updates),\n    so this function primarily serves as a placeholder for any future preference-specific handling.\n\n    Returns:\n        bool: True if successful, False if failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id for preference settings: {user_id}\")\n        return False\n\n    if not isinstance(updated, dict):\n        logger.error(f\"Invalid updated data: {type(updated)}\")\n        return False\n\n    # Settings blocks are preserved automatically through the merge process in _save_user_data__save_single_type.\n    # No additional logic needed - settings are preserved for future use when features are re-enabled.\n    return True\n\n\n@handle_errors(\"normalizing user data\", default_return=False)\ndef _save_user_data__normalize_data(dt: str, updated: dict[str, Any]) -> bool:\n    \"\"\"\n    Normalize user data with validation.\n\n    Returns:\n        bool: True if successful, False if failed\n    \"\"\"\n    # Validate inputs\n    if not dt or not isinstance(dt, str):\n        logger.error(f\"Invalid data type: {dt}\")\n        return False\n\n    if not isinstance(updated, dict):\n        logger.error(f\"Invalid updated data: {type(updated)}\")\n        return False\n    \"\"\"Apply Pydantic normalization to data.\"\"\"\n    try:\n        if dt == \"account\":\n            normalized, errors = validate_account_dict(updated)\n            if not errors and normalized:\n                updated.clear()\n                updated.update(normalized)\n        elif dt == \"preferences\":\n            normalized, errors = validate_preferences_dict(updated)\n            if not errors and normalized:\n                # Preserve any categories provided by callers even if validator trimmed them\n                try:\n                    original_categories = set(\n                        updated.get(\"categories\", [])\n                        if isinstance(updated.get(\"categories\"), list)\n                        else []\n                    )\n                    normalized_categories = set(\n                        normalized.get(\"categories\", [])\n                        if isinstance(normalized.get(\"categories\"), list)\n                        else []\n                    )\n                    merged_categories = sorted(\n                        original_categories | normalized_categories\n                    )\n                    normalized[\"categories\"] = merged_categories\n                except Exception:\n                    pass\n                updated.clear()\n                updated.update(normalized)\n        elif dt == \"schedules\":\n            normalized, errors = validate_schedules_dict(updated)\n            if not errors and normalized:\n                updated.clear()\n                updated.update(normalized)\n    except Exception:\n        pass\n\n\n@handle_errors(\"merging data type updates\", default_return=None)\ndef _save_user_data__merge_single_type(\n    user_id: str, dt: str, updates: dict[str, Any], auto_create: bool\n) -> dict[str, Any] | None:\n    \"\"\"\n    Merge updates for a single data type with current data (in-memory only, no disk write).\n\n    Returns:\n        Dict with merged data, or None if merge failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id: {user_id}\")\n        return None\n\n    if not dt or not isinstance(dt, str):\n        logger.error(f\"Invalid data type: {dt}\")\n        return None\n\n    if not isinstance(updates, dict):\n        logger.error(f\"Invalid updates: {type(updates)}\")\n        return None\n\n    try:\n        # Check if user exists when auto_create=False\n        if not auto_create:\n            from core.config import get_user_data_dir\n\n            user_dir = get_user_data_dir(user_id)\n            if not os.path.exists(user_dir):\n                logger.debug(\n                    f\"User directory doesn't exist for {user_id} and auto_create=False, skipping merge\"\n                )\n                return None\n\n        # Get current data\n        current = get_user_data(user_id, dt, auto_create=auto_create).get(dt, {})\n        updated = current.copy() if isinstance(current, dict) else {}\n\n        # Preserve caller order for categories explicitly provided\n        preserve_categories_order: list | None = None\n        if (\n            dt == \"preferences\"\n            and isinstance(updates, dict)\n            and isinstance(updates.get(\"categories\"), list)\n        ):\n            preserve_categories_order = list(\n                updates[\"categories\"]\n            )  # exact order from caller\n\n        # Handle None values in updates - remove keys that are explicitly set to None\n        # For nested dicts like features, merge instead of overwriting to preserve existing values\n        for key, value in updates.items():\n            if value is None:\n                # Explicitly remove the key if set to None\n                updated.pop(key, None)\n            elif (\n                key == \"features\"\n                and isinstance(value, dict)\n                and isinstance(updated.get(key), dict)\n            ):\n                # Merge features dict instead of overwriting (preserves existing features)\n                updated[key].update(value)\n            else:\n                updated[key] = value\n\n        # Handle field preservation and preference settings\n        if dt == \"account\":\n            # Preserve email field if provided in updates but not already in updated data\n            if \"email\" in updates and not updated.get(\"email\"):\n                updated[\"email\"] = updates[\"email\"]\n        elif dt == \"preferences\":\n            # Preserve preference settings blocks (task_settings, checkin_settings) even when features are disabled\n            _save_user_data__preserve_preference_settings(updated, updates, user_id)\n\n        # Apply Pydantic normalization\n        _save_user_data__normalize_data(dt, updated)\n\n        # Ensure critical identity fields persist for account saves\n        if dt == \"account\":\n            try:\n                if not updated.get(\"internal_username\"):\n                    prior_username = (\n                        current.get(\"internal_username\")\n                        if isinstance(current, dict)\n                        else None\n                    )\n                    updated[\"internal_username\"] = prior_username or user_id\n            except Exception:\n                pass\n\n        # Re-apply preserved category order after normalization\n        if dt == \"preferences\" and preserve_categories_order is not None:\n            try:\n                # Keep only unique items in the original order provided by caller\n                seen = set()\n                ordered_unique = []\n                for cat in preserve_categories_order:\n                    if isinstance(cat, str) and cat and cat not in seen:\n                        seen.add(cat)\n                        ordered_unique.append(cat)\n                updated[\"categories\"] = ordered_unique\n            except Exception:\n                pass\n\n        logger.debug(\n            f\"Merge {dt}: current={current}, updates={updates}, merged={updated}\"\n        )\n        return updated\n\n    except Exception as e:\n        logger.error(f\"Error merging {dt} data for user {user_id}: {e}\")\n        return None\n\n\n@handle_errors(\"saving single data type\", default_return=False)\ndef _save_user_data__save_single_type(\n    user_id: str, dt: str, updates: dict[str, Any], auto_create: bool\n) -> bool:\n    \"\"\"\n    Save single data type with enhanced validation.\n\n    Returns:\n        bool: True if successful, False if failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id: {user_id}\")\n        return False\n\n    if not dt or not isinstance(dt, str):\n        logger.error(f\"Invalid data type: {dt}\")\n        return False\n\n    if not isinstance(updates, dict):\n        logger.error(f\"Invalid updates: {type(updates)}\")\n        return False\n    \"\"\"Save a single data type for a user.\"\"\"\n    try:\n        # Use merge function to get merged data\n        updated = _save_user_data__merge_single_type(user_id, dt, updates, auto_create)\n        if updated is None:\n            return False\n\n        # Note: Cross-file invariants are now handled in save_user_data() two-phase approach\n        # This function is kept for backward compatibility but should primarily be used\n        # through save_user_data() for proper invariant handling\n\n        logger.debug(f\"Save {dt}: updates={updates}, merged={updated}\")\n\n        from core.file_operations import save_json_data\n        from core.config import get_user_file_path\n\n        file_path = get_user_file_path(user_id, dt)\n        success = save_json_data(updated, file_path)\n        logger.debug(f\"Saved {dt} data for user {user_id}: success={success}\")\n        return success\n\n    except Exception as e:\n        logger.error(f\"Error saving {dt} data for user {user_id}: {e}\")\n        logger.error(f\"Exception type: {type(e).__name__}\")\n        logger.error(f\"Exception traceback: {traceback.format_exc()}\")\n        return False\n\n\n@handle_errors(\"updating user index\", default_return=False)\ndef _save_user_data__update_index(\n    user_id: str, result: dict[str, bool], update_index: bool\n) -> bool:\n    \"\"\"\n    Update user index with validation.\n\n    Returns:\n        bool: True if successful, False if failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id for index update: {user_id}\")\n        return False\n\n    if not isinstance(result, dict):\n        logger.error(f\"Invalid result: {type(result)}\")\n        return False\n    \"\"\"Update user index and clear cache if needed.\"\"\"\n    # Update index if at least one type succeeded\n    if update_index and any(result.values()):\n        try:\n            from core.user_data_manager import update_user_index\n\n            update_user_index(user_id)\n        except Exception as e:\n            logger.warning(\n                f\"Failed to update user index after data save for user {user_id}: {e}\"\n            )\n\n    # Clear cache if any saves succeeded\n    if any(result.values()):\n        try:\n            clear_user_caches(user_id)\n            logger.debug(f\"Cleared cache for user {user_id} after data save\")\n        except Exception as e:\n            logger.warning(f\"Failed to clear cache for user {user_id}: {e}\")\n\n\n# Explicit processing order for deterministic behavior\n_DATA_TYPE_PROCESSING_ORDER = [\n    \"account\",\n    \"preferences\",\n    \"schedules\",\n    \"context\",\n    \"messages\",\n    \"tasks\",\n]\n\n\n@handle_errors(\"checking cross-file invariants\", default_return=None)\ndef _save_user_data__check_cross_file_invariants(\n    user_id: str, merged_data: dict[str, dict[str, Any]], valid_types: list[str]\n) -> dict[str, dict[str, Any]] | None:\n    \"\"\"\n    Check and enforce cross-file invariants using in-memory merged data.\n\n    Updates merged_data in-place to maintain invariants without nested saves.\n\n    Returns:\n        Updated merged_data dict, or None if invariants check failed\n    \"\"\"\n    try:\n        # Get in-memory merged data (use provided merged_data, fallback to disk reads for types not being updated)\n        account_data = merged_data.get(\"account\")\n        preferences_data = merged_data.get(\"preferences\")\n\n        # If account is being updated, use merged data; otherwise read from disk\n        # Use auto_create=False to prevent creating default accounts when checking invariants\n        if \"account\" not in valid_types:\n            account_result = get_user_data(user_id, \"account\", auto_create=False)\n            account_data = account_result.get(\"account\", {})\n\n        # If preferences is being updated, use merged data; otherwise read from disk\n        # Use auto_create=False to prevent creating default data when checking invariants\n        if \"preferences\" not in valid_types:\n            prefs_result = get_user_data(user_id, \"preferences\", auto_create=False)\n            preferences_data = prefs_result.get(\"preferences\", {})\n\n        # Invariant 1: If preferences has categories, account.features.automated_messages must be enabled\n        if preferences_data:\n            categories_list = preferences_data.get(\"categories\", [])\n            if isinstance(categories_list, list) and len(categories_list) > 0:\n                # Ensure account_data exists - if not loaded, try to load it (with auto_create=False to avoid creating new users)\n                if not account_data:\n                    account_result = get_user_data(\n                        user_id, \"account\", auto_create=False\n                    )\n                    account_data = account_result.get(\"account\", {})\n\n                # Only apply invariant if account_data exists (user was created before)\n                if account_data:\n                    feats = (\n                        dict(account_data.get(\"features\", {}))\n                        if isinstance(account_data.get(\"features\"), dict)\n                        else {}\n                    )\n                    if feats.get(\"automated_messages\") != \"enabled\":\n                        feats[\"automated_messages\"] = \"enabled\"\n                        # Update in-memory merged data instead of triggering nested save\n                        if \"account\" in valid_types:\n                            # Account is being updated, update merged data\n                            merged_data[\"account\"][\"features\"] = feats\n                        else:\n                            # Account not being updated, but we need to update it - add to merged_data\n                            if \"account\" not in merged_data:\n                                # Deep copy account data to avoid mutating the original\n                                import copy\n\n                                merged_account = (\n                                    copy.deepcopy(account_data) if account_data else {}\n                                )\n                                # Normalize the account data\n                                _save_user_data__normalize_data(\n                                    \"account\", merged_account\n                                )\n                                merged_data[\"account\"] = merged_account\n                            # Update features after adding to merged_data to ensure the update persists\n                            if \"features\" not in merged_data[\"account\"]:\n                                merged_data[\"account\"][\"features\"] = {}\n                            merged_data[\"account\"][\"features\"].update(feats)\n\n                # Ensure schedules exist for all categories\n                try:\n                    ensure_all_categories_have_schedules(user_id, suppress_logging=True)\n                except Exception:\n                    pass\n\n        # Invariant 2: If account.features.automated_messages is disabled but preferences has categories, enable it\n        # (This is essentially the same as Invariant 1, but checking from account side)\n        # Already handled above, but keeping for clarity\n\n        return merged_data\n\n    except Exception as e:\n        logger.error(f\"Error checking cross-file invariants for user {user_id}: {e}\")\n        return None\n\n\n@handle_errors(\"merging all data types in-memory\", default_return=None)\ndef _save_user_data__merge_all_types(\n    user_id: str,\n    data_updates: dict[str, dict[str, Any]],\n    valid_types: list[str],\n    auto_create: bool,\n) -> dict[str, dict[str, Any]] | None:\n    \"\"\"\n    Phase 1: Merge all data types in-memory.\n\n    Returns:\n        Dict mapping data type to merged data, or None if merge failed\n    \"\"\"\n    merged_data = {}\n\n    for dt in valid_types:\n        updates = data_updates.get(dt, {})\n        merged = _save_user_data__merge_single_type(user_id, dt, updates, auto_create)\n        if merged is None:\n            logger.error(f\"Failed to merge {dt} for user {user_id}\")\n            return None\n        merged_data[dt] = merged\n\n    return merged_data\n\n\n@handle_errors(\"writing all data types to disk\", default_return={})\ndef _save_user_data__write_all_types(\n    user_id: str, merged_data: dict[str, dict[str, Any]], valid_types: list[str]\n) -> dict[str, bool]:\n    \"\"\"\n    Phase 2: Write all merged data types to disk atomically.\n\n    Returns:\n        Dict mapping data type to success status\n    \"\"\"\n    result = {}\n\n    for dt in valid_types:\n        if dt not in merged_data:\n            result[dt] = False\n            continue\n\n        try:\n            from core.file_operations import save_json_data\n            from core.config import get_user_file_path, get_user_data_dir\n            import os\n\n            # Ensure user directory exists before writing (race condition fix)\n            user_dir = get_user_data_dir(user_id)\n            os.makedirs(user_dir, exist_ok=True)\n\n            file_path = get_user_file_path(user_id, dt)\n            success = save_json_data(merged_data[dt], file_path)\n            result[dt] = success\n            logger.debug(f\"Wrote {dt} data for user {user_id}: success={success}\")\n        except Exception as e:\n            logger.error(f\"Error writing {dt} data for user {user_id}: {e}\")\n            result[dt] = False\n\n    return result\n\n\n@handle_errors(\"saving user data\", default_return={})\ndef save_user_data(\n    user_id: str,\n    data_updates: dict[str, dict[str, Any]],\n    auto_create: bool = True,\n    update_index: bool = True,\n    create_backup: bool = True,\n    validate_data: bool = True,\n) -> dict[str, bool]:\n    \"\"\"\n    Save user data with two-phase approach: merge/validate in Phase 1, write in Phase 2.\n\n    Implements:\n    - Two-phase save (merge/validate first, then write)\n    - In-memory cross-file invariants\n    - Explicit processing order\n    - Atomic operations (all succeed or all fail)\n    - No nested saves\n    \"\"\"\n    # Validate input parameters\n    is_valid, result, invalid_types = _save_user_data__validate_input(\n        user_id, data_updates\n    )\n    if not is_valid:\n        return result\n\n    # Get valid types to process and sort by explicit order\n    valid_types_to_process = [dt for dt in data_updates if dt not in invalid_types]\n    # Sort by explicit processing order (types not in order list go to end)\n    valid_types_to_process.sort(\n        key=lambda dt: (\n            _DATA_TYPE_PROCESSING_ORDER.index(dt)\n            if dt in _DATA_TYPE_PROCESSING_ORDER\n            else len(_DATA_TYPE_PROCESSING_ORDER)\n        )\n    )\n\n    # Check if user is new (needed for validation)\n    # For new users, check if account file exists (more reliable than directory check)\n    from core.config import get_user_data_dir, get_user_file_path\n\n    user_dir = get_user_data_dir(user_id)\n    account_file = get_user_file_path(user_id, \"account\")\n    is_new_user = not os.path.exists(account_file)\n    logger.debug(\n        f\"save_user_data: user_id={user_id}, is_new_user={is_new_user}, account_file_exists={os.path.exists(account_file)}, valid_types={valid_types_to_process}\"\n    )\n\n    # PHASE 1: Merge all types in-memory\n    merged_data = _save_user_data__merge_all_types(\n        user_id, data_updates, valid_types_to_process, auto_create\n    )\n    if merged_data is None:\n        # Merge failed, return failure for all types\n        return {dt: False for dt in valid_types_to_process}\n\n    # Validate merged data\n    if validate_data:\n        # Create temporary data_updates dict with merged data for validation\n        merged_updates = {dt: merged_data[dt] for dt in valid_types_to_process}\n        invalid_types_from_validation, validation_result = (\n            _save_user_data__validate_data(\n                user_id,\n                merged_updates,\n                valid_types_to_process,\n                validate_data,\n                is_new_user,\n            )\n        )\n\n        # Update result with validation results\n        result.update(validation_result)\n\n        # For preferences, if validation failed but we have merged data, try to normalize it\n        # Pydantic might return errors but still produce valid normalized data\n        if (\n            \"preferences\" in invalid_types_from_validation\n            and \"preferences\" in merged_data\n        ):\n            try:\n                from core.schemas import validate_preferences_dict\n\n                normalized_prefs, pref_errors = validate_preferences_dict(\n                    merged_data[\"preferences\"]\n                )\n                # If normalization succeeded (no errors or only warnings), allow it\n                if not pref_errors or (\n                    len(pref_errors) == 1\n                    and \"Invalid categories\" in str(pref_errors[0])\n                ):\n                    logger.debug(\n                        \"Preferences validation had non-critical errors, allowing save after normalization\"\n                    )\n                    merged_data[\"preferences\"] = normalized_prefs\n                    invalid_types_from_validation.remove(\"preferences\")\n                    result[\"preferences\"] = True  # Mark as valid\n            except Exception as e:\n                logger.debug(f\"Could not normalize preferences: {e}\")\n\n        # For schedules, if validation failed but we have merged data, try to normalize it\n        # Pydantic might return errors but still produce valid normalized data\n        if \"schedules\" in invalid_types_from_validation and \"schedules\" in merged_data:\n            try:\n                from core.schemas import validate_schedules_dict\n\n                normalized_schedules, schedule_errors = validate_schedules_dict(\n                    merged_data[\"schedules\"]\n                )\n                # If normalization succeeded (no errors or only warnings), allow it\n                if not schedule_errors:\n                    logger.debug(\n                        \"Schedules validation had non-critical errors, allowing save after normalization\"\n                    )\n                    merged_data[\"schedules\"] = normalized_schedules\n                    invalid_types_from_validation.remove(\"schedules\")\n                    result[\"schedules\"] = True  # Mark as valid\n            except Exception as e:\n                logger.debug(f\"Could not normalize schedules: {e}\")\n\n        # Remove invalid types from processing\n        valid_types_to_process = [\n            dt\n            for dt in valid_types_to_process\n            if dt not in invalid_types_from_validation\n        ]\n        # Remove invalid types from merged_data\n        for dt in invalid_types_from_validation:\n            merged_data.pop(dt, None)\n\n    # Check cross-file invariants using in-memory merged data\n    updated_merged_data = _save_user_data__check_cross_file_invariants(\n        user_id, merged_data, valid_types_to_process\n    )\n    if updated_merged_data is None:\n        logger.error(f\"Cross-file invariants check failed for user {user_id}\")\n        return {dt: False for dt in valid_types_to_process}\n\n    merged_data = updated_merged_data\n\n    # Update valid_types_to_process if invariants added new types (e.g., account when only preferences was updated)\n    # Only add types that were intentionally modified by invariants, not types that were only read\n    # The invariants check may add 'account' to merged_data when it wasn't originally being updated,\n    # but it should NOT add 'preferences' or other types that were only read for checking invariants\n    for dt in merged_data:\n        if dt not in valid_types_to_process:\n            # Only add account if it was added by invariants (when preferences had categories but account.features.automated_messages was disabled)\n            # Do NOT add preferences or other types that were only read for invariant checking\n            if dt == \"account\" and \"account\" not in data_updates:\n                # Account was added by invariants to enable automated_messages feature\n                valid_types_to_process.append(dt)\n            # For all other types, only add if they were in the original data_updates\n            elif dt in data_updates:\n                valid_types_to_process.append(dt)\n    # Re-sort after adding new types\n    valid_types_to_process.sort(\n        key=lambda dt: (\n            _DATA_TYPE_PROCESSING_ORDER.index(dt)\n            if dt in _DATA_TYPE_PROCESSING_ORDER\n            else len(_DATA_TYPE_PROCESSING_ORDER)\n        )\n    )\n\n    logger.debug(f\"After invariants: valid_types_to_process={valid_types_to_process}\")\n\n    # Create backup if needed (AFTER validation and invariants, so we only backup valid data)\n    _save_user_data__create_backup(user_id, valid_types_to_process, create_backup)\n\n    # PHASE 2: Write all types to disk atomically\n    write_results = _save_user_data__write_all_types(\n        user_id, merged_data, valid_types_to_process\n    )\n    result.update(write_results)\n\n    # Update index and cache\n    _save_user_data__update_index(user_id, result, update_index)\n\n    return result\n\n\n@handle_errors(\"saving user data with transaction\", default_return=False)\ndef save_user_data_transaction(\n    user_id: str, data_updates: dict[str, dict[str, Any]], auto_create: bool = True\n) -> bool:\n    \"\"\"Atomic wrapper for user data updates.\"\"\"\n    if not user_id or not data_updates:\n        return False\n    try:\n        from core.user_data_manager import UserDataManager\n\n        backup_path = UserDataManager().backup_user_data(user_id)\n        logger.info(f\"Created backup before transaction: {backup_path}\")\n    except Exception as e:\n        logger.warning(f\"Failed to create backup before transaction: {e}\")\n\n    result = save_user_data(\n        user_id=user_id,\n        data_updates=data_updates,\n        auto_create=auto_create,\n        update_index=False,\n        create_backup=False,\n        validate_data=True,\n    )\n    success = all(result.values())\n    if success:\n        try:\n            from core.user_data_manager import update_user_index\n\n            update_user_index(user_id)\n        except Exception as e:\n            logger.error(f\"Transaction succeeded but failed to update index: {e}\")\n            success = False\n    else:\n        failed = [dt for dt, ok in result.items() if not ok]\n        logger.error(f\"Transaction failed for user {user_id}. Failed types: {failed}\")\n    return success\n\n\n# ---------------------------------------------------------------------------\n# CORE USER MANAGEMENT FUNCTIONS (migrated)\n# ---------------------------------------------------------------------------\n\n\n@handle_errors(\"getting all user ids\", default_return=[])\ndef get_all_user_ids() -> list[str]:\n    \"\"\"Get all user IDs from the system.\"\"\"\n    from core.config import USER_INFO_DIR_PATH\n\n    users_dir = Path(USER_INFO_DIR_PATH)\n    if not users_dir.exists():\n        return []\n\n    user_ids = []\n    for item in users_dir.iterdir():\n        # item from iterdir() is already a Path object relative to users_dir\n        # Use it directly instead of combining with users_dir to avoid double paths\n        if item.is_dir():\n            # Check if this directory has the new structure\n            account_file = item / \"account.json\"\n            if account_file.exists():\n                user_ids.append(item.name)\n\n    return user_ids\n\n\n@handle_errors(\"creating new user\", default_return=None)\ndef create_new_user(user_data: dict[str, Any]) -> str | None:\n    \"\"\"Create a new user with the new data structure.\"\"\"\n    user_id = str(uuid.uuid4())\n\n    # Use a single canonical timestamp for this creation operation so all \"created\" fields match.\n    created_ts = now_timestamp_full()\n\n    # Create account data\n    account_data = {\n        \"user_id\": user_id,\n        \"internal_username\": user_data.get(\"internal_username\", \"\"),\n        \"account_status\": \"active\",\n        \"chat_id\": user_data.get(\"chat_id\", \"\"),\n        \"phone\": user_data.get(\"phone\", \"\"),\n        \"email\": user_data.get(\"email\", \"\"),\n        \"discord_user_id\": user_data.get(\"discord_user_id\", \"\"),\n        \"created_at\": created_ts,\n        \"updated_at\": created_ts,  # account schema uses updated_at\n        \"features\": {\n            # Check for explicit messages_enabled flag first, then fall back to categories check (for backward compatibility)\n            \"automated_messages\": (\n                \"enabled\"\n                if (\n                    user_data.get(\"messages_enabled\", False)\n                    or (\n                        user_data.get(\"categories\")\n                        and len(user_data.get(\"categories\", [])) > 0\n                    )\n                )\n                else \"disabled\"\n            ),\n            \"checkins\": (\n                \"enabled\"\n                if user_data.get(\"checkin_settings\", {}).get(\"enabled\", False)\n                else \"disabled\"\n            ),\n            \"task_management\": (\n                \"enabled\"\n                if user_data.get(\"task_settings\", {}).get(\"enabled\", False)\n                else \"disabled\"\n            ),\n        },\n        \"timezone\": user_data.get(\"timezone\", \"\"),\n    }\n\n    # Create preferences data\n    preferences_data = {\n        \"categories\": user_data.get(\"categories\", []),\n        \"channel\": {\"type\": user_data.get(\"channel\", {}).get(\"type\", \"email\")},\n        \"checkin_settings\": user_data.get(\"checkin_settings\", {}),\n        \"task_settings\": user_data.get(\"task_settings\", {}),\n    }\n\n    # Remove redundant enabled flags from preferences since they're in account.json features\n    if (\n        \"checkin_settings\" in preferences_data\n        and \"enabled\" in preferences_data[\"checkin_settings\"]\n    ):\n        del preferences_data[\"checkin_settings\"][\"enabled\"]\n    if (\n        \"task_settings\" in preferences_data\n        and \"enabled\" in preferences_data[\"task_settings\"]\n    ):\n        del preferences_data[\"task_settings\"][\"enabled\"]\n\n    # Create user context data\n    context_data = {\n        \"preferred_name\": user_data.get(\"preferred_name\", \"\"),\n        \"gender_identity\": user_data.get(\"gender_identity\", []),\n        \"date_of_birth\": user_data.get(\"date_of_birth\", \"\"),\n        \"custom_fields\": {\n            \"reminders_needed\": user_data.get(\"reminders_needed\", []),\n            \"health_conditions\": user_data.get(\"custom_fields\", {}).get(\n                \"health_conditions\", []\n            ),\n            \"medications_treatments\": user_data.get(\"custom_fields\", {}).get(\n                \"medications_treatments\", []\n            ),\n            \"allergies_sensitivities\": user_data.get(\"custom_fields\", {}).get(\n                \"allergies_sensitivities\", []\n            ),\n        },\n        \"interests\": user_data.get(\"interests\", []),\n        \"goals\": user_data.get(\"goals\", []),\n        \"loved_ones\": user_data.get(\"loved_ones\", []),\n        \"activities_for_encouragement\": user_data.get(\n            \"activities_for_encouragement\", []\n        ),\n        \"notes_for_ai\": user_data.get(\"notes_for_ai\", []),\n        \"created_at\": created_ts,\n        \"last_updated\": created_ts,  # context schema uses last_updated\n    }\n\n    # Save all data using centralized save_user_data\n    save_result = save_user_data(\n        user_id,\n        {\n            \"account\": account_data,\n            \"preferences\": preferences_data,\n            \"context\": context_data,\n        },\n    )\n\n    # Check if save was successful\n    if not all(save_result.values()):\n        logger.error(f\"Failed to save user data for {user_id}: {save_result}\")\n        return None\n\n    # Create default schedule periods for initial categories\n    categories = user_data.get(\"categories\", [])\n    for category in categories:\n        ensure_category_has_default_schedule(user_id, category)\n\n    # Update user index\n    try:\n        from core.user_data_manager import update_user_index\n\n        update_user_index(user_id)\n    except Exception as e:\n        logger.warning(f\"Failed to update user index for new user {user_id}: {e}\")\n\n    logger.info(\n        f\"Created new user: {user_id} ({user_data.get('internal_username', '')})\"\n    )\n    return user_id\n\n\n@handle_errors(\"getting user categories\", default_return=[])\ndef get_user_categories(user_id: str) -> list[str]:\n    \"\"\"Get user's message categories using centralized data access.\"\"\"\n    user_data = get_user_data(user_id, \"preferences\")\n    if isinstance(user_data, dict):\n        preferences = user_data.get(\"preferences\", {})\n        if isinstance(preferences, dict):\n            categories = preferences.get(\"categories\", [])\n            if isinstance(categories, list):\n                return categories\n    elif isinstance(user_data, list):\n        return user_data\n    return []\n\n\n@handle_errors(\"getting user data with metadata\", default_return={})\ndef get_user_data_with_metadata(\n    user_id: str, data_types: str | list[str] = \"all\"\n) -> dict[str, Any]:\n    \"\"\"Get user data with file metadata using centralized system.\"\"\"\n    return get_user_data(user_id, data_types, include_metadata=True)\n\n\nPREDEFINED_OPTIONS = {\n    \"gender_identity\": [\n        \"Male\",\n        \"Female\",\n        \"Non-binary\",\n        \"Genderfluid\",\n        \"Agender\",\n        \"Bigender\",\n        \"Demiboy\",\n        \"Demigirl\",\n        \"Genderqueer\",\n        \"Two-spirit\",\n        \"Other\",\n        \"Prefer not to say\",\n    ],\n    \"health_conditions\": [\n        \"ADHD\",\n        \"Anxiety\",\n        \"Depression\",\n        \"Bipolar Disorder\",\n        \"PTSD\",\n        \"OCD\",\n        \"Autism\",\n        \"Chronic Pain\",\n        \"Diabetes\",\n        \"Asthma\",\n        \"Sleep Disorders\",\n        \"Eating Disorders\",\n        \"Substance Use Disorder\",\n    ],\n    \"medications_treatments\": [\n        \"Antidepressant\",\n        \"Anti-anxiety medication\",\n        \"Stimulant for ADHD\",\n        \"Mood stabilizer\",\n        \"Antipsychotic\",\n        \"Sleep medication\",\n        \"Therapy\",\n        \"Counseling\",\n        \"Support groups\",\n        \"Exercise\",\n        \"Meditation\",\n        \"Yoga\",\n        \"CPAP\",\n        \"Inhaler\",\n        \"Insulin\",\n    ],\n    \"reminders_needed\": [\n        \"medications_treatments\",\n        \"hydration\",\n        \"movement/stretch breaks\",\n        \"healthy meals/snacks\",\n        \"mental health check-ins\",\n        \"appointments\",\n        \"exercise\",\n        \"sleep schedule\",\n        \"self-care activities\",\n    ],\n    \"loved_one_types\": [\n        \"human\",\n        \"dog\",\n        \"cat\",\n        \"bird\",\n        \"fish\",\n        \"reptile\",\n        \"horse\",\n        \"rabbit\",\n        \"hamster\",\n        \"guinea pig\",\n        \"ferret\",\n        \"other\",\n    ],\n    \"relationship_types\": [\n        \"partner\",\n        \"spouse\",\n        \"parent\",\n        \"child\",\n        \"sibling\",\n        \"friend\",\n        \"roommate\",\n        \"colleague\",\n        \"therapist\",\n        \"doctor\",\n        \"teacher\",\n    ],\n    \"interests\": [\n        \"Reading\",\n        \"Writing\",\n        \"Gaming\",\n        \"Music\",\n        \"Art\",\n        \"Cooking\",\n        \"Baking\",\n        \"Gardening\",\n        \"Hiking\",\n        \"Swimming\",\n        \"Running\",\n        \"Yoga\",\n        \"Meditation\",\n        \"Photography\",\n        \"Crafts\",\n        \"Knitting\",\n        \"Painting\",\n        \"Drawing\",\n        \"Sewing\",\n        \"Woodworking\",\n        \"Programming\",\n        \"Math\",\n        \"Science\",\n        \"History\",\n        \"Languages\",\n        \"Travel\",\n    ],\n    \"activities_for_encouragement\": [\n        \"exercise\",\n        \"healthy eating\",\n        \"sleep hygiene\",\n        \"social activities\",\n        \"hobbies\",\n        \"work/projects\",\n        \"self-care\",\n        \"therapy appointments\",\n        \"medication adherence\",\n        \"stress management\",\n    ],\n}\n\nTIMEZONE_OPTIONS = [\n    \"America/New_York\",\n    \"America/Chicago\",\n    \"America/Denver\",\n    \"America/Los_Angeles\",\n    \"America/Regina\",\n    \"America/Toronto\",\n    \"America/Vancouver\",\n    \"America/Edmonton\",\n    \"America/Port_of_Spain\",\n    \"Europe/London\",\n    \"Europe/Paris\",\n    \"Europe/Berlin\",\n    \"Europe/Rome\",\n    \"Asia/Tokyo\",\n    \"Asia/Shanghai\",\n    \"Asia/Kolkata\",\n    \"Australia/Sydney\",\n    \"Pacific/Auckland\",\n    \"UTC\",\n]\n\n_PRESETS_CACHE: dict[str, list[str]] | None = None\n\n\n@handle_errors(\"loading presets JSON\", default_return=PREDEFINED_OPTIONS)\ndef _load_presets_json() -> dict[str, list[str]]:\n    \"\"\"Load presets from resources/presets.json (cached).\"\"\"\n    global _PRESETS_CACHE\n    if _PRESETS_CACHE is not None:\n        return _PRESETS_CACHE\n\n    import json\n\n    presets_path = Path(__file__).parent.parent / \"resources\" / \"presets.json\"\n    try:\n        with open(presets_path, encoding=\"utf-8\") as f:\n            _PRESETS_CACHE = json.load(f)\n    except FileNotFoundError:\n        logger.warning(\"presets.json not found - falling back to hard-coded options\")\n        _PRESETS_CACHE = PREDEFINED_OPTIONS\n    return _PRESETS_CACHE\n\n\n@handle_errors(\"getting predefined options\", default_return=[])\ndef get_predefined_options(field: str) -> list[str]:\n    \"\"\"Return predefined options for a personalization field.\"\"\"\n    presets = _load_presets_json()\n    return presets.get(field, [])\n\n\n@handle_errors(\"getting timezone options\", default_return=[])\ndef get_timezone_options() -> list[str]:\n    \"\"\"Get timezone options.\"\"\"\n    try:\n        import pytz\n\n        return pytz.all_timezones\n    except ImportError:\n        return TIMEZONE_OPTIONS\n\n\n@handle_errors(\"getting user id by internal username\", default_return=None)\ndef _get_user_id_by_identifier__by_internal_username(\n    internal_username: str,\n) -> str | None:\n    \"\"\"Helper function: Get user ID by internal username using the user index for fast lookup.\"\"\"\n    if not internal_username:\n        return None\n\n    try:\n        from core.config import BASE_DATA_DIR\n        from core.file_locking import safe_json_read\n\n        index_file = str(Path(BASE_DATA_DIR) / \"user_index.json\")\n        index_data = safe_json_read(index_file, default={})\n        if internal_username in index_data:\n            return index_data[internal_username]\n    except Exception as e:\n        logger.warning(\n            f\"Error looking up user by internal_username '{internal_username}' in index: {e}\"\n        )\n\n    logger.debug(\n        f\"Falling back to directory scan for internal_username '{internal_username}'\"\n    )\n    user_ids = get_all_user_ids()\n    for user_id in user_ids:\n        user_data_result = get_user_data(user_id, \"account\")\n        account_data = user_data_result.get(\"account\")\n        if account_data and account_data.get(\"internal_username\") == internal_username:\n            return user_id\n    return None\n\n\n@handle_errors(\"getting user id by email\", default_return=None)\ndef _get_user_id_by_identifier__by_email(email: str) -> str | None:\n    \"\"\"Helper function: Get user ID by email using the user index for fast lookup.\"\"\"\n    if not email:\n        return None\n\n    try:\n        from core.config import BASE_DATA_DIR\n        from core.file_locking import safe_json_read\n\n        index_file = str(Path(BASE_DATA_DIR) / \"user_index.json\")\n        index_data = safe_json_read(index_file, default={})\n        email_key = f\"email:{email}\"\n        if email_key in index_data:\n            return index_data[email_key]\n    except Exception as e:\n        logger.warning(f\"Error looking up user by email '{email}' in index: {e}\")\n\n    logger.debug(f\"Falling back to directory scan for email '{email}'\")\n    user_ids = get_all_user_ids()\n    for user_id in user_ids:\n        user_data_result = get_user_data(user_id, \"account\")\n        account_data = user_data_result.get(\"account\")\n        if account_data and account_data.get(\"email\") == email:\n            return user_id\n    return None\n\n\n@handle_errors(\"getting user id by phone\", default_return=None)\ndef _get_user_id_by_identifier__by_phone(phone: str) -> str | None:\n    \"\"\"Helper function: Get user ID by phone using the user index for fast lookup.\"\"\"\n    if not phone:\n        return None\n\n    try:\n        from core.config import BASE_DATA_DIR\n        from core.file_locking import safe_json_read\n\n        index_file = str(Path(BASE_DATA_DIR) / \"user_index.json\")\n        index_data = safe_json_read(index_file, default={})\n        phone_key = f\"phone:{phone}\"\n        if phone_key in index_data:\n            return index_data[phone_key]\n    except Exception as e:\n        logger.warning(f\"Error looking up user by phone '{phone}' in index: {e}\")\n\n    logger.debug(f\"Falling back to directory scan for phone '{phone}'\")\n    user_ids = get_all_user_ids()\n    for user_id in user_ids:\n        user_data_result = get_user_data(user_id, \"account\")\n        account_data = user_data_result.get(\"account\")\n        if account_data and account_data.get(\"phone\") == phone:\n            return user_id\n    return None\n\n\n@handle_errors(\"getting user id by chat id\", default_return=None)\ndef _get_user_id_by_identifier__by_chat_id(chat_id: str) -> str | None:\n    \"\"\"Helper function: Get user ID by chat ID.\"\"\"\n    if not chat_id:\n        return None\n\n    user_ids = get_all_user_ids()\n    for user_id in user_ids:\n        user_data_result = get_user_data(user_id, \"account\")\n        account_data = user_data_result.get(\"account\")\n        if account_data and account_data.get(\"chat_id\") == chat_id:\n            return user_id\n    return None\n\n\n@handle_errors(\"getting user id by discord user id\", default_return=None)\ndef _get_user_id_by_identifier__by_discord_user_id(\n    discord_user_id: str,\n) -> str | None:\n    \"\"\"Helper function: Get user ID by Discord user ID using the user index for fast lookup.\"\"\"\n    if not discord_user_id:\n        return None\n\n    try:\n        from core.config import BASE_DATA_DIR\n        from core.file_locking import safe_json_read\n\n        index_file = str(Path(BASE_DATA_DIR) / \"user_index.json\")\n        index_data = safe_json_read(index_file, default={})\n        discord_key = f\"discord:{discord_user_id}\"\n        if discord_key in index_data:\n            return index_data[discord_key]\n    except Exception as e:\n        logger.warning(\n            f\"Error looking up user by discord_user_id '{discord_user_id}' in index: {e}\"\n        )\n\n    logger.debug(\n        f\"Falling back to directory scan for discord_user_id '{discord_user_id}'\"\n    )\n    user_ids = get_all_user_ids()\n    for user_id in user_ids:\n        user_data_result = get_user_data(user_id, \"account\")\n        account_data = user_data_result.get(\"account\")\n        if account_data:\n            stored_discord_id = account_data.get(\"discord_user_id\", \"\")\n            if str(stored_discord_id) == str(discord_user_id):\n                return user_id\n    return None\n\n\n@handle_errors(\"getting user id by identifier\", default_return=None)\ndef get_user_id_by_identifier(identifier: str) -> str | None:\n    \"\"\"\n    Get user ID by any identifier (internal_username, email, discord_user_id, phone).\n    \"\"\"\n    if not identifier:\n        return None\n\n    try:\n        from core.config import BASE_DATA_DIR\n        from core.file_locking import safe_json_read\n\n        index_file = str(Path(BASE_DATA_DIR) / \"user_index.json\")\n        index_data = safe_json_read(index_file, default={})\n        if identifier in index_data:\n            mapped = index_data[identifier]\n            if isinstance(mapped, str) and mapped:\n                return mapped\n            try:\n                users_dir = str(Path(BASE_DATA_DIR) / \"users\" / identifier)\n                if os.path.isdir(users_dir):\n                    return identifier\n            except Exception:\n                pass\n\n        email_key = f\"email:{identifier}\"\n        if email_key in index_data:\n            return index_data[email_key]\n\n        discord_key = f\"discord:{identifier}\"\n        if discord_key in index_data:\n            return index_data[discord_key]\n\n        phone_key = f\"phone:{identifier}\"\n        if phone_key in index_data:\n            return index_data[phone_key]\n    except Exception as e:\n        logger.warning(\n            f\"Error looking up user by identifier '{identifier}' in index: {e}\"\n        )\n\n    result = _get_user_id_by_identifier__by_internal_username(identifier)\n    if result:\n        return result\n\n    result = _get_user_id_by_identifier__by_email(identifier)\n    if result:\n        return result\n\n    result = _get_user_id_by_identifier__by_discord_user_id(identifier)\n    if result:\n        return result\n\n    result = _get_user_id_by_identifier__by_phone(identifier)\n    if result:\n        return result\n\n    return None\n\n\n@handle_errors(\"updating user schedules (centralised)\", default_return=False)\ndef update_user_schedules(user_id: str, schedules_data: dict[str, Any]) -> bool:\n    \"\"\"\n    Update user schedules with validation.\n\n    Returns:\n        bool: True if successful, False if failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id: {user_id}\")\n        return False\n\n    if not user_id.strip():\n        logger.error(\"Empty user_id provided\")\n        return False\n\n    if not isinstance(schedules_data, dict):\n        logger.error(f\"Invalid schedules_data: {type(schedules_data)}\")\n        return False\n    \"\"\"Persist a complete schedules dict for *user_id*.\"\"\"\n    result = save_user_data(user_id, {\"schedules\": schedules_data})\n    return result.get(\"schedules\", False)\n\n\n# ---------------------------------------------------------------------------\n# HIGH-LEVEL UPDATE HELPERS\n# ---------------------------------------------------------------------------\n\n\n@handle_errors(\"updating user account (centralised)\", default_return=False)\ndef update_user_account(\n    user_id: str, updates: dict[str, Any], *, auto_create: bool = True\n) -> bool:\n    \"\"\"\n    Update user account with validation.\n\n    Returns:\n        bool: True if successful, False if failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id: {user_id}\")\n        return False\n\n    if not user_id.strip():\n        logger.error(\"Empty user_id provided\")\n        return False\n\n    if not isinstance(updates, dict):\n        logger.error(f\"Invalid updates: {type(updates)}\")\n        return False\n    \"\"\"Update (part of) a user\u2019s *account.json* file.\n\n    This is a thin convenience wrapper around :pyfunc:`save_user_data` that\n    scopes *updates* to the ``account`` data-type.\n    \"\"\"\n    if not user_id:\n        logger.error(\"update_user_account called with None user_id\")\n        return False\n\n    result = save_user_data(user_id, {\"account\": updates}, auto_create=auto_create)\n    return result.get(\"account\", False)\n\n\n@handle_errors(\"updating user preferences (centralised)\", default_return=False)\ndef update_user_preferences(\n    user_id: str, updates: dict[str, Any], *, auto_create: bool = True\n) -> bool:\n    \"\"\"\n    Update user preferences with validation.\n\n    Returns:\n        bool: True if successful, False if failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id: {user_id}\")\n        return False\n\n    if not user_id.strip():\n        logger.error(\"Empty user_id provided\")\n        return False\n\n    if not isinstance(updates, dict):\n        logger.error(f\"Invalid updates: {type(updates)}\")\n        return False\n    \"\"\"Update *preferences.json*.\n\n    Includes the extra bookkeeping originally implemented in the legacy\n    user management module (default schedule creation\n    for new categories, message-file creation, etc.) so behaviour remains\n    unchanged.\n    \"\"\"\n    if not user_id:\n        logger.error(\"update_user_preferences called with None user_id\")\n        return False\n\n    # When auto_create is False, ensure the user directory exists before proceeding\n    if not auto_create:\n        try:\n            from core.config import get_user_data_dir as _get_user_data_dir\n\n            if not os.path.exists(_get_user_data_dir(user_id)):\n                return False\n        except Exception:\n            return False\n\n    # -------------------------------------------------------------------\n    # Extra category bookkeeping (imported lazily to avoid circular deps)\n    # -------------------------------------------------------------------\n    if \"categories\" in updates:\n        try:\n            from core.message_management import ensure_user_message_files\n\n            preferences_data = get_user_data(user_id, \"preferences\")\n            if preferences_data is None:\n                logger.warning(\n                    f\"Could not load or create preferences for user {user_id}\"\n                )\n            else:\n                # get_user_data returns {\"preferences\": {...}}, so we need to access the nested dict\n                prefs_dict = (\n                    preferences_data.get(\"preferences\", {})\n                    if isinstance(preferences_data, dict)\n                    else {}\n                )\n                old_categories = set(\n                    prefs_dict.get(\"categories\", [])\n                    if isinstance(prefs_dict, dict)\n                    else []\n                )\n                new_categories = set(updates[\"categories\"])\n                added_categories = new_categories - old_categories\n\n                if added_categories:\n                    logger.info(\n                        f\"Categories update detected for user {user_id}: added={added_categories}\"\n                    )\n\n                # Create default schedules for any new categories\n                for category in added_categories:\n                    ensure_category_has_default_schedule(user_id, category)\n\n                # Double-check all categories have schedules\n                ensure_all_categories_have_schedules(user_id, suppress_logging=True)\n\n                # Ensure message files exist for newly added categories\n                if added_categories:\n                    try:\n                        result_files = ensure_user_message_files(\n                            user_id, list(added_categories)\n                        )\n                        if result_files.get(\"success\"):\n                            logger.info(\n                                f\"Category update for user {user_id}: created {result_files.get('files_created')} message files for {len(added_categories)} new categories (directory_created={result_files.get('directory_created')})\"\n                            )\n                        else:\n                            logger.warning(\n                                f\"Category update for user {user_id}: message file creation partially failed - created {result_files.get('files_created')}/{len(added_categories)} files\"\n                            )\n                    except Exception as e:\n                        logger.error(\n                            f\"Error creating message files for user {user_id} after category update: {e}\",\n                            exc_info=True,\n                        )\n                else:\n                    # Even if no categories were added, ensure all current categories have message files\n                    # This is a safeguard in case files were deleted or never created\n                    if new_categories:\n                        try:\n                            result_files = ensure_user_message_files(\n                                user_id, list(new_categories)\n                            )\n                            if result_files.get(\"files_created\", 0) > 0:\n                                logger.info(\n                                    f\"Category update for user {user_id}: created {result_files.get('files_created')} missing message files for existing categories\"\n                                )\n                        except Exception as e:\n                            logger.warning(\n                                f\"Error ensuring message files for user {user_id} existing categories: {e}\"\n                            )\n                # When categories exist, ensure automated_messages feature is enabled for discoverability\n                # Handled by cross-file invariants in save_user_data() two-phase approach\n                pass\n        except Exception as err:\n            logger.warning(\n                f\"Category bookkeeping skipped for user {user_id} due to import error: {err}\"\n            )\n\n    # -------------------------------------------------------------------\n    # Persist updates via the central save path\n    # -------------------------------------------------------------------\n    # When reading current state inside save flow, pass through the caller's auto_create\n    # to avoid synthesizing defaults for nonexistent users/files under strict tests\n    # Cross-file invariants will automatically ensure account.features.automated_messages is enabled\n    # if preferences has categories, without triggering nested saves\n    result = save_user_data(user_id, {\"preferences\": updates}, auto_create=auto_create)\n    return result.get(\"preferences\", False)\n\n\n@handle_errors(\"updating user context (centralised)\", default_return=False)\ndef update_user_context(\n    user_id: str, updates: dict[str, Any], *, auto_create: bool = True\n) -> bool:\n    \"\"\"\n    Update user context with validation.\n\n    Returns:\n        bool: True if successful, False if failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id: {user_id}\")\n        return False\n\n    if not user_id.strip():\n        logger.error(\"Empty user_id provided\")\n        return False\n\n    if not isinstance(updates, dict):\n        logger.error(f\"Invalid updates: {type(updates)}\")\n        return False\n    \"\"\"Update *user_context.json* for the given user.\"\"\"\n    if not user_id:\n        logger.error(\"update_user_context called with None user_id\")\n        return False\n\n    result = save_user_data(user_id, {\"context\": updates}, auto_create=auto_create)\n    return result.get(\"context\", False)\n\n\n@handle_errors(\"updating channel preferences (centralised)\", default_return=False)\ndef update_channel_preferences(\n    user_id: str, updates: dict[str, Any], *, auto_create: bool = True\n) -> bool:\n    \"\"\"\n    Update channel preferences with validation.\n\n    Returns:\n        bool: True if successful, False if failed\n    \"\"\"\n    # Validate inputs\n    if not user_id or not isinstance(user_id, str):\n        logger.error(f\"Invalid user_id: {user_id}\")\n        return False\n\n    if not user_id.strip():\n        logger.error(\"Empty user_id provided\")\n        return False\n\n    if not isinstance(updates, dict):\n        logger.error(f\"Invalid updates: {type(updates)}\")\n        return False\n    \"\"\"Specialised helper \u2013 update only the *preferences.channel* subtree.\"\"\"\n    if not user_id:\n        logger.error(\"update_channel_preferences called with None user_id\")\n        return False\n\n    result = save_user_data(user_id, {\"preferences\": updates}, auto_create=auto_create)\n    return result.get(\"preferences\", False)\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1765,
                    "line_content": "# This function is kept for backward compatibility but should primarily be used",
                    "start": 69019,
                    "end": 69041
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 2251,
                    "line_content": "# Check for explicit messages_enabled flag first, then fall back to categories check (for backward compatibility)",
                    "start": 88822,
                    "end": 88844
                  }
                ]
              ],
              [
                "core\\user_data_validation.py",
                "\"\"\"\nUser Data Validation utilities for MHM.\nThis module centralizes all rules for validating user account, preferences,\ncontext, and schedules data.\n\"\"\"\n\nimport re\nimport os\nfrom typing import Dict, Any, Tuple, List, Optional\nfrom core.logger import get_component_logger\nfrom core.error_handling import handle_errors\nfrom core.time_utilities import DATE_ONLY, TIME_ONLY_MINUTE\n\nlogger = get_component_logger(\"main\")\nvalidation_logger = get_component_logger(\"user_activity\")\n\n# ---------------------------------------------------------------------------\n# Primitive validators\n# ---------------------------------------------------------------------------\n\n\n@handle_errors(\"validating email\", default_return=False)\ndef is_valid_email(email):\n    if not email:\n        logger.debug(\"Email validation failed: empty email provided\")\n        return False\n    pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n    is_valid = bool(re.match(pattern, email))\n    if is_valid:\n        logger.debug(f\"Email validation passed: {email}\")\n    else:\n        logger.warning(f\"Email validation failed: invalid format for '{email}'\")\n    return is_valid\n\n\n@handle_errors(\"validating phone\", default_return=False)\ndef is_valid_phone(phone):\n    if not phone:\n        logger.debug(\"Phone validation failed: empty phone provided\")\n        return False\n    cleaned = re.sub(r\"[\\s\\-\\(\\)\\.]\", \"\", phone)\n    is_valid = cleaned.isdigit() and len(cleaned) >= 10\n    if is_valid:\n        logger.debug(f\"Phone validation passed: {phone}\")\n    else:\n        logger.warning(\n            f\"Phone validation failed: invalid format for '{phone}' (cleaned: '{cleaned}')\"\n        )\n    return is_valid\n\n\n@handle_errors(\"validating Discord ID\", default_return=False)\ndef is_valid_discord_id(discord_id: str) -> bool:\n    \"\"\"\n    Validate Discord user ID format.\n\n    Discord user IDs are snowflakes (numeric IDs) that are 17-19 digits long.\n    Empty strings are allowed (Discord ID is optional).\n\n    Args:\n        discord_id: The Discord user ID to validate\n\n    Returns:\n        bool: True if valid Discord ID format or empty, False otherwise\n    \"\"\"\n    # Handle None explicitly\n    if discord_id is None:\n        logger.warning(\"Discord ID validation failed: None value provided\")\n        return False\n\n    # Empty string is allowed (optional field)\n    if discord_id == \"\":\n        return True\n\n    if not isinstance(discord_id, str):\n        logger.warning(\n            f\"Discord ID validation failed: expected string, got {type(discord_id).__name__}\"\n        )\n        return False\n\n    # Remove whitespace\n    cleaned = discord_id.strip()\n\n    # Discord snowflake IDs are 17-19 digit numbers\n    # Must be all digits and between 17-19 characters\n    if cleaned.isdigit() and 17 <= len(cleaned) <= 19:\n        logger.debug(f\"Discord ID validation passed: {cleaned}\")\n        return True\n    else:\n        logger.warning(\n            f\"Discord ID validation failed: invalid format for '{discord_id}' (must be 17-19 digit number)\"\n        )\n        return False\n\n\n@handle_errors(\"validating time format\", default_return=False)\ndef validate_schedule_periods__validate_time_format(time_str: str) -> bool:\n    if not time_str:\n        logger.debug(\"Time format validation failed: empty time provided\")\n        return False\n    pattern = r\"^([01]?[0-9]|2[0-3]):[0-5][0-9]$\"\n    is_valid = bool(re.match(pattern, time_str))\n    if is_valid:\n        logger.debug(f\"Time format validation passed: {time_str}\")\n    else:\n        logger.warning(\n            f\"Time format validation failed: invalid format for '{time_str}'\"\n        )\n    return is_valid\n\n\n# ---------------------------------------------------------------------------\n# General string validation helpers\n# ---------------------------------------------------------------------------\n\n\n@handle_errors(\"validating string length\", default_return=False)\ndef is_valid_string_length(\n    text: str | None,\n    max_length: int,\n    field_name: str = \"string\",\n    allow_none: bool = False,\n) -> bool:\n    \"\"\"\n    Validate that a string is within the specified maximum length.\n\n    Args:\n        text: String to validate (can be None if allow_none=True)\n        max_length: Maximum allowed length\n        field_name: Name of the field being validated (for error messages)\n        allow_none: Whether None values are allowed\n\n    Returns:\n        True if string length is valid, False otherwise\n    \"\"\"\n    if text is None:\n        return allow_none\n\n    if not isinstance(text, str):\n        logger.warning(f\"{field_name} must be a string, got {type(text).__name__}\")\n        return False\n\n    text = text.strip()\n\n    if len(text) > max_length:\n        logger.warning(\n            f\"{field_name} exceeds maximum length of {max_length} characters\"\n        )\n        return False\n\n    return True\n\n\n@handle_errors(\"validating category name\", default_return=False)\ndef is_valid_category_name(\n    name: str | None,\n    max_length: int = 50,\n    field_name: str = \"category\",\n    allow_none: bool = True,\n) -> bool:\n    \"\"\"\n    Validate that a category/group name is valid.\n\n    Category names should be simple identifiers: alphanumeric, spaces, hyphens, underscores.\n    This is used for grouping/categorizing items (e.g., notebook groups, task categories).\n\n    Args:\n        name: Category name to validate (can be None if allow_none=True)\n        max_length: Maximum allowed length (default: 50)\n        field_name: Name of the field being validated (for error messages)\n        allow_none: Whether None values are allowed (default: True)\n\n    Returns:\n        True if category name is valid, False otherwise\n    \"\"\"\n    if name is None:\n        return allow_none\n\n    if not isinstance(name, str):\n        logger.warning(f\"{field_name} must be a string, got {type(name).__name__}\")\n        return False\n\n    name = name.strip()\n\n    if not name:\n        logger.warning(f\"{field_name} cannot be empty (use None instead)\")\n        return False\n\n    if len(name) > max_length:\n        logger.warning(\n            f\"{field_name} exceeds maximum length of {max_length} characters\"\n        )\n        return False\n\n    # Category names should be simple identifiers (alphanumeric, spaces, hyphens, underscores)\n    if not re.match(r\"^[a-zA-Z0-9\\s\\-_]+$\", name):\n        logger.warning(\n            f\"{field_name} contains invalid characters. Only alphanumeric, spaces, hyphens, and underscores are allowed.\"\n        )\n        return False\n\n    return True\n\n\n@handle_errors(\"converting to title case\", default_return=\"\")\ndef _shared__title_case(text: str) -> str:\n    \"\"\"Convert text to title case with special handling for technical terms.\"\"\"\n    if text is None:\n        return None\n    if not text:\n        return \"\"\n\n    # Special words that should be all uppercase\n    special_words = {\n        \"ai\",\n        \"api\",\n        \"ui\",\n        \"ux\",\n        \"mhm\",\n        \"id\",\n        \"url\",\n        \"http\",\n        \"https\",\n        \"json\",\n        \"xml\",\n        \"yaml\",\n        \"toml\",\n        \"ini\",\n        \"cfg\",\n        \"log\",\n        \"tmp\",\n        \"temp\",\n        \"etc\",\n        \"usr\",\n        \"var\",\n        \"bin\",\n        \"lib\",\n        \"src\",\n        \"doc\",\n        \"docs\",\n        \"test\",\n        \"backup\",\n        \"config\",\n        \"data\",\n        \"files\",\n        \"images\",\n        \"media\",\n        \"audio\",\n        \"video\",\n        \"photos\",\n        \"downloads\",\n        \"uploads\",\n        \"cache\",\n        \"logs\",\n        \"html\",\n        \"css\",\n        \"js\",\n        \"asp\",\n        \"jsp\",\n    }\n\n    # Special case mappings\n    special_mappings = {\"dotnet\": \".NET\", \"c++\": \"C++\", \"c#\": \"C#\"}\n\n    # Small words that should stay lowercase (except at beginning/end)\n    small_words = {\n        \"a\",\n        \"an\",\n        \"and\",\n        \"as\",\n        \"at\",\n        \"but\",\n        \"by\",\n        \"for\",\n        \"if\",\n        \"in\",\n        \"is\",\n        \"it\",\n        \"of\",\n        \"on\",\n        \"or\",\n        \"the\",\n        \"to\",\n        \"up\",\n        \"via\",\n    }\n\n    words = text.lower().split()\n    result = []\n\n    for i, word in enumerate(words):\n        # Check special mappings first\n        if word in special_mappings:\n            result.append(special_mappings[word])\n        # Check if it's a special technical word that should be uppercase\n        elif word in special_words:\n            result.append(word.upper())\n        # First/last word or not a small word - capitalize normally\n        elif i == 0 or i == len(words) - 1 or word not in small_words:\n            result.append(word.capitalize())\n        # Small word in middle - keep lowercase\n        else:\n            result.append(word)\n\n    return \" \".join(result)\n\n\n# ---------------------------------------------------------------------------\n# High-level validators (moved & improved)\n# ---------------------------------------------------------------------------\n\n\n@handle_errors(\"validating user update\", default_return=(False, [\"Validation failed\"]))\ndef validate_user_update(\n    user_id: str, data_type: str, updates: dict[str, Any]\n) -> tuple[bool, list[str]]:\n    \"\"\"Validate partial updates to an existing user's data.\"\"\"\n    logger.debug(\n        f\"Validating user update for user {user_id}, data_type: {data_type}, fields: {list(updates.keys())}\"\n    )\n    errors: list[str] = []\n    if not user_id:\n        errors.append(\"user_id is required\")\n    if not updates:\n        errors.append(\"updates cannot be empty\")\n\n    # ACCOUNT ----------------------------------------------------------------\n    if data_type == \"account\":\n        # Account validation is now handled by Pydantic models in core/schemas.py\n        # This function is kept for backward compatibility but delegates to Pydantic\n        from core.schemas import validate_account_dict\n\n        try:\n            # This maintains backward compatibility with the old validation approach\n            try:\n                from core.user_data_handlers import get_user_data\n\n                current_account = get_user_data(user_id, \"account\").get(\"account\", {})\n            except Exception:\n                current_account = {}\n\n            # Merge updates with current account for validation\n            merged_account = current_account.copy()\n            merged_account.update(updates)\n\n            # Add user_id if not present (required by Pydantic)\n            if \"user_id\" not in merged_account:\n                merged_account[\"user_id\"] = user_id\n\n            # Use Pydantic validation for account data\n            _, validation_errors = validate_account_dict(merged_account)\n            if validation_errors:\n                errors.extend(validation_errors)\n\n            # Additional strict validation for critical fields\n            if (\n                \"internal_username\" in updates\n                and not updates[\"internal_username\"].strip()\n            ):\n                errors.append(\"internal_username cannot be empty\")\n\n            if \"channel\" in updates:\n                channel = updates[\"channel\"]\n                if isinstance(channel, dict) and \"type\" in channel:\n                    valid_channel_types = [\"email\", \"discord\", \"sms\", \"webhook\"]\n                    if channel[\"type\"] not in valid_channel_types:\n                        errors.append(\n                            f\"Invalid channel type: {channel['type']}. Must be one of: {valid_channel_types}\"\n                        )\n        except Exception as e:\n            errors.append(f\"Account validation error: {e}\")\n\n    # PREFERENCES -------------------------------------------------------------\n    elif data_type == \"preferences\":\n        # Preferences validation is now handled by Pydantic models in core/schemas.py\n        # This function is kept for backward compatibility but delegates to Pydantic\n        from core.schemas import validate_preferences_dict\n\n        try:\n            # Retry in case of race conditions with file reads in parallel execution\n            import time\n\n            current_preferences = {}\n            for attempt in range(3):\n                try:\n                    from core.user_data_handlers import get_user_data\n\n                    current_preferences = get_user_data(user_id, \"preferences\").get(\n                        \"preferences\", {}\n                    )\n                    if (\n                        current_preferences or attempt == 2\n                    ):  # Accept empty on last attempt\n                        break\n                except Exception:\n                    pass\n                if attempt < 2:\n                    time.sleep(0.05)  # Brief delay before retry\n\n            merged_preferences = (\n                current_preferences.copy()\n                if isinstance(current_preferences, dict)\n                else {}\n            )\n            merged_preferences.update(updates)\n\n            # Use Pydantic validation for preferences data\n            _, validation_errors = validate_preferences_dict(merged_preferences)\n            if validation_errors:\n                errors.extend(validation_errors)\n        except Exception as e:\n            logger.warning(f\"Preferences validation error for user {user_id}: {e}\")\n            # Don't fail validation on exceptions - let Pydantic handle it\n            pass\n\n    # CONTEXT -----------------------------------------------------------------\n    elif data_type == \"context\":\n        dob = updates.get(\"date_of_birth\")\n        if dob:\n            try:\n                from datetime import datetime as _dt\n\n                _dt.strptime(dob, DATE_ONLY)\n            except ValueError:\n                errors.append(\"date_of_birth must be in YYYY-MM-DD format\")\n        if \"custom_fields\" in updates and not isinstance(\n            updates[\"custom_fields\"], dict\n        ):\n            errors.append(\"custom_fields must be a dictionary\")\n\n    # SCHEDULES ---------------------------------------------------------------\n    elif data_type == \"schedules\":\n        # Schedules validation is now handled by Pydantic models in core/schemas.py\n        # This function is kept for backward compatibility but delegates to Pydantic\n        from core.schemas import validate_schedules_dict\n\n        try:\n            try:\n                from core.user_data_handlers import get_user_data\n\n                current_schedules = get_user_data(user_id, \"schedules\").get(\n                    \"schedules\", {}\n                )\n            except Exception:\n                current_schedules = {}\n\n            merged_schedules = current_schedules.copy()\n            merged_schedules.update(updates)\n\n            # Use Pydantic validation for schedules data\n            _, validation_errors = validate_schedules_dict(merged_schedules)\n            if validation_errors:\n                errors.extend(validation_errors)\n        except Exception as e:\n            errors.append(f\"Schedules validation error: {e}\")\n\n    is_valid = len(errors) == 0\n    if is_valid:\n        logger.debug(\n            f\"User update validation passed for user {user_id}, data_type: {data_type}\"\n        )\n    else:\n        logger.warning(\n            f\"User update validation failed for user {user_id}, data_type: {data_type} - errors: {errors}\"\n        )\n    return is_valid, errors\n\n\n@handle_errors(\n    \"validating schedule periods\", default_return=(False, [\"Validation failed\"])\n)\ndef validate_schedule_periods(\n    periods: dict[str, dict[str, Any]], category: str = \"unknown\"\n) -> tuple[bool, list[str]]:\n    \"\"\"Validate schedule periods and return (is_valid, error_messages).\n\n    Args:\n        periods: Dictionary of period_name -> period_data\n        category: Category name for error messages (e.g., \"tasks\", \"check-ins\")\n\n    Returns:\n        Tuple of (is_valid, list_of_error_messages)\n    \"\"\"\n    errors: list[str] = []\n\n    if not periods:\n        return False, [f\"At least one time period is required for {category}.\"]\n\n    # Check if any periods are active (excluding \"ALL\" period)\n    active_periods = [\n        name\n        for name, data in periods.items()\n        if isinstance(data, dict)\n        and data.get(\"active\", False)\n        and name.upper() != \"ALL\"\n    ]\n    if not active_periods:\n        return False, [f\"At least one time period must be enabled for {category}.\"]\n\n    # Validate each period\n    valid_days = [\n        \"ALL\",\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\",\n    ]\n\n    for period_name, period_data in periods.items():\n        if not isinstance(period_data, dict):\n            errors.append(f\"Period '{period_name}' data must be a dictionary\")\n            continue\n\n        # Check required fields - name is the dictionary key, so we don't need to check period_data.get('name')\n        # The period_name is already validated by being a valid dictionary key\n\n        # Validate times\n        start_time = period_data.get(\"start_time\")\n        end_time = period_data.get(\"end_time\")\n\n        if not start_time or not end_time:\n            errors.append(\n                f\"Period '{period_name}' must have both start_time and end_time\"\n            )\n            continue\n\n        if not validate_schedule_periods__validate_time_format(start_time):\n            errors.append(\n                f\"Period '{period_name}' has invalid start_time format: {start_time}\"\n            )\n\n        if not validate_schedule_periods__validate_time_format(end_time):\n            errors.append(\n                f\"Period '{period_name}' has invalid end_time format: {end_time}\"\n            )\n\n        # Validate time ordering\n        if (\n            start_time\n            and end_time\n            and validate_schedule_periods__validate_time_format(start_time)\n            and validate_schedule_periods__validate_time_format(end_time)\n        ):\n            try:\n                from datetime import datetime as _dt\n\n                st = _dt.strptime(start_time, TIME_ONLY_MINUTE)\n                et = _dt.strptime(end_time, TIME_ONLY_MINUTE)\n                if st >= et:\n                    errors.append(\n                        f\"Period '{period_name}' start_time must be before end_time\"\n                    )\n            except ValueError:\n                # Already caught by format validation\n                pass\n\n        # Validate days for active periods\n        if period_data.get(\"active\", False):\n            days = period_data.get(\"days\", [])\n            if not isinstance(days, list):\n                errors.append(f\"Period '{period_name}' days must be a list\")\n            elif not days:\n                errors.append(\n                    f\"Active period '{period_name}' must have at least one day selected\"\n                )\n            else:\n                invalid_days = [d for d in days if d not in valid_days]\n                if invalid_days:\n                    errors.append(\n                        f\"Period '{period_name}' has invalid days: {invalid_days}\"\n                    )\n\n    return len(errors) == 0, errors\n\n\n@handle_errors(\n    \"validating new user data\", default_return=(False, [\"Validation failed\"])\n)\ndef validate_new_user_data(\n    user_id: str, data_updates: dict[str, dict[str, Any]]\n) -> tuple[bool, list[str]]:\n    \"\"\"Validate complete dataset required for a brand-new user.\"\"\"\n    errors: list[str] = []\n    if not user_id:\n        errors.append(\"user_id is required\")\n    if not data_updates:\n        errors.append(\"data_updates cannot be empty\")\n\n    from core.config import get_user_data_dir\n\n    if os.path.exists(get_user_data_dir(user_id)):\n        errors.append(f\"User {user_id} already exists\")\n\n    # ACCOUNT (mandatory)\n    account = data_updates.get(\"account\")\n    if not account:\n        errors.append(\"account data is required for new user creation\")\n    else:\n        if not account.get(\"internal_username\"):\n            errors.append(\"internal_username is required for new user creation\")\n        if account.get(\"email\") and not is_valid_email(account[\"email\"]):\n            errors.append(\"Invalid email format\")\n        if \"account_status\" in account and account[\"account_status\"] not in [\n            \"active\",\n            \"inactive\",\n            \"suspended\",\n        ]:\n            errors.append(\n                \"Invalid account_status. Must be one of: active, inactive, suspended\"\n            )\n\n    # PREFERENCES (mandatory for channel type)\n    prefs = data_updates.get(\"preferences\", {})\n    if \"categories\" in prefs and isinstance(prefs[\"categories\"], list):\n        from core.message_management import get_message_categories\n\n        invalid = [c for c in prefs[\"categories\"] if c not in get_message_categories()]\n        if invalid:\n            errors.append(f\"Invalid categories: {invalid}\")\n\n    # Channel type validation\n    channel = prefs.get(\"channel\")\n    if not isinstance(channel, dict) or not channel.get(\"type\"):\n        errors.append(\"channel.type is required for new user creation\")\n    elif channel[\"type\"] not in [\"email\", \"discord\"]:\n        errors.append(\"Invalid channel type. Must be one of: email, discord\")\n\n    # CONTEXT (optional)\n    context = data_updates.get(\"context\", {})\n    if (\n        context.get(\"date_of_birth\")\n        and not validate_user_update(\n            user_id, \"context\", {\"date_of_birth\": context[\"date_of_birth\"]}\n        )[0]\n    ):\n        errors.append(\"Invalid date_of_birth format\")\n\n    return len(errors) == 0, errors\n\n\n# ---------------------------------------------------------------------------\n# PERSONALIZATION VALIDATOR\n# ---------------------------------------------------------------------------\n\n\n@handle_errors(\n    \"validating personalization data\", default_return=(False, [\"Validation failed\"])\n)\ndef validate_personalization_data(data: dict[str, Any]) -> tuple[bool, list[str]]:\n    \"\"\"Validate *context/personalization* structure.\n\n    No field is required; we only type-check fields that are present.\n    This logic previously lived in the legacy user management utilities.\n    \"\"\"\n\n    errors: list[str] = []\n\n    # Optional string fields\n    for field in (\"date_of_birth\", \"timezone\"):\n        if field in data and not isinstance(data[field], str):\n            errors.append(f\"Field {field} must be a string if present\")\n\n    # Optional list fields\n    optional_lists = [\n        \"gender_identity\",\n        \"reminders_needed\",\n        \"loved_ones\",\n        \"interests\",\n        \"activities_for_encouragement\",\n        \"notes_for_ai\",\n        \"goals\",\n    ]\n    for field in optional_lists:\n        if field in data and not isinstance(data[field], list):\n            errors.append(f\"Field {field} must be a list if present\")\n\n    # custom_fields (dict of lists)\n    custom_fields = data.get(\"custom_fields\", {})\n    if \"custom_fields\" in data and not isinstance(custom_fields, dict):\n        errors.append(\"custom_fields must be a dictionary if present\")\n    else:\n        for key in (\"health_conditions\", \"medications_treatments\"):\n            if key in custom_fields and not isinstance(custom_fields[key], list):\n                errors.append(\n                    f\"Field {key} (in custom_fields) must be a list if present\"\n                )\n\n    # date_of_birth format\n    if dob := data.get(\"date_of_birth\"):\n        try:\n            from datetime import datetime as _dt\n\n            _dt.strptime(dob, DATE_ONLY)\n        except ValueError:\n            errors.append(\"date_of_birth must be in YYYY-MM-DD format\")\n\n    # loved_ones list of dicts\n    loved_ones = data.get(\"loved_ones\", [])\n    if not isinstance(loved_ones, list):\n        errors.append(\"loved_ones must be a list if present\")\n    else:\n        for idx, item in enumerate(loved_ones):\n            if not isinstance(item, dict):\n                errors.append(f\"loved_one at index {idx} must be a dictionary\")\n\n    return len(errors) == 0, errors\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 330,
                    "line_content": "# This function is kept for backward compatibility but delegates to Pydantic",
                    "start": 9660,
                    "end": 9682
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 334,
                    "line_content": "# This maintains backward compatibility with the old validation approach",
                    "start": 9807,
                    "end": 9829
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 376,
                    "line_content": "# This function is kept for backward compatibility but delegates to Pydantic",
                    "start": 11714,
                    "end": 11736
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 434,
                    "line_content": "# This function is kept for backward compatibility but delegates to Pydantic",
                    "start": 14047,
                    "end": 14069
                  }
                ]
              ],
              [
                "core\\__init__.py",
                "\"\"\"Core functionality package for the MHM application.\n\nContains foundational modules for configuration, logging, error handling,\nservice management, user data operations, scheduling, and system utilities.\n\"\"\"\n\n# Main public API - package-level exports for easier refactoring\n# Logger exports\nfrom .logger import (\n    get_component_logger,\n    setup_logging,\n    ComponentLogger,\n    BackupDirectoryRotatingFileHandler,\n    ExcludeLoggerNamesFilter,\n    PytestContextLogFormatter,\n    setup_third_party_error_logging,\n    suppress_noisy_logging,\n    set_console_log_level,\n    toggle_verbose_logging,\n    get_verbose_mode,\n    set_verbose_mode,\n    get_logger,\n)\n\n# Error handling exports\nfrom .error_handling import (\n    handle_errors,\n    MHMError,\n    DataError,\n    FileOperationError,\n    ConfigurationError,\n    CommunicationError,\n    SchedulerError,\n    UserInterfaceError,\n    AIError,\n    ValidationError,\n    RecoveryError,\n)\n\n# User data handlers exports\nfrom .user_data_handlers import (\n    get_user_data,\n    save_user_data,\n    save_user_data_transaction,\n    get_all_user_ids,\n    update_user_account,\n    update_user_preferences,\n    update_user_schedules,\n    update_user_context,\n    update_channel_preferences,\n    register_data_loader,\n)\n\n# Config exports - commonly used constants and functions\n# Note: We import the module rather than individual constants to avoid\n# exposing too many items. Users can do: from core import config; config.BASE_DATA_DIR\nfrom . import config\n\n# File operations exports (no circular dependencies)\nfrom .file_operations import (\n    load_json_data,\n    save_json_data,\n    determine_file_path,\n    verify_file_access,\n)\n\n# Message management exports (no circular dependencies)\nfrom .message_management import (\n    get_recent_messages,\n    store_sent_message,\n    get_message_categories,\n    load_user_messages,\n    add_message,\n    archive_old_messages,\n)\n\n# Response tracking exports (no circular dependencies)\nfrom .response_tracking import (\n    get_recent_responses,\n    store_chat_interaction,\n    get_recent_checkins,\n    is_user_checkins_enabled,\n)\n\n# User data validation exports (no circular dependencies)\nfrom .user_data_validation import (\n    validate_schedule_periods,\n    is_valid_email,  # Medium usage\n)\n\n# Error handling additional exports (medium usage)\nfrom .error_handling import handle_network_error\n\n# Config constants exports (medium usage)\n# Note: Some config constants are imported directly, so we export commonly used ones\nfrom .config import (\n    DISCORD_BOT_TOKEN,\n    EMAIL_SMTP_SERVER,\n    EMAIL_IMAP_SERVER,\n    EMAIL_SMTP_USERNAME,\n    LM_STUDIO_BASE_URL,\n    LM_STUDIO_API_KEY,\n    LM_STUDIO_MODEL,\n    SCHEDULER_INTERVAL,\n    get_available_channels,\n)\n\n# UI management exports (medium usage)\nfrom .ui_management import (\n    collect_period_data_from_widgets,\n    load_period_widgets_for_category,\n)\n\n# User data handlers additional exports (medium usage)\nfrom .user_data_handlers import (\n    ensure_all_categories_have_schedules,\n    get_user_id_by_identifier,\n)\n\n# Schema validation exports (medium usage)\nfrom .schemas import (\n    validate_account_dict,\n    validate_preferences_dict,\n    validate_schedules_dict,\n    validate_messages_file_dict,\n)\n\n# Schema models exports (public API)\nfrom .schemas import (\n    AccountModel,\n    ChannelModel,\n    PreferencesModel,\n    CategoryScheduleModel,\n    FeaturesModel,\n    PeriodModel,\n    MessagesFileModel,\n    MessageModel,\n    SchedulesModel,\n)\n\n# Config additional constants exports (low usage)\nfrom .config import (\n    CONTEXT_CACHE_TTL,\n    DISCORD_APPLICATION_ID,\n    EMAIL_SMTP_PASSWORD,\n)\n\n# Config additional functions exports (public API)\nfrom .config import (\n    validate_all_configuration,\n    validate_and_raise_if_invalid,\n    get_backups_dir,\n    ensure_user_directory,\n    validate_email_config,\n    validate_discord_config,\n    validate_minimum_config,\n    get_channel_class_mapping,\n    validate_core_paths,\n    validate_ai_configuration,\n    validate_communication_channels,\n    validate_logging_configuration,\n    validate_scheduler_configuration,\n    validate_file_organization_settings,\n    validate_environment_variables,\n    print_configuration_report,\n)\n\n# Error handling additional classes and functions (public API)\nfrom .error_handling import (\n    ErrorHandler,\n    ErrorRecoveryStrategy,\n    ConfigurationRecovery,\n)\n\n# Config validation error class (public API)\nfrom .config import ConfigValidationError\n\n# Service classes exports (public API)\n# Note: Some service items may have circular dependencies\nfrom .service import (\n    MHMService,\n    InitializationError,\n)\n\n# Service utilities exports (public API)\nfrom .service_utilities import (\n    Throttler,\n    InvalidTimeFormatError,\n    create_reschedule_request,\n    is_service_running,\n    get_service_processes,\n    is_headless_service_running,\n    is_ui_service_running,\n    wait_for_network,\n    load_and_localize_datetime,\n)\n\n# Headless service exports (public API)\nfrom .headless_service import HeadlessServiceManager\n\n# File operations additional exports (public API)\nfrom .file_operations import create_user_files\n\n# File auditor exports (public API)\nfrom .file_auditor import (\n    FileAuditor,\n    start_auditor,\n    stop_auditor,\n    record_created,\n)\n\n# Schedule utilities exports (public API)\n# Note: schedule_utilities may have dependencies on schedule_management\nfrom .schedule_utilities import (\n    get_active_schedules,\n    is_schedule_active,\n    get_current_active_schedules,\n)\n\n# Auto cleanup exports (public API)\nfrom .auto_cleanup import (\n    get_last_cleanup_timestamp,\n    update_cleanup_timestamp,\n    should_run_cleanup,\n    perform_cleanup,\n    auto_cleanup_if_needed,\n    cleanup_data_directory,\n    cleanup_tests_data_directory,\n    archive_old_messages_for_all_users,\n    get_cleanup_status,\n)\n\n# Backup manager exports (public API)\nfrom .backup_manager import BackupManager\n\n# Check-in dynamic manager class exports (public API)\nfrom .checkin_dynamic_manager import DynamicCheckinManager\n\n# Check-in analytics exports (high usage)\nfrom .checkin_analytics import CheckinAnalytics\n\n# Error handling additional exports (high usage)\nfrom .error_handling import handle_ai_error\n\n# Config function exports (high usage)\nfrom .config import get_user_data_dir, get_user_file_path\n\n# Dynamic check-in manager exports (high usage)\nfrom .checkin_dynamic_manager import dynamic_checkin_manager\n\n# Schedule management exports - lazy import due to circular dependencies (medium usage)\n# Note: schedule_management has circular dependencies with user package\n# Functions available: get_schedule_time_periods, set_schedule_periods (high usage),\n# clear_schedule_periods_cache (medium usage), add_schedule_period (low usage)\n# Note: add_schedule_period has circular dependencies, documented as lazy import\n\n# User data manager exports (high usage)\n# Note: user_data_manager may have circular dependencies\n# Attempting direct import - if this causes circular import errors, this will be\n# documented as lazy import in comments\nfrom .user_data_manager import (\n    update_user_index,\n    rebuild_user_index,\n    UserDataManager,\n    update_message_references,\n    backup_user_data,\n    export_user_data,\n    delete_user_completely,\n    get_user_data_summary,\n    get_user_info_for_data_manager,\n    build_user_index,\n    get_user_summary,\n    get_all_user_summaries,\n    get_user_analytics_summary,\n)\n\n# User data handlers exports (high usage)\nfrom .user_data_handlers import (\n    get_user_categories,\n    clear_user_caches,\n    register_default_loaders,\n    get_available_data_types,\n    get_data_type_info,\n    create_default_schedule_periods,\n    migrate_legacy_schedules_structure,\n    ensure_category_has_default_schedule,\n)\n\n# Service exports - lazy import due to circular dependencies (medium usage)\n# Note: service has circular dependencies with scheduler\n# Functions available: get_scheduler_manager (medium usage), MHMService, InitializationError\n# Use: from core.service import get_scheduler_manager\n\n# User data handlers exports - lazy import to avoid circular dependencies\n# Functions available via direct import: get_user_categories (high usage),\n# ensure_all_categories_have_schedules, get_user_id_by_identifier (medium usage)\n# Other functions: clear_user_caches (use: from core.user_data_handlers import clear_user_caches)\n\n# User data manager exports - lazy import to avoid circular dependencies\n# Note: user_data_manager may have circular dependencies\n# Functions available via direct import: update_user_index (high usage)\n# Other functions: rebuild_user_index (use: from core.user_data_manager import rebuild_user_index)\n\n\n# Scheduler exports - lazy import due to circular dependencies (medium usage)\n# Note: SchedulerManager and add_schedule_period have circular dependencies\n# Use lazy import pattern for these items\n# ERROR_HANDLING_EXCLUDE: Special Python method for dynamic attribute access\ndef __getattr__(name: str):\n    \"\"\"Lazy import handler for items with circular dependencies\"\"\"\n    if name == \"SchedulerManager\":\n        from .scheduler import SchedulerManager\n\n        return SchedulerManager\n    elif name == \"add_schedule_period\":\n        from .schedule_management import add_schedule_period\n\n        return add_schedule_period\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\n__all__ = [\n    # Logger\n    \"get_component_logger\",\n    \"setup_logging\",\n    \"ComponentLogger\",\n    \"BackupDirectoryRotatingFileHandler\",\n    \"ExcludeLoggerNamesFilter\",\n    \"PytestContextLogFormatter\",\n    \"setup_third_party_error_logging\",\n    \"suppress_noisy_logging\",\n    \"set_console_log_level\",\n    \"toggle_verbose_logging\",\n    \"get_verbose_mode\",\n    \"set_verbose_mode\",\n    \"get_logger\",\n    # Error handling\n    \"handle_errors\",\n    \"MHMError\",\n    \"DataError\",\n    \"FileOperationError\",\n    \"ConfigurationError\",\n    \"CommunicationError\",\n    \"SchedulerError\",\n    \"UserInterfaceError\",\n    \"AIError\",\n    \"ValidationError\",\n    \"RecoveryError\",\n    # User data handlers\n    \"get_user_data\",\n    \"save_user_data\",\n    \"save_user_data_transaction\",\n    \"get_all_user_ids\",\n    \"update_user_account\",\n    \"update_user_preferences\",\n    \"update_user_schedules\",\n    \"update_user_context\",\n    \"update_channel_preferences\",\n    \"register_data_loader\",\n    # File operations\n    \"load_json_data\",\n    \"save_json_data\",\n    \"determine_file_path\",\n    \"verify_file_access\",\n    # Message management\n    \"get_recent_messages\",\n    \"store_sent_message\",\n    \"get_message_categories\",\n    \"load_user_messages\",\n    # Response tracking\n    \"get_recent_responses\",\n    \"store_chat_interaction\",\n    \"get_recent_checkins\",\n    \"is_user_checkins_enabled\",\n    # User data validation\n    \"validate_schedule_periods\",\n    \"is_valid_email\",  # Medium usage\n    # Error handling additional (medium usage)\n    \"handle_network_error\",\n    # Config constants (medium usage)\n    \"DISCORD_BOT_TOKEN\",\n    \"EMAIL_SMTP_SERVER\",\n    \"EMAIL_IMAP_SERVER\",\n    \"EMAIL_SMTP_USERNAME\",\n    \"LM_STUDIO_BASE_URL\",\n    \"LM_STUDIO_API_KEY\",\n    \"LM_STUDIO_MODEL\",\n    \"SCHEDULER_INTERVAL\",\n    \"get_available_channels\",\n    # UI management (medium usage)\n    \"collect_period_data_from_widgets\",\n    \"load_period_widgets_for_category\",\n    # User management additional (medium usage)\n    \"ensure_all_categories_have_schedules\",\n    \"get_user_id_by_identifier\",\n    # Schedule management - lazy import (circular dependencies)\n    \"add_schedule_period\",\n    # Scheduler - lazy import (circular dependencies)\n    \"SchedulerManager\",\n    # Schema validation (medium usage)\n    \"validate_account_dict\",\n    \"validate_preferences_dict\",\n    \"validate_schedules_dict\",\n    \"validate_messages_file_dict\",\n    # Schema models (public API)\n    \"AccountModel\",\n    \"ChannelModel\",\n    \"PreferencesModel\",\n    \"CategoryScheduleModel\",\n    \"FeaturesModel\",\n    \"PeriodModel\",\n    \"MessagesFileModel\",\n    \"MessageModel\",\n    \"SchedulesModel\",\n    # Config additional constants (low usage)\n    \"CONTEXT_CACHE_TTL\",\n    \"DISCORD_APPLICATION_ID\",\n    \"EMAIL_SMTP_PASSWORD\",\n    # Config additional functions (public API)\n    \"validate_all_configuration\",\n    \"validate_and_raise_if_invalid\",\n    \"get_backups_dir\",\n    \"ensure_user_directory\",\n    \"validate_email_config\",\n    \"validate_discord_config\",\n    \"validate_minimum_config\",\n    \"get_channel_class_mapping\",\n    \"validate_core_paths\",\n    \"validate_ai_configuration\",\n    \"validate_communication_channels\",\n    \"validate_logging_configuration\",\n    \"validate_scheduler_configuration\",\n    \"validate_file_organization_settings\",\n    \"validate_environment_variables\",\n    \"print_configuration_report\",\n    # Error handling additional classes and functions (public API)\n    \"ErrorHandler\",\n    \"ErrorRecoveryStrategy\",\n    \"ConfigurationRecovery\",\n    # Config validation error class (public API)\n    \"ConfigValidationError\",\n    # Service classes (public API)\n    \"MHMService\",\n    \"InitializationError\",\n    # Service utilities (public API)\n    \"Throttler\",\n    \"InvalidTimeFormatError\",\n    \"create_reschedule_request\",\n    \"is_service_running\",\n    \"get_service_processes\",\n    \"is_headless_service_running\",\n    \"is_ui_service_running\",\n    \"wait_for_network\",\n    \"load_and_localize_datetime\",\n    # Headless service (public API)\n    \"HeadlessServiceManager\",\n    # File operations additional (public API)\n    \"create_user_files\",\n    # File auditor (public API)\n    \"FileAuditor\",\n    \"start_auditor\",\n    \"stop_auditor\",\n    \"record_created\",\n    # Schedule utilities (public API)\n    \"get_active_schedules\",\n    \"is_schedule_active\",\n    \"get_current_active_schedules\",\n    # Auto cleanup (public API)\n    \"get_last_cleanup_timestamp\",\n    \"update_cleanup_timestamp\",\n    \"should_run_cleanup\",\n    \"perform_cleanup\",\n    \"auto_cleanup_if_needed\",\n    \"cleanup_data_directory\",\n    \"cleanup_tests_data_directory\",\n    \"archive_old_messages_for_all_users\",\n    \"get_cleanup_status\",\n    # Backup manager (public API)\n    \"BackupManager\",\n    # Check-in dynamic manager class (public API)\n    \"DynamicCheckinManager\",\n    # Check-in analytics (high usage)\n    \"CheckinAnalytics\",\n    # Error handling additional (high usage)\n    \"handle_ai_error\",\n    # Config functions (high usage)\n    \"get_user_data_dir\",\n    \"get_user_file_path\",\n    # Dynamic check-in manager (high usage)\n    \"dynamic_checkin_manager\",\n    # Schedule management (high usage) - lazy import due to circular dependencies\n    # Note: get_schedule_time_periods, set_schedule_periods not in __all__ due to circular dependencies\n    # Use: from core.schedule_management import get_schedule_time_periods, set_schedule_periods\n    # User data manager (high usage and public API)\n    \"update_user_index\",\n    \"rebuild_user_index\",\n    \"UserDataManager\",\n    \"update_message_references\",\n    \"backup_user_data\",\n    \"export_user_data\",\n    \"delete_user_completely\",\n    \"get_user_data_summary\",\n    \"get_user_info_for_data_manager\",\n    \"build_user_index\",\n    \"get_user_summary\",\n    \"get_all_user_summaries\",\n    \"get_user_analytics_summary\",\n    # User management (high usage and public API)\n    \"get_user_categories\",\n    \"clear_user_caches\",\n    \"register_default_loaders\",\n    \"get_available_data_types\",\n    \"get_data_type_info\",\n    \"create_default_schedule_periods\",\n    \"migrate_legacy_schedules_structure\",\n    \"ensure_category_has_default_schedule\",\n    # Modules (for module-level access: from core import config)\n    \"config\",\n    # Legacy module names (for backward compatibility)\n    \"auto_cleanup\",\n    \"backup_manager\",\n    \"checkin_analytics\",\n    \"checkin_dynamic_manager\",\n    \"error_handling\",\n    \"file_auditor\",\n    \"file_operations\",\n    \"headless_service\",\n    \"logger\",\n    \"message_management\",\n    \"response_tracking\",\n    \"schedule_management\",\n    \"schedule_utilities\",\n    \"scheduler\",\n    \"service\",\n    \"service_utilities\",\n    \"ui_management\",\n    \"user_data_handlers\",\n    \"user_data_manager\",\n    \"user_data_validation\",\n]\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 519,
                    "line_content": "# Legacy module names (for backward compatibility)",
                    "start": 15542,
                    "end": 15564
                  },
                  {
                    "pattern": "(?i)legacy module",
                    "match": "Legacy module",
                    "line": 519,
                    "line_content": "# Legacy module names (for backward compatibility)",
                    "start": 15517,
                    "end": 15530
                  }
                ]
              ],
              [
                "tests\\conftest.py",
                "\"\"\"\nPytest configuration and shared fixtures for MHM testing framework.\n\nThis file provides:\n- Test configuration\n- Shared fixtures for common test data\n- Temporary directory management\n- Mock configurations for testing\n- Dedicated testing log configuration\n\"\"\"\n\nimport pytest\nimport sys\nimport os\n\nos.environ[\"QT_QPA_PLATFORM\"] = \"offscreen\"\n# Set environment variable for consolidated logging very early, before any logging initialization\n# Allow override via environment variable for individual component logging\nos.environ[\"TEST_CONSOLIDATED_LOGGING\"] = os.environ.get(\n    \"TEST_CONSOLIDATED_LOGGING\", \"1\"\n)\nimport tempfile\nimport shutil\nimport json\nimport logging\nimport warnings\nfrom typing import List, Optional, Type\nimport time\nimport re\nfrom pathlib import Path\nfrom unittest.mock import Mock, patch\nfrom datetime import datetime\nfrom core.time_utilities import (\n    now_timestamp_filename,\n    now_timestamp_full,\n    TIMESTAMP_FULL,\n)\n\n# CRITICAL: Suppress __package__ != __spec__.parent warnings immediately after importing warnings\n# These warnings are emitted during module import, so they must be filtered before any other imports\n# Use simplefilter to catch all DeprecationWarnings, then add specific filters\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\nwarnings.filterwarnings(\n    \"ignore\", message=\".*__package__.*\", category=DeprecationWarning\n)\n\n\ndef ensure_qt_runtime():\n    \"\"\"Ensure PySide6 can load in the current environment.\n\n    Qt-dependent tests rely on libGL/GLX libraries that may be absent in\n    containerized or headless environments. Import the critical PySide6\n    modules and skip those tests gracefully when the runtime is missing.\n    \"\"\"\n\n    try:\n        from PySide6 import QtWidgets  # noqa: F401 - import verifies availability\n        from PySide6.QtWidgets import QApplication  # noqa: F401\n    except (ImportError, OSError) as exc:\n        message = str(exc)\n        lower_message = message.lower()\n        gl_indicators = (\"libgl\", \"opengl\", \"libegl\", \"libglu\", \"glx\")\n        if any(token in lower_message for token in gl_indicators):\n            pytest.skip(f\"Qt runtime unavailable: {exc}\", allow_module_level=True)\n        raise\n\n\n# Suppress Discord library warnings\nwarnings.filterwarnings(\n    \"ignore\", message=\".*audioop.*is deprecated.*\", category=DeprecationWarning\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*parameter 'timeout' of type 'float' is deprecated.*\",\n    category=DeprecationWarning,\n)\nwarnings.filterwarnings(\"ignore\", category=pytest.PytestUnhandledThreadExceptionWarning)\nwarnings.filterwarnings(\"ignore\", category=pytest.PytestUnraisableExceptionWarning)\n# Suppress PytestCollectionWarning for development tools implementation classes\n# These classes (TestCoverageAnalyzer, TestCoverageReportGenerator) are implementation classes, not test classes\n# They start with \"Test\" which makes pytest try to collect them, but they have __init__ constructors\n# Apply filters early to catch warnings during collection\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*cannot collect test class.*TestCoverage.*\",\n    category=pytest.PytestCollectionWarning,\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*cannot collect test class.*because it has a __init__ constructor.*\",\n    category=pytest.PytestCollectionWarning,\n)\n# General filter for all PytestCollectionWarning from development_tools modules\nwarnings.filterwarnings(\n    \"ignore\",\n    category=pytest.PytestCollectionWarning,\n    module=\"development_tools.tests.*\",\n)\n\n# Additional __package__ warning filters (redundant but explicit for clarity)\n# The main filter is above, right after importing warnings\nwarnings.filterwarnings(\n    \"ignore\", message=\".*__package__.*\", category=DeprecationWarning\n)\n\n# Suppress specific Discord library warnings more broadly\nwarnings.filterwarnings(\"ignore\", module=\"discord.player\")\nwarnings.filterwarnings(\"ignore\", module=\"discord.http\")\nwarnings.filterwarnings(\"ignore\", message=\".*audioop.*\", category=DeprecationWarning)\nwarnings.filterwarnings(\n    \"ignore\", message=\".*timeout.*deprecated.*\", category=DeprecationWarning\n)\n\n# Additional comprehensive warning suppression\n# Suppress audioop deprecation warning from discord.player (Python 3.13 deprecation)\n# Note: This warning comes from discord library's use of deprecated audioop module\n# It will be fixed when discord.py updates, but we suppress it in tests for now\nwarnings.filterwarnings(\n    \"ignore\", message=\".*audioop.*deprecated.*\", category=DeprecationWarning\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*audioop.*\",\n    category=DeprecationWarning,\n    module=\"discord.player\",\n)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"discord.player\")\nwarnings.filterwarnings(\n    \"ignore\", message=\".*timeout.*\", category=DeprecationWarning, module=\"discord.*\"\n)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"discord.http\")\n\n# Suppress aiohttp client session warnings\nwarnings.filterwarnings(\n    \"ignore\", message=\".*Unclosed client session.*\", category=ResourceWarning\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*Task was destroyed but it is pending.*\",\n    category=RuntimeWarning,\n)\n# Suppress unawaited coroutine warnings from Discord bot event handlers in test environments\n# This is expected when using mocks - the coroutines are created but never executed\n# The coroutine is registered with @bot.event but may not be awaited in test environments\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*coroutine.*_on_ready_internal.*was never awaited.*\",\n    category=RuntimeWarning,\n)\nwarnings.filterwarnings(\n    \"ignore\",\n    message=\".*coroutine.*was never awaited.*\",\n    category=RuntimeWarning,\n    module=\"communication.communication_channels.discord.bot\",\n)\n\n# Note: Do not override BASE_DATA_DIR/USER_INFO_DIR_PATH via environment here,\n# as some unit tests assert the library defaults. Session fixtures below\n# patch core.config attributes to isolate user data under tests/data/users.\n\n# Ensure project root is on sys.path ONCE for all tests\nproject_root = Path(__file__).parent.parent\n\n\n# CRITICAL: Set up logging isolation BEFORE importing any core modules\ndef setup_logging_isolation():\n    \"\"\"Set up logging isolation before any core modules are imported.\"\"\"\n    # Remove all handlers from root logger to prevent test logs from going to app.log\n    root_logger = logging.getLogger()\n    for handler in root_logger.handlers[:]:\n        if isinstance(handler, logging.StreamHandler) and handler.stream in (\n            sys.stdout,\n            sys.stderr,\n        ):\n            root_logger.removeHandler(handler)\n            continue\n        handler.close()\n        root_logger.removeHandler(handler)\n\n    # Also clear any handlers from the main application logger if it exists\n    main_logger = logging.getLogger(\"mhm\")\n    for handler in main_logger.handlers[:]:\n        if isinstance(handler, logging.StreamHandler) and handler.stream in (\n            sys.stdout,\n            sys.stderr,\n        ):\n            main_logger.removeHandler(handler)\n            continue\n        handler.close()\n        main_logger.removeHandler(handler)\n\n    # Set propagate to False for main loggers to prevent test logs from bubbling up\n    root_logger.propagate = False\n    main_logger.propagate = False\n\n\n# Set up logging isolation immediately\nsetup_logging_isolation()\n\n# Set environment variable to indicate we're running tests\n# TEST_VERBOSE_LOGS levels:\n#   '0' (quiet/default): Component loggers INFO, Test loggers WARNING\n#   '1' (medium): Component loggers INFO, Test loggers INFO\n#   '2' (verbose): Component loggers DEBUG, Test loggers DEBUG\nos.environ[\"MHM_TESTING\"] = \"1\"\nos.environ[\"TEST_VERBOSE_LOGS\"] = os.environ.get(\"TEST_VERBOSE_LOGS\", \"0\")\n# Disable core app log rotation during tests to avoid Windows file-in-use issues\nos.environ[\"DISABLE_LOG_ROTATION\"] = \"1\"\n\n# Force all log paths to tests/logs for absolute isolation, even if modules read env at import time\ntests_logs_dir = (Path(__file__).parent / \"logs\").resolve()\ntests_logs_dir.mkdir(exist_ok=True)\nos.environ[\"LOGS_DIR\"] = str(tests_logs_dir)\nos.environ[\"LOG_BACKUP_DIR\"] = str(tests_logs_dir / \"backups\")\nos.environ[\"LOG_ARCHIVE_DIR\"] = str(tests_logs_dir / \"archive\")\n(tests_logs_dir / \"backups\").mkdir(exist_ok=True)\n(tests_logs_dir / \"archive\").mkdir(exist_ok=True)\n\n# In test mode, all component logs go to test_consolidated.log\n# We'll set these up properly in setup_consolidated_test_logging fixture\n# For now, just point them to a placeholder - the fixture will redirect them\nconsolidated_log_placeholder = str(tests_logs_dir / \"test_consolidated.log\")\nos.environ[\"LOG_MAIN_FILE\"] = consolidated_log_placeholder\nos.environ[\"LOG_DISCORD_FILE\"] = consolidated_log_placeholder\nos.environ[\"LOG_AI_FILE\"] = consolidated_log_placeholder\nos.environ[\"LOG_USER_ACTIVITY_FILE\"] = consolidated_log_placeholder\nos.environ[\"LOG_ERRORS_FILE\"] = consolidated_log_placeholder\nos.environ[\"LOG_COMMUNICATION_MANAGER_FILE\"] = consolidated_log_placeholder\nos.environ[\"LOG_EMAIL_FILE\"] = consolidated_log_placeholder\nos.environ[\"LOG_UI_FILE\"] = consolidated_log_placeholder\nos.environ[\"LOG_FILE_OPS_FILE\"] = consolidated_log_placeholder\nos.environ[\"LOG_SCHEDULER_FILE\"] = consolidated_log_placeholder\n\n# Ensure all user data for tests is stored under tests/data to avoid\n# accidental writes to system directories like /tmp or the real data\n# directory. The environment variable must be set before importing\n# core.config so that BASE_DATA_DIR resolves correctly.\ntests_data_dir = (Path(__file__).parent / \"data\").resolve()\ntests_data_dir.mkdir(exist_ok=True)\n(tests_data_dir / \"users\").mkdir(parents=True, exist_ok=True)\nos.environ[\"TEST_DATA_DIR\"] = os.environ.get(\"TEST_DATA_DIR\", str(tests_data_dir))\n# Also set BASE_DATA_DIR for any code that reads it directly\nos.environ[\"BASE_DATA_DIR\"] = str(tests_data_dir)\n# Route service flags to tests/data/flags in test mode\nflags_dir = tests_data_dir / \"flags\"\nflags_dir.mkdir(parents=True, exist_ok=True)\nos.environ[\"MHM_FLAGS_DIR\"] = str(flags_dir)\n\n# Import core modules for testing (after logging isolation is set up)\n# Force core config paths to tests/data early so all modules see test isolation\n# Note: This import may fail for development tools tests that don't need core modules\ntry:\n    import core.config as _core_config\n\n    _core_config.BASE_DATA_DIR = str(tests_data_dir)\n    _core_config.USER_INFO_DIR_PATH = str(tests_data_dir / \"users\")\nexcept (ImportError, ModuleNotFoundError):\n    # Core modules not available (e.g., in development tools tests)\n    # This is expected and safe to ignore for tests that don't use core functionality\n    _core_config = None\n\n# CRITICAL: Re-initialize UserDataManager module-level instance if it was already imported\n# This ensures the module-level instance uses the test directory\ntry:\n    import core.user_data_manager as udm_module\n    from core.user_data_manager import UserDataManager\n\n    # Recreate the module-level instance with updated BASE_DATA_DIR\n    if hasattr(udm_module, \"user_data_manager\"):\n        udm_module.user_data_manager = UserDataManager()\nexcept (ImportError, NameError):\n    pass  # UserDataManager not imported yet, will use correct paths when imported\n\n\n# Session-start guard: ensure loader registry identity and completeness\n@pytest.fixture(scope=\"session\", autouse=True)\ndef verify_user_data_loader_registry():\n    import importlib\n    import core.user_data_handlers as um\n    import core.user_data_handlers as udh\n\n    # Align module state early to avoid split registries from import order\n    um = importlib.reload(um)\n    udh = importlib.reload(udh)\n\n    # Identity check: both reloads should reference the same registry object\n    if um.USER_DATA_LOADERS is not udh.USER_DATA_LOADERS:\n        raise AssertionError(\n            \"USER_DATA_LOADERS mismatch: core.user_data_handlers reloads hold different dict objects.\"\n        )\n\n    # Completeness check: attempt registration once if any missing\n    def _missing_keys():\n        return [k for k, v in um.USER_DATA_LOADERS.items() if not v.get(\"loader\")]\n\n    missing = _missing_keys()\n    if missing:\n        try:\n            if hasattr(um, \"register_default_loaders\"):\n                um.register_default_loaders()\n            elif hasattr(udh, \"register_default_loaders\"):\n                udh.register_default_loaders()\n        except Exception as e:\n            raise AssertionError(f\"Failed to register default loaders: {e}\")\n\n        # Re-evaluate after registration attempt\n        missing_after = _missing_keys()\n        if missing_after:\n            raise AssertionError(\n                f\"Missing user data loaders after registration attempt: {missing_after}\"\n            )\n\n    # All good; continue tests\n    yield\n\n\n# Ensure import order and perform a single default loader registration at session start\n@pytest.fixture(scope=\"session\", autouse=True)\ndef initialize_loader_import_order(request):\n    \"\"\"Reload core.user_data_handlers and register loaders once.\n\n    This ensures both modules share the same USER_DATA_LOADERS dict and that required\n    loaders are present without relying on the data shim.\n\n    Skip this fixture for development tools tests that don't have core modules available.\n    \"\"\"\n    import importlib\n\n    # Try to import core modules - skip if not available (e.g., development tools tests)\n    try:\n        import core.user_data_handlers as um\n    except (ImportError, ModuleNotFoundError):\n        # Core modules not available (e.g., in development tools test environment)\n        yield\n        return\n    um = importlib.reload(um)\n    try:\n        import core.user_data_handlers as udh\n\n        udh = importlib.reload(udh)\n    except Exception:\n        udh = None\n\n    # Single registration pass if available\n    try:\n        if hasattr(um, \"register_default_loaders\"):\n            um.register_default_loaders()\n        elif udh is not None and hasattr(udh, \"register_default_loaders\"):\n            udh.register_default_loaders()\n    except Exception:\n        # Do not fail session start; verify_user_data_loader_registry will enforce later\n        pass\n    yield\n\n\n# Apply user-data shim immediately so tests cannot capture pre-patch references\ndef _apply_get_user_data_shim_early():\n    # Gate with env flag to allow disabling after burn-in\n    if os.getenv(\"ENABLE_TEST_DATA_SHIM\", \"1\") != \"1\":\n        return\n    try:\n        import core.user_data_handlers as um\n    except Exception:\n        return\n    try:\n        import core.user_data_handlers as udh\n    except Exception:\n        udh = None\n\n    # Prefer core.user_data_handlers.get_user_data so the shim always applies.\n    original_get_user_data = getattr(um, \"get_user_data\", None)\n    if (\n        original_get_user_data is None\n        and udh is not None\n        and hasattr(udh, \"get_user_data\")\n    ):\n        original_get_user_data = getattr(udh, \"get_user_data\", None)\n    if original_get_user_data is None:\n        return\n\n    def _load_single_type(user_id: str, key: str, *, auto_create: bool):\n        try:\n            entry = um.USER_DATA_LOADERS.get(key)\n            loader = entry.get(\"loader\") if entry else None\n            if loader is None:\n                key_to_func_and_file = {\n                    \"account\": (um._get_user_data__load_account, \"account\"),\n                    \"preferences\": (um._get_user_data__load_preferences, \"preferences\"),\n                    \"context\": (um._get_user_data__load_context, \"user_context\"),\n                    \"schedules\": (um._get_user_data__load_schedules, \"schedules\"),\n                }\n                func_file = key_to_func_and_file.get(key)\n                if func_file is None:\n                    return None\n                func, file_type = func_file\n                try:\n                    um.register_data_loader(key, func, file_type)\n                    entry = um.USER_DATA_LOADERS.get(key)\n                    loader = entry.get(\"loader\") if entry else None\n                except Exception:\n                    loader = func\n            if loader is None:\n                return None\n            return loader(user_id, auto_create)\n        except Exception:\n            return None\n\n    def wrapped_get_user_data(user_id: str, data_type: str = \"all\", *args, **kwargs):\n        auto_create = True\n        try:\n            auto_create = bool(kwargs.get(\"auto_create\", True))\n        except Exception:\n            auto_create = True\n        result = original_get_user_data(user_id, data_type, *args, **kwargs)\n        try:\n            if data_type == \"all\":\n                if not isinstance(result, dict):\n                    result = {} if result is None else {\"value\": result}\n                for key in (\"account\", \"preferences\", \"context\", \"schedules\"):\n                    if key not in result or not result.get(key):\n                        loaded = _load_single_type(\n                            user_id, key, auto_create=auto_create\n                        )\n                        if loaded is not None:\n                            result[key] = loaded\n                return result\n            if isinstance(data_type, list):\n                # Ensure a dict containing requested keys; fill missing via loaders\n                if not isinstance(result, dict):\n                    result = {}\n                for key in data_type:\n                    if key not in result or not result.get(key):\n                        loaded = _load_single_type(\n                            user_id, key, auto_create=auto_create\n                        )\n                        if loaded is not None:\n                            result[key] = loaded\n                return result\n            if isinstance(data_type, str):\n                key = data_type\n                if isinstance(result, dict) and result.get(key):\n                    return result\n                loaded = _load_single_type(user_id, key, auto_create=auto_create)\n                if loaded is not None:\n                    return {key: loaded}\n                return result\n        except Exception:\n            return result\n\n    # Patch both modules so call sites using either path receive the shim\n    try:\n        setattr(um, \"get_user_data\", wrapped_get_user_data)\n    except Exception:\n        pass\n    try:\n        if udh is not None and hasattr(udh, \"get_user_data\"):\n            setattr(udh, \"get_user_data\", wrapped_get_user_data)\n    except Exception:\n        pass\n\n\n_apply_get_user_data_shim_early()\n\n\n# Allow tests to opt-out of the data shim via marker: @pytest.mark.no_data_shim\n@pytest.fixture(scope=\"function\", autouse=True)\ndef toggle_data_shim_per_marker(request, monkeypatch):\n    marker = request.node.get_closest_marker(\"no_data_shim\")\n    if marker is not None:\n        monkeypatch.setenv(\"ENABLE_TEST_DATA_SHIM\", \"0\")\n    yield\n\n\n# Global QMessageBox patch to prevent popup dialogs during testing\ndef setup_qmessagebox_patches():\n    \"\"\"Set up global QMessageBox patches to prevent popup dialogs during testing.\"\"\"\n    try:\n        from PySide6.QtWidgets import QMessageBox\n\n        # Create a mock QMessageBox that returns appropriate values\n        class MockQMessageBox:\n            @staticmethod\n            def information(*args, **kwargs):\n                return QMessageBox.StandardButton.Ok\n\n            @staticmethod\n            def warning(*args, **kwargs):\n                return QMessageBox.StandardButton.Ok\n\n            @staticmethod\n            def critical(*args, **kwargs):\n                return QMessageBox.StandardButton.Ok\n\n            @staticmethod\n            def question(*args, **kwargs):\n                return QMessageBox.StandardButton.Yes\n\n            @staticmethod\n            def about(*args, **kwargs):\n                return QMessageBox.StandardButton.Ok\n\n        # Apply the patch globally\n        QMessageBox.information = MockQMessageBox.information\n        QMessageBox.warning = MockQMessageBox.warning\n        QMessageBox.critical = MockQMessageBox.critical\n        QMessageBox.question = MockQMessageBox.question\n        QMessageBox.about = MockQMessageBox.about\n\n        print(\"Global QMessageBox patches applied to prevent popup dialogs\")\n    except ImportError:\n        # PySide6 not available, skip QMessageBox patches\n        print(\"PySide6 not available, skipping QMessageBox patches\")\n    except Exception as e:\n        print(f\"Failed to apply QMessageBox patches: {e}\")\n\n\n# Set up QMessageBox patches\nsetup_qmessagebox_patches()\n\n# Import the formatter from core.logger instead of duplicating it\n# Note: This import may fail for development tools tests that don't need core modules\ntry:\n    from core.logger import PytestContextLogFormatter as PytestContextLogFormatter\nexcept (ImportError, ModuleNotFoundError):\n    # Core modules not available (e.g., in development tools tests)\n    # Define a minimal formatter for tests that don't use core.logger available.\n    class _FallbackPytestContextLogFormatter(logging.Formatter):\n        \"\"\"Minimal formatter for tests that don't have core.logger available.\"\"\"\n\n        def __init__(self, fmt=None, datefmt=None):\n            super().__init__(fmt=fmt, datefmt=datefmt)\n\n    PytestContextLogFormatter: type[logging.Formatter] = (\n        _FallbackPytestContextLogFormatter\n    )\n\n# Global flag to prevent multiple test logging setups\n_test_logging_setup_done = False\n_test_logger_global = None\n_test_log_file_global = None\n\n\n# Set up dedicated testing logging\ndef setup_test_logging():\n    \"\"\"Set up dedicated logging for tests with complete isolation from main app logging.\"\"\"\n    global _test_logging_setup_done, _test_logger_global, _test_log_file_global\n\n    # Prevent multiple setup calls\n    if _test_logging_setup_done:\n        return _test_logger_global, _test_log_file_global\n\n    _test_logging_setup_done = True\n\n    # Suppress Discord library warnings\n    import warnings\n\n    warnings.filterwarnings(\n        \"ignore\",\n        message=\"'audioop' is deprecated and slated for removal in Python 3.13\",\n        category=DeprecationWarning,\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        message=\"parameter 'timeout' of type 'float' is deprecated\",\n        category=DeprecationWarning,\n    )\n\n    # Create test logs directory\n    test_logs_dir = Path(project_root) / \"tests\" / \"logs\"\n    test_logs_dir.mkdir(exist_ok=True)\n    (test_logs_dir / \"backups\").mkdir(exist_ok=True)\n\n    # Create test log filename with consistent naming (one per session)\n    test_log_file = test_logs_dir / \"test_run.log\"\n\n    # Configure test logger\n    test_logger = logging.getLogger(\"mhm_tests\")\n    # Respect TEST_VERBOSE_LOGS:\n    #   0 = WARNING (failures/warnings/skips only), 1 = INFO (test execution details), 2 = DEBUG (everything)\n    # All levels log failures, warnings, and skips - the difference is in infrastructure logging\n    verbose_logs = os.getenv(\"TEST_VERBOSE_LOGS\", \"0\")\n    if verbose_logs == \"2\":\n        test_logger_level = logging.DEBUG\n    elif verbose_logs == \"1\":\n        test_logger_level = logging.INFO\n    else:\n        test_logger_level = logging.WARNING  # Level 0: Only failures, warnings, skips\n    test_logger.setLevel(test_logger_level)\n\n    # Clear any existing handlers\n    test_logger.handlers.clear()\n\n    # Use simple file handler for test logs (no size-based rotation during session)\n    file_handler = logging.FileHandler(test_log_file, encoding=\"utf-8\")\n    # File handler level matches logger level to respect verbose setting\n    file_handler.setLevel(test_logger_level)\n\n    # Console handler for test output\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.ERROR)  # Minimize console spam during full runs\n\n    # Create formatter with test context\n    formatter = PytestContextLogFormatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n    file_handler.setFormatter(formatter)\n    console_handler.setFormatter(formatter)\n\n    # Add handlers to test logger only\n    test_logger.addHandler(file_handler)\n    test_logger.addHandler(console_handler)\n\n    # Prevent test logger from propagating to root logger\n    test_logger.propagate = False\n\n    # Also set up a handler for any \"mhm\" loggers to go to test logs\n    mhm_logger = logging.getLogger(\"mhm\")\n    # Keep component loggers quiet at levels 0 and 1 to avoid excessive logging\n    # Level 1 focuses on test execution details, not component infrastructure chatter\n    # 0 = WARNING, 1 = WARNING (quiet), 2 = DEBUG\n    verbose_logs = os.getenv(\"TEST_VERBOSE_LOGS\", \"0\")\n    if verbose_logs == \"2\":\n        mhm_logger.setLevel(logging.DEBUG)\n    else:\n        mhm_logger.setLevel(logging.WARNING)  # Levels 0 and 1: Only warnings and errors\n    mhm_logger.handlers.clear()\n    mhm_logger.addHandler(file_handler)\n    mhm_logger.propagate = False\n\n    # Store for reuse\n    _test_logger_global = test_logger\n    _test_log_file_global = test_log_file\n\n    return test_logger, test_log_file\n\n\n# Set up test logging\ntest_logger, test_log_file = setup_test_logging()\n\n\n# Session-based log rotation management\nclass SessionLogRotationManager:\n    \"\"\"Manages session-based log rotation that rotates ALL logs together if any exceed size limits.\"\"\"\n\n    def __init__(\n        self, max_size_mb=2\n    ):  # 2MB for test logs (lower than production 5MB for more frequent rotation)\n        self.max_size_bytes = max_size_mb * 1024 * 1024\n        self.log_files = []\n        self.rotation_needed = False\n        self.last_rotation_check = self._load_last_rotation_time()\n\n    def _get_rotation_state_file(self):\n        \"\"\"Get the path to the file that stores last rotation time.\"\"\"\n        rotation_state_file = (\n            Path(os.environ.get(\"LOG_BACKUP_DIR\", \"tests/logs/backups\"))\n            / \".last_rotation\"\n        )\n        return rotation_state_file\n\n    def _load_last_rotation_time(self):\n        \"\"\"Load the last rotation time from persistent storage.\"\"\"\n        rotation_state_file = self._get_rotation_state_file()\n        try:\n            if rotation_state_file.exists():\n                with open(rotation_state_file, \"r\", encoding=\"utf-8\") as f:\n                    timestamp_str = f.read().strip()\n                    if timestamp_str:\n                        return datetime.fromisoformat(timestamp_str)\n        except (OSError, ValueError) as e:\n            test_logger.debug(f\"Could not load last rotation time: {e}\")\n        return None\n\n    def _save_last_rotation_time(self, timestamp):\n        \"\"\"Save the last rotation time to persistent storage.\"\"\"\n        rotation_state_file = self._get_rotation_state_file()\n        try:\n            rotation_state_file.parent.mkdir(parents=True, exist_ok=True)\n            with open(rotation_state_file, \"w\", encoding=\"utf-8\") as f:\n                f.write(timestamp.isoformat())\n        except OSError as e:\n            test_logger.debug(f\"Could not save last rotation time: {e}\")\n\n    def register_log_file(self, file_path):\n        \"\"\"Register a log file for session-based rotation monitoring.\n\n        Files are registered even if they don't exist yet - they'll be checked\n        for rotation when they're created.\n        \"\"\"\n        if file_path:\n            # Convert to absolute path for consistency\n            abs_path = os.path.abspath(file_path)\n            if abs_path not in self.log_files:\n                self.log_files.append(abs_path)\n\n    def check_rotation_needed(self):\n        \"\"\"Check if any log file exceeds the size limit or if time-based rotation is needed.\n\n        This method checks rotation conditions but does NOT update last_rotation_check.\n        The timestamp is only updated after rotation actually completes in rotate_all_logs().\n        \"\"\"\n        from datetime import datetime, timedelta\n\n        now = datetime.now()\n\n        # Check time-based rotation (daily rotation for test logs)\n        if self.last_rotation_check is not None:\n            # We have a previous rotation timestamp - check elapsed time\n            time_since_last = now - self.last_rotation_check\n            if time_since_last > timedelta(hours=24):\n                self.rotation_needed = True\n                test_logger.info(\n                    f\"Time-based rotation needed (last rotation: {self.last_rotation_check}, elapsed: {time_since_last})\"\n                )\n                return True\n        else:\n            # First time checking - use the oldest log entry timestamp to determine if rotation is needed\n            # This handles the case where logs are old but we've never rotated before\n            # File modification time is unreliable because it gets updated on every write\n            for log_file in self.log_files:\n                try:\n                    if os.path.exists(log_file):\n                        # Try to parse the header timestamp to find when logs actually started\n                        oldest_timestamp = self._get_oldest_log_timestamp(log_file)\n                        if oldest_timestamp:\n                            time_since_oldest = now - oldest_timestamp\n                            if time_since_oldest > timedelta(hours=24):\n                                self.rotation_needed = True\n                                test_logger.info(\n                                    f\"Time-based rotation needed (log file {log_file} has entries from {oldest_timestamp}, {time_since_oldest} old, no previous rotation)\"\n                                )\n                                return True\n                except (OSError, FileNotFoundError) as e:\n                    test_logger.debug(\n                        f\"Could not check oldest log timestamp for {log_file}: {e}\"\n                    )\n                    continue\n\n        # Check size-based rotation\n        for log_file in self.log_files:\n            try:\n                if os.path.exists(log_file):\n                    file_size = os.path.getsize(log_file)\n                    if file_size > self.max_size_bytes:\n                        self.rotation_needed = True\n                        size_mb = file_size / (1024 * 1024)\n                        test_logger.info(\n                            f\"Log file {log_file} exceeds limit ({size_mb:.2f}MB > {self.max_size_bytes / (1024 * 1024):.2f}MB), rotation needed\"\n                        )\n                        return True\n            except (OSError, FileNotFoundError) as e:\n                test_logger.debug(f\"Could not check size for {log_file}: {e}\")\n                continue\n\n        return False\n\n    def _get_oldest_log_timestamp(self, log_file: str):\n        \"\"\"Extract the oldest timestamp from a log file by checking the header or first log entry.\n\n        Returns the datetime of the oldest entry, or None if it can't be determined.\n        \"\"\"\n        try:\n            with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                # Read first few lines to find header or first log entry\n                for i, line in enumerate(f):\n                    if i > 20:  # Don't read too far\n                        break\n\n                    # Check for header timestamp: \"# TEST RUN STARTED: YYYY-MM-DD HH:MM:SS\"\n                    if \"TEST RUN STARTED:\" in line:\n                        try:\n                            # Extract timestamp from header\n                            match = re.search(\n                                r\"(\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2})\", line\n                            )\n                            if match:\n                                timestamp_str = match.group(1)\n                                return datetime.strptime(timestamp_str, TIMESTAMP_FULL)\n                        except (ValueError, AttributeError):\n                            pass\n\n                    # Check for log entry timestamp: \"YYYY-MM-DD HH:MM:SS - ...\"\n                    if re.match(r\"^\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}\", line):\n                        try:\n                            timestamp_str = line[\n                                :19\n                            ]  # First 19 chars are \"YYYY-MM-DD HH:MM:SS\"\n                            return datetime.strptime(timestamp_str, TIMESTAMP_FULL)\n                        except ValueError:\n                            pass\n        except (OSError, FileNotFoundError, UnicodeDecodeError):\n            pass\n        return None\n\n    def _write_log_header(self, log_file: str, timestamp: str):\n        \"\"\"Write a formatted header to a log file during rotation.\n\n        Args:\n            log_file: Path to the log file\n            timestamp: Timestamp string to include in header\n        \"\"\"\n        log_filename = Path(log_file).name\n\n        if \"test_run\" in log_filename:\n            header_text = (\n                f\"{'='*80}\\n\"\n                f\"# TEST RUN STARTED: {timestamp}\\n\"\n                f\"# Test Execution Logging Active\\n\"\n                f\"# Test execution and framework logs are captured here\\n\"\n                f\"{'='*80}\\n\\n\"\n            )\n        elif \"consolidated\" in log_filename:\n            header_text = (\n                f\"{'='*80}\\n\"\n                f\"# TEST RUN STARTED: {timestamp}\\n\"\n                f\"# Component Logging Active\\n\"\n                f\"# Real component logs from application components are captured here\\n\"\n                f\"{'='*80}\\n\\n\"\n            )\n        else:\n            # Default header for other log files\n            header_text = f\"# Log rotated at {timestamp}\\n\"\n\n        # Use 'w' mode for rotation (creates new file)\n        # Add retry logic to handle file locking\n        import time\n\n        max_retries = 3\n        retry_delay = 0.1\n\n        for attempt in range(max_retries):\n            try:\n                with open(log_file, \"w\", encoding=\"utf-8\") as f:\n                    f.write(header_text)\n                break  # Success\n            except (OSError, PermissionError) as e:\n                if attempt < max_retries - 1:\n                    time.sleep(retry_delay)\n                    continue\n                # Last attempt failed - log warning but don't raise\n                test_logger.warning(\n                    f\"Failed to write header to {log_file} after {max_retries} attempts: {e}\"\n                )\n\n    def rotate_all_logs(self, rotation_context=\"session\"):\n        \"\"\"Rotate all registered log files together to maintain continuity.\n\n        Args:\n            rotation_context: Context string for logging (e.g., \"session start\", \"session end\")\n        \"\"\"\n        # Check if rotation is needed first\n        if not self.check_rotation_needed():\n            # If no rotation needed, initialize last_rotation_check on first call\n            # This prevents time-based rotation from triggering immediately on next check\n            if self.last_rotation_check is None:\n                self.last_rotation_check = datetime.now()\n            return\n\n        test_logger.info(f\"Starting {rotation_context} log rotation for all log files\")\n        timestamp = now_timestamp_filename()\n        timestamp_display = now_timestamp_full()\n\n        # Ensure backups directory exists\n        backup_dir = Path(os.environ.get(\"LOG_BACKUP_DIR\", \"tests/logs/backups\"))\n        backup_dir.mkdir(parents=True, exist_ok=True)\n\n        rotated_files = []\n        failed_files = []\n\n        for log_file in self.log_files:\n            try:\n                if not os.path.exists(log_file):\n                    test_logger.debug(f\"Skipping {log_file} - file does not exist\")\n                    continue\n\n                file_size = os.path.getsize(log_file)\n                if file_size == 0:\n                    test_logger.debug(f\"Skipping {log_file} - file is empty\")\n                    continue\n\n                # Create backup filename with timestamp\n                log_filename = Path(log_file).name\n                backup_filename = f\"{log_filename}.{timestamp}.bak\"\n                backup_file = backup_dir / backup_filename\n\n                # Handle Windows file locking by copying instead of moving\n                # Add retry logic with timeout to prevent hanging\n                import time\n\n                max_retries = 5  # Increased retries\n                retry_delay = 0.2  # Longer delay between retries\n                rotation_success = False\n\n                for attempt in range(max_retries):\n                    try:\n                        # Try to move first (faster and atomic)\n                        shutil.move(log_file, backup_file)\n                        # Verify backup was created and has content\n                        if backup_file.exists():\n                            backup_size = backup_file.stat().st_size\n                            if backup_size > 0:\n                                test_logger.info(\n                                    f\"Rotated {log_file} ({file_size} bytes) to {backup_file} ({backup_size} bytes)\"\n                                )\n                                rotation_success = True\n                                rotated_files.append(log_file)\n                                break\n                            else:\n                                test_logger.warning(\n                                    f\"Move succeeded but backup file is empty: {backup_file}\"\n                                )\n                        else:\n                            test_logger.warning(\n                                f\"Move succeeded but backup file is missing: {backup_file}\"\n                            )\n                    except (OSError, PermissionError) as move_error:\n                        if attempt < max_retries - 1:\n                            time.sleep(retry_delay)\n                            continue\n                        # If move fails after retries, try copy method\n                        try:\n                            test_logger.warning(\n                                f\"Move failed for {log_file} due to file locking, using copy method: {move_error}\"\n                            )\n                            # Read entire file first to ensure we get all content\n                            with open(log_file, \"rb\") as src:\n                                with open(backup_file, \"wb\") as dst:\n                                    shutil.copyfileobj(src, dst)\n                            # Verify backup was created and has content\n                            if backup_file.exists():\n                                backup_size = backup_file.stat().st_size\n                                if backup_size > 0:\n                                    test_logger.info(\n                                        f\"Copied {log_file} ({file_size} bytes) to {backup_file} ({backup_size} bytes)\"\n                                    )\n                                    rotation_success = True\n                                    rotated_files.append(log_file)\n                                    break\n                                else:\n                                    test_logger.warning(\n                                        f\"Copy succeeded but backup file is empty: {backup_file}\"\n                                    )\n                            else:\n                                test_logger.warning(\n                                    f\"Copy succeeded but backup file is missing: {backup_file}\"\n                                )\n                        except (OSError, PermissionError) as copy_error:\n                            test_logger.warning(\n                                f\"Copy also failed for {log_file}: {copy_error}\"\n                            )\n                            failed_files.append((log_file, str(copy_error)))\n                            continue\n\n                if rotation_success:\n                    # Truncate the original file and write formatted header\n                    # Add retry for header writing too\n                    for attempt in range(max_retries):\n                        try:\n                            self._write_log_header(log_file, timestamp_display)\n                            if attempt > 0:\n                                test_logger.info(f\"Truncated {log_file} after copy\")\n                            break\n                        except (OSError, PermissionError) as header_error:\n                            if attempt < max_retries - 1:\n                                time.sleep(retry_delay)\n                                continue\n                            test_logger.warning(\n                                f\"Failed to write header to {log_file}: {header_error}\"\n                            )\n                else:\n                    failed_files.append((log_file, \"Rotation failed after all retries\"))\n\n            except (OSError, FileNotFoundError) as e:\n                test_logger.warning(f\"Failed to rotate {log_file}: {e}\")\n                failed_files.append((log_file, str(e)))\n\n        # Log summary\n        if rotated_files:\n            test_logger.info(\n                f\"Successfully rotated {len(rotated_files)} log file(s): {', '.join(Path(f).name for f in rotated_files)}\"\n            )\n        if failed_files:\n            test_logger.warning(\n                f\"Failed to rotate {len(failed_files)} log file(s): {', '.join(f'{Path(f).name} ({err})' for f, err in failed_files)}\"\n            )\n\n        self.rotation_needed = False\n        rotation_time = datetime.now()\n        self.last_rotation_check = rotation_time\n        self._save_last_rotation_time(rotation_time)\n        test_logger.info(f\"{rotation_context.capitalize()} log rotation completed\")\n\n\n# Helper function for writing log headers\ndef _write_test_log_header(log_file: str, timestamp: str):\n    \"\"\"Write a formatted header to a test log file.\n\n    Args:\n        log_file: Path to the log file\n        timestamp: Timestamp string to include in header (format: 'YYYY-MM-DD HH:MM:SS')\n    \"\"\"\n    # Skip header writing in parallel worker processes to avoid duplicate headers\n    # pytest-xdist sets PYTEST_XDIST_WORKER for worker processes\n    if os.environ.get(\"PYTEST_XDIST_WORKER\"):\n        return\n\n    log_filename = Path(log_file).name\n\n    if \"test_run\" in log_filename:\n        header_text = (\n            f\"\\n{'='*80}\\n\"\n            f\"# TEST RUN STARTED: {timestamp}\\n\"\n            f\"# Test Execution Logging Active\\n\"\n            f\"# Test execution and framework logs are captured here\\n\"\n            f\"{'='*80}\\n\\n\"\n        )\n    elif \"consolidated\" in log_filename:\n        header_text = (\n            f\"\\n{'='*80}\\n\"\n            f\"# TEST RUN STARTED: {timestamp}\\n\"\n            f\"# Component Logging Active\\n\"\n            f\"# Real component logs from application components are captured here\\n\"\n            f\"{'='*80}\\n\\n\"\n        )\n    else:\n        # Default header for other log files\n        header_text = f\"# Log rotated at {timestamp}\\n\"\n\n    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n        f.write(header_text)\n\n\n# Global session rotation manager\nsession_rotation_manager = SessionLogRotationManager()\n\n# Global consolidated handler for new component loggers\n_consolidated_handler = None\n\n# Register the test_run file with session rotation manager\nif test_log_file and test_log_file.exists():\n    session_rotation_manager.register_log_file(str(test_log_file))\n\n\n# Log lifecycle management\nclass LogLifecycleManager:\n    \"\"\"Manages log file lifecycle including backup, archive, and cleanup operations.\"\"\"\n\n    def __init__(self, archive_days=30):\n        self.archive_days = archive_days\n        self.backup_dir = Path(os.environ.get(\"LOG_BACKUP_DIR\", \"tests/logs/backups\"))\n        self.archive_dir = Path(os.environ.get(\"LOG_ARCHIVE_DIR\", \"tests/logs/archive\"))\n\n        # Ensure directories exist\n        self.backup_dir.mkdir(parents=True, exist_ok=True)\n        self.archive_dir.mkdir(parents=True, exist_ok=True)\n\n    def cleanup_old_archives(self):\n        \"\"\"Remove archive files older than the specified number of days.\"\"\"\n        cutoff_date = datetime.now().timestamp() - (self.archive_days * 24 * 60 * 60)\n\n        cleaned_count = 0\n        for archive_file in self.archive_dir.glob(\"*\"):\n            try:\n                if (\n                    archive_file.is_file()\n                    and archive_file.stat().st_mtime < cutoff_date\n                ):\n                    archive_file.unlink()\n                    cleaned_count += 1\n                    test_logger.debug(f\"Removed old archive: {archive_file}\")\n            except (OSError, FileNotFoundError) as e:\n                test_logger.warning(f\"Failed to remove archive {archive_file}: {e}\")\n\n        if cleaned_count > 0:\n            test_logger.info(\n                f\"Cleaned up {cleaned_count} old archive files (older than {self.archive_days} days)\"\n            )\n\n        return cleaned_count\n\n    def archive_old_backups(self):\n        \"\"\"Move old backup files to archive directory.\"\"\"\n        cutoff_date = datetime.now().timestamp() - (7 * 24 * 60 * 60)  # 7 days\n\n        archived_count = 0\n        for backup_file in self.backup_dir.glob(\"*\"):\n            try:\n                if backup_file.is_file() and backup_file.stat().st_mtime < cutoff_date:\n                    # Create archive filename with timestamp\n                    timestamp = now_timestamp_filename()\n                    archive_filename = (\n                        f\"{backup_file.stem}_{timestamp}{backup_file.suffix}\"\n                    )\n                    archive_path = self.archive_dir / archive_filename\n\n                    shutil.move(str(backup_file), str(archive_path))\n                    archived_count += 1\n                    test_logger.debug(\n                        f\"Archived backup: {backup_file} -> {archive_path}\"\n                    )\n            except (OSError, FileNotFoundError) as e:\n                test_logger.warning(f\"Failed to archive backup {backup_file}: {e}\")\n\n        if archived_count > 0:\n            test_logger.info(f\"Archived {archived_count} old backup files\")\n\n        return archived_count\n\n    def perform_lifecycle_maintenance(self):\n        \"\"\"Perform all lifecycle maintenance operations.\"\"\"\n        # Reduce verbosity - only log if something actually happens\n        archived_count = self.archive_old_backups()\n        cleanup_count = self.cleanup_old_archives()\n        if archived_count > 0 or cleanup_count > 0:\n            test_logger.debug(\n                f\"Log lifecycle maintenance: archived {archived_count}, cleaned {cleanup_count}\"\n            )\n\n\n# Global log lifecycle manager\nlog_lifecycle_manager = LogLifecycleManager()\n\n\n# Configure size-based rotation for component logs during tests to avoid growth\n@pytest.fixture(scope=\"session\", autouse=True)\ndef setup_consolidated_test_logging():\n    \"\"\"Set up consolidated test logging - all component logs go to a single file.\n\n    This replaces the complex multi-file logging system with a single consolidated log file\n    that contains all component logs, making it much easier to manage and debug.\n\n    In parallel execution mode (pytest-xdist), uses per-worker log files to avoid\n    file locking issues and interleaved log entries.\n    \"\"\"\n    # Detect parallel execution mode\n    worker_id = os.environ.get(\"PYTEST_XDIST_WORKER\")\n    is_parallel = worker_id is not None\n\n    # Use per-worker log files in parallel mode to avoid file locking issues\n    if is_parallel:\n        # Per-worker log files: test_run_gw0.log, test_consolidated_gw0.log, etc.\n        consolidated_log_file = (\n            Path(project_root) / \"tests\" / \"logs\" / f\"test_consolidated_{worker_id}.log\"\n        )\n        test_run_log_file = (\n            Path(project_root) / \"tests\" / \"logs\" / f\"test_run_{worker_id}.log\"\n        )\n    else:\n        # Sequential mode: use standard log files\n        consolidated_log_file = (\n            Path(project_root) / \"tests\" / \"logs\" / \"test_consolidated.log\"\n        )\n        test_run_log_file = Path(project_root) / \"tests\" / \"logs\" / \"test_run.log\"\n\n    consolidated_log_file.parent.mkdir(exist_ok=True)\n\n    # CRITICAL: Only rotate in the main process, not in worker processes\n    # Workers use per-worker log files that get consolidated later\n    # Rotation should only happen on the main log files (test_run.log, test_consolidated.log)\n    rotation_happened = False\n\n    # Only check rotation in main process (not in workers)\n    # Workers use per-worker files that don't need rotation\n    if not is_parallel:\n        # Register both log files with session rotation manager BEFORE checking rotation\n        session_rotation_manager.register_log_file(str(consolidated_log_file))\n        session_rotation_manager.register_log_file(str(test_run_log_file))\n\n        # Check for rotation BEFORE writing headers (rotation will create backups and write headers to new files)\n        # CRITICAL: Only rotate if files are safe to rotate (not locked, not being written to)\n        rotation_needed = session_rotation_manager.check_rotation_needed()\n    else:\n        rotation_needed = False\n\n    if rotation_needed and not is_parallel:\n        # Check if files are safe to rotate (not locked by active handlers)\n        # We can only safely rotate if no handlers are currently writing to the files\n        files_safe_to_rotate = True\n\n        for log_file in session_rotation_manager.log_files:\n            if os.path.exists(log_file):\n                # Try to open the file in exclusive mode to check if it's locked\n                try:\n                    # On Windows, opening in 'r+b' mode will fail if file is locked\n                    with open(log_file, \"r+b\") as f:\n                        f.seek(0, 2)  # Seek to end\n                        f.flush()\n                except (OSError, PermissionError) as e:\n                    test_logger.warning(\n                        f\"Cannot rotate {log_file} - file is locked (likely still being written to): {e}\"\n                    )\n                    files_safe_to_rotate = False\n                    break\n\n        if files_safe_to_rotate:\n            # Close any existing handlers that might be writing to these files\n            # This prevents \"file in use\" errors during rotation\n            loggers_to_check = [\n                test_logger,\n                logging.getLogger(\"mhm\"),\n                logging.getLogger(\"mhm.error_handler\"),\n                logging.getLogger(\"mhm_tests\"),\n                logging.getLogger(\"mhm_tests.run_tests\"),\n                logging.root,\n            ]\n\n            # Also check all existing loggers\n            for logger_name in logging.Logger.manager.loggerDict:\n                loggers_to_check.append(logging.getLogger(logger_name))\n\n            handlers_closed = 0\n            for logger in loggers_to_check:\n                for handler in list(logger.handlers):\n                    if isinstance(handler, logging.FileHandler):\n                        try:\n                            handler.flush()\n                            handler.close()\n                            logger.removeHandler(handler)\n                            handlers_closed += 1\n                        except Exception:\n                            pass\n\n            if handlers_closed > 0:\n                test_logger.debug(\n                    f\"Closed {handlers_closed} file handlers before rotation\"\n                )\n\n            # Short delay to ensure Windows releases file handles\n            time.sleep(0.2)\n\n            # Attempt rotation with timeout to prevent hanging\n            # Use threading to ensure rotation doesn't block test execution\n            import threading\n\n            rotation_complete = threading.Event()\n            rotation_error: list[Exception | None] = [None]\n            rotation_success = [False]\n\n            def do_rotation():\n                try:\n                    session_rotation_manager.rotate_all_logs(\n                        rotation_context=\"session start\"\n                    )\n                    rotation_success[0] = True\n                    test_logger.info(\n                        \"Performed automatic log rotation at session start\"\n                    )\n                except Exception as e:\n                    rotation_error[0] = e\n                    test_logger.warning(f\"Log rotation failed: {e}\")\n                finally:\n                    rotation_complete.set()\n\n            rotation_thread = threading.Thread(target=do_rotation, daemon=True)\n            rotation_thread.start()\n\n            # Wait up to 5 seconds for rotation to complete (non-blocking)\n            if rotation_complete.wait(timeout=5.0):\n                if rotation_error[0]:\n                    test_logger.warning(\n                        f\"Log rotation completed with errors: {rotation_error[0]}\"\n                    )\n                elif rotation_success[0]:\n                    rotation_happened = True\n            else:\n                test_logger.warning(\n                    \"Log rotation timed out after 5 seconds - continuing without rotation (logs will rotate on next run)\"\n                )\n        else:\n            test_logger.info(\n                \"Skipping log rotation - files are locked (likely from previous session still cleaning up)\"\n            )\n\n    # Ensure test_run.log exists so rotation manager can write headers to it\n    if not test_run_log_file.exists():\n        test_run_log_file.touch()\n\n    # Write headers only if rotation didn't happen (rotation already wrote headers)\n    # or if files are empty/new\n    from datetime import datetime\n\n    timestamp = now_timestamp_full()\n\n    if not rotation_happened:\n        # Check if files are empty or don't exist - only write headers if needed\n        write_header_run = (\n            not test_run_log_file.exists() or test_run_log_file.stat().st_size == 0\n        )\n        write_header_consolidated = (\n            not consolidated_log_file.exists()\n            or consolidated_log_file.stat().st_size == 0\n        )\n\n        if write_header_run:\n            _write_test_log_header(str(test_run_log_file), timestamp)\n        if write_header_consolidated:\n            _write_test_log_header(str(consolidated_log_file), timestamp)\n\n    # In parallel mode, update environment variables to use per-worker consolidated log\n    # This prevents file locking issues - all component logs go to the per-worker consolidated file\n    if is_parallel:\n        logs_dir = Path(project_root) / \"tests\" / \"logs\"\n        worker_consolidated = str(consolidated_log_file)\n        # Point all component loggers to the per-worker consolidated log\n        os.environ[\"LOG_MAIN_FILE\"] = worker_consolidated\n        os.environ[\"LOG_ERRORS_FILE\"] = worker_consolidated\n        os.environ[\"LOG_DISCORD_FILE\"] = worker_consolidated\n        os.environ[\"LOG_AI_FILE\"] = worker_consolidated\n        os.environ[\"LOG_USER_ACTIVITY_FILE\"] = worker_consolidated\n        os.environ[\"LOG_COMMUNICATION_MANAGER_FILE\"] = worker_consolidated\n        os.environ[\"LOG_EMAIL_FILE\"] = worker_consolidated\n        os.environ[\"LOG_UI_FILE\"] = worker_consolidated\n        os.environ[\"LOG_FILE_OPS_FILE\"] = worker_consolidated\n        os.environ[\"LOG_SCHEDULER_FILE\"] = worker_consolidated\n\n        # CRITICAL: Update test_logger to write to per-worker test_run log file\n        # test_logger was set up at import time with test_run.log, but in parallel mode\n        # we need it to write to test_run_gw*.log so [WORKER-TEST] entries are captured\n        # test_logger is already a global variable, so we can reference it directly\n        if test_logger:\n            # CRITICAL: Ensure logger level is at least INFO to capture [WORKER-TEST] entries\n            # These are essential for debugging memory leaks, regardless of verbose setting\n            if test_logger.level > logging.INFO:\n                test_logger.setLevel(logging.INFO)\n\n            # Find and update the existing file handler instead of removing/recreating\n            # This preserves any other handlers (like console handler) and avoids timing issues\n            file_handler_found = False\n            for handler in list(test_logger.handlers):\n                if isinstance(handler, logging.FileHandler):\n                    # Close the old file handler\n                    handler.close()\n                    test_logger.removeHandler(handler)\n                    file_handler_found = True\n\n            # Add new file handler for per-worker log file\n            worker_test_run_handler = logging.FileHandler(\n                test_run_log_file, encoding=\"utf-8\", mode=\"a\"\n            )\n            # CRITICAL: Always set handler to INFO level to capture [WORKER-TEST] entries\n            # These are essential for debugging memory leaks, regardless of verbose setting\n            worker_test_run_handler.setLevel(logging.INFO)\n\n            # Use same formatter as before\n            # PytestContextLogFormatter is defined at module level (line 434)\n            # Reference it from the current module's globals to avoid UnboundLocalError in parallel mode\n            import sys\n\n            current_module = sys.modules[__name__]\n            formatter = current_module.PytestContextLogFormatter(\n                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n            )\n            worker_test_run_handler.setFormatter(formatter)\n            test_logger.addHandler(worker_test_run_handler)\n    else:\n        # Sequential mode: point all component loggers to the consolidated log\n        consolidated_path = str(consolidated_log_file)\n        os.environ[\"LOG_MAIN_FILE\"] = consolidated_path\n        os.environ[\"LOG_ERRORS_FILE\"] = consolidated_path\n        os.environ[\"LOG_DISCORD_FILE\"] = consolidated_path\n        os.environ[\"LOG_AI_FILE\"] = consolidated_path\n        os.environ[\"LOG_USER_ACTIVITY_FILE\"] = consolidated_path\n        os.environ[\"LOG_COMMUNICATION_MANAGER_FILE\"] = consolidated_path\n        os.environ[\"LOG_EMAIL_FILE\"] = consolidated_path\n        os.environ[\"LOG_UI_FILE\"] = consolidated_path\n        os.environ[\"LOG_FILE_OPS_FILE\"] = consolidated_path\n        os.environ[\"LOG_SCHEDULER_FILE\"] = consolidated_path\n\n    # Set up separate handlers for component logs vs test execution logs\n    # Component logs go directly to test_consolidated.log (no test context)\n    # Test execution logs go directly to test_run.log (with test context)\n\n    # Create handler for component logs (no test context)\n    component_handler = logging.FileHandler(\n        str(consolidated_log_file), mode=\"a\", encoding=\"utf-8\"\n    )\n    # CRITICAL: Set handler level to WARNING for levels 0 and 1 to prevent excessive logging\n    # Only level 2 (DEBUG) will log everything from components\n    verbose_logs = os.getenv(\"TEST_VERBOSE_LOGS\", \"0\")\n    if verbose_logs == \"2\":\n        component_handler.setLevel(logging.DEBUG)\n    else:\n        component_handler.setLevel(\n            logging.WARNING\n        )  # Levels 0 and 1: Only warnings and errors\n    component_formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    component_handler.setFormatter(component_formatter)\n\n    # Create handler for test execution logs (with test context)\n    test_handler = logging.FileHandler(\n        str(test_run_log_file), mode=\"a\", encoding=\"utf-8\"\n    )\n    try:\n        from core.logger import PytestContextLogFormatter\n\n        test_formatter = PytestContextLogFormatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\",\n        )\n    except (ImportError, ModuleNotFoundError):\n        # Core modules not available (e.g., in development tools tests)\n        # Use standard formatter instead\n        test_formatter = logging.Formatter(\n            \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\",\n        )\n    test_handler.setFormatter(test_formatter)\n\n    # CRITICAL: Ensure root logger has a valid level FIRST (child loggers inherit from root)\n    # This must happen before we configure other loggers to prevent None comparisons\n    root_logger = logging.getLogger()\n    if not isinstance(root_logger.level, int) or root_logger.level == logging.NOTSET:\n        root_logger.setLevel(logging.WARNING)  # Set a safe default level\n\n    # Also ensure all existing loggers have valid levels before we start configuring\n    # Walk through logger hierarchy and ensure no None levels\n    for logger_name in list(logging.Logger.manager.loggerDict.keys()):\n        try:\n            logger_obj = logging.getLogger(logger_name)\n            if isinstance(logger_obj, logging.Logger):\n                # Ensure logger has a valid level (not None, not NOTSET without parent)\n                if logger_obj.level == logging.NOTSET:\n                    # If level is NOTSET, ensure parent has valid level\n                    parent = logger_obj.parent\n                    if parent and parent.level == logging.NOTSET:\n                        # Both logger and parent are NOTSET - set logger level\n                        logger_obj.setLevel(logging.WARNING)\n                elif logger_obj.level is None:\n                    # Level is None (invalid) - set to safe default\n                    logger_obj.setLevel(logging.WARNING)\n        except Exception:\n            continue  # Skip problematic loggers\n\n    # Force component loggers to reconfigure with new log paths\n    # Clear the component logger cache so they'll be recreated with updated paths\n    try:\n        from core.logger import _component_loggers\n\n        _component_loggers.clear()\n    except (ImportError, AttributeError):\n        pass  # If cache doesn't exist or isn't accessible, that's okay\n\n    # Ensure component loggers are created by importing the modules that create them\n    # This will recreate them with the updated environment variables\n    try:\n        from core.scheduler import SchedulerManager\n        from core.service import MHMService\n        from communication.core.channel_orchestrator import CommunicationManager\n        from ai.chatbot import AIChatbot\n    except ImportError:\n        pass  # Some modules might not be available during tests\n\n    # Component loggers are now available for configuration\n\n    # Configure loggers\n    for logger_name in list(\n        logging.Logger.manager.loggerDict.keys()\n    ):  # Use list() to avoid modification during iteration\n        try:\n            # Skip pytest's internal loggers to avoid breaking pytest's logging\n            if logger_name.startswith(\"_pytest\") or logger_name == \"pytest\":\n                continue\n\n            logger_obj = logging.getLogger(logger_name)\n\n            # Skip if logger_obj is not actually a Logger instance (could be PlaceHolder or other type)\n            if not isinstance(logger_obj, logging.Logger):\n                continue\n\n            # Clear existing handlers\n            for h in logger_obj.handlers[:]:\n                try:\n                    h.close()\n                except Exception:\n                    pass\n                logger_obj.removeHandler(h)\n\n            # Set appropriate log level\n            # Validate logging constants are actually integers (defensive check)\n            if (\n                not isinstance(logging.DEBUG, int)\n                or not isinstance(logging.INFO, int)\n                or not isinstance(logging.WARNING, int)\n            ):\n                # Logging module is in invalid state - skip configuration\n                test_logger.warning(\n                    \"Logging constants are invalid - skipping logger configuration\"\n                )\n                continue\n\n            if logger_name.startswith(\"mhm.\"):\n                # Component loggers: Keep quiet at levels 0 and 1 to avoid excessive logging\n                #   0 = WARNING (quiet), 1 = WARNING (quiet - focus on test execution, not component chatter), 2 = DEBUG (verbose)\n                verbose_logs = os.getenv(\"TEST_VERBOSE_LOGS\", \"0\")\n                if verbose_logs == \"2\":\n                    level = logging.DEBUG\n                else:\n                    level = (\n                        logging.WARNING\n                    )  # Levels 0 and 1: Only warnings and errors from components\n            else:\n                # Test execution loggers:\n                #   0 = WARNING (failures/warnings/skips only), 1 = INFO (test execution details), 2 = DEBUG (everything)\n                verbose_logs = os.getenv(\"TEST_VERBOSE_LOGS\", \"0\")\n                if verbose_logs == \"2\":\n                    level = logging.DEBUG\n                elif verbose_logs == \"1\":\n                    level = logging.INFO\n                else:\n                    level = logging.WARNING  # Level 0: Only failures, warnings, skips\n\n            # Ensure level is a valid integer before setting\n            if level is None or not isinstance(level, int):\n                level = logging.WARNING  # Safe default\n\n            # Set level with error handling in case logger is in invalid state\n            try:\n                logger_obj.setLevel(level)\n            except (TypeError, AttributeError) as e:\n                # Skip loggers that can't have their level set (might be in invalid state)\n                test_logger.debug(\n                    f\"Skipping logger {logger_name} - cannot set level: {e}\"\n                )\n                continue\n\n            # Component loggers go to test_consolidated.log (no test context)\n            if logger_name.startswith(\"mhm.\"):\n                logger_obj.addHandler(component_handler)\n                logger_obj.propagate = False\n            # Test execution loggers go to test_run.log (with test context)\n            else:\n                logger_obj.addHandler(test_handler)\n                logger_obj.propagate = True\n\n                # Special handling for mhm_tests logger: ensure it respects verbose setting\n                if logger_name == \"mhm_tests\":\n                    verbose_logs = os.getenv(\"TEST_VERBOSE_LOGS\", \"0\")\n                    if verbose_logs == \"2\":\n                        test_level = logging.DEBUG\n                    elif verbose_logs == \"1\":\n                        test_level = logging.INFO\n                    else:\n                        test_level = (\n                            logging.WARNING\n                        )  # Level 0: Only failures, warnings, skips\n                    logger_obj.setLevel(test_level)\n                    # Also update handler level\n                    for handler in logger_obj.handlers:\n                        if isinstance(handler, logging.FileHandler):\n                            handler.setLevel(test_level)\n\n        except Exception:\n            # Never fail tests due to logging configuration issues\n            continue\n\n    # Final check: ensure root logger still has valid level after configuration\n    root_logger = logging.getLogger()\n    if not isinstance(root_logger.level, int) or root_logger.level == logging.NOTSET:\n        root_logger.setLevel(logging.WARNING)  # Set a safe default level\n\n    # CRITICAL: Configure root logger to catch unnamed log entries (empty string logger name)\n    # Unnamed entries with test context should go to test_run.log, not test_consolidated.log\n    # Clear any existing handlers on root logger first\n    for handler in root_logger.handlers[:]:\n        try:\n            handler.close()\n        except Exception:\n            pass\n        root_logger.removeHandler(handler)\n    # Add test handler to root logger to catch unnamed logs (they should have test context)\n    root_logger.addHandler(test_handler)\n    root_logger.propagate = False  # Prevent double logging\n\n    # Note: Log files are already registered and rotation checked above, before headers are written\n\n    # Clean up any individual log files that were created before consolidated mode was enabled\n    _cleanup_individual_log_files()\n\n\n# --- HOUSEKEEPING: Prune old test artifacts to keep repo tidy ---\ndef _cleanup_test_log_files():\n    \"\"\"Clean up excessive test log files to prevent accumulation.\"\"\"\n    try:\n        logs_dir = Path(project_root) / \"tests\" / \"logs\"\n        if not logs_dir.exists():\n            return\n\n        # Remove old individual component log files (now that we use consolidated logging)\n        for log_file in logs_dir.glob(\"*.log\"):\n            # Skip the consolidated log file and test_run.log\n            if log_file.name in [\"test_consolidated.log\", \"test_run.log\"]:\n                continue\n\n            try:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    content = f.read().strip()\n                    # If file only contains rotation message or is empty, remove it\n                    if (\n                        content.startswith(\"# Log rotated at\")\n                        and len(content.split(\"\\n\")) <= 2\n                    ) or len(content) == 0:\n                        log_file.unlink()\n                        # Don't log cleanup operations - focus on test results\n            except Exception:\n                continue\n\n        # Clean up backup files older than 1 day\n        backups_dir = logs_dir / \"backups\"\n        if backups_dir.exists():\n            for backup_file in backups_dir.glob(\"*.bak\"):\n                try:\n                    if backup_file.stat().st_mtime < time.time() - 86400:  # 1 day\n                        backup_file.unlink()\n                        # Don't log cleanup operations - focus on test results\n                except Exception:\n                    continue\n\n    except Exception as e:\n        test_logger.warning(f\"Error during test log cleanup: {e}\")\n\n\ndef _cleanup_individual_log_files():\n    \"\"\"Clean up individual log files that were created before consolidated mode was enabled.\n\n    NOTE: We no longer copy content from these files because component loggers write directly\n    to test_consolidated.log via environment variables. These files should not exist in\n    consolidated logging mode, so we just delete them.\n    \"\"\"\n    try:\n        logs_dir = Path(project_root) / \"tests\" / \"logs\"\n        if not logs_dir.exists():\n            return\n\n        # List of individual log files that should be cleaned up in consolidated mode\n        individual_log_files = [\n            \"app.log\",\n            \"errors.log\",\n            \"ai.log\",\n            \"discord.log\",\n            \"user_activity.log\",\n            \"communication_manager.log\",\n            \"email.log\",\n            \"ui.log\",\n            \"file_ops.log\",\n            \"scheduler.log\",\n            \"schedule_utilities.log\",\n            \"analytics.log\",\n            \"message.log\",\n            \"backup.log\",\n            \"checkin_dynamic.log\",\n        ]\n\n        for log_file_name in individual_log_files:\n            log_file = logs_dir / log_file_name\n            if log_file.exists():\n                try:\n                    # Just delete the file - component loggers write directly to consolidated log\n                    # No need to copy content as it's already in test_consolidated.log\n                    log_file.unlink()\n                    test_logger.debug(\n                        f\"Cleaned up individual log file: {log_file_name}\"\n                    )\n                except Exception as e:\n                    test_logger.warning(f\"Error cleaning up {log_file_name}: {e}\")\n\n    except Exception as e:\n        test_logger.warning(f\"Error during individual log file cleanup: {e}\")\n\n\ndef _consolidate_and_cleanup_main_logs():\n    \"\"\"DEPRECATED: No longer consolidates from app.log/errors.log.\n\n    Component loggers now write directly to test_consolidated.log via environment variables.\n    This function is kept for backward compatibility but does nothing.\n    \"\"\"\n    # Component loggers write directly to consolidated log, no consolidation needed\n    pass\n\n\ndef _add_test_run_start_markers():\n    \"\"\"Add clear test run start markers to both consolidated and test_run log files.\"\"\"\n    # Headers are now written immediately when consolidated logging is set up\n    # No need to write duplicate headers here\n    pass\n\n\ndef _prune_old_files(\n    target_dir: Path, patterns: list[str], older_than_days: int\n) -> int:\n    \"\"\"Remove files in target_dir matching any pattern older than N days.\n\n    Returns the number of files removed.\n    \"\"\"\n    removed_count = 0\n    try:\n        if older_than_days <= 0:\n            return 0\n        cutoff = datetime.now().timestamp() - (older_than_days * 24 * 3600)\n        for pattern in patterns:\n            for file_path in target_dir.rglob(pattern):\n                try:\n                    if file_path.is_file() and file_path.stat().st_mtime < cutoff:\n                        file_path.unlink(missing_ok=True)\n                        removed_count += 1\n                except Exception:\n                    # Best-effort cleanup; ignore individual file errors\n                    pass\n    except Exception:\n        # Never fail tests due to cleanup issues\n        pass\n    return removed_count\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef clear_test_user_factory_cache():\n    \"\"\"Clear test user factory cache at the end of the test session.\"\"\"\n    yield\n    # Clear cache after all tests complete\n    try:\n        from tests.test_utilities import TestUserFactory\n\n        TestUserFactory.clear_cache()\n    except Exception:\n        pass  # Ignore errors during cleanup\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef prune_test_artifacts_before_and_after_session():\n    \"\"\"Prune old logs (tests/logs) and backups (tests/data/backups) before and after the session.\n\n    Defaults: logs older than 14 days, test backups older than 7 days.\n    Override via TEST_LOG_RETENTION_DAYS and TEST_BACKUP_RETENTION_DAYS env vars.\n    \"\"\"\n    project_root_path = Path(project_root)\n    logs_dir = project_root_path / \"tests\" / \"logs\"\n    test_backups_dir = project_root_path / \"tests\" / \"data\" / \"backups\"\n\n    log_retention_days = int(os.getenv(\"TEST_LOG_RETENTION_DAYS\", \"14\"))\n    backup_retention_days = int(os.getenv(\"TEST_BACKUP_RETENTION_DAYS\", \"7\"))\n\n    # Prune before tests\n    if logs_dir.exists():\n        removed = _prune_old_files(\n            logs_dir,\n            patterns=[\"*.log\", \"*.log.*\", \"*.gz\"],\n            older_than_days=log_retention_days,\n        )\n        # Don't log cleanup operations - focus on test results\n\n    if test_backups_dir.exists():\n        removed = _prune_old_files(\n            test_backups_dir,\n            patterns=[\"*.zip\"],\n            older_than_days=backup_retention_days,\n        )\n        # Don't log cleanup operations - focus on test results\n\n    # Clean up excessive test log files to prevent accumulation\n    _cleanup_test_log_files()\n\n    # Pre-run purge of stray pytest-of-* under tests/data and leftover tmp children\n    data_dir = project_root_path / \"tests\" / \"data\"\n    try:\n        # Clean up conversation_states.json before tests\n        conversation_states_file = data_dir / \"conversation_states.json\"\n        if conversation_states_file.exists():\n            try:\n                conversation_states_file.unlink(missing_ok=True)\n                # Don't log cleanup operations - focus on test results\n            except Exception:\n                pass\n\n        # Remove any pytest-of-* and tmp_* directories (pytest creates these when using tmpdir fixtures)\n        import glob\n\n        pytest_pattern = str(data_dir / \"pytest-of-*\")\n        pytest_dirs = glob.glob(pytest_pattern)\n        for stray in pytest_dirs:\n            stray_path = Path(stray)\n            if stray_path.exists():\n                shutil.rmtree(stray_path, ignore_errors=True)\n                # Don't log cleanup operations - focus on test results\n\n        # Remove tmp* directories directly in tests/data (but not the \"tmp\" directory itself)\n        # Matches: tmp_*, tmp* (but not just \"tmp\"), pytest-of-*\n        for item in data_dir.iterdir():\n            try:\n                if item.is_dir():\n                    if (\n                        item.name.startswith(\"tmp_\")\n                        or (item.name.startswith(\"tmp\") and item.name != \"tmp\")\n                        or item.name.startswith(\"pytest-of-\")\n                    ):\n                        shutil.rmtree(item, ignore_errors=True)\n                        # Don't log cleanup operations - focus on test results\n                elif item.is_file() and item.name == \".last_cache_cleanup\":\n                    item.unlink(missing_ok=True)\n                    # Don't log cleanup operations - focus on test results\n            except Exception:\n                pass\n\n        # Clean up test JSON files (test_*.json, .tmp_*.json, welcome_tracking.json)\n        test_json_patterns = [\"test_\", \".tmp_\", \"welcome_tracking.json\"]\n        for item in data_dir.iterdir():\n            try:\n                if item.is_file() and item.suffix == \".json\":\n                    if any(\n                        item.name.startswith(pattern) for pattern in test_json_patterns\n                    ):\n                        item.unlink(missing_ok=True)\n                        # Don't log cleanup operations - focus on test results\n            except Exception:\n                pass\n\n        tmp_dir = data_dir / \"tmp\"\n        if tmp_dir.exists():\n            for child in tmp_dir.iterdir():\n                if child.is_dir() or child.is_file():\n                    try:\n                        if child.is_dir():\n                            shutil.rmtree(child, ignore_errors=True)\n                        else:\n                            child.unlink(missing_ok=True)\n                    except Exception:\n                        pass\n            # Don't log cleanup operations - they're not interesting for test results\n    except Exception:\n        pass\n\n    yield\n\n    # Prune again after tests\n    if logs_dir.exists():\n        removed = _prune_old_files(\n            logs_dir,\n            patterns=[\"*.log\", \"*.log.*\", \"*.gz\"],\n            older_than_days=log_retention_days,\n        )\n        # Don't log cleanup operations - focus on test results (removed logging to prevent hangs)\n\n    if test_backups_dir.exists():\n        removed = _prune_old_files(\n            test_backups_dir,\n            patterns=[\"*.zip\"],\n            older_than_days=backup_retention_days,\n        )\n        # Don't log cleanup operations - focus on test results (removed logging to prevent hangs)\n\n    # Session-end purge: flags, tmp children, pytest-of-* directories, and tmp_* directories\n    try:\n        data_dir = project_root_path / \"tests\" / \"data\"\n\n        # Remove pytest-of-* directories (pytest creates these when using tmpdir fixtures)\n        # Also remove tmp* directories (created when TMPDIR is set to tests/data)\n        # Matches: tmp_*, tmp* (but not just \"tmp\"), pytest-of-*\n        # Use direct directory iteration instead of glob for Windows compatibility\n        if data_dir.exists():\n            for item in data_dir.iterdir():\n                try:\n                    if item.is_dir():\n                        if (\n                            item.name.startswith(\"pytest-of-\")\n                            or item.name.startswith(\"tmp_\")\n                            or (item.name.startswith(\"tmp\") and item.name != \"tmp\")\n                        ):\n                            shutil.rmtree(item, ignore_errors=True)\n                            # Don't log cleanup operations - focus on test results\n                    elif item.is_file():\n                        # Clean up test JSON files (test_*.json, .tmp_*.json, welcome_tracking.json)\n                        if item.suffix == \".json\":\n                            test_json_patterns = [\n                                \"test_\",\n                                \".tmp_\",\n                                \"welcome_tracking.json\",\n                            ]\n                            if any(\n                                item.name.startswith(pattern)\n                                for pattern in test_json_patterns\n                            ):\n                                item.unlink(missing_ok=True)\n                                # Don't log cleanup operations - focus on test results\n                        # Clean up .last_cache_cleanup file\n                        elif item.name == \".last_cache_cleanup\":\n                            item.unlink(missing_ok=True)\n                            # Don't log cleanup operations - focus on test results\n                except Exception:\n                    pass\n\n        # Clear flags directory\n        flags_dir = data_dir / \"flags\"\n        if flags_dir.exists():\n            for child in flags_dir.iterdir():\n                try:\n                    if child.is_dir():\n                        shutil.rmtree(child, ignore_errors=True)\n                    else:\n                        child.unlink(missing_ok=True)\n                except Exception:\n                    pass\n            # Don't log cleanup operations - focus on test results (failures, warnings, skips)\n\n        # Clear tmp directory\n        tmp_dir = data_dir / \"tmp\"\n        if tmp_dir.exists():\n            for child in tmp_dir.iterdir():\n                try:\n                    if child.is_dir():\n                        shutil.rmtree(child, ignore_errors=True)\n                    else:\n                        child.unlink(missing_ok=True)\n                except Exception:\n                    pass\n            # Don't log cleanup operations - they're not interesting for test results\n\n        # Clear requests directory\n        requests_dir = data_dir / \"requests\"\n        if requests_dir.exists():\n            for child in requests_dir.iterdir():\n                try:\n                    if child.is_dir():\n                        shutil.rmtree(child, ignore_errors=True)\n                    else:\n                        child.unlink(missing_ok=True)\n                except Exception:\n                    pass\n            # Don't log cleanup operations - focus on test results (failures, warnings, skips)\n\n        # Clean up conversation_states.json\n        conversation_states_file = data_dir / \"conversation_states.json\"\n        if conversation_states_file.exists():\n            try:\n                conversation_states_file.unlink(missing_ok=True)\n                # Don't log cleanup operations - focus on test results\n            except Exception:\n                pass\n    except Exception:\n        pass\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef log_lifecycle_maintenance():\n    \"\"\"Perform log lifecycle maintenance at session start.\"\"\"\n    # Perform lifecycle maintenance (archive old backups, cleanup old archives)\n    log_lifecycle_manager.perform_lifecycle_maintenance()\n\n    yield\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef session_log_rotation_check():\n    \"\"\"Perform final log cleanup at session end.\n\n    NOTE: Log rotation only happens at session START in setup_consolidated_test_logging.\n    This ensures rotation only occurs between test runs, never during an active session.\n    \"\"\"\n    yield\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef isolate_logging():\n    \"\"\"Ensure complete logging isolation during tests to prevent test logs from appearing in main app.log.\"\"\"\n    # Store original handlers\n    root_logger = logging.getLogger()\n    original_root_handlers = root_logger.handlers[:]\n\n    main_logger = logging.getLogger(\"mhm\")\n    original_main_handlers = main_logger.handlers[:]\n\n    # Remove all handlers from main loggers to prevent test logs from going to app.log\n    for handler in root_logger.handlers[:]:\n        if isinstance(handler, logging.StreamHandler) and handler.stream in (\n            sys.stdout,\n            sys.stderr,\n        ):\n            root_logger.removeHandler(handler)\n            continue\n        handler.close()\n        root_logger.removeHandler(handler)\n\n    for handler in main_logger.handlers[:]:\n        if isinstance(handler, logging.StreamHandler) and handler.stream in (\n            sys.stdout,\n            sys.stderr,\n        ):\n            main_logger.removeHandler(handler)\n            continue\n        handler.close()\n        main_logger.removeHandler(handler)\n\n    # Set propagate to False for main loggers to prevent test logs from bubbling up\n    root_logger.propagate = False\n    main_logger.propagate = False\n\n    # Reduce verbosity - this is expected behavior, no need to log it\n    test_logger.debug(\n        \"Logging isolation activated - test logs will not appear in main app.log\"\n    )\n\n    yield\n\n    # Restore original handlers after tests complete\n    for handler in original_root_handlers:\n        root_logger.addHandler(handler)\n\n    for handler in original_main_handlers:\n        main_logger.addHandler(handler)\n\n    # Restore propagate settings\n    root_logger.propagate = True\n    main_logger.propagate = True\n\n    test_logger.info(\"Logging isolation deactivated - main app logging restored\")\n\n\n# Test helper: wait until a predicate returns True within a timeout\ndef wait_until(predicate, timeout_seconds: float = 1.0, poll_seconds: float = 0.005):\n    \"\"\"Poll predicate() until it returns True or timeout elapses.\n\n    Returns True if predicate succeeds within timeout, otherwise False.\n    \"\"\"\n    import time as _time\n\n    deadline = _time.perf_counter() + timeout_seconds\n    while _time.perf_counter() < deadline:\n        try:\n            if predicate():\n                return True\n        except Exception:\n            # Ignore transient errors while waiting\n            pass\n        _time.sleep(poll_seconds)\n    return False\n\n\n# Test helper: materialize minimal user structures via public APIs\ndef materialize_user_minimal_via_public_apis(user_id: str) -> dict:\n    \"\"\"Ensure minimal structures exist without overwriting existing data.\n\n    - Merges into existing account (preserves internal_username and enabled features)\n    - Adds missing preferences keys (keeps existing categories/channel)\n    - Adds a default motivational/morning period if schedules missing\n    \"\"\"\n    from core.user_data_handlers import (\n        get_user_data,\n        update_user_account,\n        update_user_preferences,\n        update_user_schedules,\n    )\n    from core.user_data_handlers import get_user_id_by_identifier\n    from core.config import get_user_data_dir\n    import os\n\n    # Resolve UUID if user_id is an internal username (race condition fix)\n    # get_user_data_dir uses user_id directly, so we need to resolve UUIDs first\n    resolved_user_id = user_id\n    if not os.path.exists(get_user_data_dir(user_id)):\n        # Try to resolve UUID from internal username\n        from tests.test_utilities import TestUserFactory as TUF\n\n        uuid_resolved = get_user_id_by_identifier(\n            user_id\n        ) or TUF.get_test_user_id_by_internal_username(\n            user_id, os.getenv(\"TEST_DATA_DIR\", \"tests/data\")\n        )\n        if uuid_resolved and uuid_resolved != user_id:\n            resolved_user_id = uuid_resolved\n            # Verify resolved UUID directory exists\n            if os.path.exists(get_user_data_dir(resolved_user_id)):\n                user_id = resolved_user_id\n\n    # Ensure user directory exists before proceeding (race condition fix)\n    user_dir = get_user_data_dir(user_id)\n    if not os.path.exists(user_dir):\n        os.makedirs(user_dir, exist_ok=True)\n\n    # Load current state\n    current_all = get_user_data(user_id, \"all\") or {}\n    current_account = current_all.get(\"account\") or {}\n    current_prefs = current_all.get(\"preferences\") or {}\n    current_schedules = current_all.get(\"schedules\") or {}\n\n    # Account: preserve existing values; set sensible defaults where missing\n    merged_features = dict(current_account.get(\"features\") or {})\n    if \"automated_messages\" not in merged_features:\n        merged_features[\"automated_messages\"] = \"enabled\"\n    if \"task_management\" not in merged_features:\n        merged_features[\"task_management\"] = \"disabled\"\n    if \"checkins\" not in merged_features:\n        merged_features[\"checkins\"] = \"disabled\"\n\n    account_updates = {\n        \"user_id\": current_account.get(\"user_id\") or user_id,\n        \"internal_username\": current_account.get(\"internal_username\") or user_id,\n        \"account_status\": current_account.get(\"account_status\") or \"active\",\n        \"features\": merged_features,\n    }\n    update_user_account(user_id, account_updates)\n\n    # Preferences: add missing keys only\n    # Always ensure preferences exist (even if empty) to prevent get_user_data returning empty dict\n    prefs_updates = {}\n    if not current_prefs.get(\"categories\"):\n        prefs_updates[\"categories\"] = [\"motivational\"]\n    if not current_prefs.get(\"channel\"):\n        prefs_updates[\"channel\"] = {\"type\": \"discord\", \"contact\": \"test#1234\"}\n    # Always update preferences to ensure file exists (race condition fix)\n    if prefs_updates or not current_prefs:\n        update_user_preferences(user_id, prefs_updates)\n\n    # Schedules: ensure schedules exist for all categories in preferences; merge into existing\n    schedules_updates = current_schedules if isinstance(current_schedules, dict) else {}\n    # Get categories from preferences (or default to motivational)\n    categories = current_prefs.get(\"categories\", [\"motivational\"])\n    if not categories:\n        categories = [\"motivational\"]\n\n    # Ensure schedules exist for all categories\n    for category in categories:\n        schedules_updates.setdefault(category, {}).setdefault(\"periods\", {})\n        # Add a default morning period if none exists\n        if \"morning\" not in schedules_updates[category][\"periods\"]:\n            schedules_updates[category][\"periods\"][\"morning\"] = {\n                \"active\": True,\n                \"days\": [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\"],\n                \"start_time\": \"09:00\",\n                \"end_time\": \"12:00\",\n            }\n    update_user_schedules(user_id, schedules_updates)\n\n    # Ensure context exists\n    get_user_data(user_id, \"context\")\n    return get_user_data(user_id, \"all\")\n\n\n@pytest.fixture(scope=\"session\")\ndef test_data_dir():\n    \"\"\"Provide the repository-scoped test data directory for all tests.\"\"\"\n    test_logger.info(f\"Using test data directory: {tests_data_dir}\")\n    return str(tests_data_dir)\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef mock_config(test_data_dir):\n    \"\"\"Mock configuration for testing with proper test data directory.\"\"\"\n    test_logger.debug(f\"Setting up mock config with test data dir: {test_data_dir}\")\n    import core.config\n\n    # Always patch to ensure consistent test environment\n    # This ensures that even if patch_user_data_dirs is not active, we have a consistent config\n    default_categories = (\n        '[\"motivational\", \"health\", \"quotes_to_ponder\", \"word_of_the_day\", \"fun_facts\"]'\n    )\n\n    with (\n        patch.object(core.config, \"BASE_DATA_DIR\", test_data_dir),\n        patch.object(\n            core.config, \"USER_INFO_DIR_PATH\", os.path.join(test_data_dir, \"users\")\n        ),\n        patch.object(\n            core.config,\n            \"DEFAULT_MESSAGES_DIR_PATH\",\n            os.path.join(project_root, \"resources\", \"default_messages\"),\n        ),\n        patch.dict(os.environ, {\"CATEGORIES\": default_categories}, clear=False),\n    ):\n        yield\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef ensure_mock_config_applied(mock_config, test_data_dir):\n    \"\"\"Verify mock_config fixture is active for every test.\"\"\"\n    import os\n    import core.config\n\n    assert os.path.samefile(core.config.BASE_DATA_DIR, test_data_dir)\n    yield\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef clear_user_caches_between_tests():\n    \"\"\"Ensure user data caches don't leak between tests.\"\"\"\n    from core.user_data_handlers import clear_user_caches\n\n    clear_user_caches()\n    yield\n    clear_user_caches()\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef register_user_data_loaders_session():\n    \"\"\"Ensure core user data loaders are present without overwriting metadata.\"\"\"\n    import core.user_data_handlers as um\n\n    # Set only missing loaders to avoid clobbering metadata\n    for key, func, ftype in [\n        (\"account\", um._get_user_data__load_account, \"account\"),\n        (\"preferences\", um._get_user_data__load_preferences, \"preferences\"),\n        (\"context\", um._get_user_data__load_context, \"user_context\"),\n        (\"schedules\", um._get_user_data__load_schedules, \"schedules\"),\n    ]:\n        try:\n            entry = um.USER_DATA_LOADERS.get(key)\n            if entry and entry.get(\"loader\") is None:\n                um.register_data_loader(key, func, ftype)\n        except Exception:\n            # As a fallback, if the dict is missing, register minimally\n            um.register_data_loader(key, func, ftype)\n    yield\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef fix_user_data_loaders():\n    \"\"\"Ensure loaders stay correctly registered for each test without overwriting metadata.\"\"\"\n    import core.user_data_handlers as um\n\n    for key, func, ftype in [\n        (\"account\", um._get_user_data__load_account, \"account\"),\n        (\"preferences\", um._get_user_data__load_preferences, \"preferences\"),\n        (\"context\", um._get_user_data__load_context, \"user_context\"),\n        (\"schedules\", um._get_user_data__load_schedules, \"schedules\"),\n    ]:\n        entry = um.USER_DATA_LOADERS.get(key)\n        if entry and entry.get(\"loader\") is None:\n            um.register_data_loader(key, func, ftype)\n    yield\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef shim_get_user_data_to_invoke_loaders():\n    \"\"\"Shim core.user_data_handlers.get_user_data to ensure structured dicts.\n\n    If a test calls get_user_data with 'all' or a specific type and the result is\n    empty/missing, invoke the registered loaders in USER_DATA_LOADERS to assemble\n    the expected structure. This preserves production behavior when everything is\n    wired correctly, but guards against import-order timing in tests.\n    \"\"\"\n    import core.user_data_handlers as um\n\n    # Safety net: always provide structural dicts during tests regardless of loader state\n    # Also patch the public helpers module used by many tests\n    try:\n        import core.user_data_handlers as udh\n    except Exception:\n        udh = None\n\n    # Prefer core.user_data_handlers.get_user_data; fall back to handlers if missing\n    original_get_user_data = getattr(um, \"get_user_data\", None)\n    if (\n        original_get_user_data is None\n        and udh is not None\n        and hasattr(udh, \"get_user_data\")\n    ):\n        original_get_user_data = getattr(udh, \"get_user_data\", None)\n    if original_get_user_data is None:\n        yield\n        return\n\n    def _load_single_type(user_id: str, key: str, *, auto_create: bool):\n        try:\n            entry = um.USER_DATA_LOADERS.get(key)\n            loader = None\n            if entry:\n                loader = entry.get(\"loader\")\n            # If loader is missing, attempt to self-heal by (re)registering\n            if loader is None:\n                key_to_func_and_file = {\n                    \"account\": (um._get_user_data__load_account, \"account\"),\n                    \"preferences\": (um._get_user_data__load_preferences, \"preferences\"),\n                    \"context\": (um._get_user_data__load_context, \"user_context\"),\n                    \"schedules\": (um._get_user_data__load_schedules, \"schedules\"),\n                }\n                func_file = key_to_func_and_file.get(key)\n                if func_file is None:\n                    return None\n                func, file_type = func_file\n                try:\n                    um.register_data_loader(key, func, file_type)\n                    entry = um.USER_DATA_LOADERS.get(key)\n                    loader = entry.get(\"loader\") if entry else None\n                except Exception:\n                    loader = func\n            if loader is None:\n                return None\n            # Loaders accept (user_id, auto_create)\n            return loader(user_id, auto_create)\n        except Exception:\n            return None\n\n    def _fallback_read_from_files(user_id: str, key: str):\n        \"\"\"Read requested type directly from user JSON files as a last resort.\"\"\"\n        try:\n            import core.config as _cfg\n            from core.config import get_user_data_dir as _get_user_data_dir\n        except Exception:\n            return None\n\n        # Resolve actual user directory via config helper (handles UUID mapping)\n        try:\n            user_dir = _get_user_data_dir(user_id)\n        except Exception:\n            user_dir = os.path.join(_cfg.USER_INFO_DIR_PATH, user_id)\n        filename_map = {\n            \"account\": \"account.json\",\n            \"preferences\": \"preferences.json\",\n            \"context\": \"user_context.json\",\n            \"schedules\": \"schedules.json\",\n        }\n        filename = filename_map.get(key)\n        if not filename:\n            return None\n        file_path = os.path.join(user_dir, filename)\n        if not os.path.exists(file_path):\n            return None\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as fh:\n                return json.load(fh)\n        except Exception:\n            return None\n\n    def wrapped_get_user_data(user_id: str, data_type: str = \"all\", *args, **kwargs):\n        auto_create = True\n        try:\n            auto_create = bool(kwargs.get(\"auto_create\", True))\n        except Exception:\n            auto_create = True\n        result = original_get_user_data(user_id, data_type, *args, **kwargs)\n        try:\n            # If asking for all, ensure a dict with expected keys\n            if data_type == \"all\":\n                if not isinstance(result, dict):\n                    test_logger.debug(\n                        f\"shim_get_user_data: Coercing non-dict 'all' result for {user_id} -> assembling structure\"\n                    )\n                    result = {} if result is None else {\"value\": result}\n                # Respect auto_create and only assemble when user dir exists\n                should_assemble = auto_create\n                if should_assemble:\n                    try:\n                        from core.config import get_user_data_dir as _get_user_data_dir\n\n                        if not os.path.exists(_get_user_data_dir(user_id)):\n                            should_assemble = False\n                    except Exception:\n                        should_assemble = False\n                if should_assemble:\n                    for key in (\"account\", \"preferences\", \"context\", \"schedules\"):\n                        if key not in result or not result.get(key):\n                            test_logger.debug(\n                                f\"shim_get_user_data: '{key}' missing/empty for {user_id}; invoking loader\"\n                            )\n                            loaded = _load_single_type(\n                                user_id, key, auto_create=auto_create\n                            )\n                            if loaded is not None:\n                                result[key] = loaded\n                            else:\n                                # Fallback: direct file read\n                                fb = _fallback_read_from_files(user_id, key)\n                                if fb is not None:\n                                    result[key] = fb\n                                else:\n                                    test_logger.warning(\n                                        f\"shim_get_user_data: loader and file fallback could not provide '{key}' for {user_id}\"\n                                    )\n                return result\n\n            # Specific type request: ensure structure present\n            if isinstance(data_type, str):\n                key = data_type\n                # If result already a dict containing the key with a value, return as-is\n                if isinstance(result, dict) and result.get(key):\n                    return result\n                # Otherwise, attempt to load and return {key: value} if allowed\n                should_assemble = auto_create\n                if should_assemble:\n                    try:\n                        from core.config import get_user_data_dir as _get_user_data_dir\n\n                        if not os.path.exists(_get_user_data_dir(user_id)):\n                            should_assemble = False\n                    except Exception:\n                        should_assemble = False\n                if should_assemble:\n                    test_logger.debug(\n                        f\"shim_get_user_data: '{key}' request returned empty for {user_id}; invoking loader\"\n                    )\n                    loaded = _load_single_type(user_id, key, auto_create=auto_create)\n                    if loaded is not None:\n                        return {key: loaded}\n                    fb = _fallback_read_from_files(user_id, key)\n                    if fb is not None:\n                        return {key: fb}\n                return result\n        except Exception:\n            test_logger.exception(\n                \"shim_get_user_data: unexpected error while assembling result\"\n            )\n            return result\n\n        return result\n\n    # Patch in place for the duration of the test\n    # Patch both modules so all call sites are covered\n    setattr(um, \"get_user_data\", wrapped_get_user_data)\n    original_handlers_get = None\n    if udh is not None and hasattr(udh, \"get_user_data\"):\n        original_handlers_get = udh.get_user_data\n        setattr(udh, \"get_user_data\", wrapped_get_user_data)\n    try:\n        yield\n    finally:\n        # Restore originals at end of session\n        try:\n            setattr(um, \"get_user_data\", original_get_user_data)\n        except Exception:\n            pass\n        if udh is not None and original_handlers_get is not None:\n            try:\n                setattr(udh, \"get_user_data\", original_handlers_get)\n            except Exception:\n                pass\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef verify_required_loaders_present():\n    \"\"\"Fail fast if required user-data loaders are missing at session start.\"\"\"\n    try:\n        import core.user_data_handlers as um\n\n        required = (\"account\", \"preferences\", \"context\", \"schedules\")\n        missing = []\n        for k in required:\n            entry = um.USER_DATA_LOADERS.get(k)\n            if not (isinstance(entry, dict) and entry.get(\"loader\")):\n                missing.append(k)\n        if missing:\n            raise AssertionError(\n                f\"Required user-data loaders missing or None: {missing}. \"\n                f\"Present keys: {list(um.USER_DATA_LOADERS.keys())}\"\n            )\n    except Exception as e:\n        raise AssertionError(f\"Loader self-check failed: {e}\")\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef env_guard_and_restore(monkeypatch):\n    \"\"\"Snapshot and restore critical environment variables to prevent test leakage.\n\n    Restores after each test to ensure environment stability across the suite.\n    \"\"\"\n    critical_keys = [\n        \"MHM_TESTING\",\n        \"CATEGORIES\",\n        \"LOGS_DIR\",\n        \"LOG_BACKUP_DIR\",\n        \"LOG_ARCHIVE_DIR\",\n        \"LOG_MAIN_FILE\",\n        \"LOG_DISCORD_FILE\",\n        \"LOG_AI_FILE\",\n        \"LOG_USER_ACTIVITY_FILE\",\n        \"LOG_ERRORS_FILE\",\n        \"LOG_COMMUNICATION_MANAGER_FILE\",\n        \"LOG_EMAIL_FILE\",\n        \"LOG_UI_FILE\",\n        \"LOG_FILE_OPS_FILE\",\n        \"LOG_SCHEDULER_FILE\",\n        \"BASE_DATA_DIR\",\n        \"USER_INFO_DIR_PATH\",\n        \"TEMP\",\n        \"TMP\",\n        \"TMPDIR\",\n    ]\n    snapshot = {k: os.environ.get(k) for k in critical_keys}\n    try:\n        yield\n    finally:\n        for k, v in snapshot.items():\n            if v is None:\n                if k in os.environ:\n                    monkeypatch.delenv(k, raising=False)\n            else:\n                monkeypatch.setenv(k, v)\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef ensure_tmp_base_directory():\n    \"\"\"Ensure base tmp directory exists at session start (optimization: create once).\"\"\"\n    # Use the session-scoped test_data_dir directly to avoid scope mismatch\n    # with function-scoped test_data_dir fixtures in some test files\n    base_tmp = os.path.join(tests_data_dir, \"tmp\")\n    os.makedirs(base_tmp, exist_ok=True)\n    yield\n\n\n@pytest.fixture(scope=\"function\")\ndef test_path_factory(test_data_dir, ensure_tmp_base_directory):\n    \"\"\"Provide a per-test directory under tests/data/tmp/<uuid> for ad-hoc temp usage.\n\n    Prefer this over raw tempfile.mkdtemp/TemporaryDirectory to keep paths within the repo.\n    Optimized: base tmp directory is created once at session start.\n    \"\"\"\n    import uuid\n\n    base_tmp = os.path.join(test_data_dir, \"tmp\")\n    # Base directory already exists from session fixture, just create subdirectory\n    path = os.path.join(base_tmp, uuid.uuid4().hex)\n    os.makedirs(path, exist_ok=True)\n    return path\n\n\n@pytest.fixture(scope=\"function\")\ndef ensure_user_materialized(test_data_dir):\n    \"\"\"Return a helper to ensure account/preferences/context files exist for a user.\n\n    If the user directory is missing, uses TestUserFactory to create a basic user.\n    If present but missing files, writes minimal JSON structures to materialize them.\n    \"\"\"\n    from pathlib import Path as _Path\n    import json as _json\n    import os as _os\n\n    def _helper(user_id: str):\n        users_dir = _Path(test_data_dir) / \"users\"\n        user_dir = users_dir / user_id\n        if not user_dir.exists():\n            try:\n                from tests.test_utilities import TestUserFactory\n\n                TestUserFactory.create_basic_user(\n                    user_id, test_data_dir=str(test_data_dir)\n                )\n            except Exception:\n                user_dir.mkdir(parents=True, exist_ok=True)\n        # Materialize minimal files if missing\n        acct_path = user_dir / \"account.json\"\n        prefs_path = user_dir / \"preferences.json\"\n        ctx_path = user_dir / \"user_context.json\"\n        if not acct_path.exists():\n            _json.dump(\n                {\n                    \"user_id\": user_id,\n                    \"internal_username\": user_id,\n                    \"account_status\": \"active\",\n                    \"features\": {\n                        \"automated_messages\": \"disabled\",\n                        \"task_management\": \"disabled\",\n                        \"checkins\": \"disabled\",\n                    },\n                },\n                open(acct_path, \"w\", encoding=\"utf-8\"),\n                indent=2,\n            )\n        if not prefs_path.exists():\n            _json.dump(\n                {\n                    \"channel\": {\"type\": \"email\"},\n                    \"checkin_settings\": {\"enabled\": False},\n                    \"task_settings\": {\"enabled\": False},\n                },\n                open(prefs_path, \"w\", encoding=\"utf-8\"),\n                indent=2,\n            )\n        if not ctx_path.exists():\n            _json.dump(\n                {\"preferred_name\": user_id, \"pronouns\": [], \"custom_fields\": {}},\n                open(ctx_path, \"w\", encoding=\"utf-8\"),\n                indent=2,\n            )\n        return str(user_dir)\n\n    return _helper\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef path_sanitizer():\n    \"\"\"Guardrail: ensure temp resolution stays within tests/data and detect escapes.\n\n    Fails fast if the active temp directory is outside tests/data.\n    \"\"\"\n    import tempfile\n\n    # Always enforce repository-scoped tests/data as the allowed root\n    allowed_root = os.path.abspath(str(tests_data_dir))\n    # Validate Python's temp resolution points to tests/data\n    current_tmp = os.path.abspath(tempfile.gettempdir())\n    if not current_tmp.startswith(allowed_root):\n        raise AssertionError(\n            f\"Temp directory escaped repo: {current_tmp} (expected under {allowed_root}).\"\n        )\n    yield\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef enforce_user_dir_locations():\n    \"\"\"Ensure tests only create user dirs under tests/data/users.\n\n    - Fails if a top-level tests/data/test-user* directory appears.\n    - Fails if any test-user* directory is created under tests/data/tmp.\n    Cleans stray dirs to keep workspace tidy before failing.\n    \"\"\"\n    base = tests_data_dir\n    users_dir = base / \"users\"\n    tmp_dir = base / \"tmp\"\n    try:\n        pre_top = set(x.name for x in base.iterdir() if x.is_dir())\n        pre_tmp_children = set(\n            x.name\n            for x in (tmp_dir.iterdir() if tmp_dir.exists() else [])\n            if x.is_dir()\n        )\n    except Exception:\n        pre_top, pre_tmp_children = set(), set()\n\n    yield\n\n    # Check for misplaced top-level test users\n    try:\n        for entry in base.iterdir():\n            if not entry.is_dir():\n                continue\n            if (\n                entry.name.startswith(\"test-user\")\n                and entry.parent == base\n                and entry != users_dir\n            ):\n                try:\n                    shutil.rmtree(entry, ignore_errors=True)\n                finally:\n                    pytest.fail(\n                        f\"Misplaced test user directory detected: {entry}. \"\n                        f\"User directories must be under {users_dir}.\"\n                    )\n    except Exception:\n        # Do not mask test results if scan fails\n        pass\n\n    # Check for user-like dirs directly under tmp (non-recursive for performance)\n    try:\n        if tmp_dir.exists():\n            for child in tmp_dir.iterdir():\n                if not child.is_dir():\n                    continue\n                # Heuristics: tmp dir is a misplaced user dir if it has user-signature files\n                # or looks like a test-user name\n                looks_like_user = (\n                    child.name.startswith(\"test-user\")\n                    or (child / \"account.json\").exists()\n                    or (child / \"preferences.json\").exists()\n                    or (child / \"user_context.json\").exists()\n                    or (child / \"checkins.json\").exists()\n                    or (child / \"schedules.json\").exists()\n                    or (child / \"messages\").is_dir()\n                )\n                if looks_like_user:\n                    try:\n                        shutil.rmtree(child, ignore_errors=True)\n                    finally:\n                        pytest.fail(\n                            f\"User directory artifacts detected under tmp: {child}. \"\n                            f\"All user data must be created under {users_dir}.\"\n                        )\n    except Exception:\n        pass\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef cleanup_tmp_at_session_end():\n    \"\"\"Clear tests/data/tmp contents at session end to keep the workspace tidy.\"\"\"\n    yield\n    try:\n        tmp_dir = tests_data_dir / \"tmp\"\n        if tmp_dir.exists():\n            for child in tmp_dir.iterdir():\n                if child.is_dir():\n                    shutil.rmtree(child, ignore_errors=True)\n                else:\n                    try:\n                        child.unlink(missing_ok=True)\n                    except Exception:\n                        pass\n    except Exception:\n        pass\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef force_test_data_directory():\n    \"\"\"Route all system temp usage into tests/data for the entire session.\"\"\"\n    import tempfile\n\n    root = str(tests_data_dir)\n    # Set common env vars so any native/library lookups resolve under tests/data\n    os.environ[\"TMPDIR\"] = root\n    os.environ[\"TEMP\"] = root\n    os.environ[\"TMP\"] = root\n    # Patch Python's temp resolution\n    original_tempdir = tempfile.tempdir\n    tempfile.tempdir = root\n    try:\n        yield\n    finally:\n        tempfile.tempdir = original_tempdir\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_user_data(mock_config, test_data_dir, request):\n    \"\"\"Create mock user data for testing with unique user ID for each test.\"\"\"\n    import uuid\n    import core.config\n\n    # Generate unique user ID for each test to prevent interference\n    user_id = f\"test-user-{uuid.uuid4().hex[:8]}\"\n    user_dir = os.path.join(test_data_dir, \"users\", user_id)\n    # Ensure parent directory exists first (race condition fix for parallel execution)\n    os.makedirs(os.path.dirname(user_dir), exist_ok=True)\n    os.makedirs(user_dir, exist_ok=True)\n\n    test_logger.debug(f\"Creating mock user data for user: {user_id}\")\n\n    # Create mock account.json with current timestamp\n    current_time = datetime.now().isoformat() + \"Z\"\n    account_data = {\n        \"user_id\": user_id,\n        \"internal_username\": f\"testuser_{user_id[-4:]}\",\n        \"account_status\": \"active\",\n        \"chat_id\": \"\",\n        \"phone\": \"\",\n        \"email\": f\"test_{user_id[-4:]}@example.com\",\n        \"discord_user_id\": \"\",\n        \"created_at\": current_time,\n        \"updated_at\": current_time,\n        \"features\": {\n            \"automated_messages\": \"disabled\",\n            \"checkins\": \"disabled\",\n            \"task_management\": \"disabled\",\n        },\n    }\n\n    # Create mock preferences.json - categories only included if automated_messages enabled\n    preferences_data = {\n        \"channel\": {\"type\": \"email\"},\n        \"checkin_settings\": {\n            \"enabled\": False,\n            \"frequency\": \"daily\",\n            \"time\": \"09:00\",\n            \"categories\": [\"mood\", \"energy\", \"sleep\"],\n        },\n        \"task_settings\": {\n            \"enabled\": False,\n            \"reminder_frequency\": \"daily\",\n            \"reminder_time\": \"10:00\",\n        },\n    }\n\n    # Only add categories if automated_messages is enabled\n    if account_data[\"features\"][\"automated_messages\"] == \"enabled\":\n        preferences_data[\"categories\"] = [\"motivational\", \"health\", \"quotes_to_ponder\"]\n\n    # Create mock user_context.json\n    context_data = {\n        \"preferred_name\": f\"Test User {user_id[-4:]}\",\n        \"pronouns\": [\"they/them\"],\n        \"date_of_birth\": \"\",\n        \"custom_fields\": {\n            \"health_conditions\": [],\n            \"medications_treatments\": [],\n            \"reminders_needed\": [],\n            \"gender_identity\": \"\",\n            \"accessibility_needs\": [],\n        },\n        \"interests\": [\"reading\", \"music\"],\n        \"goals\": [\"Improve mental health\", \"Stay organized\"],\n        \"loved_ones\": [],\n        \"activities_for_encouragement\": [\"exercise\", \"socializing\"],\n        \"notes_for_ai\": [\"Prefers gentle encouragement\", \"Responds well to structure\"],\n        \"created_at\": current_time,\n        \"last_updated\": current_time,\n    }\n\n    # Create mock checkins.json\n    checkins_data = {\"checkins\": [], \"last_checkin_date\": None, \"streak_count\": 0}\n\n    # Create minimal schedules.json so schedule reads/writes have a base file\n    schedules_data = {\"categories\": {}}\n\n    # Create mock chat_interactions.json\n    chat_data = {\"interactions\": [], \"total_interactions\": 0, \"last_interaction\": None}\n\n    # Create messages directory and sent_messages.json only if automated_messages enabled\n    messages_dir = os.path.join(user_dir, \"messages\")\n    if account_data[\"features\"][\"automated_messages\"] == \"enabled\":\n        os.makedirs(messages_dir, exist_ok=True)\n\n        sent_messages_data = {\"messages\": [], \"total_sent\": 0, \"last_sent\": None}\n\n        with open(os.path.join(messages_dir, \"sent_messages.json\"), \"w\") as f:\n            json.dump(sent_messages_data, f, indent=2)\n    else:\n        sent_messages_data = None\n\n    # Save all mock data\n    with open(os.path.join(user_dir, \"account.json\"), \"w\") as f:\n        json.dump(account_data, f, indent=2)\n\n    with open(os.path.join(user_dir, \"preferences.json\"), \"w\") as f:\n        json.dump(preferences_data, f, indent=2)\n\n    with open(os.path.join(user_dir, \"user_context.json\"), \"w\") as f:\n        json.dump(context_data, f, indent=2)\n\n    with open(os.path.join(user_dir, \"checkins.json\"), \"w\") as f:\n        json.dump(checkins_data, f, indent=2)\n\n    with open(os.path.join(user_dir, \"chat_interactions.json\"), \"w\") as f:\n        json.dump(chat_data, f, indent=2)\n    with open(os.path.join(user_dir, \"schedules.json\"), \"w\") as f:\n        json.dump(schedules_data, f, indent=2)\n\n    # Ensure user is discoverable via identifier lookups\n    # Use file locking-aware update and retry if needed\n    try:\n        from core.user_data_manager import update_user_index\n\n        # Retry update_user_index in case of race conditions in parallel execution\n        max_retries = 3\n        for attempt in range(max_retries):\n            success = update_user_index(user_id)\n            if success:\n                break\n            if attempt < max_retries - 1:\n                import time\n\n                time.sleep(0.1)  # Brief delay before retry\n        if not success:\n            test_logger.warning(\n                f\"Failed to update user index for {user_id} after {max_retries} attempts\"\n            )\n    except Exception as e:\n        test_logger.warning(f\"Error updating user index for {user_id}: {e}\")\n        # Don't fail the test, but log the issue\n\n    test_logger.debug(f\"Created complete mock user data files in: {user_dir}\")\n\n    # Store user_id for cleanup\n    request.node.user_id = user_id\n\n    return {\n        \"user_id\": user_id,\n        \"user_dir\": user_dir,\n        \"account_data\": account_data,\n        \"preferences_data\": preferences_data,\n        \"context_data\": context_data,\n        \"checkins_data\": checkins_data,\n        \"schedules_data\": schedules_data,\n        \"chat_data\": chat_data,\n        \"sent_messages_data\": sent_messages_data,\n    }\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_user_data_with_messages(test_data_dir, mock_config, request):\n    \"\"\"Create mock user data for testing with automated_messages enabled and categories.\"\"\"\n    import uuid\n\n    # Generate unique user ID for each test to prevent interference\n    user_id = f\"test-user-messages-{uuid.uuid4().hex[:8]}\"\n    user_dir = os.path.join(test_data_dir, \"users\", user_id)\n    os.makedirs(user_dir, exist_ok=True)\n\n    test_logger.debug(f\"Creating mock user data with messages for user: {user_id}\")\n\n    # Create mock account.json with automated_messages enabled\n    current_time = datetime.now().isoformat() + \"Z\"\n    account_data = {\n        \"user_id\": user_id,\n        \"internal_username\": f\"testuser_{user_id[-4:]}\",\n        \"account_status\": \"active\",\n        \"chat_id\": \"\",\n        \"phone\": \"\",\n        \"email\": f\"test_{user_id[-4:]}@example.com\",\n        \"discord_user_id\": \"\",\n        \"created_at\": current_time,\n        \"updated_at\": current_time,\n        \"features\": {\n            \"automated_messages\": \"enabled\",\n            \"checkins\": \"disabled\",\n            \"task_management\": \"disabled\",\n        },\n    }\n\n    # Create mock preferences.json with categories (automated_messages enabled)\n    preferences_data = {\n        \"channel\": {\"type\": \"email\"},\n        \"categories\": [\"motivational\", \"health\", \"quotes_to_ponder\"],\n        \"checkin_settings\": {\n            \"enabled\": False,\n            \"frequency\": \"daily\",\n            \"time\": \"09:00\",\n            \"categories\": [\"mood\", \"energy\", \"sleep\"],\n        },\n        \"task_settings\": {\n            \"enabled\": False,\n            \"reminder_frequency\": \"daily\",\n            \"reminder_time\": \"10:00\",\n        },\n    }\n\n    # Create mock user_context.json\n    context_data = {\n        \"preferred_name\": f\"Test User {user_id[-4:]}\",\n        \"pronouns\": [\"they/them\"],\n        \"date_of_birth\": \"\",\n        \"custom_fields\": {\n            \"health_conditions\": [],\n            \"medications_treatments\": [],\n            \"reminders_needed\": [],\n            \"gender_identity\": \"\",\n            \"accessibility_needs\": [],\n        },\n        \"interests\": [\"reading\", \"music\"],\n        \"goals\": [\"Improve mental health\", \"Stay organized\"],\n        \"loved_ones\": [],\n        \"activities_for_encouragement\": [\"exercise\", \"socializing\"],\n        \"notes_for_ai\": [\"Prefers gentle encouragement\", \"Responds well to structure\"],\n        \"created_at\": current_time,\n        \"last_updated\": current_time,\n    }\n\n    # Create mock checkins.json\n    checkins_data = {\"checkins\": [], \"last_checkin_date\": None, \"streak_count\": 0}\n\n    # Create mock chat_interactions.json\n    chat_data = {\"interactions\": [], \"total_interactions\": 0, \"last_interaction\": None}\n\n    # Create messages directory and sent_messages.json (automated_messages enabled)\n    messages_dir = os.path.join(user_dir, \"messages\")\n    os.makedirs(messages_dir, exist_ok=True)\n\n    sent_messages_data = {\"messages\": [], \"total_sent\": 0, \"last_sent\": None}\n\n    # Save all mock data\n    with open(os.path.join(user_dir, \"account.json\"), \"w\") as f:\n        json.dump(account_data, f, indent=2)\n\n    with open(os.path.join(user_dir, \"preferences.json\"), \"w\") as f:\n        json.dump(preferences_data, f, indent=2)\n\n    with open(os.path.join(user_dir, \"user_context.json\"), \"w\") as f:\n        json.dump(context_data, f, indent=2)\n\n    with open(os.path.join(user_dir, \"checkins.json\"), \"w\") as f:\n        json.dump(checkins_data, f, indent=2)\n\n    with open(os.path.join(user_dir, \"chat_interactions.json\"), \"w\") as f:\n        json.dump(chat_data, f, indent=2)\n\n    with open(os.path.join(messages_dir, \"sent_messages.json\"), \"w\") as f:\n        json.dump(sent_messages_data, f, indent=2)\n\n    test_logger.debug(f\"Created complete mock user data with messages in: {user_dir}\")\n\n    # Store user_id for cleanup\n    request.node.user_id = user_id\n\n    return {\n        \"user_id\": user_id,\n        \"user_dir\": user_dir,\n        \"account_data\": account_data,\n        \"preferences_data\": preferences_data,\n        \"context_data\": context_data,\n        \"checkins_data\": checkins_data,\n        \"chat_data\": chat_data,\n        \"sent_messages_data\": sent_messages_data,\n    }\n\n\n@pytest.fixture(scope=\"function\")\ndef update_user_index_for_test(test_data_dir):\n    \"\"\"Helper fixture to update user index for test users.\"\"\"\n\n    def _update_index(user_id):\n        try:\n            from core.user_data_manager import update_user_index\n\n            success = update_user_index(user_id)\n            if success:\n                test_logger.debug(f\"Updated user index for test user: {user_id}\")\n            else:\n                test_logger.warning(\n                    f\"Failed to update user index for test user: {user_id}\"\n                )\n            return success\n        except Exception as e:\n            test_logger.warning(\n                f\"Error updating user index for test user {user_id}: {e}\"\n            )\n            return False\n\n    return _update_index\n\n\n# --- GLOBAL PATCH: Force all user data to tests/data/users/ for all tests ---\n# DISABLED: This fixture was causing test isolation issues\n# @pytest.fixture(autouse=True, scope=\"session\")\n# def patch_user_data_dirs():\n#     \"\"\"Patch BASE_DATA_DIR and USER_INFO_DIR_PATH to use tests/data/users/ for all tests.\"\"\"\n#     from unittest.mock import patch\n#     import core.config\n#     test_data_dir = os.path.abspath(\"tests/data\")\n#     users_dir = os.path.join(test_data_dir, \"users\")\n#     os.makedirs(users_dir, exist_ok=True)\n#\n#     # Patch the module attributes directly\n#     with patch.object(core.config, \"BASE_DATA_DIR\", test_data_dir), \\\n#          patch.object(core.config, \"USER_INFO_DIR_PATH\", users_dir):\n#         yield\n\n\n# --- CLEANUP FIXTURE: Remove test users from tests/data/users/ after all tests (NEVER touches real user data) ---\n@pytest.fixture(scope=\"session\", autouse=True)\ndef cleanup_test_users_after_session():\n    \"\"\"Remove test users from tests/data/users/ after all tests. NEVER touches real user data.\"\"\"\n    yield  # Run all tests first\n\n    # Clear all user caches to prevent state pollution between test runs\n    try:\n        from core.user_data_handlers import clear_user_caches\n\n        clear_user_caches()  # Clear all caches\n    except Exception:\n        pass  # Ignore errors during cleanup\n\n    # Only clean up test directories, NEVER real user data\n    for base_dir in [\"tests/data/users\"]:\n        abs_dir = os.path.abspath(base_dir)\n        if os.path.exists(abs_dir):\n            for item in os.listdir(abs_dir):\n                # Clean up ONLY test directories (test-*, test_*, testuser*)\n                # NEVER clean up UUID directories - these are real users!\n                if (\n                    item.startswith(\"test-\")\n                    or item.startswith(\"test_\")\n                    or item.startswith(\"testuser\")\n                ):\n                    item_path = os.path.join(abs_dir, item)\n                    try:\n                        if os.path.isdir(item_path):\n                            shutil.rmtree(item_path)\n                        else:\n                            os.remove(item_path)\n                    except Exception:\n                        pass\n\n    # Additional cleanup: Remove ALL directories in tests/data/users/ for complete isolation\n    # This ensures no test data persists between test runs\n    test_users_dir = os.path.abspath(\"tests/data/users\")\n    if os.path.exists(test_users_dir):\n        for item in os.listdir(test_users_dir):\n            item_path = os.path.join(test_users_dir, item)\n            try:\n                if os.path.isdir(item_path):\n                    shutil.rmtree(item_path)\n                else:\n                    os.remove(item_path)\n            except Exception:\n                pass\n\n    # Also clean up the user index file to prevent stale entries\n    test_data_dir = os.path.abspath(\"tests/data\")\n    user_index_file = os.path.join(test_data_dir, \"user_index.json\")\n    if os.path.exists(user_index_file):\n        try:\n            os.remove(user_index_file)\n        except Exception:\n            pass\n\n    # Clean up test request files from schedule editor tests\n    requests_dir = os.path.join(test_data_dir, \"requests\")\n    if os.path.exists(requests_dir):\n        try:\n            for item in os.listdir(requests_dir):\n                # Clean up test request files (reschedule_test_*)\n                if item.startswith(\"reschedule_test_\"):\n                    item_path = os.path.join(requests_dir, item)\n                    try:\n                        os.remove(item_path)\n                    except Exception:\n                        pass\n        except Exception:\n            pass\n\n    # Clean up test backup files to prevent clutter\n    backup_dir = os.path.join(test_data_dir, \"backups\")\n    if os.path.exists(backup_dir):\n        try:\n            for item in os.listdir(backup_dir):\n                item_path = os.path.join(backup_dir, item)\n                try:\n                    if os.path.isfile(item_path):\n                        os.remove(item_path)\n                    elif os.path.isdir(item_path):\n                        shutil.rmtree(item_path, ignore_errors=True)\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n    # Clean up pytest-of-* and tmp* directories (pytest creates these when using tmpdir fixtures)\n    # Matches: tmp_*, tmp* (but not just \"tmp\"), pytest-of-*\n    # Use direct directory iteration instead of glob for Windows compatibility\n    try:\n        if os.path.exists(test_data_dir):\n            for item in os.listdir(test_data_dir):\n                item_path = os.path.join(test_data_dir, item)\n                try:\n                    if os.path.isdir(item_path):\n                        if (\n                            item.startswith(\"pytest-of-\")\n                            or item.startswith(\"tmp_\")\n                            or (item.startswith(\"tmp\") and item != \"tmp\")\n                        ):\n                            shutil.rmtree(item_path, ignore_errors=True)\n                    elif os.path.isfile(item_path):\n                        # Clean up test JSON files (test_*.json, .tmp_*.json, welcome_tracking.json)\n                        if item.endswith(\".json\"):\n                            test_json_patterns = [\n                                \"test_\",\n                                \".tmp_\",\n                                \"welcome_tracking.json\",\n                            ]\n                            if any(\n                                item.startswith(pattern)\n                                for pattern in test_json_patterns\n                            ):\n                                os.remove(item_path)\n                        # Clean up .last_cache_cleanup file\n                        elif item == \".last_cache_cleanup\":\n                            os.remove(item_path)\n                except Exception:\n                    pass\n    except Exception:\n        pass\n\n    # Clean up flags directory\n    flags_dir = os.path.join(test_data_dir, \"flags\")\n    if os.path.exists(flags_dir):\n        try:\n            for item in os.listdir(flags_dir):\n                item_path = os.path.join(flags_dir, item)\n                try:\n                    if os.path.isfile(item_path):\n                        os.remove(item_path)\n                    elif os.path.isdir(item_path):\n                        shutil.rmtree(item_path, ignore_errors=True)\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n    # Clean up tmp directory\n    tmp_dir = os.path.join(test_data_dir, \"tmp\")\n    if os.path.exists(tmp_dir):\n        try:\n            for item in os.listdir(tmp_dir):\n                item_path = os.path.join(tmp_dir, item)\n                try:\n                    if os.path.isfile(item_path):\n                        os.remove(item_path)\n                    elif os.path.isdir(item_path):\n                        shutil.rmtree(item_path, ignore_errors=True)\n                except Exception:\n                    pass\n        except Exception:\n            pass\n\n    # Clean up other test artifacts according to cleanup standards\n    try:\n        # Remove pytest temporary directories (pytest-of-* and tmp* directories created by pytest's tmpdir plugin)\n        # Matches: tmp_*, tmp* (but not just \"tmp\"), pytest-of-*\n        # Use direct directory iteration instead of glob for Windows compatibility\n        if os.path.exists(test_data_dir):\n            for item in os.listdir(test_data_dir):\n                item_path = os.path.join(test_data_dir, item)\n                try:\n                    if os.path.isdir(item_path):\n                        if (\n                            item.startswith(\"pytest-of-\")\n                            or item.startswith(\"tmp_\")\n                            or (item.startswith(\"tmp\") and item != \"tmp\")\n                        ):\n                            shutil.rmtree(item_path, ignore_errors=True)\n                    elif os.path.isfile(item_path):\n                        # Clean up test JSON files (test_*.json, .tmp_*.json, welcome_tracking.json)\n                        if item.endswith(\".json\"):\n                            test_json_patterns = [\n                                \"test_\",\n                                \".tmp_\",\n                                \"welcome_tracking.json\",\n                            ]\n                            if any(\n                                item.startswith(pattern)\n                                for pattern in test_json_patterns\n                            ):\n                                os.remove(item_path)\n                        # Clean up .last_cache_cleanup file\n                        elif item == \".last_cache_cleanup\":\n                            os.remove(item_path)\n                except Exception:\n                    pass\n\n        # Remove stray config directory\n        config_dir = os.path.join(test_data_dir, \"config\")\n        if os.path.exists(config_dir):\n            shutil.rmtree(config_dir, ignore_errors=True)\n\n        # Remove root files\n        for filename in [\".env\", \"requirements.txt\", \"test_file.json\"]:\n            file_path = os.path.join(test_data_dir, filename)\n            if os.path.exists(file_path):\n                os.remove(file_path)\n\n        # Remove legacy nested directory\n        nested_dir = os.path.join(test_data_dir, \"nested\")\n        if os.path.exists(nested_dir):\n            shutil.rmtree(nested_dir, ignore_errors=True)\n\n        # Remove corrupted files\n        for item in os.listdir(test_data_dir):\n            if (\n                item.startswith(\"tmp\") and \".corrupted_\" in item\n            ) or \".corrupted_\" in item:\n                item_path = os.path.join(test_data_dir, item)\n                if os.path.isfile(item_path):\n                    os.remove(item_path)\n\n        # Clear tmp directory completely - remove all subdirectories and files\n        tmp_dir = os.path.join(test_data_dir, \"tmp\")\n        if os.path.exists(tmp_dir):\n            try:\n                # Remove all contents (subdirectories and files)\n                for item in os.listdir(tmp_dir):\n                    item_path = os.path.join(tmp_dir, item)\n                    try:\n                        if os.path.isdir(item_path):\n                            shutil.rmtree(item_path, ignore_errors=True)\n                        else:\n                            os.remove(item_path)\n                    except Exception:\n                        pass\n                # Directory itself stays but is now empty\n            except Exception:\n                pass\n\n        # Clear flags directory completely\n        flags_dir = os.path.join(test_data_dir, \"flags\")\n        if os.path.exists(flags_dir):\n            try:\n                for item in os.listdir(flags_dir):\n                    item_path = os.path.join(flags_dir, item)\n                    try:\n                        if os.path.isfile(item_path):\n                            os.remove(item_path)\n                        elif os.path.isdir(item_path):\n                            shutil.rmtree(item_path, ignore_errors=True)\n                    except Exception:\n                        pass\n            except Exception:\n                pass\n\n        # Clear requests directory completely\n        requests_dir = os.path.join(test_data_dir, \"requests\")\n        if os.path.exists(requests_dir):\n            try:\n                for item in os.listdir(requests_dir):\n                    item_path = os.path.join(requests_dir, item)\n                    try:\n                        if os.path.isfile(item_path):\n                            os.remove(item_path)\n                        elif os.path.isdir(item_path):\n                            shutil.rmtree(item_path, ignore_errors=True)\n                    except Exception:\n                        pass\n            except Exception:\n                pass\n\n        # Clean up conversation_states.json\n        conversation_states_file = os.path.join(\n            test_data_dir, \"conversation_states.json\"\n        )\n        if os.path.exists(conversation_states_file):\n            try:\n                os.remove(conversation_states_file)\n            except Exception:\n                pass\n\n        # Clear logs directory\n        logs_dir = os.path.join(test_data_dir, \"logs\")\n        if os.path.exists(logs_dir):\n            shutil.rmtree(logs_dir, ignore_errors=True)\n\n        # Clean up old test_run files in tests/logs (but preserve current session files)\n        test_logs_dir = os.path.join(project_root, \"tests\", \"logs\")\n        if os.path.exists(test_logs_dir):\n            try:\n                for item in os.listdir(test_logs_dir):\n                    # Only clean up old test_run files, not the current session's test_run.log\n                    if (\n                        item.startswith(\"test_run_\")\n                        and item.endswith(\".log\")\n                        and item != \"test_run.log\"\n                    ):\n                        item_path = os.path.join(test_logs_dir, item)\n                        if os.path.isfile(item_path):\n                            os.remove(item_path)\n            except Exception:\n                pass\n\n        # Clean up any stray test.log files in tests/data\n        try:\n            for root, dirs, files in os.walk(test_data_dir):\n                for file in files:\n                    if file == \"test.log\":\n                        file_path = os.path.join(root, file)\n                        try:\n                            os.remove(file_path)\n                        except Exception:\n                            pass\n        except Exception:\n            pass\n\n    except Exception:\n        pass  # Ignore cleanup errors\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_logger():\n    \"\"\"Mock logger for testing.\"\"\"\n    with patch(\"core.logger.get_logger\") as mock_logger:\n        mock_logger.return_value = Mock()\n        yield mock_logger\n\n\n@pytest.fixture(scope=\"function\")\ndef temp_file():\n    \"\"\"Create a temporary file for testing.\"\"\"\n    with tempfile.NamedTemporaryFile(mode=\"w+\", delete=False) as f:\n        yield f.name\n    # Cleanup\n    os.unlink(f.name)\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_ai_response():\n    \"\"\"Mock AI response for testing.\"\"\"\n    return {\n        \"response\": \"This is a test AI response.\",\n        \"confidence\": 0.85,\n        \"model\": \"test-model\",\n        \"timestamp\": \"2025-01-01T12:00:00Z\",\n    }\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_task_data():\n    \"\"\"Mock task data for testing.\"\"\"\n    return {\n        \"task_id\": \"test-task-123\",\n        \"title\": \"Test Task\",\n        \"description\": \"This is a test task\",\n        \"priority\": \"medium\",\n        \"due_date\": \"2025-01-15\",\n        \"completed\": False,\n        \"created_at\": \"2025-01-01T12:00:00Z\",\n        \"updated_at\": \"2025-01-01T12:00:00Z\",\n    }\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_message_data():\n    \"\"\"Mock message data for testing.\"\"\"\n    return {\n        \"message_id\": \"test-message-123\",\n        \"text\": \"This is a test message\",\n        \"category\": \"motivational\",\n        \"days\": [\"monday\", \"wednesday\", \"friday\"],\n        \"time_periods\": [\"18:00-20:00\"],\n        \"active\": True,\n    }\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_service_data():\n    \"\"\"Mock service data for testing.\"\"\"\n    return {\n        \"service_id\": \"test-service-123\",\n        \"name\": \"Test Service\",\n        \"status\": \"running\",\n        \"pid\": 12345,\n        \"start_time\": \"2025-01-01T12:00:00Z\",\n        \"config\": {\"port\": 8080, \"host\": \"localhost\"},\n    }\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_communication_data():\n    \"\"\"Mock communication data for testing.\"\"\"\n    return {\n        \"message_id\": \"test-msg-123\",\n        \"user_id\": \"test-user-123\",\n        \"channel\": \"email\",\n        \"content\": \"Test message content\",\n        \"sent_at\": \"2025-01-01T12:00:00Z\",\n        \"status\": \"sent\",\n    }\n\n\n@pytest.fixture(scope=\"function\")\ndef mock_schedule_data():\n    \"\"\"Mock schedule data for testing.\"\"\"\n    return {\n        \"category\": \"motivational\",\n        \"periods\": {\n            \"morning\": {\n                \"start_time\": \"08:00\",\n                \"end_time\": \"10:00\",\n                \"days\": [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\"],\n                \"active\": True,\n            },\n            \"evening\": {\n                \"start_time\": \"18:00\",\n                \"end_time\": \"20:00\",\n                \"days\": [\"monday\", \"wednesday\", \"friday\"],\n                \"active\": True,\n            },\n        },\n    }\n\n\n# Configure pytest\ndef pytest_collection_modifyitems(config, items):\n    \"\"\"Exclude AI test files that are run via run_ai_functionality_tests.py from pytest collection and add default markers.\"\"\"\n    # These test classes are designed to be run via run_ai_functionality_tests.py, not pytest\n    # They have __init__ constructors that require parameters, which causes pytest collection warnings\n    ai_test_files = [\n        \"tests/ai/test_ai_core.py\",\n        \"tests/ai/test_ai_integration.py\",\n        \"tests/ai/test_ai_errors.py\",\n        \"tests/ai/test_ai_cache.py\",\n        \"tests/ai/test_ai_performance.py\",\n        \"tests/ai/test_ai_quality.py\",\n        \"tests/ai/test_ai_advanced.py\",\n        \"tests/ai/test_ai_functionality_manual.py\",  # Manual test runner, not a pytest test\n    ]\n\n    items_to_remove = []\n    for item in items:\n        # Get the file path from the item\n        item_path = (\n            str(item.fspath)\n            if hasattr(item, \"fspath\")\n            else str(Path(item.nodeid.split(\"::\")[0]))\n        )\n        # Normalize path separators\n        normalized_item_path = item_path.replace(\"\\\\\", os.sep).replace(\"/\", os.sep)\n\n        for ai_file in ai_test_files:\n            normalized_ai_file = ai_file.replace(\"\\\\\", os.sep).replace(\"/\", os.sep)\n            if normalized_ai_file in normalized_item_path or os.path.basename(\n                normalized_ai_file\n            ) == os.path.basename(normalized_item_path):\n                items_to_remove.append(item)\n                break\n\n    # Remove excluded items\n    for item in items_to_remove:\n        items.remove(item)\n\n    # Add default markers to remaining items\n    for item in items:\n        # Add unit marker by default if no marker is present\n        if not any(item.iter_markers()):\n            item.add_marker(pytest.mark.unit)\n\n\ndef pytest_ignore_collect(collection_path, config):\n    \"\"\"Ignore temp directories created by parallel coverage runs to prevent collection errors.\"\"\"\n    path_str = str(collection_path)\n    # Ignore temp directories created by coverage runs\n    if \"pytest-tmp-\" in path_str or \"pytest-of-\" in path_str:\n        # Only ignore if it's under tests/data (our temp directory location)\n        if \"tests\" + os.sep + \"data\" in path_str:\n            return True\n    return None  # Let other ignore rules handle it\n\n\ndef pytest_configure(config):\n    \"\"\"Configure pytest for MHM testing and suppress collection warnings for development tools implementation classes.\"\"\"\n    # Register custom markers to avoid warnings\n    config.addinivalue_line(\n        \"markers\",\n        \"notebook: Notebook functionality tests (notes, lists, journal entries)\",\n    )\n\n    # Suppress PytestCollectionWarning for development tools implementation classes\n    # These warnings are emitted during collection, so we need to filter them in the hook\n    # Note: pytest is already imported at module level, don't import it again here\n    import warnings\n\n    warnings.filterwarnings(\n        \"ignore\",\n        message=\".*cannot collect test class.*TestCoverage.*\",\n        category=pytest.PytestCollectionWarning,\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        message=\".*cannot collect test class.*because it has a __init__ constructor.*\",\n        category=pytest.PytestCollectionWarning,\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        category=pytest.PytestCollectionWarning,\n        module=\"development_tools.tests.*\",\n    )\n\n    # Only log from main process to avoid duplicate messages in parallel mode\n    if not os.environ.get(\"PYTEST_XDIST_WORKER\"):\n        test_logger.debug(\"Configuring pytest for MHM testing\")\n\n    # Configure pytest tmpdir to use tests/data/tmp instead of creating pytest-of-* directories\n    # This ensures all temporary files stay within tests/data\n    try:\n        # Set the base temporary directory for pytest's tmpdir fixture\n        if hasattr(config, \"option\") and hasattr(config.option, \"tmp_path_factory\"):\n            config.option.tmp_path_factory = str(tests_data_dir / \"tmp\")\n    except Exception:\n        pass  # Ignore if pytest version doesn't support this\n\n    # Core test type markers\n    config.addinivalue_line(\"markers\", \"unit: mark test as a unit test\")\n    config.addinivalue_line(\"markers\", \"integration: mark test as an integration test\")\n    config.addinivalue_line(\"markers\", \"behavior: mark test as a behavior test\")\n    config.addinivalue_line(\"markers\", \"ui: mark test as testing UI components\")\n\n    # Speed markers\n    config.addinivalue_line(\"markers\", \"slow: mark test as slow running (>1 second)\")\n    config.addinivalue_line(\n        \"markers\", \"fast: mark test as fast running (<100ms, optional)\"\n    )\n\n    # Resource markers\n    config.addinivalue_line(\n        \"markers\", \"asyncio: mark test as requiring asyncio support\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"file_io: mark test as having heavy file operations\"\n    )\n\n    # Feature-specific markers (aligned with component loggers)\n    config.addinivalue_line(\"markers\", \"ai: mark test as requiring AI functionality\")\n    config.addinivalue_line(\n        \"markers\", \"communication: mark test as testing communication channels\"\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"tasks: mark test as task management functionality (includes reminders)\",\n    )\n    config.addinivalue_line(\n        \"markers\", \"checkins: mark test as check-in system functionality\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"messages: mark test as message system functionality\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"analytics: mark test as analytics and reporting functionality\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"user_management: mark test as user account management functionality\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"scheduler: mark test as scheduler functionality\"\n    )\n\n    # Test quality markers\n    config.addinivalue_line(\n        \"markers\", \"regression: mark test as preventing regression issues\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"smoke: mark test as basic functionality smoke test\"\n    )\n    config.addinivalue_line(\"markers\", \"critical: mark test as critical path test\")\n    config.addinivalue_line(\n        \"markers\",\n        \"no_parallel: mark test as must not run under pytest-xdist parallel execution\",\n    )\n    config.addinivalue_line(\n        \"markers\",\n        \"e2e: End-to-end tests with real tool execution (slow, excluded from regular runs)\",\n    )\n\n\ndef pytest_sessionstart(session):\n    \"\"\"Log test session start.\"\"\"\n    # Only log from main process to avoid duplicate messages in parallel mode\n    if not os.environ.get(\"PYTEST_XDIST_WORKER\"):\n        test_logger.debug(f\"Starting test session with {len(session.items)} tests\")\n        test_logger.debug(f\"Test log file: {test_log_file}\")\n\n        # Register atexit handler for crash recovery\n        import atexit\n\n        def crash_recovery_handler():\n            \"\"\"Ensure cleanup on unexpected exit.\"\"\"\n            try:\n                from communication.core.channel_orchestrator import CommunicationManager\n\n                if CommunicationManager._instance is not None:\n                    test_logger.warning(\n                        \"Crash recovery: Cleaning up CommunicationManager on unexpected exit\"\n                    )\n                    try:\n                        CommunicationManager._instance.stop_all()\n                    except Exception:\n                        pass\n                    CommunicationManager._instance = None\n            except (ImportError, ModuleNotFoundError):\n                pass\n            except Exception as e:\n                test_logger.warning(f\"Error in crash recovery handler: {e}\")\n\n        atexit.register(crash_recovery_handler)\n\n        # Diagnostic logging for DLL error investigation (0xC0000135 - STATUS_DLL_NOT_FOUND)\n        try:\n            import sys\n            import pytest\n\n            test_logger.debug(f\"Python executable: {sys.executable}\")\n            test_logger.debug(f\"Python version: {sys.version}\")\n            test_logger.debug(f\"Python path: {sys.path[:3]}...\")  # First 3 entries only\n            test_logger.debug(f\"Pytest version: {pytest.__version__}\")\n            try:\n                import pytest_xdist  # type: ignore[reportMissingImports]\n\n                test_logger.debug(f\"Pytest-xdist version: {pytest_xdist.__version__}\")\n            except (ImportError, AttributeError):\n                test_logger.debug(\"Pytest-xdist: not available\")\n        except Exception as e:\n            test_logger.warning(f\"Failed to log diagnostic information: {e}\")\n\n\ndef _consolidate_worker_logs():\n    \"\"\"Consolidate per-worker log files into main log files at the end of parallel test runs.\n\n    This function should only be called from the main process (not workers).\n    It finds all worker log files (test_run_gw*.log, test_consolidated_gw*.log, etc.)\n    and combines them into the main log files.\n    \"\"\"\n    # Only run in main process (not in workers)\n    if os.environ.get(\"PYTEST_XDIST_WORKER\"):\n        return\n\n    try:\n        logs_dir = Path(project_root) / \"tests\" / \"logs\"\n        if not logs_dir.exists():\n            return\n\n        # CRITICAL: Small delay to allow worker processes to close their file handles\n        # This prevents file locking issues during consolidation\n        import time\n\n        time.sleep(0.5)  # 500ms delay to allow workers to close files\n\n        # Find all worker log files\n        worker_test_run_logs = sorted(logs_dir.glob(\"test_run_gw*.log\"))\n        worker_consolidated_logs = sorted(logs_dir.glob(\"test_consolidated_gw*.log\"))\n\n        # If no worker logs found, nothing to consolidate\n        if not (worker_test_run_logs or worker_consolidated_logs):\n            return\n\n        # CRITICAL: Backup worker log files BEFORE consolidation for debugging memory leaks\n        # This allows us to analyze which tests each worker ran even after consolidation\n        backup_dir = logs_dir / \"worker_logs_backup\"\n        backup_dir.mkdir(exist_ok=True)\n        for log_file in worker_test_run_logs + worker_consolidated_logs:\n            try:\n                backup_path = backup_dir / log_file.name\n                import shutil\n\n                shutil.copy2(log_file, backup_path)\n            except Exception as e:\n                test_logger.debug(f\"Failed to backup worker log {log_file}: {e}\")\n\n        test_logger.info(\"Consolidating worker log files into main log files...\")\n\n        # Consolidate test_run logs\n        main_test_run_log = logs_dir / \"test_run.log\"\n        if worker_test_run_logs:\n            try:\n                with open(main_test_run_log, \"a\", encoding=\"utf-8\") as main_file:\n                    main_file.write(f\"\\n{'='*80}\\n\")\n                    main_file.write(\"# CONSOLIDATED FROM PARALLEL WORKERS\\n\")\n                    main_file.write(f\"{'='*80}\\n\\n\")\n\n                    for worker_log in worker_test_run_logs:\n                        worker_id = worker_log.stem.replace(\"test_run_\", \"\")\n                        main_file.write(f\"\\n# --- Worker {worker_id} ---\\n\")\n                        try:\n                            # Retry mechanism to handle file locking issues\n                            # Workers might still have file handles open\n                            import time\n\n                            max_retries = 5\n                            retry_delay = 0.2  # 200ms between retries\n\n                            for attempt in range(max_retries):\n                                try:\n                                    # Stream file content in chunks to avoid loading entire file into memory\n                                    # Limit to 50MB per worker log to prevent memory issues\n                                    max_size = 50 * 1024 * 1024  # 50MB\n                                    file_size = worker_log.stat().st_size\n                                    chunk_size = 1024 * 1024  # 1MB chunks for streaming\n\n                                    if file_size > max_size:\n                                        test_logger.warning(\n                                            f\"Worker log {worker_log.name} is {file_size / (1024*1024):.1f}MB, truncating to last 50MB\"\n                                        )\n                                        # Read only the last 50MB in streaming chunks\n                                        with open(worker_log, \"rb\") as f:\n                                            f.seek(\n                                                -max_size, 2\n                                            )  # Seek to 50MB from end\n                                            while True:\n                                                chunk = f.read(chunk_size)\n                                                if not chunk:\n                                                    break\n                                                main_file.write(\n                                                    chunk.decode(\n                                                        \"utf-8\", errors=\"replace\"\n                                                    )\n                                                )\n                                                main_file.flush()  # Flush to disk to free memory\n                                    else:\n                                        # Stream entire file in chunks to avoid memory accumulation\n                                        with open(\n                                            worker_log, \"r\", encoding=\"utf-8\"\n                                        ) as worker_file:\n                                            while True:\n                                                chunk = worker_file.read(chunk_size)\n                                                if not chunk:\n                                                    break\n                                                main_file.write(chunk)\n                                                main_file.flush()  # Flush to disk to free memory\n                                    break  # Success, exit retry loop\n                                except (IOError, PermissionError, OSError) as e:\n                                    if attempt < max_retries - 1:\n                                        time.sleep(retry_delay)\n                                        continue\n                                    else:\n                                        # Last attempt failed, log warning and skip this file\n                                        test_logger.warning(\n                                            f\"Failed to read worker log {worker_log.name} after {max_retries} attempts: {e}\"\n                                        )\n                                        main_file.write(\n                                            f\"# Error: Could not read {worker_log.name} - file may be locked\\n\"\n                                        )\n                                        break\n                        except Exception as e:\n                            main_file.write(f\"# Error reading {worker_log.name}: {e}\\n\")\n                            test_logger.warning(\n                                f\"Error reading worker log {worker_log.name}: {e}\"\n                            )\n                        main_file.write(f\"\\n# --- End Worker {worker_id} ---\\n\\n\")\n                        main_file.flush()  # Ensure worker section is written before next\n\n                test_logger.info(\n                    f\"Consolidated {len(worker_test_run_logs)} worker test_run logs into {main_test_run_log.name}\"\n                )\n            except Exception as e:\n                test_logger.warning(f\"Error consolidating test_run logs: {e}\")\n\n        # Consolidate consolidated logs\n        main_consolidated_log = logs_dir / \"test_consolidated.log\"\n        if worker_consolidated_logs:\n            try:\n                with open(main_consolidated_log, \"a\", encoding=\"utf-8\") as main_file:\n                    main_file.write(f\"\\n{'='*80}\\n\")\n                    main_file.write(\"# CONSOLIDATED FROM PARALLEL WORKERS\\n\")\n                    main_file.write(f\"{'='*80}\\n\\n\")\n\n                    for worker_log in worker_consolidated_logs:\n                        worker_id = worker_log.stem.replace(\"test_consolidated_\", \"\")\n                        main_file.write(f\"\\n# --- Worker {worker_id} ---\\n\")\n                        try:\n                            # Retry mechanism to handle file locking issues\n                            # Workers might still have file handles open\n                            import time\n\n                            max_retries = 5\n                            retry_delay = 0.2  # 200ms between retries\n                            content = None\n\n                            for attempt in range(max_retries):\n                                try:\n                                    # Stream file content in chunks to avoid loading entire file into memory\n                                    # Limit to 10MB per worker log to prevent memory issues\n                                    max_size = 10 * 1024 * 1024  # 10MB\n                                    file_size = worker_log.stat().st_size\n                                    chunk_size = 1024 * 1024  # 1MB chunks for streaming\n\n                                    if file_size > max_size:\n                                        test_logger.warning(\n                                            f\"Worker log {worker_log.name} is {file_size / (1024*1024):.1f}MB, truncating to last 10MB\"\n                                        )\n                                        # Read only the last 10MB in streaming chunks\n                                        with open(worker_log, \"rb\") as f:\n                                            f.seek(\n                                                -max_size, 2\n                                            )  # Seek to 10MB from end\n                                            while True:\n                                                chunk = f.read(chunk_size)\n                                                if not chunk:\n                                                    break\n                                                main_file.write(\n                                                    chunk.decode(\n                                                        \"utf-8\", errors=\"replace\"\n                                                    )\n                                                )\n                                                main_file.flush()  # Flush to disk to free memory\n                                    else:\n                                        # Stream entire file in chunks to avoid memory accumulation\n                                        with open(\n                                            worker_log, \"r\", encoding=\"utf-8\"\n                                        ) as worker_file:\n                                            while True:\n                                                chunk = worker_file.read(chunk_size)\n                                                if not chunk:\n                                                    break\n                                                main_file.write(chunk)\n                                                main_file.flush()  # Flush to disk to free memory\n                                    break  # Success, exit retry loop\n                                except (IOError, PermissionError, OSError) as e:\n                                    if attempt < max_retries - 1:\n                                        time.sleep(retry_delay)\n                                        continue\n                                    else:\n                                        # Last attempt failed, log warning and skip this file\n                                        test_logger.warning(\n                                            f\"Failed to read worker log {worker_log.name} after {max_retries} attempts: {e}\"\n                                        )\n                                        main_file.write(\n                                            f\"# Error: Could not read {worker_log.name} - file may be locked\\n\"\n                                        )\n                                        break\n                        except Exception as e:\n                            main_file.write(f\"# Error reading {worker_log.name}: {e}\\n\")\n                            test_logger.warning(\n                                f\"Error reading worker log {worker_log.name}: {e}\"\n                            )\n                        main_file.write(f\"\\n# --- End Worker {worker_id} ---\\n\\n\")\n\n                test_logger.info(\n                    f\"Consolidated {len(worker_consolidated_logs)} worker consolidated logs into {main_consolidated_log.name}\"\n                )\n\n                # CRITICAL: Register main consolidated log for rotation AFTER consolidation\n                # (consolidation can make the file huge, so we need to check rotation after)\n                session_rotation_manager.register_log_file(str(main_consolidated_log))\n            except Exception as e:\n                test_logger.warning(f\"Error consolidating consolidated logs: {e}\")\n\n        # Clean up worker log files after consolidation\n        # CRITICAL: Add delay before cleanup to allow workers to close file handles\n        # Windows file locking can prevent immediate deletion\n        import time\n\n        time.sleep(0.5)  # 500ms delay to allow workers to close file handles\n\n        all_worker_logs = worker_test_run_logs + worker_consolidated_logs\n        cleaned_count = 0\n        failed_logs = []\n\n        for worker_log in all_worker_logs:\n            # Retry deletion with exponential backoff for Windows file locking\n            max_retries = 5\n            retry_delay = 0.1  # Start with 100ms\n            deleted = False\n\n            for attempt in range(max_retries):\n                try:\n                    worker_log.unlink()\n                    cleaned_count += 1\n                    deleted = True\n                    break\n                except (OSError, PermissionError) as e:\n                    # Windows file locking error - retry with delay\n                    if attempt < max_retries - 1:\n                        time.sleep(retry_delay)\n                        retry_delay *= (\n                            2  # Exponential backoff: 0.1s, 0.2s, 0.4s, 0.8s, 1.6s\n                        )\n                    else:\n                        # Last attempt failed - log but don't fail\n                        failed_logs.append(worker_log.name)\n                        test_logger.debug(\n                            f\"Could not remove worker log {worker_log.name} after {max_retries} attempts: {e}\"\n                        )\n                except Exception as e:\n                    # Other errors - don't retry\n                    failed_logs.append(worker_log.name)\n                    test_logger.debug(\n                        f\"Error removing worker log {worker_log.name}: {e}\"\n                    )\n                    break\n\n        if cleaned_count > 0:\n            test_logger.debug(\n                f\"Cleaned up {cleaned_count} worker log files after consolidation\"\n            )\n        if failed_logs:\n            # Only log warning if some files failed (not critical - they'll be cleaned up next run)\n            test_logger.debug(\n                f\"Could not clean up {len(failed_logs)} worker log files (locked by workers): {', '.join(failed_logs[:5])}{'...' if len(failed_logs) > 5 else ''}\"\n            )\n\n        test_logger.info(\"Worker log consolidation completed\")\n\n    except Exception as e:\n        test_logger.warning(f\"Error during worker log consolidation: {e}\")\n\n\ndef pytest_sessionfinish(session, exitstatus):\n    \"\"\"Log test session finish, check for log rotation, consolidate worker logs, and handle crash recovery.\"\"\"\n    test_logger.info(f\"Test session finished with exit status: {exitstatus}\")\n    if hasattr(session, \"testscollected\"):\n        test_logger.info(f\"Tests collected: {session.testscollected}\")\n\n    # Only run in main process\n    if os.environ.get(\"PYTEST_XDIST_WORKER\"):\n        return\n\n    # Detect crash (non-zero exit status that's not a normal failure)\n    # Exit status 1 = tests failed (normal), 2 = interrupted (normal), 3 = internal error (crash)\n    if exitstatus == 3:\n        test_logger.error(\n            \"Test session crashed (internal error) - saving crash diagnostics\"\n        )\n\n        # Save crash diagnostics\n        crash_log_file = Path(\"tests/logs/crash_diagnostics.log\")\n        try:\n            crash_log_file.parent.mkdir(parents=True, exist_ok=True)\n            with open(crash_log_file, \"a\", encoding=\"utf-8\") as f:\n                f.write(f\"\\n{'='*80}\\n\")\n                f.write(f\"CRASH DETECTED: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n                f.write(f\"Exit Status: {exitstatus}\\n\")\n                f.write(\n                    f\"Tests Run: {getattr(session, 'testsfailed', 0)} failed, {getattr(session, 'testscollected', 0)} collected\\n\"\n                )\n                f.write(f\"{'='*80}\\n\")\n        except Exception as e:\n            test_logger.warning(f\"Could not save crash diagnostics: {e}\")\n\n    # Ensure cleanup even on crash\n    try:\n        from communication.core.channel_orchestrator import CommunicationManager\n\n        if CommunicationManager._instance is not None:\n            test_logger.debug(\"Session finish: Ensuring CommunicationManager cleanup\")\n            try:\n                CommunicationManager._instance.stop_all()\n            except Exception:\n                pass\n            CommunicationManager._instance = None\n    except (ImportError, ModuleNotFoundError):\n        pass\n    except Exception as e:\n        test_logger.warning(f\"Error during session finish cleanup: {e}\")\n\n    # Consolidate worker logs FIRST (this can make main log files huge)\n    _consolidate_worker_logs()\n\n    # NOTE: Log rotation only happens at session START in setup_consolidated_test_logging.\n    # We do NOT rotate at session end because:\n    # 1. File handlers are still active and writing, causing corruption\n    # 2. Rotation at session start (before logging begins) is safer\n    # 3. Consolidation happens here, but rotation should wait until next session start\n\n\n# Track test start times for duration calculation (for debugging)\n_test_start_times = {}\n\n\ndef pytest_runtest_setup(item):\n    \"\"\"Log when a test starts with timestamp (DEBUG level only).\"\"\"\n    test_id = item.nodeid\n    start_time = datetime.now()\n    _test_start_times[test_id] = start_time\n    timestamp = start_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]  # Include milliseconds\n\n    # Include worker ID if running in parallel mode\n    worker_id = os.environ.get(\"PYTEST_XDIST_WORKER\", \"main\")\n\n    # CRITICAL: Always log test starts with worker ID at INFO level for worker tracking\n    # This is essential for debugging memory leaks - we need to know which worker ran which test\n    test_logger.info(f\"[WORKER-TEST] [{worker_id}] START: {test_id}\")\n\n    # Use DEBUG level to reduce log noise - only visible with TEST_VERBOSE_LOGS=2\n    test_logger.debug(f\"[TEST-START] [{worker_id}] {timestamp} - {test_id}\")\n\n\ndef pytest_runtest_teardown(item, nextitem):\n    \"\"\"Log when a test ends with timestamp and duration (DEBUG level only).\"\"\"\n    test_id = item.nodeid\n    end_time = datetime.now()\n    timestamp = end_time.strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]  # Include milliseconds\n\n    # Calculate duration if we have start time\n    duration = None\n    if test_id in _test_start_times:\n        start_time = _test_start_times[test_id]\n        duration_seconds = (end_time - start_time).total_seconds()\n        duration = f\"{duration_seconds:.3f}s\"\n        del _test_start_times[test_id]\n\n    # Use DEBUG level to reduce log noise - only visible with TEST_VERBOSE_LOGS=2\n    if duration:\n        test_logger.debug(f\"[TEST-END] {timestamp} - {test_id} (duration: {duration})\")\n    else:\n        test_logger.debug(f\"[TEST-END] {timestamp} - {test_id}\")\n\n\ndef pytest_runtest_logreport(report):\n    \"\"\"Log individual test results with timestamps.\"\"\"\n    if report.when == \"call\":\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[\n            :-3\n        ]  # Include milliseconds\n        # Only log PASSED tests when verbose mode is enabled (to reduce log noise)\n        verbose_logs = os.getenv(\"TEST_VERBOSE_LOGS\", \"0\")\n        if report.passed:\n            # Only log passed tests at DEBUG level (level 2) to avoid I/O overhead\n            # Level 1 and 0: don't log passed tests - focus on failures, warnings, and skips\n            if verbose_logs == \"2\":\n                test_logger.debug(\n                    f\"[TEST-RESULT] {timestamp} - PASSED: {report.nodeid}\"\n                )\n            # Don't log passed tests at other levels - they're not interesting\n        elif report.failed:\n            # Always log failures - these are important\n            worker_id = os.environ.get(\"PYTEST_XDIST_WORKER\", \"main\")\n            # CRITICAL: Also log at INFO level for worker tracking (even failures)\n            test_logger.info(f\"[WORKER-TEST] [{worker_id}] FAILED: {report.nodeid}\")\n            test_logger.error(\n                f\"[TEST-RESULT] [{worker_id}] {timestamp} - FAILED: {report.nodeid}\"\n            )\n            if report.longrepr:\n                test_logger.error(f\"Error details: {report.longrepr}\")\n        elif report.skipped:\n            # Always log skips - these might indicate issues\n            worker_id = os.environ.get(\"PYTEST_XDIST_WORKER\", \"main\")\n            test_logger.warning(\n                f\"[TEST-RESULT] [{worker_id}] {timestamp} - SKIPPED: {report.nodeid}\"\n            )\n        elif report.passed:\n            # CRITICAL: Log passed tests at INFO level for worker tracking (memory leak debugging)\n            # This is essential to know which tests each worker ran\n            worker_id = os.environ.get(\"PYTEST_XDIST_WORKER\", \"main\")\n            test_logger.info(f\"[WORKER-TEST] [{worker_id}] PASSED: {report.nodeid}\")\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef cleanup_communication_manager():\n    \"\"\"Clean up CommunicationManager singleton after all tests complete.\"\"\"\n    yield\n\n    # Clean up CommunicationManager singleton to prevent access violations\n    # CRITICAL: Add timeout to prevent hanging during cleanup\n    import threading\n    import time\n\n    cleanup_complete = threading.Event()\n    cleanup_error: list[Exception | None] = [None]\n\n    def do_cleanup():\n        try:\n            from communication.core.channel_orchestrator import CommunicationManager\n\n            if CommunicationManager._instance is not None:\n                test_logger.debug(\"Cleaning up CommunicationManager singleton...\")\n                # Add timeout wrapper to prevent hanging\n                import signal\n                import sys\n\n                # On Windows, use threading timeout instead of signal\n                stop_complete = threading.Event()\n                stop_error: list[Exception | None] = [None]\n\n                def stop_worker():\n                    try:\n                        CommunicationManager._instance.stop_all()\n                    except Exception as e:\n                        stop_error[0] = e\n                    finally:\n                        stop_complete.set()\n\n                stop_thread = threading.Thread(target=stop_worker, daemon=True)\n                stop_thread.start()\n\n                # Wait up to 10 seconds for stop_all to complete\n                if not stop_complete.wait(timeout=10.0):\n                    test_logger.warning(\n                        \"CommunicationManager.stop_all() timed out after 10 seconds - forcing cleanup\"\n                    )\n                    # Force cleanup even if stop_all didn't complete\n                    try:\n                        CommunicationManager._instance._running = False\n                        if hasattr(CommunicationManager._instance, \"_channels_dict\"):\n                            CommunicationManager._instance._channels_dict.clear()\n                    except Exception:\n                        pass\n                elif stop_error[0]:\n                    test_logger.warning(f\"Error during stop_all(): {stop_error[0]}\")\n\n                CommunicationManager._instance = None\n                test_logger.debug(\"CommunicationManager cleanup completed\")\n        except (ImportError, ModuleNotFoundError):\n            # Silently skip if core/communication modules aren't available (e.g., development tools tests)\n            pass\n        except Exception as e:\n            cleanup_error[0] = e\n        finally:\n            cleanup_complete.set()\n\n    cleanup_thread = threading.Thread(target=do_cleanup, daemon=True)\n    cleanup_thread.start()\n\n    # Wait up to 15 seconds for cleanup to complete (increased from 5 to account for stop_all timeout)\n    if not cleanup_complete.wait(timeout=15.0):\n        test_logger.warning(\"CommunicationManager cleanup timed out after 15 seconds\")\n    elif cleanup_error[0]:\n        test_logger.warning(\n            f\"Error during CommunicationManager cleanup: {cleanup_error[0]}\"\n        )\n\n\n@pytest.fixture(autouse=True)\ndef cleanup_conversation_manager():\n    \"\"\"Clean up ConversationManager state before each test.\"\"\"\n    # Clear state before test\n    try:\n        from communication.message_processing.conversation_flow_manager import (\n            conversation_manager,\n        )\n\n        conversation_manager.clear_all_states()\n    except (ImportError, ModuleNotFoundError):\n        # Silently skip if core/communication modules aren't available (e.g., development tools tests)\n        pass\n    except Exception as e:\n        test_logger.warning(f\"Error clearing conversation manager state: {e}\")\n\n    yield\n\n    # Clear state after test\n    try:\n        from communication.message_processing.conversation_flow_manager import (\n            conversation_manager,\n        )\n\n        conversation_manager.clear_all_states()\n    except (ImportError, ModuleNotFoundError):\n        # Silently skip if core/communication modules aren't available (e.g., development tools tests)\n        pass\n    except Exception as e:\n        test_logger.warning(f\"Error clearing conversation manager state: {e}\")\n\n\n@pytest.fixture(autouse=True)\ndef cleanup_conversation_history():\n    \"\"\"Clean up ConversationHistory singleton state before and after each test.\n\n    This prevents conversation history from accumulating across tests, which was\n    causing memory leaks in batch runs (especially batch 8 with AI conversation tests).\n    \"\"\"\n    # Clear state before test\n    try:\n        from ai.conversation_history import get_conversation_history\n\n        history = get_conversation_history()\n        if history is not None:\n            # Clear all sessions for all users\n            if hasattr(history, \"_sessions\"):\n                history._sessions.clear()\n            if hasattr(history, \"_active_sessions\"):\n                history._active_sessions.clear()\n    except (ImportError, ModuleNotFoundError):\n        # Silently skip if AI modules aren't available\n        pass\n    except Exception as e:\n        test_logger.warning(f\"Error clearing conversation history state: {e}\")\n\n    yield\n\n    # Clear state after test\n    try:\n        from ai.conversation_history import get_conversation_history\n\n        history = get_conversation_history()\n        if history is not None:\n            # Clear all sessions for all users\n            if hasattr(history, \"_sessions\"):\n                history._sessions.clear()\n            if hasattr(history, \"_active_sessions\"):\n                history._active_sessions.clear()\n    except (ImportError, ModuleNotFoundError):\n        # Silently skip if AI modules aren't available\n        pass\n    except Exception as e:\n        test_logger.warning(f\"Error clearing conversation history state: {e}\")\n\n\n@pytest.fixture(autouse=True)\ndef cleanup_singletons():\n    \"\"\"Clean up singleton instances before each test to ensure isolation.\"\"\"\n    # Store original singleton instances\n    original_instances = {}\n\n    try:\n        # Store AI Chatbot singleton\n        try:\n            from ai.chatbot import AIChatBotSingleton\n\n            original_instances[\"ai_chatbot\"] = AIChatBotSingleton._instance\n        except (ImportError, AttributeError):\n            pass\n\n        # Store MessageRouter singleton\n        try:\n            import communication.message_processing.message_router as router_module\n\n            original_instances[\"message_router\"] = router_module._message_router\n        except (ImportError, AttributeError):\n            pass\n\n        # Store cache instances\n        try:\n            import ai.cache_manager as cache_module\n\n            original_instances[\"response_cache\"] = getattr(\n                cache_module, \"_response_cache\", None\n            )\n            original_instances[\"context_cache\"] = getattr(\n                cache_module, \"_context_cache\", None\n            )\n        except (ImportError, AttributeError):\n            pass\n\n        yield\n\n    finally:\n        # Restore original singleton instances to prevent state pollution\n        try:\n            from ai.chatbot import AIChatBotSingleton\n\n            if \"ai_chatbot\" in original_instances:\n                AIChatBotSingleton._instance = original_instances[\"ai_chatbot\"]\n                # Also clear the locks_by_user defaultdict which can accumulate\n                if AIChatBotSingleton._instance is not None:\n                    if hasattr(AIChatBotSingleton._instance, \"_locks_by_user\"):\n                        AIChatBotSingleton._instance._locks_by_user.clear()\n        except (ImportError, AttributeError):\n            pass\n\n        try:\n            import communication.message_processing.message_router as router_module\n\n            if \"message_router\" in original_instances:\n                router_module._message_router = original_instances[\"message_router\"]\n        except (ImportError, AttributeError):\n            pass\n\n        # Clear cache instances (not restore - caches should be fresh for each test)\n        try:\n            import ai.cache_manager as cache_module\n\n            # Clear caches if they exist and have a clear method\n            if (\n                hasattr(cache_module, \"_response_cache\")\n                and cache_module._response_cache is not None\n            ):\n                if hasattr(cache_module._response_cache, \"clear\"):\n                    cache_module._response_cache.clear()\n                cache_module._response_cache = None\n            if (\n                hasattr(cache_module, \"_context_cache\")\n                and cache_module._context_cache is not None\n            ):\n                if hasattr(cache_module._context_cache, \"clear\"):\n                    cache_module._context_cache.clear()\n                cache_module._context_cache = None\n        except (ImportError, AttributeError):\n            pass\n\n\n@pytest.fixture(autouse=True)\ndef cleanup_communication_threads():\n    \"\"\"\n    Lightweight cleanup of CommunicationManager state between tests.\n\n    This fixture performs only lightweight state clearing to prevent resource\n    accumulation. Full cleanup with thread joins and async operations is handled\n    by the session-scoped cleanup_communication_manager fixture at the end of\n    the test session.\n\n    Rationale: Per-test cleanup must be fast and non-blocking. Calling stop_all()\n    after every test causes resource exhaustion (threads, file handles, memory)\n    that accumulates over hundreds of tests, eventually making the system unresponsive.\n    \"\"\"\n    yield\n\n    # Lightweight state clearing only - no blocking operations\n    # Full cleanup is handled by cleanup_communication_manager fixture at session end\n    try:\n        from communication.core.channel_orchestrator import CommunicationManager\n\n        if CommunicationManager._instance is not None:\n            try:\n                # Only clear state without blocking operations\n                # This prevents resource accumulation while keeping cleanup fast\n                if hasattr(CommunicationManager._instance, \"_channels_dict\"):\n                    # Clear channels dict to prevent state leakage between tests\n                    # This is safe because channels will be recreated if needed\n                    CommunicationManager._instance._channels_dict.clear()\n            except Exception:\n                pass  # Ignore errors during lightweight cleanup\n    except (ImportError, ModuleNotFoundError):\n        # Silently skip if core/communication modules aren't available (e.g., development tools tests)\n        pass\n    except Exception:\n        pass  # Ignore other errors\n\n\n@pytest.fixture(autouse=True)\ndef periodic_memory_cleanup(request):\n    \"\"\"\n    Periodic memory cleanup to prevent accumulation during long test runs.\n\n    Performs garbage collection every N tests to help prevent memory accumulation.\n    This is especially important for long-running test suites and parallel execution.\n    \"\"\"\n    yield\n\n    # Run garbage collection more frequently to prevent memory accumulation\n    # Reduced from every 100 tests to every 20 tests for better memory management\n    # This is especially important for parallel test execution with pytest-xdist\n    test_count = getattr(periodic_memory_cleanup, \"_test_count\", 0) + 1\n    periodic_memory_cleanup._test_count = test_count\n\n    # More frequent GC for parallel runs, less frequent for sequential\n    import os\n\n    is_parallel = \"PYTEST_XDIST_WORKER\" in os.environ\n    gc_interval = (\n        5 if is_parallel else 20\n    )  # Every 5 tests in parallel (more aggressive), 20 in sequential\n\n    if test_count % gc_interval == 0:\n        import gc\n\n        # Force garbage collection to free up memory\n        collected = gc.collect()\n        if collected > 0:\n            test_logger.debug(\n                f\"Garbage collection freed {collected} objects after {test_count} tests\"\n            )\n\n        # Also clear any module-level caches that might accumulate\n        try:\n            # Clear AI caches\n            import ai.cache_manager as cache_module\n\n            if (\n                hasattr(cache_module, \"_response_cache\")\n                and cache_module._response_cache is not None\n            ):\n                if hasattr(cache_module._response_cache, \"clear\"):\n                    cache_module._response_cache.clear()\n            if (\n                hasattr(cache_module, \"_context_cache\")\n                and cache_module._context_cache is not None\n            ):\n                if hasattr(cache_module._context_cache, \"clear\"):\n                    cache_module._context_cache.clear()\n        except (ImportError, AttributeError):\n            pass\n\n        # Clear AI chatbot locks dictionary (defaultdict can accumulate)\n        try:\n            from ai.chatbot import AIChatBotSingleton\n\n            if AIChatBotSingleton._instance is not None:\n                if hasattr(AIChatBotSingleton._instance, \"_locks_by_user\"):\n                    # Clear old locks to prevent accumulation\n                    AIChatBotSingleton._instance._locks_by_user.clear()\n        except (ImportError, AttributeError):\n            pass\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1677,
                    "line_content": "This function is kept for backward compatibility but does nothing.",
                    "start": 71560,
                    "end": 71582
                  }
                ]
              ],
              [
                "tests\\test_utilities.py",
                "#!/usr/bin/env python3\n\"\"\"\nTest Utilities for MHM\nCentralized test helper functions to eliminate redundancy across test files\n\"\"\"\n\nimport os\nimport tempfile\nimport shutil\nimport json\nimport logging\nimport copy\nimport uuid\n\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional, List\nfrom datetime import datetime\n\n# Do not modify sys.path; rely on package imports\n\nfrom core.user_data_handlers import create_new_user, save_user_data, get_user_data\nfrom core.file_operations import ensure_user_directory\nfrom core.time_utilities import now_timestamp_full, DATE_ONLY\n\n# Setup logger for test utilities\nlogger = logging.getLogger(__name__)\n\n\nclass TestUserFactory:\n    \"\"\"Factory for creating test users with different configurations\"\"\"\n\n    # Cache for pre-created user data structures to avoid recreating identical data\n    _user_data_cache: dict[str, dict[str, Any]] = {}\n\n    @staticmethod\n    def _get_cache_key(\n        enable_checkins: bool = None,\n        enable_tasks: bool = None,\n        user_type: str = \"basic\",\n        **kwargs,\n    ) -> str:\n        \"\"\"Generate a cache key for user data structures (configuration only, not user_id).\"\"\"\n        # For user types with fixed configurations, use just the type\n        if user_type in (\n            \"minimal\",\n            \"full\",\n            \"discord\",\n            \"email\",\n            \"health\",\n            \"task\",\n            \"complex_checkins\",\n            \"disability\",\n            \"limited_data\",\n            \"inconsistent\",\n        ):\n            return f\"{user_type}\"\n        # For basic users, include checkins and tasks in the key\n        return f\"{user_type}:checkins={enable_checkins}:tasks={enable_tasks}\"\n\n    @staticmethod\n    def _get_cached_user_data(cache_key: str) -> dict[str, Any] | None:\n        \"\"\"Get cached user data structure if available.\"\"\"\n        return TestUserFactory._user_data_cache.get(cache_key)\n\n    @staticmethod\n    def _cache_user_data(cache_key: str, user_data: dict[str, Any]):\n        \"\"\"Cache a user data structure for reuse.\"\"\"\n        # Use deepcopy to ensure the cached template is independent\n        TestUserFactory._user_data_cache[cache_key] = copy.deepcopy(user_data)\n\n    @staticmethod\n    def clear_cache():\n        \"\"\"Clear the user data cache (useful for test cleanup).\"\"\"\n        TestUserFactory._user_data_cache.clear()\n\n    @staticmethod\n    def create_basic_user(\n        user_id: str,\n        enable_checkins: bool = True,\n        enable_tasks: bool = True,\n        test_data_dir: str = None,\n    ) -> bool:\n        \"\"\"\n        Create a test user with basic functionality enabled\n\n        Args:\n            user_id: Unique identifier for the test user\n            enable_checkins: Whether to enable check-ins for this user\n            enable_tasks: Whether to enable task management for this user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # test_data_dir is now required - all tests should use the modern approach\n            if not test_data_dir:\n                raise ValueError(\n                    \"test_data_dir parameter is required - use modern test approach\"\n                )\n            return TestUserFactory.create_basic_user__with_test_dir(\n                user_id, enable_checkins, enable_tasks, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(f\"Error creating basic user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def _create_user_files_directly__directory_structure(\n        test_data_dir: str, user_id: str\n    ) -> tuple[str, str]:\n        \"\"\"Create the user directory structure and return paths.\"\"\"\n        # Create test users directory\n        test_users_dir = os.path.join(test_data_dir, \"users\")\n        os.makedirs(test_users_dir, exist_ok=True)\n\n        # Generate UUID for the user\n        import uuid\n\n        actual_user_id = str(uuid.uuid4())\n\n        # Create user directory\n        user_dir = os.path.join(test_users_dir, actual_user_id)\n        os.makedirs(user_dir, exist_ok=True)\n\n        return actual_user_id, user_dir\n\n    @staticmethod\n    def _create_user_files_directly__account_data(\n        actual_user_id: str, user_id: str, user_data: dict\n    ) -> dict:\n        \"\"\"Create account data structure.\"\"\"\n        return {\n            \"user_id\": actual_user_id,\n            \"internal_username\": user_id,\n            \"account_status\": \"active\",\n            \"chat_id\": user_data.get(\"chat_id\", \"\"),\n            \"phone\": user_data.get(\"phone\", \"\"),\n            \"email\": user_data.get(\"email\", \"\"),\n            \"discord_user_id\": user_data.get(\"discord_user_id\", \"\"),\n            \"created_at\": now_timestamp_full(),\n            \"updated_at\": now_timestamp_full(),\n            \"features\": {\n                \"automated_messages\": (\n                    \"enabled\" if user_data.get(\"categories\") else \"disabled\"\n                ),\n                \"checkins\": (\n                    \"enabled\"\n                    if user_data.get(\"checkin_settings\", {}).get(\"enabled\", False)\n                    else \"disabled\"\n                ),\n                \"task_management\": (\n                    \"enabled\"\n                    if user_data.get(\"task_settings\", {}).get(\"enabled\", False)\n                    else \"disabled\"\n                ),\n            },\n            \"timezone\": user_data.get(\"timezone\", \"\"),\n        }\n\n    @staticmethod\n    def _create_user_files_directly__preferences_data(user_data: dict) -> dict:\n        \"\"\"Create preferences data structure.\"\"\"\n        preferences_data = {\n            \"categories\": user_data.get(\"categories\", []),\n            \"channel\": {\"type\": user_data.get(\"channel\", {}).get(\"type\", \"email\")},\n            \"checkin_settings\": user_data.get(\"checkin_settings\", {}),\n            \"task_settings\": user_data.get(\"task_settings\", {}),\n        }\n\n        # Remove redundant enabled flags from preferences since they're in account.json features\n        if (\n            \"checkin_settings\" in preferences_data\n            and \"enabled\" in preferences_data[\"checkin_settings\"]\n        ):\n            del preferences_data[\"checkin_settings\"][\"enabled\"]\n        if (\n            \"task_settings\" in preferences_data\n            and \"enabled\" in preferences_data[\"task_settings\"]\n        ):\n            del preferences_data[\"task_settings\"][\"enabled\"]\n\n        return preferences_data\n\n    @staticmethod\n    def _create_user_files_directly__context_data(user_data: dict) -> dict:\n        \"\"\"Create user context data structure.\"\"\"\n        return {\n            \"preferred_name\": user_data.get(\"preferred_name\", \"\"),\n            \"gender_identity\": user_data.get(\"gender_identity\", []),\n            \"date_of_birth\": user_data.get(\"date_of_birth\", \"\"),\n            \"custom_fields\": {\n                \"reminders_needed\": user_data.get(\"reminders_needed\", []),\n                \"health_conditions\": user_data.get(\"custom_fields\", {}).get(\n                    \"health_conditions\", []\n                ),\n                \"medications_treatments\": user_data.get(\"custom_fields\", {}).get(\n                    \"medications_treatments\", []\n                ),\n                \"allergies_sensitivities\": user_data.get(\"custom_fields\", {}).get(\n                    \"allergies_sensitivities\", []\n                ),\n            },\n            \"interests\": user_data.get(\"interests\", []),\n            \"goals\": user_data.get(\"goals\", []),\n            \"loved_ones\": user_data.get(\"loved_ones\", []),\n            \"activities_for_encouragement\": user_data.get(\n                \"activities_for_encouragement\", []\n            ),\n            \"notes_for_ai\": user_data.get(\"notes_for_ai\", []),\n            \"created_at\": now_timestamp_full(),\n            \"last_updated\": now_timestamp_full(),\n        }\n\n    @staticmethod\n    def _create_user_files_directly__save_json(file_path: str, data: dict):\n        \"\"\"Save data to a JSON file.\"\"\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(data, f, indent=2, ensure_ascii=False)\n\n    @staticmethod\n    def _create_user_files_directly__schedules_data(categories: list) -> dict:\n        \"\"\"Create default schedule periods for categories.\"\"\"\n        if not categories:\n            return {}\n\n        schedules_data = {}\n        for category in categories:\n            schedules_data[category] = {\n                \"periods\": {\n                    \"Morning\": {\n                        \"active\": True,\n                        \"days\": [\"ALL\"],\n                        \"start_time\": \"09:00\",\n                        \"end_time\": \"12:00\",\n                    },\n                    \"Afternoon\": {\n                        \"active\": True,\n                        \"days\": [\"ALL\"],\n                        \"start_time\": \"13:00\",\n                        \"end_time\": \"17:00\",\n                    },\n                    \"Evening\": {\n                        \"active\": True,\n                        \"days\": [\"ALL\"],\n                        \"start_time\": \"18:00\",\n                        \"end_time\": \"21:00\",\n                    },\n                }\n            }\n        return schedules_data\n\n    @staticmethod\n    def _create_user_files_directly__message_files(user_dir: str, categories: list):\n        \"\"\"Create message directory and default message files.\"\"\"\n        # Ensure user directory exists before creating messages subdirectory (race condition fix)\n        os.makedirs(user_dir, exist_ok=True)\n        messages_dir = os.path.join(user_dir, \"messages\")\n        os.makedirs(messages_dir, exist_ok=True)\n\n        for category in categories:\n            category_file = os.path.join(messages_dir, f\"{category}.json\")\n            if not os.path.exists(category_file):\n                # Ensure parent directory exists before writing (race condition fix)\n                os.makedirs(os.path.dirname(category_file), exist_ok=True)\n                with open(category_file, \"w\", encoding=\"utf-8\") as f:\n                    json.dump([], f, indent=2, ensure_ascii=False)\n\n        # Create sent_messages.json\n        sent_messages_file = os.path.join(messages_dir, \"sent_messages.json\")\n        if not os.path.exists(sent_messages_file):\n            # Ensure parent directory exists before writing (race condition fix)\n            os.makedirs(os.path.dirname(sent_messages_file), exist_ok=True)\n            with open(sent_messages_file, \"w\", encoding=\"utf-8\") as f:\n                json.dump([], f, indent=2, ensure_ascii=False)\n\n    @staticmethod\n    def create_basic_user__update_index(\n        test_data_dir: str,\n        user_id: str,\n        actual_user_id: str,\n        discord_user_id: str = None,\n        email: str = None,\n    ):\n        \"\"\"Update user index to map internal_username to UUID.\n\n        Also adds mappings for Discord user ID and email if provided.\n        Uses file locking to prevent race conditions in parallel test execution.\n        \"\"\"\n        from core.file_locking import safe_json_read, safe_json_write\n\n        user_index_file = os.path.join(test_data_dir, \"user_index.json\")\n\n        # Read existing index with file locking\n        user_index = safe_json_read(user_index_file, default={})\n\n        # Add the new user to the index\n        user_index[user_id] = actual_user_id\n\n        # Add Discord user ID mapping if provided\n        if discord_user_id:\n            user_index[f\"discord:{discord_user_id}\"] = actual_user_id\n\n        # Add email mapping if provided\n        if email:\n            user_index[f\"email:{email}\"] = actual_user_id\n\n        # Save the updated index with file locking\n        safe_json_write(user_index_file, user_index, indent=2)\n\n    @staticmethod\n    def _create_user_files_directly(\n        user_id: str, user_data: dict[str, Any], test_data_dir: str\n    ) -> str:\n        \"\"\"Helper function to create user files directly in test directory\"\"\"\n        # Create user directory structure\n        actual_user_id, user_dir = (\n            TestUserFactory._create_user_files_directly__directory_structure(\n                test_data_dir, user_id\n            )\n        )\n\n        # Create data structures\n        account_data = TestUserFactory._create_user_files_directly__account_data(\n            actual_user_id, user_id, user_data\n        )\n        preferences_data = (\n            TestUserFactory._create_user_files_directly__preferences_data(user_data)\n        )\n        context_data = TestUserFactory._create_user_files_directly__context_data(\n            user_data\n        )\n\n        # Save main files\n        account_file = os.path.join(user_dir, \"account.json\")\n        preferences_file = os.path.join(user_dir, \"preferences.json\")\n        context_file = os.path.join(user_dir, \"user_context.json\")\n\n        TestUserFactory._create_user_files_directly__save_json(\n            account_file, account_data\n        )\n        TestUserFactory._create_user_files_directly__save_json(\n            preferences_file, preferences_data\n        )\n        TestUserFactory._create_user_files_directly__save_json(\n            context_file, context_data\n        )\n\n        # Create schedules if categories exist\n        categories = user_data.get(\"categories\", [])\n        if categories:\n            schedules_data = (\n                TestUserFactory._create_user_files_directly__schedules_data(categories)\n            )\n            schedules_file = os.path.join(user_dir, \"schedules.json\")\n            TestUserFactory._create_user_files_directly__save_json(\n                schedules_file, schedules_data\n            )\n\n        # Create message files\n        TestUserFactory._create_user_files_directly__message_files(user_dir, categories)\n\n        # Update user index with all mappings (username, Discord ID, email)\n        discord_user_id = user_data.get(\"discord_user_id\")\n        email = user_data.get(\"email\")\n        TestUserFactory.create_basic_user__update_index(\n            test_data_dir,\n            user_id,\n            actual_user_id,\n            discord_user_id=discord_user_id,\n            email=email,\n        )\n\n        return actual_user_id\n\n    @staticmethod\n    def create_basic_user__with_test_dir(\n        user_id: str,\n        enable_checkins: bool = True,\n        enable_tasks: bool = True,\n        test_data_dir: str = None,\n    ) -> bool:\n        \"\"\"Create basic user with test directory by directly saving files\"\"\"\n        try:\n            # Check cache first to avoid recreating identical user data structures\n            # Cache key excludes user_id to allow reuse across tests with same configuration\n            cache_key = TestUserFactory._get_cache_key(\n                enable_checkins, enable_tasks, \"basic\"\n            )\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # AND ensure checkin/task settings match current parameters (for correctness)\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"email\"] = f\"{user_id}@example.com\"\n                user_data[\"preferred_name\"] = f\"Test User {user_id}\"\n                # Ensure checkin and task settings match current parameters\n                user_data[\"checkin_settings\"][\"enabled\"] = enable_checkins\n                user_data[\"task_settings\"][\"enabled\"] = enable_tasks\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": f\"{user_id}@example.com\",\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"UTC\",\n                    \"categories\": [\"motivational\", \"health\"],\n                    \"channel\": {\"type\": \"discord\"},\n                    \"checkin_settings\": {\n                        \"enabled\": enable_checkins,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                    },\n                    \"task_settings\": {\n                        \"enabled\": enable_tasks,\n                        \"default_priority\": \"medium\",\n                        \"reminder_enabled\": True,\n                    },\n                    \"preferred_name\": f\"Test User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"\",\n                    \"reminders_needed\": [],\n                    \"custom_fields\": {\n                        \"health_conditions\": [],\n                        \"medications_treatments\": [],\n                        \"allergies_sensitivities\": [],\n                    },\n                    \"interests\": [\"Technology\", \"Gaming\"],\n                    \"goals\": [\"Improve mental health\", \"Stay organized\"],\n                    \"loved_ones\": [],\n                    \"activities_for_encouragement\": [],\n                    \"notes_for_ai\": [],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(f\"Error creating basic user with test dir {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_discord_user(\n        user_id: str, discord_user_id: str = None, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"\n        Create a test user specifically configured for Discord testing\n\n        Args:\n            user_id: Unique identifier for the test user\n            discord_user_id: Discord user ID (defaults to user_id if not provided)\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # test_data_dir is now required - all tests should use the modern approach\n            if not test_data_dir:\n                raise ValueError(\n                    \"test_data_dir parameter is required - use modern test approach\"\n                )\n            return TestUserFactory.create_discord_user__with_test_dir(\n                user_id, discord_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(f\"Error creating discord user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_discord_user__with_test_dir(\n        user_id: str, discord_user_id: str = None, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"Create discord user with test directory by directly saving files\"\"\"\n        try:\n            if discord_user_id is None:\n                discord_user_id = user_id\n\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"discord\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"email\"] = f\"{user_id}@example.com\"\n                # CRITICAL: Always set discord_user_id explicitly, don't rely on cached value\n                user_data[\"discord_user_id\"] = discord_user_id\n                user_data[\"preferred_name\"] = f\"Discord User {user_id}\"\n                # Ensure channel type is set correctly\n                if \"channel\" not in user_data:\n                    user_data[\"channel\"] = {\"type\": \"discord\"}\n                else:\n                    user_data[\"channel\"][\"type\"] = \"discord\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": f\"{user_id}@example.com\",\n                    \"discord_user_id\": discord_user_id,\n                    \"timezone\": \"UTC\",\n                    \"categories\": [\"motivational\", \"health\"],\n                    \"channel\": {\"type\": \"discord\"},\n                    \"checkin_settings\": {\n                        \"enabled\": True,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                    },\n                    \"task_settings\": {\n                        \"enabled\": True,\n                        \"default_priority\": \"medium\",\n                        \"reminder_enabled\": True,\n                    },\n                    \"preferred_name\": f\"Discord User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"\",\n                    \"reminders_needed\": [],\n                    \"custom_fields\": {\n                        \"health_conditions\": [],\n                        \"medications_treatments\": [],\n                        \"allergies_sensitivities\": [],\n                    },\n                    \"interests\": [\"Technology\", \"Gaming\"],\n                    \"goals\": [\"Improve mental health\", \"Stay organized\"],\n                    \"loved_ones\": [],\n                    \"activities_for_encouragement\": [],\n                    \"notes_for_ai\": [],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            # This already updates the index with username, Discord ID, and email mappings\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(f\"Error creating discord user with test dir {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_full_featured_user(user_id: str, test_data_dir: str = None) -> bool:\n        \"\"\"\n        Create a test user with all features enabled and comprehensive data\n\n        Args:\n            user_id: Unique identifier for the test user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return TestUserFactory.create_full_featured_user__with_test_dir(\n                    user_id, test_data_dir\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_full_featured_user__impl(user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating full featured user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_full_featured_user__with_test_dir(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"Create full featured user with test directory by directly saving files\"\"\"\n        try:\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"full\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"email\"] = f\"{user_id}@example.com\"\n                user_data[\"preferred_name\"] = f\"Full Featured User {user_id}\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": f\"{user_id}@example.com\",\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"America/New_York\",\n                    \"categories\": [\n                        \"motivational\",\n                        \"health\",\n                        \"fun_facts\",\n                        \"quotes_to_ponder\",\n                        \"word_of_the_day\",\n                    ],\n                    \"channel\": {\"type\": \"discord\"},\n                    \"checkin_settings\": {\n                        \"enabled\": True,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                        \"custom_questions\": [\n                            \"How are you feeling?\",\n                            \"Did you eat today?\",\n                            \"Did you take your medication?\",\n                        ],\n                    },\n                    \"task_settings\": {\n                        \"enabled\": True,\n                        \"default_priority\": \"high\",\n                        \"reminder_enabled\": True,\n                        \"auto_escalation\": True,\n                    },\n                    \"preferred_name\": f\"Full Featured User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"1990-01-01\",\n                    \"reminders_needed\": [\"medication\", \"appointments\", \"exercise\"],\n                    \"custom_fields\": {\n                        \"health_conditions\": [\"ADHD\", \"Depression\"],\n                        \"medications_treatments\": [\"Adderall\", \"Therapy\"],\n                        \"allergies_sensitivities\": [\"Peanuts\"],\n                    },\n                    \"interests\": [\"Technology\", \"Gaming\", \"Reading\", \"Cooking\"],\n                    \"goals\": [\n                        \"Improve mental health\",\n                        \"Stay organized\",\n                        \"Build better habits\",\n                    ],\n                    \"loved_ones\": [\"Family\", \"Friends\"],\n                    \"activities_for_encouragement\": [\n                        \"Exercise\",\n                        \"Socializing\",\n                        \"Creative projects\",\n                    ],\n                    \"notes_for_ai\": [\n                        \"Prefers gentle encouragement\",\n                        \"Responds well to humor\",\n                    ],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n\n            # Update user index to map internal_username to UUID (critical for UUID resolution)\n            TestUserFactory.create_basic_user__update_index(\n                test_data_dir, user_id, actual_user_id\n            )\n\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Error creating full featured user with test dir {user_id}: {e}\"\n            )\n            return False\n\n    @staticmethod\n    def create_full_featured_user__impl(user_id: str) -> bool:\n        \"\"\"Internal implementation of full featured user creation\"\"\"\n        try:\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": f\"{user_id}@example.com\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"America/New_York\",\n                \"categories\": [\n                    \"motivational\",\n                    \"health\",\n                    \"fun_facts\",\n                    \"quotes_to_ponder\",\n                    \"word_of_the_day\",\n                ],\n                \"channel\": {\"type\": \"discord\"},\n                \"checkin_settings\": {\n                    \"enabled\": True,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                    \"custom_questions\": [\n                        \"How are you feeling?\",\n                        \"Did you eat today?\",\n                        \"Did you take your medication?\",\n                    ],\n                },\n                \"task_settings\": {\n                    \"enabled\": True,\n                    \"default_priority\": \"high\",\n                    \"reminder_enabled\": True,\n                    \"auto_escalation\": True,\n                },\n                \"preferred_name\": f\"Full Featured User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"1990-01-01\",\n                \"reminders_needed\": [\"medication\", \"appointments\", \"exercise\"],\n                \"custom_fields\": {\n                    \"health_conditions\": [\"ADHD\", \"Depression\"],\n                    \"medications_treatments\": [\"Adderall\", \"Therapy\"],\n                    \"allergies_sensitivities\": [\"Peanuts\"],\n                },\n                \"interests\": [\"Technology\", \"Gaming\", \"Reading\", \"Cooking\"],\n                \"goals\": [\n                    \"Improve mental health\",\n                    \"Stay organized\",\n                    \"Build better habits\",\n                ],\n                \"loved_ones\": [\"Family\", \"Friends\"],\n                \"activities_for_encouragement\": [\n                    \"Exercise\",\n                    \"Socializing\",\n                    \"Creative projects\",\n                ],\n                \"notes_for_ai\": [\n                    \"Prefers gentle encouragement\",\n                    \"Responds well to humor\",\n                ],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if actual_user_id:\n                return True\n            else:\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error creating full featured user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_email_user(\n        user_id: str, email: str = None, test_data_dir: str = None\n    ) -> str:\n        \"\"\"\n        Create a test user specifically configured for email testing\n\n        Args:\n            user_id: Unique identifier for the test user\n            email: Email address (defaults to user_id@example.com if not provided)\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            str: User ID if user was created successfully, None otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return TestUserFactory.create_email_user__with_test_dir(\n                    user_id, email, test_data_dir\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_email_user__impl(user_id, email)\n\n        except Exception as e:\n            logger.error(f\"Error creating email user {user_id}: {e}\")\n            return None\n\n    @staticmethod\n    def create_email_user__with_test_dir(\n        user_id: str, email: str = None, test_data_dir: str = None\n    ) -> str:\n        \"\"\"Create email user with test directory by directly saving files\"\"\"\n        try:\n            if email is None:\n                email = f\"{user_id}@example.com\"\n\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"email\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"email\"] = email\n                user_data[\"preferred_name\"] = f\"Email User {user_id}\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": email,\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"UTC\",\n                    \"categories\": [\"motivational\", \"health\"],\n                    \"channel\": {\"type\": \"email\"},\n                    \"checkin_settings\": {\n                        \"enabled\": True,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                    },\n                    \"task_settings\": {\n                        \"enabled\": True,\n                        \"default_priority\": \"medium\",\n                        \"reminder_enabled\": True,\n                    },\n                    \"preferred_name\": f\"Email User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"\",\n                    \"reminders_needed\": [],\n                    \"custom_fields\": {\n                        \"health_conditions\": [],\n                        \"medications_treatments\": [],\n                        \"allergies_sensitivities\": [],\n                    },\n                    \"interests\": [\"Technology\", \"Reading\"],\n                    \"goals\": [\"Improve mental health\", \"Stay organized\"],\n                    \"loved_ones\": [],\n                    \"activities_for_encouragement\": [],\n                    \"notes_for_ai\": [],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.verify_email_user_creation__with_test_dir(\n                user_id, actual_user_id, test_data_dir, email=email\n            )\n\n        except Exception as e:\n            logger.error(f\"Error creating email user with test dir {user_id}: {e}\")\n            return None\n\n    @staticmethod\n    def create_email_user__impl(user_id: str, email: str = None) -> str:\n        \"\"\"Internal implementation of email user creation\"\"\"\n        try:\n            if email is None:\n                email = f\"{user_id}@example.com\"\n\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": email,\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"motivational\", \"health\"],\n                \"channel\": {\"type\": \"email\"},\n                \"checkin_settings\": {\n                    \"enabled\": True,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                },\n                \"task_settings\": {\n                    \"enabled\": True,\n                    \"default_priority\": \"medium\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": f\"Email User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": [],\n                \"custom_fields\": {\n                    \"health_conditions\": [],\n                    \"medications_treatments\": [],\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [\"Technology\", \"Reading\"],\n                \"goals\": [\"Improve mental health\", \"Stay organized\"],\n                \"loved_ones\": [],\n                \"activities_for_encouragement\": [],\n                \"notes_for_ai\": [],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            return actual_user_id\n\n        except Exception as e:\n            logger.error(f\"Error creating email user {user_id}: {e}\")\n            return None\n\n    @staticmethod\n    def create_user_with_custom_fields(\n        user_id: str, custom_fields: dict[str, Any] = None, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"\n        Create a test user with custom fields for testing custom field functionality\n\n        Args:\n            user_id: Unique identifier for the test user\n            custom_fields: Dictionary of custom fields to add to user context\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, temporarily patch the config to use it\n            if test_data_dir:\n                from unittest.mock import patch\n                import core.config\n\n                # Create test users directory\n                test_users_dir = os.path.join(test_data_dir, \"users\")\n                os.makedirs(test_users_dir, exist_ok=True)\n\n                # Temporarily patch the config to use test directory\n                with (\n                    patch.object(core.config, \"BASE_DATA_DIR\", test_data_dir),\n                    patch.object(core.config, \"USER_INFO_DIR_PATH\", test_users_dir),\n                ):\n                    return TestUserFactory.create_user_with_custom_fields__impl(\n                        user_id, custom_fields\n                    )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_user_with_custom_fields__impl(\n                    user_id, custom_fields\n                )\n\n        except Exception as e:\n            logger.error(f\"Error creating custom fields test user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_custom_fields__impl(\n        user_id: str, custom_fields: dict[str, Any] = None\n    ) -> bool:\n        \"\"\"Internal implementation of custom fields user creation\"\"\"\n        try:\n            if custom_fields is None:\n                custom_fields = {\n                    \"health_conditions\": [\"ADHD\", \"Depression\"],\n                    \"medications_treatments\": [\"Medication A\", \"Therapy\"],\n                    \"reminders_needed\": [\"Take medication\", \"Exercise\"],\n                }\n\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": f\"{user_id}@example.com\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"motivational\", \"health\"],\n                \"channel\": {\"type\": \"discord\"},\n                \"checkin_settings\": {\n                    \"enabled\": True,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                },\n                \"task_settings\": {\n                    \"enabled\": True,\n                    \"default_priority\": \"medium\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": f\"Test User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": custom_fields.get(\"reminders_needed\", []),\n                \"custom_fields\": {\n                    \"health_conditions\": custom_fields.get(\"health_conditions\", []),\n                    \"medications_treatments\": custom_fields.get(\n                        \"medications_treatments\", []\n                    ),\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [\"Technology\", \"Gaming\"],\n                \"goals\": [\"Improve executive functioning\", \"Stay organized\"],\n                \"loved_ones\": [\"Family\", \"Friends\"],\n                \"activities_for_encouragement\": [\"Exercise\", \"Reading\"],\n                \"notes_for_ai\": [\n                    \"Prefers gentle reminders\",\n                    \"Responds well to encouragement\",\n                ],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if not actual_user_id:\n                logger.error(f\"create_new_user returned None for user {user_id}\")\n                return False\n\n            # Ensure user index is updated (race condition fix for parallel execution)\n            from core.user_data_manager import update_user_index\n            import time\n\n            max_retries = 3\n            for attempt in range(max_retries):\n                success = update_user_index(actual_user_id)\n                if success:\n                    break\n                if attempt < max_retries - 1:\n                    time.sleep(0.1)  # Brief delay before retry\n\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error creating custom fields test user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_schedules(\n        user_id: str, schedule_config: dict[str, Any] = None, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"\n        Create a test user with comprehensive schedule configuration\n\n        Args:\n            user_id: Unique identifier for the test user\n            schedule_config: Custom schedule configuration\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, temporarily patch the config to use it\n            if test_data_dir:\n                from unittest.mock import patch\n                import core.config\n\n                # Create test users directory\n                test_users_dir = os.path.join(test_data_dir, \"users\")\n                os.makedirs(test_users_dir, exist_ok=True)\n\n                # Temporarily patch the config to use test directory\n                with (\n                    patch.object(core.config, \"BASE_DATA_DIR\", test_data_dir),\n                    patch.object(core.config, \"USER_INFO_DIR_PATH\", test_users_dir),\n                ):\n                    return TestUserFactory.create_user_with_schedules__impl(\n                        user_id, schedule_config\n                    )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_user_with_schedules__impl(\n                    user_id, schedule_config\n                )\n\n        except Exception as e:\n            logger.error(f\"Error creating schedules test user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_schedules__impl(\n        user_id: str, schedule_config: dict[str, Any] = None\n    ) -> bool:\n        \"\"\"Internal implementation of schedules user creation\"\"\"\n        try:\n            if schedule_config is None:\n                schedule_config = {\n                    \"motivational\": {\n                        \"periods\": {\n                            \"Default\": {\n                                \"active\": True,\n                                \"days\": [\"ALL\"],\n                                \"start_time\": \"18:00\",\n                                \"end_time\": \"21:30\",\n                            }\n                        }\n                    },\n                    \"health\": {\n                        \"periods\": {\n                            \"Default\": {\n                                \"active\": True,\n                                \"days\": [\"ALL\"],\n                                \"start_time\": \"18:00\",\n                                \"end_time\": \"20:00\",\n                            }\n                        }\n                    },\n                }\n\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": f\"{user_id}@example.com\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"motivational\", \"health\"],\n                \"channel\": {\"type\": \"discord\"},\n                \"checkin_settings\": {\n                    \"enabled\": True,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                },\n                \"task_settings\": {\n                    \"enabled\": True,\n                    \"default_priority\": \"medium\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": f\"Test User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": [],\n                \"custom_fields\": {\n                    \"health_conditions\": [],\n                    \"medications_treatments\": [],\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [\"Technology\", \"Gaming\"],\n                \"goals\": [\"Improve mental health\", \"Stay organized\"],\n                \"loved_ones\": [],\n                \"activities_for_encouragement\": [],\n                \"notes_for_ai\": [],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if not actual_user_id:\n                logger.error(f\"create_new_user returned None for user {user_id}\")\n                return False\n\n            # Ensure user directory exists before proceeding\n            from core.config import get_user_data_dir\n            import os\n\n            user_dir = get_user_data_dir(actual_user_id)\n            if not os.path.exists(user_dir):\n                os.makedirs(user_dir, exist_ok=True)\n                import time\n\n                time.sleep(0.1)  # Brief delay to ensure directory is created\n\n            # Ensure user index is updated (race condition fix for parallel execution)\n            from core.user_data_manager import update_user_index\n            import time\n\n            max_retries = 3\n            for attempt in range(max_retries):\n                success = update_user_index(actual_user_id)\n                if success:\n                    break\n                if attempt < max_retries - 1:\n                    time.sleep(0.1)  # Brief delay before retry\n\n            # Ensure user account and preferences files are fully written before saving schedules\n            # This is important for cross-file invariants that might validate schedules\n            # Schedule validation may require preferences with categories to exist\n            time.sleep(0.2)  # Brief delay to ensure account file is flushed\n\n            # Ensure preferences with categories exist (required for schedule validation)\n            # Schedule validation may require that schedule categories match preference categories\n            from core.user_data_handlers import get_user_data, save_user_data\n\n            categories = user_data.get(\"categories\", [\"motivational\", \"health\"])\n\n            # Always save preferences first to ensure they exist for validation\n            # create_new_user should have created preferences, but ensure they exist with categories\n            pref_result = save_user_data(\n                actual_user_id,\n                {\n                    \"preferences\": {\n                        \"categories\": categories,\n                        \"channel\": user_data.get(\"channel\", {\"type\": \"discord\"}),\n                    }\n                },\n                auto_create=True,\n            )\n\n            if not pref_result.get(\"preferences\", False):\n                logger.warning(\n                    f\"Failed to save preferences for user {user_id} before schedules. Result: {pref_result}\"\n                )\n                # Try once more with a delay\n                import time\n\n                time.sleep(0.2)\n                pref_result = save_user_data(\n                    actual_user_id,\n                    {\n                        \"preferences\": {\n                            \"categories\": categories,\n                            \"channel\": user_data.get(\"channel\", {\"type\": \"discord\"}),\n                        }\n                    },\n                    auto_create=True,\n                )\n                if not pref_result.get(\"preferences\", False):\n                    logger.error(\n                        f\"Failed to save preferences for user {user_id} after retry. Result: {pref_result}\"\n                    )\n                    return False\n\n            # Verify preferences were saved - retry with longer delays\n            prefs_data = {}\n            for attempt in range(5):\n                prefs_data = get_user_data(\n                    actual_user_id, \"preferences\", auto_create=True\n                )\n                if (\n                    prefs_data\n                    and \"preferences\" in prefs_data\n                    and prefs_data.get(\"preferences\", {}).get(\"categories\")\n                ):\n                    break\n                if attempt < 4:\n                    time.sleep(0.2)  # Longer delay for file system operations\n\n            if (\n                not prefs_data\n                or \"preferences\" not in prefs_data\n                or not prefs_data.get(\"preferences\", {}).get(\"categories\")\n            ):\n                # Check if preferences file exists\n                from core.config import get_user_file_path\n                import os\n\n                prefs_file = get_user_file_path(actual_user_id, \"preferences\")\n                file_exists = os.path.exists(prefs_file) if prefs_file else False\n                logger.warning(\n                    f\"Preferences with categories not found for user {user_id} after save. Got: {prefs_data}, File exists: {file_exists}, File path: {prefs_file}\"\n                )\n                # If file exists but data isn't loading, try reading directly\n                if file_exists:\n                    try:\n                        from core.file_operations import load_json_data\n\n                        direct_data = load_json_data(prefs_file)\n                        logger.debug(f\"Direct file read: {direct_data}\")\n                    except Exception as e:\n                        logger.debug(f\"Error reading preferences file directly: {e}\")\n                return False\n\n            time.sleep(0.2)  # Brief delay to ensure preferences are flushed\n\n            # Ensure schedule categories match preference categories (required for validation)\n            # Extract categories from schedule_config\n            schedule_categories = (\n                list(schedule_config.keys()) if schedule_config else []\n            )\n            pref_categories = prefs_data.get(\"preferences\", {}).get(\"categories\", [])\n\n            # If schedule has categories not in preferences, add them\n            # Also ensure we use the categories from schedule_config if provided, otherwise use user_data categories\n            if schedule_categories:\n                # Use schedule categories as the source of truth\n                final_categories = list(set(pref_categories + schedule_categories))\n            else:\n                # Use user_data categories\n                final_categories = categories\n\n            if set(schedule_categories) != set(pref_categories):\n                updated_prefs = prefs_data.get(\"preferences\", {}).copy()\n                updated_prefs[\"categories\"] = final_categories\n                pref_result = save_user_data(\n                    actual_user_id, {\"preferences\": updated_prefs}, auto_create=True\n                )\n                if not pref_result.get(\"preferences\", False):\n                    logger.warning(\n                        f\"Failed to update preferences with schedule categories for user {user_id}\"\n                    )\n                else:\n                    # Verify update\n                    for verify_attempt in range(3):\n                        prefs_data = get_user_data(\n                            actual_user_id, \"preferences\", auto_create=True\n                        )\n                        if prefs_data and \"preferences\" in prefs_data:\n                            updated_pref_categories = prefs_data.get(\n                                \"preferences\", {}\n                            ).get(\"categories\", [])\n                            if set(schedule_categories).issubset(\n                                set(updated_pref_categories)\n                            ):\n                                break\n                        if verify_attempt < 2:\n                            time.sleep(0.1)\n                time.sleep(0.2)  # Brief delay to ensure preferences are flushed\n\n            # Add schedule data using the correct function\n            # Retry in case of race conditions with file writes in parallel execution\n            schedule_success = False\n            last_result = {}\n            for attempt in range(5):\n                result = save_user_data(\n                    actual_user_id,\n                    {\"schedules\": schedule_config},\n                    auto_create=True,\n                    validate_data=True,\n                )\n                last_result = result\n                schedule_success = result.get(\"schedules\", False)\n                if schedule_success:\n                    break\n                if attempt < 4:\n                    time.sleep(0.2)  # Increased delay before retry\n\n            if not schedule_success:\n                # Try with validate_data=False as a last resort (for test purposes)\n                logger.warning(\n                    f\"Failed to save schedule data for user {user_id} after 5 attempts with validation. Result: {last_result}. Trying without validation.\"\n                )\n                result_no_validate = save_user_data(\n                    actual_user_id,\n                    {\"schedules\": schedule_config},\n                    auto_create=True,\n                    validate_data=False,\n                )\n                schedule_success = result_no_validate.get(\"schedules\", False)\n                if schedule_success:\n                    logger.info(\n                        f\"Schedule save succeeded without validation for user {user_id}\"\n                    )\n                else:\n                    logger.warning(\n                        f\"Failed to save schedule data for user {user_id} even without validation. Result: {result_no_validate}\"\n                    )\n\n            return schedule_success\n        except Exception as e:\n            logger.error(f\"Error creating scheduled test user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_minimal_user(user_id: str, test_data_dir: str = None) -> bool:\n        \"\"\"\n        Create a minimal test user with only basic messaging enabled\n\n        Args:\n            user_id: Unique identifier for the test user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return TestUserFactory.create_minimal_user__with_test_dir(\n                    user_id, test_data_dir\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_minimal_user__impl(user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating minimal user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_minimal_user_and_get_id(\n        user_id: str, test_data_dir: str = None\n    ) -> tuple[bool, str]:\n        \"\"\"\n        Create a minimal test user and return the actual UUID\n\n        Args:\n            user_id: Unique identifier for the test user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            tuple[bool, str]: (success, actual_user_id) where actual_user_id is the UUID\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return TestUserFactory.create_minimal_user__with_test_dir_and_get_id(\n                    user_id, test_data_dir\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                actual_user_id = TestUserFactory.create_minimal_user__impl_and_get_id(\n                    user_id\n                )\n                return (actual_user_id is not None, actual_user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating minimal user {user_id}: {e}\")\n            return (False, None)\n\n    @staticmethod\n    def create_minimal_user__with_test_dir(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"Create minimal user with test directory by directly saving files\"\"\"\n        try:\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"minimal\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"email\"] = f\"{user_id}@example.com\"\n                user_data[\"preferred_name\"] = f\"Minimal User {user_id}\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": f\"{user_id}@example.com\",\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"UTC\",\n                    \"categories\": [\"motivational\"],\n                    \"channel\": {\"type\": \"email\"},\n                    \"checkin_settings\": {\n                        \"enabled\": False,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                    },\n                    \"task_settings\": {\n                        \"enabled\": False,\n                        \"default_priority\": \"medium\",\n                        \"reminder_enabled\": True,\n                    },\n                    \"preferred_name\": f\"Minimal User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"\",\n                    \"reminders_needed\": [],\n                    \"custom_fields\": {\n                        \"health_conditions\": [],\n                        \"medications_treatments\": [],\n                        \"allergies_sensitivities\": [],\n                    },\n                    \"interests\": [],\n                    \"goals\": [],\n                    \"loved_ones\": [],\n                    \"activities_for_encouragement\": [],\n                    \"notes_for_ai\": [],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(f\"Error creating minimal user with test dir {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_minimal_user__with_test_dir_and_get_id(\n        user_id: str, test_data_dir: str = None\n    ) -> tuple[bool, str]:\n        \"\"\"Create minimal user with test directory and return the actual UUID\"\"\"\n        try:\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": f\"{user_id}@example.com\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"motivational\"],\n                \"channel\": {\"type\": \"email\"},\n                \"checkin_settings\": {\n                    \"enabled\": False,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                },\n                \"task_settings\": {\n                    \"enabled\": False,\n                    \"default_priority\": \"medium\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": f\"Minimal User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": [],\n                \"custom_fields\": {\n                    \"health_conditions\": [],\n                    \"medications_treatments\": [],\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [],\n                \"goals\": [],\n                \"loved_ones\": [],\n                \"activities_for_encouragement\": [],\n                \"notes_for_ai\": [],\n            }\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n\n            # Verify user creation with proper configuration patching\n            success = TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n            return (success, actual_user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating minimal user with test dir {user_id}: {e}\")\n            return (False, None)\n\n    @staticmethod\n    def create_minimal_user__impl(user_id: str) -> bool:\n        \"\"\"Internal implementation of minimal user creation\"\"\"\n        try:\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": f\"{user_id}@example.com\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"motivational\"],\n                \"channel\": {\"type\": \"email\"},\n                \"checkin_settings\": {\n                    \"enabled\": False,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                },\n                \"task_settings\": {\n                    \"enabled\": False,\n                    \"default_priority\": \"medium\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": f\"Minimal User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": [],\n                \"custom_fields\": {\n                    \"health_conditions\": [],\n                    \"medications_treatments\": [],\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [],\n                \"goals\": [],\n                \"loved_ones\": [],\n                \"activities_for_encouragement\": [],\n                \"notes_for_ai\": [],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if actual_user_id:\n                return True\n            else:\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error creating minimal user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_complex_checkins(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"\n        Create a test user with complex check-in configurations\n\n        Args:\n            user_id: Unique identifier for the test user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return TestUserFactory.create_user_with_complex_checkins__with_test_dir(\n                    user_id, test_data_dir\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_user_with_complex_checkins__impl(user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating complex checkins user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_complex_checkins__with_test_dir(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"Create complex checkins user with test directory by directly saving files\"\"\"\n        try:\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"complex_checkins\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"email\"] = f\"{user_id}@example.com\"\n                user_data[\"preferred_name\"] = f\"Complex Checkins User {user_id}\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": f\"{user_id}@example.com\",\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"UTC\",\n                    \"categories\": [\"motivational\", \"health\"],\n                    \"channel\": {\"type\": \"discord\"},\n                    \"checkin_settings\": {\n                        \"enabled\": True,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                        \"questions\": {\n                            \"mood\": {\"enabled\": True, \"type\": \"scale_1_5\"},\n                            \"energy\": {\"enabled\": True, \"type\": \"scale_1_5\"},\n                            \"ate_breakfast\": {\"enabled\": True, \"type\": \"yes_no\"},\n                            \"medication_taken\": {\"enabled\": True, \"type\": \"yes_no\"},\n                            \"sleep_quality\": {\"enabled\": True, \"type\": \"scale_1_5\"},\n                        },\n                    },\n                    \"task_settings\": {\n                        \"enabled\": False,\n                        \"default_priority\": \"medium\",\n                        \"reminder_enabled\": True,\n                    },\n                    \"preferred_name\": f\"Complex Checkins User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"\",\n                    \"reminders_needed\": [\"medication\", \"meals\", \"sleep\"],\n                    \"custom_fields\": {\n                        \"health_conditions\": [\"Depression\", \"Anxiety\"],\n                        \"medications_treatments\": [\"Therapy\", \"Medication\"],\n                        \"allergies_sensitivities\": [],\n                    },\n                    \"interests\": [\"Health\", \"Wellness\", \"Self-care\"],\n                    \"goals\": [\n                        \"Improve mental health\",\n                        \"Build healthy habits\",\n                        \"Track progress\",\n                    ],\n                    \"loved_ones\": [\"Family\", \"Friends\"],\n                    \"activities_for_encouragement\": [\n                        \"Exercise\",\n                        \"Socializing\",\n                        \"Creative projects\",\n                    ],\n                    \"notes_for_ai\": [\n                        \"Prefers detailed check-ins\",\n                        \"Health-focused approach\",\n                    ],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Error creating complex checkins user with test dir {user_id}: {e}\"\n            )\n            return False\n\n    @staticmethod\n    def create_user_with_complex_checkins__impl(user_id: str) -> bool:\n        \"\"\"Internal implementation of complex checkins user creation\"\"\"\n        try:\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": f\"{user_id}@example.com\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"motivational\", \"health\"],\n                \"channel\": {\"type\": \"discord\"},\n                \"checkin_settings\": {\n                    \"enabled\": True,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                    \"questions\": {\n                        \"mood\": {\"enabled\": True, \"type\": \"scale_1_5\"},\n                        \"energy\": {\"enabled\": True, \"type\": \"scale_1_5\"},\n                        \"ate_breakfast\": {\"enabled\": True, \"type\": \"yes_no\"},\n                        \"medication_taken\": {\"enabled\": True, \"type\": \"yes_no\"},\n                        \"sleep_quality\": {\"enabled\": True, \"type\": \"scale_1_5\"},\n                    },\n                },\n                \"task_settings\": {\n                    \"enabled\": False,\n                    \"default_priority\": \"medium\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": f\"Complex Checkins User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": [\"medication\", \"meals\", \"sleep\"],\n                \"custom_fields\": {\n                    \"health_conditions\": [\"Depression\", \"Anxiety\"],\n                    \"medications_treatments\": [\"Therapy\", \"Medication\"],\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [\"Health\", \"Wellness\", \"Self-care\"],\n                \"goals\": [\n                    \"Improve mental health\",\n                    \"Build healthy habits\",\n                    \"Track progress\",\n                ],\n                \"loved_ones\": [\"Family\", \"Friends\"],\n                \"activities_for_encouragement\": [\n                    \"Exercise\",\n                    \"Socializing\",\n                    \"Creative projects\",\n                ],\n                \"notes_for_ai\": [\n                    \"Prefers detailed check-ins\",\n                    \"Health-focused approach\",\n                ],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if actual_user_id:\n                return True\n            else:\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error creating complex checkins user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_health_focus(user_id: str, test_data_dir: str = None) -> bool:\n        \"\"\"\n        Create a test user with health-focused features and data\n\n        Args:\n            user_id: Unique identifier for the test user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return TestUserFactory.create_user_with_health_focus__with_test_dir(\n                    user_id, test_data_dir\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_user_with_health_focus__impl(user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating health focus user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_health_focus__with_test_dir(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"Create health focus user with test directory by directly saving files\"\"\"\n        try:\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"health\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"email\"] = f\"{user_id}@example.com\"\n                user_data[\"preferred_name\"] = f\"Health Focus User {user_id}\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": f\"{user_id}@example.com\",\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"UTC\",\n                    \"categories\": [\"health\", \"motivational\"],\n                    \"channel\": {\"type\": \"discord\"},\n                    \"checkin_settings\": {\n                        \"enabled\": True,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                        \"custom_questions\": [\n                            \"How are you feeling today?\",\n                            \"Did you take your medication?\",\n                            \"How did you sleep?\",\n                        ],\n                    },\n                    \"task_settings\": {\n                        \"enabled\": True,\n                        \"default_priority\": \"high\",\n                        \"reminder_enabled\": True,\n                    },\n                    \"preferred_name\": f\"Health Focus User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"1990-01-01\",\n                    \"reminders_needed\": [\n                        \"medication\",\n                        \"appointments\",\n                        \"exercise\",\n                        \"sleep\",\n                    ],\n                    \"custom_fields\": {\n                        \"health_conditions\": [\"Anxiety\", \"Depression\"],\n                        \"medications_treatments\": [\"Therapy\", \"Medication\"],\n                        \"allergies_sensitivities\": [\"Pollen\"],\n                    },\n                    \"interests\": [\"Health\", \"Exercise\", \"Meditation\"],\n                    \"goals\": [\n                        \"Improve mental health\",\n                        \"Build healthy habits\",\n                        \"Manage stress\",\n                    ],\n                    \"loved_ones\": [\"Family\", \"Friends\"],\n                    \"activities_for_encouragement\": [\n                        \"Exercise\",\n                        \"Meditation\",\n                        \"Socializing\",\n                    ],\n                    \"notes_for_ai\": [\n                        \"Prefers gentle encouragement\",\n                        \"Health-focused approach\",\n                    ],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Error creating health focus user with test dir {user_id}: {e}\"\n            )\n            return False\n\n    @staticmethod\n    def create_user_with_health_focus__impl(user_id: str) -> bool:\n        \"\"\"Internal implementation of health focus user creation\"\"\"\n        try:\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": f\"{user_id}@example.com\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"health\", \"motivational\"],\n                \"channel\": {\"type\": \"discord\"},\n                \"checkin_settings\": {\n                    \"enabled\": True,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                    \"custom_questions\": [\n                        \"How are you feeling today?\",\n                        \"Did you take your medication?\",\n                        \"How did you sleep?\",\n                    ],\n                },\n                \"task_settings\": {\n                    \"enabled\": True,\n                    \"default_priority\": \"high\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": f\"Health Focus User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"1990-01-01\",\n                \"reminders_needed\": [\"medication\", \"appointments\", \"exercise\", \"sleep\"],\n                \"custom_fields\": {\n                    \"health_conditions\": [\"Anxiety\", \"Depression\"],\n                    \"medications_treatments\": [\"Therapy\", \"Medication\"],\n                    \"allergies_sensitivities\": [\"Pollen\"],\n                },\n                \"interests\": [\"Health\", \"Exercise\", \"Meditation\"],\n                \"goals\": [\n                    \"Improve mental health\",\n                    \"Build healthy habits\",\n                    \"Manage stress\",\n                ],\n                \"loved_ones\": [\"Family\", \"Friends\"],\n                \"activities_for_encouragement\": [\n                    \"Exercise\",\n                    \"Meditation\",\n                    \"Socializing\",\n                ],\n                \"notes_for_ai\": [\n                    \"Prefers gentle encouragement\",\n                    \"Health-focused approach\",\n                ],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if actual_user_id:\n                return True\n            else:\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error creating health focus user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_task_focus(user_id: str, test_data_dir: str = None) -> bool:\n        \"\"\"\n        Create a test user with task management focus\n\n        Args:\n            user_id: Unique identifier for the test user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return TestUserFactory.create_user_with_task_focus__with_test_dir(\n                    user_id, test_data_dir\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_user_with_task_focus__impl(user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating task focus user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_task_focus__with_test_dir(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"Create task focus user with test directory by directly saving files\"\"\"\n        try:\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"task\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"email\"] = f\"{user_id}@example.com\"\n                user_data[\"preferred_name\"] = f\"Task Focus User {user_id}\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": f\"{user_id}@example.com\",\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"UTC\",\n                    \"categories\": [\"motivational\"],\n                    \"channel\": {\"type\": \"discord\"},\n                    \"checkin_settings\": {\n                        \"enabled\": False,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                    },\n                    \"task_settings\": {\n                        \"enabled\": True,\n                        \"default_priority\": \"high\",\n                        \"reminder_enabled\": True,\n                        \"auto_escalation\": True,\n                    },\n                    \"preferred_name\": f\"Task Focus User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"\",\n                    \"reminders_needed\": [\"deadlines\", \"meetings\", \"follow-ups\"],\n                    \"custom_fields\": {\n                        \"health_conditions\": [\"ADHD\"],\n                        \"medications_treatments\": [],\n                        \"allergies_sensitivities\": [],\n                    },\n                    \"interests\": [\"Productivity\", \"Organization\", \"Technology\"],\n                    \"goals\": [\n                        \"Improve productivity\",\n                        \"Stay organized\",\n                        \"Meet deadlines\",\n                    ],\n                    \"loved_ones\": [],\n                    \"activities_for_encouragement\": [\n                        \"Planning\",\n                        \"Organization\",\n                        \"Time management\",\n                    ],\n                    \"notes_for_ai\": [\n                        \"Task-focused approach\",\n                        \"Prefers clear deadlines\",\n                    ],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(f\"Error creating task focus user with test dir {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_task_focus__impl(user_id: str) -> bool:\n        \"\"\"Internal implementation of task focus user creation\"\"\"\n        try:\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": f\"{user_id}@example.com\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"motivational\"],\n                \"channel\": {\"type\": \"discord\"},\n                \"checkin_settings\": {\n                    \"enabled\": False,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                },\n                \"task_settings\": {\n                    \"enabled\": True,\n                    \"default_priority\": \"high\",\n                    \"reminder_enabled\": True,\n                    \"auto_escalation\": True,\n                },\n                \"preferred_name\": f\"Task Focus User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": [\"deadlines\", \"meetings\", \"follow-ups\"],\n                \"custom_fields\": {\n                    \"health_conditions\": [\"ADHD\"],\n                    \"medications_treatments\": [],\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [\"Productivity\", \"Organization\", \"Technology\"],\n                \"goals\": [\"Improve productivity\", \"Stay organized\", \"Meet deadlines\"],\n                \"loved_ones\": [],\n                \"activities_for_encouragement\": [\n                    \"Planning\",\n                    \"Organization\",\n                    \"Time management\",\n                ],\n                \"notes_for_ai\": [\"Task-focused approach\", \"Prefers clear deadlines\"],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if actual_user_id:\n                return True\n            else:\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error creating task focus user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_disabilities(user_id: str, test_data_dir: str = None) -> bool:\n        \"\"\"\n        Create a test user with disability-focused features and data\n\n        Args:\n            user_id: Unique identifier for the test user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return TestUserFactory.create_user_with_disabilities__with_test_dir(\n                    user_id, test_data_dir\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_user_with_disabilities__impl(user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating disability user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_disabilities__with_test_dir(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"Create disability user with test directory by directly saving files\"\"\"\n        try:\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"disability\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"email\"] = f\"{user_id}@example.com\"\n                user_data[\"preferred_name\"] = f\"Disability User {user_id}\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": f\"{user_id}@example.com\",\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"UTC\",\n                    \"categories\": [\"motivational\", \"health\"],\n                    \"channel\": {\"type\": \"discord\"},\n                    \"checkin_settings\": {\n                        \"enabled\": True,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                    },\n                    \"task_settings\": {\n                        \"enabled\": True,\n                        \"default_priority\": \"medium\",\n                        \"reminder_enabled\": True,\n                    },\n                    \"preferred_name\": f\"Disability User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"\",\n                    \"reminders_needed\": [\"medication\", \"appointments\", \"accessibility\"],\n                    \"custom_fields\": {\n                        \"health_conditions\": [\"ADHD\", \"Autism\"],\n                        \"medications_treatments\": [\"Therapy\", \"Medication\"],\n                        \"allergies_sensitivities\": [],\n                    },\n                    \"interests\": [\"Technology\", \"Gaming\", \"Art\"],\n                    \"goals\": [\n                        \"Improve executive functioning\",\n                        \"Build routines\",\n                        \"Stay organized\",\n                    ],\n                    \"loved_ones\": [\"Family\", \"Friends\"],\n                    \"activities_for_encouragement\": [\n                        \"Creative projects\",\n                        \"Socializing\",\n                        \"Exercise\",\n                    ],\n                    \"notes_for_ai\": [\n                        \"Prefers clear instructions\",\n                        \"Needs routine reminders\",\n                    ],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(f\"Error creating disability user with test dir {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_disabilities__impl(user_id: str) -> bool:\n        \"\"\"Internal implementation of disability user creation\"\"\"\n        try:\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": f\"{user_id}@example.com\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"motivational\", \"health\"],\n                \"channel\": {\"type\": \"discord\"},\n                \"checkin_settings\": {\n                    \"enabled\": True,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                },\n                \"task_settings\": {\n                    \"enabled\": True,\n                    \"default_priority\": \"medium\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": f\"Disability User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": [\"medication\", \"appointments\", \"accessibility\"],\n                \"custom_fields\": {\n                    \"health_conditions\": [\"ADHD\", \"Autism\"],\n                    \"medications_treatments\": [\"Therapy\", \"Medication\"],\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [\"Technology\", \"Gaming\", \"Art\"],\n                \"goals\": [\n                    \"Improve executive functioning\",\n                    \"Build routines\",\n                    \"Stay organized\",\n                ],\n                \"loved_ones\": [\"Family\", \"Friends\"],\n                \"activities_for_encouragement\": [\n                    \"Creative projects\",\n                    \"Socializing\",\n                    \"Exercise\",\n                ],\n                \"notes_for_ai\": [\n                    \"Prefers clear instructions\",\n                    \"Needs routine reminders\",\n                ],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if actual_user_id:\n                return True\n            else:\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error creating disability user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_limited_data(user_id: str, test_data_dir: str = None) -> bool:\n        \"\"\"\n        Create a test user with minimal data for testing edge cases\n\n        Args:\n            user_id: Unique identifier for the test user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return TestUserFactory.create_user_with_limited_data__with_test_dir(\n                    user_id, test_data_dir\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_user_with_limited_data__impl(user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating limited data user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_limited_data__with_test_dir(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"Create limited data user with test directory by directly saving files\"\"\"\n        try:\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"limited_data\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                # Limited data users should have empty preferred_name (not a generated name)\n                user_data[\"preferred_name\"] = \"\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"\",\n                    \"email\": \"\",\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"UTC\",\n                    \"categories\": [\"motivational\"],\n                    \"channel\": {\"type\": \"email\"},\n                    \"checkin_settings\": {\n                        \"enabled\": False,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                    },\n                    \"task_settings\": {\n                        \"enabled\": False,\n                        \"default_priority\": \"medium\",\n                        \"reminder_enabled\": True,\n                    },\n                    \"preferred_name\": \"\",\n                    \"gender_identity\": [],\n                    \"date_of_birth\": \"\",\n                    \"reminders_needed\": [],\n                    \"custom_fields\": {\n                        \"health_conditions\": [],\n                        \"medications_treatments\": [],\n                        \"allergies_sensitivities\": [],\n                    },\n                    \"interests\": [],\n                    \"goals\": [],\n                    \"loved_ones\": [],\n                    \"activities_for_encouragement\": [],\n                    \"notes_for_ai\": [],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Error creating limited data user with test dir {user_id}: {e}\"\n            )\n            return False\n\n    @staticmethod\n    def create_user_with_limited_data__impl(user_id: str) -> bool:\n        \"\"\"Internal implementation of limited data user creation\"\"\"\n        try:\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"\",\n                \"email\": \"\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"UTC\",\n                \"categories\": [\"motivational\"],\n                \"channel\": {\"type\": \"email\"},\n                \"checkin_settings\": {\n                    \"enabled\": False,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                },\n                \"task_settings\": {\n                    \"enabled\": False,\n                    \"default_priority\": \"medium\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": \"\",\n                \"gender_identity\": [],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": [],\n                \"custom_fields\": {\n                    \"health_conditions\": [],\n                    \"medications_treatments\": [],\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [],\n                \"goals\": [],\n                \"loved_ones\": [],\n                \"activities_for_encouragement\": [],\n                \"notes_for_ai\": [],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if actual_user_id:\n                return True\n            else:\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error creating limited data user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_inconsistent_data(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"\n        Create a test user with inconsistent data for testing edge cases\n\n        Args:\n            user_id: Unique identifier for the test user\n            test_data_dir: Test data directory to use (if None, uses real user directory)\n\n        Returns:\n            bool: True if user was created successfully, False otherwise\n        \"\"\"\n        try:\n            # If test_data_dir is provided, use direct file creation\n            if test_data_dir:\n                return (\n                    TestUserFactory.create_user_with_inconsistent_data__with_test_dir(\n                        user_id, test_data_dir\n                    )\n                )\n            else:\n                # Use real user directory (for backward compatibility)\n                return TestUserFactory.create_user_with_inconsistent_data__impl(user_id)\n\n        except Exception as e:\n            logger.error(f\"Error creating inconsistent data user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_user_with_inconsistent_data__with_test_dir(\n        user_id: str, test_data_dir: str = None\n    ) -> bool:\n        \"\"\"Create inconsistent data user with test directory by directly saving files\"\"\"\n        try:\n            # Check cache first to avoid recreating identical user data structures\n            cache_key = TestUserFactory._get_cache_key(user_type=\"inconsistent\")\n            cached_data = TestUserFactory._get_cached_user_data(cache_key)\n\n            if cached_data:\n                # Use cached data structure but update user_id-specific fields\n                # Use deepcopy to avoid modifying the cached template\n                user_data = copy.deepcopy(cached_data)\n                user_data[\"internal_username\"] = user_id\n                user_data[\"preferred_name\"] = f\"Inconsistent User {user_id}\"\n            else:\n                # Create user data in the format expected by create_new_user\n                user_data = {\n                    \"internal_username\": user_id,\n                    \"chat_id\": \"\",\n                    \"phone\": \"3062619228\",\n                    \"email\": \"\",\n                    \"discord_user_id\": \"\",\n                    \"timezone\": \"America/Regina\",\n                    \"categories\": [\"motivational\"],\n                    \"channel\": {\"type\": \"discord\"},\n                    \"checkin_settings\": {\n                        \"enabled\": True,\n                        \"frequency\": \"daily\",\n                        \"reminder_time\": \"09:00\",\n                    },\n                    \"task_settings\": {\n                        \"enabled\": True,\n                        \"default_priority\": \"medium\",\n                        \"reminder_enabled\": True,\n                    },\n                    \"preferred_name\": f\"Inconsistent User {user_id}\",\n                    \"gender_identity\": [\"they/them\"],\n                    \"date_of_birth\": \"\",\n                    \"reminders_needed\": [],\n                    \"custom_fields\": {\n                        \"health_conditions\": [],\n                        \"medications_treatments\": [],\n                        \"allergies_sensitivities\": [],\n                    },\n                    \"interests\": [\"Technology\"],\n                    \"goals\": [\"Stay organized\"],\n                    \"loved_ones\": [],\n                    \"activities_for_encouragement\": [],\n                    \"notes_for_ai\": [],\n                }\n                # Cache the template (without user_id-specific fields) for reuse\n                template_data = user_data.copy()\n                TestUserFactory._cache_user_data(cache_key, template_data)\n\n            # Use helper function to create files\n            actual_user_id = TestUserFactory._create_user_files_directly(\n                user_id, user_data, test_data_dir\n            )\n            # Verify user creation with proper configuration patching\n            return TestUserFactory.create_basic_user__verify_creation(\n                user_id, actual_user_id, test_data_dir\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Error creating inconsistent data user with test dir {user_id}: {e}\"\n            )\n            return False\n\n    @staticmethod\n    def create_user_with_inconsistent_data__impl(user_id: str) -> bool:\n        \"\"\"Internal implementation of inconsistent data user creation\"\"\"\n        try:\n            # Use the proper create_new_user function to generate UUID and register user\n            from core.user_data_handlers import create_new_user\n\n            # Create user data in the format expected by create_new_user\n            user_data = {\n                \"internal_username\": user_id,\n                \"chat_id\": \"\",\n                \"phone\": \"3062619228\",\n                \"email\": \"\",\n                \"discord_user_id\": \"\",\n                \"timezone\": \"America/Regina\",\n                \"categories\": [\"motivational\"],\n                \"channel\": {\"type\": \"discord\"},\n                \"checkin_settings\": {\n                    \"enabled\": True,\n                    \"frequency\": \"daily\",\n                    \"reminder_time\": \"09:00\",\n                },\n                \"task_settings\": {\n                    \"enabled\": True,\n                    \"default_priority\": \"medium\",\n                    \"reminder_enabled\": True,\n                },\n                \"preferred_name\": f\"Inconsistent User {user_id}\",\n                \"gender_identity\": [\"they/them\"],\n                \"date_of_birth\": \"\",\n                \"reminders_needed\": [],\n                \"custom_fields\": {\n                    \"health_conditions\": [],\n                    \"medications_treatments\": [],\n                    \"allergies_sensitivities\": [],\n                },\n                \"interests\": [\"Technology\"],\n                \"goals\": [\"Stay organized\"],\n                \"loved_ones\": [],\n                \"activities_for_encouragement\": [],\n                \"notes_for_ai\": [],\n            }\n\n            # Create the user using the proper function\n            actual_user_id = create_new_user(user_data)\n\n            if actual_user_id:\n                return True\n            else:\n                return False\n\n        except Exception as e:\n            logger.error(f\"Error creating inconsistent data user {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def get_test_user_data(user_id: str, test_data_dir: str) -> dict[str, Any]:\n        \"\"\"Get user data from test directory\"\"\"\n        try:\n            # First try to find the user by internal username in the user index\n            user_index_file = os.path.join(test_data_dir, \"user_index.json\")\n            if os.path.exists(user_index_file):\n                with open(user_index_file, \"r\", encoding=\"utf-8\") as f:\n                    user_index = json.load(f)\n\n                if user_id in user_index:\n                    actual_user_id = user_index[user_id]\n                    user_dir = os.path.join(test_data_dir, \"users\", actual_user_id)\n\n                    # Load all user data files\n                    result = {}\n\n                    # Load account data\n                    account_file = os.path.join(user_dir, \"account.json\")\n                    if os.path.exists(account_file):\n                        with open(account_file, \"r\", encoding=\"utf-8\") as f:\n                            result[\"account\"] = json.load(f)\n\n                    # Load preferences data\n                    preferences_file = os.path.join(user_dir, \"preferences.json\")\n                    if os.path.exists(preferences_file):\n                        with open(preferences_file, \"r\", encoding=\"utf-8\") as f:\n                            result[\"preferences\"] = json.load(f)\n\n                    # Load context data\n                    context_file = os.path.join(user_dir, \"user_context.json\")\n                    if os.path.exists(context_file):\n                        with open(context_file, \"r\", encoding=\"utf-8\") as f:\n                            result[\"context\"] = json.load(f)\n\n                    # Load schedules data\n                    schedules_file = os.path.join(user_dir, \"schedules.json\")\n                    if os.path.exists(schedules_file):\n                        with open(schedules_file, \"r\", encoding=\"utf-8\") as f:\n                            result[\"schedules\"] = json.load(f)\n\n                    return result\n\n            return {}\n\n        except Exception as e:\n            logger.error(f\"Error getting test user data for {user_id}: {e}\")\n            return {}\n\n    @staticmethod\n    def get_test_user_id_by_internal_username(\n        internal_username: str, test_data_dir: str\n    ) -> str | None:\n        \"\"\"Get user ID by internal username from test directory\"\"\"\n        try:\n            user_index_file = os.path.join(test_data_dir, \"user_index.json\")\n            if os.path.exists(user_index_file):\n                # Use safe_json_read to prevent race conditions in parallel execution\n                from core.file_locking import safe_json_read\n\n                user_index = safe_json_read(user_index_file, default={})\n\n                return user_index.get(internal_username)\n\n            # Fallback: scan user directories when index is missing or out-of-date\n            users_dir = os.path.join(test_data_dir, \"users\")\n            if os.path.exists(users_dir):\n                from core.file_locking import safe_json_read  # reuse helper\n\n                for entry in os.listdir(users_dir):\n                    user_dir = os.path.join(users_dir, entry)\n                    account_file = os.path.join(user_dir, \"account.json\")\n                    if not os.path.exists(account_file):\n                        continue\n                    account_data = safe_json_read(account_file, default={})\n                    if account_data.get(\"internal_username\") == internal_username:\n                        # Update the index so future lookups are fast and consistent\n                        TestUserFactory.create_basic_user__update_index(\n                            test_data_dir, internal_username, entry\n                        )\n                        return entry\n\n            return None\n\n        except Exception as e:\n            logger.error(f\"Error getting test user ID for {internal_username}: {e}\")\n            return None\n\n    @staticmethod\n    def create_basic_user__verify_creation(\n        user_id: str, actual_user_id: str, test_data_dir: str\n    ) -> bool:\n        \"\"\"Helper function to verify user creation with proper configuration patching\"\"\"\n        # CRITICAL: Update user index FIRST so get_user_id_by_identifier can find the user\n        TestUserFactory.create_basic_user__update_index(\n            test_data_dir, user_id, actual_user_id\n        )\n\n        # CRITICAL FIX: Patch the configuration to use the test data directory\n        # This ensures that system functions can find the created users\n        from unittest.mock import patch\n        import core.config\n\n        with (\n            patch.object(core.config, \"BASE_DATA_DIR\", test_data_dir),\n            patch.object(\n                core.config, \"USER_INFO_DIR_PATH\", os.path.join(test_data_dir, \"users\")\n            ),\n        ):\n\n            # Verify the user was created successfully by trying to find it\n            from core.user_data_handlers import get_user_id_by_identifier\n\n            found_user_id = get_user_id_by_identifier(user_id)\n\n            if found_user_id:\n                return True\n            else:\n                logger.warning(\n                    f\"User {user_id} was created but not found by get_user_id_by_identifier\"\n                )\n                return actual_user_id is not None\n\n    @staticmethod\n    def verify_email_user_creation__with_test_dir(\n        user_id: str, actual_user_id: str, test_data_dir: str, email: str = None\n    ) -> str:\n        \"\"\"Helper function to verify email user creation with proper configuration patching\"\"\"\n        # CRITICAL: Ensure user index is updated FIRST so get_user_id_by_identifier can find the user\n        # Extract email from user data if not provided (for cases where verification is called separately)\n        if email is None:\n            # Try to get email from account file\n            user_dir = os.path.join(test_data_dir, \"users\", actual_user_id)\n            account_file = os.path.join(user_dir, \"account.json\")\n            if os.path.exists(account_file):\n                try:\n                    with open(account_file, \"r\", encoding=\"utf-8\") as f:\n                        account_data = json.load(f)\n                        email = account_data.get(\"email\")\n                except Exception:\n                    pass\n\n        # Update user index to ensure the user can be found\n        TestUserFactory.create_basic_user__update_index(\n            test_data_dir, user_id, actual_user_id, email=email\n        )\n\n        # CRITICAL FIX: Patch the configuration to use the test data directory\n        # This ensures that system functions can find the created users\n        from unittest.mock import patch\n        import core.config\n\n        with (\n            patch.object(core.config, \"BASE_DATA_DIR\", test_data_dir),\n            patch.object(\n                core.config, \"USER_INFO_DIR_PATH\", os.path.join(test_data_dir, \"users\")\n            ),\n        ):\n\n            # Verify the user was created successfully by trying to find it\n            from core.user_data_handlers import get_user_id_by_identifier\n\n            found_user_id = get_user_id_by_identifier(user_id)\n\n            if found_user_id:\n                return actual_user_id\n            else:\n                logger.warning(\n                    f\"Email user {user_id} was created but not found by get_user_id_by_identifier\"\n                )\n                return actual_user_id if actual_user_id else None\n\n\nclass TestDataManager:\n    \"\"\"Manages test data directories and cleanup\"\"\"\n\n    @staticmethod\n    def setup_test_environment() -> tuple:\n        \"\"\"\n        Create isolated test environment with temporary directories.\n        Optimized: Uses session-scoped base tmp directory (created once).\n\n        Returns:\n            tuple: (test_dir, test_data_dir, test_test_data_dir)\n        \"\"\"\n        # Create temporary test directory\n        # Use per-test path under tests/data instead of system temp\n        from tests.conftest import tests_data_dir  # reuse base\n        import uuid, os\n\n        base_tmp = os.path.join(tests_data_dir, \"tmp\")\n        # Base tmp directory should already exist from session fixture, but ensure it exists\n        os.makedirs(base_tmp, exist_ok=True)\n        test_dir = os.path.join(base_tmp, f\"mhm_test_{uuid.uuid4().hex}\")\n        os.makedirs(test_dir, exist_ok=True)\n        test_data_dir = os.path.join(test_dir, \"data\")\n        test_test_data_dir = os.path.join(test_dir, \"tests\", \"data\")\n\n        # Create directory structure (batch creation for efficiency)\n        dirs_to_create = [\n            test_data_dir,\n            test_test_data_dir,\n            os.path.join(test_data_dir, \"users\"),\n            os.path.join(test_test_data_dir, \"users\"),\n        ]\n        for dir_path in dirs_to_create:\n            os.makedirs(dir_path, exist_ok=True)\n\n        # Create test user index with flat lookup structure\n        # Use file locking to prevent race conditions in parallel test execution\n        from core.file_locking import safe_json_write\n\n        user_index = {\n            \"last_updated\": \"2025-01-01T00:00:00\",\n            \"test-user-basic\": \"test-user-basic\",  # username \u2192 UUID\n            \"test-user-full\": \"test-user-full\",  # username \u2192 UUID\n        }\n\n        safe_json_write(\n            os.path.join(test_data_dir, \"user_index.json\"), user_index, indent=2\n        )\n\n        return test_dir, test_data_dir, test_test_data_dir\n\n    @staticmethod\n    def cleanup_test_environment(test_dir: str) -> None:\n        \"\"\"\n        Clean up test environment and remove temporary files\n\n        Args:\n            test_dir: Path to the test directory to clean up\n        \"\"\"\n        try:\n            if not test_dir:\n                return\n            # Prefer letting the per-test factory and session cleanup handle removal.\n            # As a fallback, remove with ignore_errors to avoid intermittent failures on Windows.\n            if os.path.exists(test_dir):\n                shutil.rmtree(test_dir, ignore_errors=True)\n        except Exception as e:\n            logger.warning(f\"Could not clean up test directory {test_dir}: {e}\")\n\n\nclass TestUserDataFactory:\n    \"\"\"Factory for creating specific test user data structures\"\"\"\n\n    @staticmethod\n    def create_account_data(user_id: str, **overrides) -> dict[str, Any]:\n        \"\"\"\n        Create standard account data structure with optional overrides\n\n        Args:\n            user_id: User identifier\n            **overrides: Optional field overrides\n\n        Returns:\n            Dict containing account data\n        \"\"\"\n        base_data = {\n            \"user_id\": user_id,\n            \"internal_username\": user_id,\n            \"account_status\": \"active\",\n            \"name\": f\"Test User {user_id}\",\n            \"pronouns\": \"they/them\",\n            \"timezone\": \"UTC\",\n            \"created_at\": \"2025-01-01 00:00:00\",\n            \"updated_at\": \"2025-01-01 00:00:00\",\n        }\n        base_data.update(overrides)\n        return base_data\n\n    @staticmethod\n    def create_preferences_data(user_id: str, **overrides) -> dict[str, Any]:\n        \"\"\"\n        Create standard preferences data structure with optional overrides\n\n        Args:\n            user_id: User identifier\n            **overrides: Optional field overrides\n\n        Returns:\n            Dict containing preferences data\n        \"\"\"\n        base_data = {\n            \"categories\": [\"motivational\", \"health\"],\n            \"channel\": {\"type\": \"discord\"},\n            \"notification_settings\": {\n                \"morning_reminders\": True,\n                \"task_reminders\": True,\n                \"checkin_reminders\": True,\n            },\n        }\n        base_data.update(overrides)\n        return base_data\n\n    @staticmethod\n    def create_schedules_data(**overrides) -> dict[str, Any]:\n        \"\"\"\n        Create standard schedules data structure with optional overrides\n\n        Args:\n            **overrides: Optional field overrides\n\n        Returns:\n            Dict containing schedules data\n        \"\"\"\n        base_data = {\n            \"motivational\": {\n                \"periods\": {\n                    \"morning\": {\n                        \"active\": True,\n                        \"days\": [\n                            \"monday\",\n                            \"tuesday\",\n                            \"wednesday\",\n                            \"thursday\",\n                            \"friday\",\n                        ],\n                        \"start_time\": \"09:00\",\n                        \"end_time\": \"12:00\",\n                    }\n                }\n            }\n        }\n        base_data.update(overrides)\n        return base_data\n\n    @staticmethod\n    def create_context_data(**overrides) -> dict[str, Any]:\n        \"\"\"\n        Create standard context data structure with optional overrides\n\n        Args:\n            **overrides: Optional field overrides\n\n        Returns:\n            Dict containing context data\n        \"\"\"\n        base_data = {\n            \"preferred_name\": \"Test User\",\n            \"timezone\": \"UTC\",\n            \"language\": \"en\",\n            \"created_date\": \"2025-01-01\",\n        }\n        base_data.update(overrides)\n        return base_data\n\n\n# Convenience functions for backward compatibility\ndef create_test_user(\n    user_id: str, user_type: str = \"basic\", test_data_dir: str = None, **kwargs\n) -> bool:\n    \"\"\"\n    Convenience function to create test users with different configurations\n\n    Args:\n        user_id: Unique identifier for the test user\n        user_type: Type of user to create. Options:\n            - \"basic\": Basic user with configurable features\n            - \"discord\": Discord-specific user\n            - \"email\": Email-specific user\n\n            - \"full\": Full featured user with all capabilities\n            - \"minimal\": Minimal user with only messaging\n            - \"health\": Health-focused user\n            - \"task\": Task/productivity-focused user\n            - \"disability\": User with accessibility considerations\n            - \"complex_checkins\": User with complex check-in configurations\n            - \"limited_data\": User with minimal data (like real users)\n            - \"inconsistent\": User with inconsistent/partial data\n            - \"custom_fields\": User with custom field configurations\n            - \"scheduled\": User with custom schedule configurations\n        test_data_dir: Test data directory to use (required for modern test approach)\n        **kwargs: Additional arguments passed to the specific creation method\n\n    Returns:\n        bool: True if user was created successfully, False otherwise\n    \"\"\"\n    try:\n        if user_type == \"basic\":\n            enable_checkins = kwargs.get(\"enable_checkins\", True)\n            enable_tasks = kwargs.get(\"enable_tasks\", True)\n            return TestUserFactory.create_basic_user(\n                user_id, enable_checkins, enable_tasks, test_data_dir\n            )\n\n        elif user_type == \"discord\":\n            discord_user_id = kwargs.get(\"discord_user_id\")\n            return TestUserFactory.create_discord_user(\n                user_id, discord_user_id, test_data_dir\n            )\n\n        elif user_type == \"email\":\n            email = kwargs.get(\"email\")\n            return TestUserFactory.create_email_user(user_id, email, test_data_dir)\n\n        elif user_type == \"full\":\n            return TestUserFactory.create_full_featured_user(user_id, test_data_dir)\n\n        elif user_type == \"minimal\":\n            return TestUserFactory.create_minimal_user(user_id, test_data_dir)\n\n        elif user_type == \"health\":\n            return TestUserFactory.create_user_with_health_focus(user_id, test_data_dir)\n\n        elif user_type == \"task\":\n            return TestUserFactory.create_user_with_task_focus(user_id, test_data_dir)\n\n        elif user_type == \"disability\":\n            return TestUserFactory.create_user_with_disabilities(user_id, test_data_dir)\n\n        elif user_type == \"complex_checkins\":\n            return TestUserFactory.create_user_with_complex_checkins(\n                user_id, test_data_dir\n            )\n\n        elif user_type == \"limited_data\":\n            return TestUserFactory.create_user_with_limited_data(user_id, test_data_dir)\n\n        elif user_type == \"inconsistent\":\n            return TestUserFactory.create_user_with_inconsistent_data(\n                user_id, test_data_dir\n            )\n\n        elif user_type == \"custom_fields\":\n            custom_fields = kwargs.get(\"custom_fields\")\n            return TestUserFactory.create_user_with_custom_fields(\n                user_id, custom_fields, test_data_dir\n            )\n\n        elif user_type == \"scheduled\":\n            schedule_config = kwargs.get(\"schedule_config\")\n            return TestUserFactory.create_user_with_schedules(\n                user_id, schedule_config, test_data_dir\n            )\n\n        else:\n            logger.error(f\"Unknown user type: {user_type}\")\n            return False\n\n    except Exception as e:\n        logger.error(f\"Error creating test user {user_id} of type {user_type}: {e}\")\n        return False\n\n\ndef setup_test_data_environment() -> tuple:\n    \"\"\"\n    Convenience function to set up test data environment\n\n    Returns:\n        tuple: (test_dir, test_data_dir, test_test_data_dir)\n    \"\"\"\n    return TestDataManager.setup_test_environment()\n\n\ndef cleanup_test_data_environment(test_dir: str) -> None:\n    \"\"\"\n    Convenience function to clean up test data environment\n\n    Args:\n        test_dir: Path to the test directory to clean up\n    \"\"\"\n    TestDataManager.cleanup_test_environment(test_dir)\n\n\nclass TestDataFactory:\n    \"\"\"Factory for creating test data for various scenarios\"\"\"\n\n    @staticmethod\n    def create_corrupted_user_data(\n        user_id: str, corruption_type: str = \"invalid_json\"\n    ) -> bool:\n        \"\"\"\n        Create a user with corrupted data for testing error handling\n\n        Args:\n            user_id: Unique identifier for the test user\n            corruption_type: Type of corruption (\"invalid_json\", \"missing_file\", \"empty_file\")\n\n        Returns:\n            bool: True if corrupted user was created successfully, False otherwise\n        \"\"\"\n        try:\n            from core.config import get_user_data_dir\n            import os\n\n            # Create user directory\n            user_dir = get_user_data_dir(user_id)\n            os.makedirs(user_dir, exist_ok=True)\n\n            if corruption_type == \"invalid_json\":\n                # Create file with invalid JSON\n                with open(os.path.join(user_dir, \"account.json\"), \"w\") as f:\n                    f.write(\"invalid json content\")\n                with open(os.path.join(user_dir, \"preferences.json\"), \"w\") as f:\n                    f.write(\"{ invalid json }\")\n                with open(os.path.join(user_dir, \"user_context.json\"), \"w\") as f:\n                    f.write(\"not json at all\")\n\n            elif corruption_type == \"missing_file\":\n                # Create only some files, leave others missing\n                with open(os.path.join(user_dir, \"account.json\"), \"w\") as f:\n                    json.dump({\"user_id\": user_id}, f)\n                # Don't create preferences.json or user_context.json\n\n            elif corruption_type == \"empty_file\":\n                # Create empty files\n                with open(os.path.join(user_dir, \"account.json\"), \"w\") as f:\n                    f.write(\"\")\n                with open(os.path.join(user_dir, \"preferences.json\"), \"w\") as f:\n                    f.write(\"\")\n                with open(os.path.join(user_dir, \"user_context.json\"), \"w\") as f:\n                    f.write(\"\")\n\n            return True\n\n        except Exception as e:\n            logger.error(f\"Error creating corrupted user data for {user_id}: {e}\")\n            return False\n\n    @staticmethod\n    def create_test_schedule_data(categories: list[str] = None) -> dict[str, Any]:\n        \"\"\"\n        Create test schedule data for testing schedule management\n\n        Args:\n            categories: List of categories to create schedules for\n\n        Returns:\n            Dict containing schedule data\n        \"\"\"\n        if categories is None:\n            categories = [\"motivational\", \"health\"]\n\n        schedule_data = {}\n        for category in categories:\n            schedule_data[category] = {\n                \"periods\": {\n                    \"Default\": {\n                        \"active\": True,\n                        \"days\": [\"ALL\"],\n                        \"start_time\": \"18:00\",\n                        \"end_time\": \"21:30\",\n                    }\n                }\n            }\n\n        return schedule_data\n\n    @staticmethod\n    def create_test_task_data(task_count: int = 3) -> list[dict[str, Any]]:\n        \"\"\"\n        Create test task data for testing task management\n\n        Args:\n            task_count: Number of tasks to create\n\n        Returns:\n            List of task dictionaries\n        \"\"\"\n        import uuid\n        from datetime import datetime, timedelta\n        from core.time_utilities import now_timestamp_full\n\n        tasks = []\n        for i in range(task_count):\n            task = {\n                \"task_id\": str(uuid.uuid4()),\n                \"title\": f\"Test Task {i+1}\",\n                \"description\": f\"Description for test task {i+1}\",\n                \"priority\": \"medium\",\n                \"status\": \"active\",\n                \"due_date\": (datetime.now() + timedelta(days=i + 1)).strftime(\n                    DATE_ONLY\n                ),\n                \"created_at\": now_timestamp_full(),\n                \"updated_at\": now_timestamp_full(),\n            }\n            tasks.append(task)\n\n        return tasks\n\n    @staticmethod\n    def create_test_message_data(\n        category: str = \"motivational\", message_count: int = 5\n    ) -> list[dict[str, Any]]:\n        \"\"\"\n        Create test message data for testing message management\n\n        Args:\n            category: Message category\n            message_count: Number of messages to create\n\n        Returns:\n            List of message dictionaries\n        \"\"\"\n\n        messages = []\n        for i in range(message_count):\n            message = {\n                \"message_id\": str(uuid.uuid4()),\n                \"content\": f\"Test message {i+1} for {category}\",\n                \"category\": category,\n                \"created_at\": now_timestamp_full(),\n                \"sent\": False,\n                \"scheduled_for\": now_timestamp_full(),\n            }\n            messages.append(message)\n\n        return messages\n\n\nclass TestLogPathMocks:\n    \"\"\"Helper class for creating complete log path mocks for tests\"\"\"\n\n    @staticmethod\n    def create_complete_log_paths_mock(base_dir: str) -> dict[str, str]:\n        \"\"\"\n        Create a complete mock dictionary for _get_log_paths_for_environment()\n        that includes all required keys including ai_dev_tools_file.\n\n        Args:\n            base_dir: Base directory for logs (e.g., test_data_dir / \"logs\")\n\n        Returns:\n            Dict with all log path keys required by the logger system\n        \"\"\"\n        from pathlib import Path\n\n        base_path = Path(base_dir)\n\n        return {\n            \"base_dir\": str(base_path),\n            \"backup_dir\": str(base_path / \"backups\"),\n            \"archive_dir\": str(base_path / \"archive\"),\n            \"main_file\": str(base_path / \"app.log\"),\n            \"discord_file\": str(base_path / \"discord.log\"),\n            \"ai_file\": str(base_path / \"ai.log\"),\n            \"user_activity_file\": str(base_path / \"user_activity.log\"),\n            \"errors_file\": str(base_path / \"errors.log\"),\n            \"communication_manager_file\": str(base_path / \"communication_manager.log\"),\n            \"email_file\": str(base_path / \"email.log\"),\n            \"ui_file\": str(base_path / \"ui.log\"),\n            \"file_ops_file\": str(base_path / \"file_ops.log\"),\n            \"scheduler_file\": str(base_path / \"scheduler.log\"),\n            \"schedule_utilities_file\": str(base_path / \"schedule_utilities.log\"),\n            \"analytics_file\": str(base_path / \"analytics.log\"),\n            \"message_file\": str(base_path / \"message.log\"),\n            \"backup_file\": str(base_path / \"backup.log\"),\n            \"checkin_dynamic_file\": str(base_path / \"checkin_dynamic.log\"),\n            \"ai_dev_tools_file\": str(base_path / \"ai_dev_tools.log\"),\n        }\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 588,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 23676,
                    "end": 23698
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 796,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 32387,
                    "end": 32409
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 966,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 39500,
                    "end": 39522
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1091,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 44608,
                    "end": 44630
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1396,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 57824,
                    "end": 57846
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1424,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 58864,
                    "end": 58886
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1638,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 67441,
                    "end": 67463
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1827,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 75795,
                    "end": 75817
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 2017,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 83805,
                    "end": 83827
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 2185,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 90963,
                    "end": 90985
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 2358,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 98318,
                    "end": 98340
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 2515,
                    "line_content": "# Use real user directory (for backward compatibility)",
                    "start": 104581,
                    "end": 104603
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 2996,
                    "line_content": "# Convenience functions for backward compatibility",
                    "start": 123410,
                    "end": 123432
                  }
                ]
              ],
              [
                "user\\user_context.py",
                "# user_context.py\n\nimport threading\nfrom core.logger import get_component_logger\nfrom core.user_data_handlers import get_user_data\nfrom core.error_handling import handle_errors\nfrom core.schedule_utilities import get_active_schedules\n\nlogger = get_component_logger(\"user_activity\")\ncontext_logger = get_component_logger(\"user_activity\")\n\n\nclass UserContext:\n    _instance = None\n    _lock = threading.Lock()\n\n    def __new__(cls):\n        \"\"\"Create a new instance.\"\"\"\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = super(UserContext, cls).__new__(cls)\n                cls._instance.user_data = {}\n        return cls._instance\n\n    @handle_errors(\"loading user data\")\n    def load_user_data(self, user_id):\n        \"\"\"\n        Loads user data using the new user management functions.\n\n        Args:\n            user_id (str): The user ID whose data needs to be loaded.\n        \"\"\"\n        if not user_id:\n            logger.error(\"Attempted to load user data with None user_id\")\n            return\n\n        # Use the new user management functions directly\n        user_data_result = get_user_data(user_id, \"account\", normalize_on_read=True)\n        account_data = user_data_result.get(\"account\") or {}\n        prefs_result = get_user_data(user_id, \"preferences\", normalize_on_read=True)\n        preferences_data = prefs_result.get(\"preferences\") or {}\n        context_result = get_user_data(user_id, \"context\")\n        context_data = context_result.get(\"context\") or {}\n\n        # Store data in the new format directly - no legacy conversion needed\n        self.user_data = {\n            \"user_id\": account_data.get(\"user_id\", user_id),\n            \"internal_username\": account_data.get(\"internal_username\", \"\"),\n            \"active\": account_data.get(\"account_status\") == \"active\",\n            \"preferred_name\": context_data.get(\"preferred_name\", \"\"),\n            \"chat_id\": account_data.get(\"chat_id\", \"\"),\n            \"phone\": account_data.get(\"phone\", \"\"),\n            \"email\": account_data.get(\"email\", \"\"),\n            \"discord_user_id\": account_data.get(\"discord_user_id\", \"\"),\n            \"created_at\": account_data.get(\"created_at\", \"\"),\n            \"last_updated\": account_data.get(\"updated_at\", \"\"),\n            \"preferences\": preferences_data,\n            \"schedules\": {},  # Schedules are handled separately\n        }\n\n        logger.info(f\"User data loaded for user_id {user_id}\")\n\n    @handle_errors(\"saving user data\")\n    def save_user_data(self, user_id):\n        \"\"\"\n        Saves user data using the new user management functions.\n\n        Args:\n            user_id (str): The user ID whose data needs to be saved.\n        \"\"\"\n        if not user_id:\n            logger.error(\"Attempted to save user data with None user_id\")\n            return\n\n        # Extract data and update using new functions directly - no legacy extraction needed\n        account_updates = {\n            \"user_id\": self.user_data.get(\"user_id\", user_id),\n            \"internal_username\": self.user_data.get(\"internal_username\", \"\"),\n            \"account_status\": (\n                \"active\" if self.user_data.get(\"active\", True) else \"inactive\"\n            ),\n            \"chat_id\": self.user_data.get(\"chat_id\", \"\"),\n            \"phone\": self.user_data.get(\"phone\", \"\"),\n            \"email\": self.user_data.get(\"email\", \"\"),\n        }\n\n        preferences_updates = self.user_data.get(\"preferences\", {})\n\n        context_updates = {\n            \"preferred_name\": self.user_data.get(\"preferred_name\", \"\"),\n        }\n\n        # Update all data using new functions\n        from core.user_data_handlers import save_user_data\n\n        # Save account data\n        save_user_data(user_id, {\"account\": account_updates})\n\n        # Save preferences data\n        save_user_data(user_id, {\"preferences\": preferences_updates})\n\n        # Save context data\n        save_user_data(user_id, {\"context\": context_updates})\n\n        logger.info(f\"User data saved for user_id {user_id}\")\n\n    @handle_errors(\"setting user ID\")\n    def set_user_id(self, user_id):\n        \"\"\"\n        Sets the user_id in the user_data dictionary.\n\n        Args:\n            user_id (str): The user ID to be set.\n        \"\"\"\n        if user_id:\n            self.user_data[\"user_id\"] = user_id\n            logger.debug(f\"UserContext: set_user_id called with {user_id}\")\n        else:\n            logger.debug(\"UserContext: Clearing user_id (set to None during logout)\")\n            self.user_data[\"user_id\"] = None\n\n    @handle_errors(\"getting user ID\", default_return=None)\n    def get_user_id(self):\n        \"\"\"\n        Retrieves the user_id from the user_data dictionary.\n\n        Returns:\n            str: The current user ID, or None if not set.\n        \"\"\"\n        return self.user_data.get(\"user_id\")\n\n    @handle_errors(\"setting internal username\")\n    def set_internal_username(self, internal_username):\n        \"\"\"\n        Sets the internal_username in the user_data dictionary.\n\n        Args:\n            internal_username (str): The internal username to be set.\n        \"\"\"\n        if internal_username:\n            self.user_data[\"internal_username\"] = internal_username\n            logger.debug(\n                f\"UserContext: set_internal_username called with {internal_username}\"\n            )\n        else:\n            logger.debug(\n                \"UserContext: Clearing internal_username (set to None during logout)\"\n            )\n            self.user_data[\"internal_username\"] = None\n\n    @handle_errors(\"getting internal username\", default_return=None)\n    def get_internal_username(self):\n        \"\"\"\n        Retrieves the internal_username from the user_data dictionary.\n\n        Returns:\n            str: The current internal username, or None if not set.\n        \"\"\"\n        return self.user_data.get(\"internal_username\")\n\n    @handle_errors(\"setting preferred name\")\n    def set_preferred_name(self, preferred_name):\n        \"\"\"\n        Sets the preferred_name in the user_data dictionary.\n\n        Args:\n            preferred_name (str): The preferred name to be set.\n        \"\"\"\n        self.user_data[\"preferred_name\"] = preferred_name\n        logger.debug(f\"UserContext: set_preferred_name called with {preferred_name}\")\n\n    @handle_errors(\"getting preferred name\", default_return=None)\n    def get_preferred_name(self):\n        \"\"\"\n        Retrieves the preferred_name from the user_data dictionary.\n\n        Returns:\n            str: The current preferred name, or None if not set.\n        \"\"\"\n        return self.user_data.get(\"preferred_name\")\n\n    @handle_errors(\"getting instance context\")\n    def get_instance_context(self):\n        \"\"\"\n        Get basic user context from the current UserContext instance.\n\n        Returns:\n            dict: Dictionary containing basic user context information\n        \"\"\"\n        user_id = self.get_user_id()\n        if not user_id:\n            return {}\n\n        # Get user data\n        user_data_result = get_user_data(user_id, \"account\")\n        account_data = user_data_result.get(\"account\") or {}\n\n        # Get preferences\n        prefs_result = get_user_data(user_id, \"preferences\")\n        preferences_data = prefs_result.get(\"preferences\") or {}\n\n        # Get context\n        context_result = get_user_data(user_id, \"context\")\n        context_data = context_result.get(\"context\") or {}\n\n        # Get schedules for active schedules check\n        schedules_result = get_user_data(user_id, \"schedules\", normalize_on_read=True)\n        schedules_data = (\n            schedules_result.get(\"schedules\", {})\n            if isinstance(schedules_result, dict) and \"schedules\" in schedules_result\n            else (schedules_result if isinstance(schedules_result, dict) else {})\n        )\n\n        # Build basic context\n        context = {\n            \"user_id\": user_id,\n            \"preferred_name\": context_data.get(\"preferred_name\", \"\"),\n            \"account_status\": account_data.get(\"account_status\", \"unknown\"),\n            \"preferences\": preferences_data,\n            \"active_schedules\": get_active_schedules(schedules_data),\n        }\n\n        return context\n",
                [
                  {
                    "pattern": "(?i)legacy conversion",
                    "match": "legacy conversion",
                    "line": 45,
                    "line_content": "# Store data in the new format directly - no legacy conversion needed",
                    "start": 1570,
                    "end": 1587
                  },
                  {
                    "pattern": "(?i)legacy extraction",
                    "match": "legacy extraction",
                    "line": 75,
                    "line_content": "# Extract data and update using new functions directly - no legacy extraction needed",
                    "start": 2879,
                    "end": 2896
                  }
                ]
              ],
              [
                "communication\\command_handlers\\account_handler.py",
                "\"\"\"\nAccount Management Handler\n\nChannel-agnostic handler for account creation and linking operations.\n\"\"\"\n\nfrom typing import Dict, Any, List, Optional\nfrom core.logger import get_component_logger\nfrom core.error_handling import handle_errors\nfrom core.user_data_handlers import get_user_id_by_identifier, create_new_user\nfrom core.user_data_handlers import get_user_data, get_all_user_ids, update_user_account\nfrom core.user_data_manager import update_user_index\nfrom communication.command_handlers.base_handler import InteractionHandler\nfrom communication.command_handlers.shared_types import (\n    InteractionResponse,\n    ParsedCommand,\n)\n\nlogger = get_component_logger(\"account_handler\")\n\n\nclass AccountManagementHandler(InteractionHandler):\n    \"\"\"Handler for account management interactions\"\"\"\n\n    @handle_errors(\"checking if account handler can handle intent\")\n    def can_handle(self, intent: str) -> bool:\n        \"\"\"Check if this handler can handle the given intent\"\"\"\n        return intent in [\"create_account\", \"link_account\", \"check_account_status\"]\n\n    @handle_errors(\n        \"handling account interaction\",\n        default_return=InteractionResponse(\n            \"I'm having trouble with account management right now. Please try again.\",\n            True,\n        ),\n    )\n    def handle(\n        self, user_id: str, parsed_command: ParsedCommand\n    ) -> InteractionResponse:\n        \"\"\"Handle account management interactions\"\"\"\n        intent = parsed_command.intent\n        entities = parsed_command.entities\n\n        if intent == \"create_account\":\n            return self._handle_create_account(user_id, entities)\n        elif intent == \"link_account\":\n            return self._handle_link_account(user_id, entities)\n        elif intent == \"check_account_status\":\n            return self._handle_check_account_status(user_id)\n        else:\n            return InteractionResponse(\n                f\"I don't understand that account command. Try: {', '.join(self.get_examples())}\",\n                True,\n            )\n\n    @handle_errors(\"handling account creation\")\n    def _handle_create_account(\n        self, user_id: str, entities: dict[str, Any]\n    ) -> InteractionResponse:\n        \"\"\"\n        Handle account creation request.\n\n        Args:\n            user_id: The user's internal ID (if they already have one) or channel identifier\n            entities: Command entities containing username and other account data\n\n        Returns:\n            InteractionResponse with account creation result\n        \"\"\"\n        username = entities.get(\"username\")\n        if not username:\n            return InteractionResponse(\n                \"To create an account, please provide a username. What username would you like to use?\",\n                completed=False,\n                suggestions=[\"Use my Discord username\", \"Create account\"],\n            )\n\n        username = username.strip()\n\n        # Validate username\n        if len(username) < 3:\n            return InteractionResponse(\n                \"\u274c Username must be at least 3 characters long. Please choose a different username.\",\n                completed=False,\n            )\n\n        # Check if username already exists\n        if self._username_exists(username):\n            return InteractionResponse(\n                f\"\u274c Username '{username}' is already taken. Please choose a different username.\",\n                completed=False,\n            )\n\n        # Extract channel-specific identifier (Discord ID, email, etc.)\n        channel_identifier = entities.get(\"channel_identifier\", \"\")\n        channel_type = entities.get(\"channel_type\", \"discord\")\n\n        # Extract feature selection (default to True for backward compatibility)\n        tasks_enabled = entities.get(\"tasks_enabled\", True)\n        checkins_enabled = entities.get(\"checkins_enabled\", True)\n        messages_enabled = entities.get(\"messages_enabled\", False)\n        timezone = entities.get(\"timezone\", \"America/Regina\")\n\n        try:\n            # Categories should be empty list initially (user can add categories later via UI)\n            # The messages_enabled flag will be used to set automated_messages feature\n            user_data = {\n                \"internal_username\": username,\n                \"categories\": [],  # Start with empty - user can add categories later\n                \"task_settings\": {\"enabled\": tasks_enabled},\n                \"checkin_settings\": {\"enabled\": checkins_enabled},\n                \"channel\": {\"type\": channel_type},\n                \"timezone\": timezone,\n                \"messages_enabled\": messages_enabled,  # Explicit flag for automated_messages feature\n            }\n\n            # Add channel-specific identifier\n            if channel_type == \"discord\" and channel_identifier:\n                user_data[\"discord_user_id\"] = channel_identifier\n            elif channel_type == \"email\" and channel_identifier:\n                user_data[\"email\"] = channel_identifier\n\n            new_user_id = create_new_user(user_data)\n\n            if new_user_id:\n                logger.info(\n                    f\"Created new MHM account: {username} (user_id: {new_user_id}, channel: {channel_type})\"\n                )\n\n                return InteractionResponse(\n                    f\"\u2705 **Account created successfully!**\\n\\n\"\n                    f\"Your MHM username: `{username}`\\n\"\n                    f\"Your account is now linked to your {channel_type} account.\\n\\n\"\n                    f\"You can now use commands like:\\n\"\n                    f\"\u2022 `/help` - See all available commands\\n\"\n                    f\"\u2022 `/profile` - View your profile\\n\"\n                    f\"\u2022 `create task [description]` - Create a new task\\n\"\n                    f\"\u2022 `show my tasks` - View your tasks\\n\\n\"\n                    f\"Welcome to MHM! \ud83d\ude80\",\n                    completed=True,\n                    rich_data={\n                        \"type\": \"account_created\",\n                        \"username\": username,\n                        \"user_id\": new_user_id,\n                    },\n                )\n            else:\n                return InteractionResponse(\n                    \"\u274c Failed to create account. Please try again or contact support.\",\n                    completed=False,\n                )\n        except Exception as e:\n            logger.error(f\"Error creating account: {e}\")\n            return InteractionResponse(\n                \"\u274c An error occurred while creating your account. Please try again.\",\n                completed=False,\n            )\n\n    @handle_errors(\"handling account linking\")\n    def _handle_link_account(\n        self, user_id: str, entities: dict[str, Any]\n    ) -> InteractionResponse:\n        \"\"\"\n        Handle account linking request.\n\n        Args:\n            user_id: The channel identifier (Discord ID, email, etc.)\n            entities: Command entities containing username and confirmation code\n\n        Returns:\n            InteractionResponse with account linking result\n        \"\"\"\n        username = entities.get(\"username\")\n        confirmation_code = entities.get(\"confirmation_code\")\n        channel_identifier = entities.get(\"channel_identifier\", user_id)\n        channel_type = entities.get(\"channel_type\", \"discord\")\n\n        # Step 1: User provides username\n        if not username:\n            return InteractionResponse(\n                \"To link your account, please provide your existing MHM username.\",\n                completed=False,\n                suggestions=[\"Link my account\"],\n            )\n\n        username = username.strip()\n\n        # Validate username\n        if len(username) < 3:\n            return InteractionResponse(\n                \"\u274c Username must be at least 3 characters long. Please check your username and try again.\",\n                completed=False,\n            )\n\n        # Check if username exists\n        existing_user_id = self._get_user_id_by_username(username)\n        if not existing_user_id:\n            return InteractionResponse(\n                f\"\u274c Username '{username}' not found. Please check your username and try again.\",\n                completed=False,\n            )\n\n        # Check if account already has a channel identifier\n        user_data_result = get_user_data(existing_user_id, \"account\")\n        account_data = user_data_result.get(\"account\", {})\n\n        # Check for existing link based on channel type\n        if channel_type == \"discord\":\n            existing_discord_id = account_data.get(\"discord_user_id\", \"\")\n            if existing_discord_id and existing_discord_id != channel_identifier:\n                return InteractionResponse(\n                    f\"\u274c This account is already linked to a different Discord account.\\n\"\n                    f\"If this is your account, please contact support.\",\n                    completed=False,\n                )\n        elif channel_type == \"email\":\n            existing_email = account_data.get(\"email\", \"\")\n            if existing_email and existing_email.lower() != channel_identifier.lower():\n                return InteractionResponse(\n                    f\"\u274c This account is already linked to a different email address.\\n\"\n                    f\"If this is your account, please contact support.\",\n                    completed=False,\n                )\n\n        # Step 2: User provides confirmation code\n        if not confirmation_code:\n            # Generate and send confirmation code\n            from communication.command_handlers.account_handler import (\n                _generate_confirmation_code,\n                _send_confirmation_code,\n            )\n\n            code = _generate_confirmation_code()\n\n            # Store pending operation (in-memory for now, could be moved to a proper store)\n            from communication.command_handlers.account_handler import (\n                _pending_link_operations,\n            )\n\n            _pending_link_operations[channel_identifier] = {\n                \"operation_type\": \"link\",\n                \"username\": username,\n                \"user_id\": existing_user_id,\n                \"confirmation_code\": code,\n                \"channel_type\": channel_type,\n            }\n\n            # Send confirmation code (pass channel_identifier for Discord)\n            code_sent = _send_confirmation_code(\n                existing_user_id,\n                code,\n                channel_type,\n                channel_identifier=channel_identifier,\n            )\n\n            if code_sent:\n                return InteractionResponse(\n                    f\"\u2705 **Confirmation code sent!**\\n\\n\"\n                    f\"A confirmation code has been sent to the email address associated with your MHM account.\\n\"\n                    f\"Please check your email and enter the code here to complete the linking process.\",\n                    completed=False,\n                    rich_data={\n                        \"type\": \"confirmation_code_sent\",\n                        \"username\": username,\n                        \"channel_type\": channel_type,\n                    },\n                    suggestions=[\"Enter confirmation code\"],\n                )\n            else:\n                return InteractionResponse(\n                    f\"\u274c Could not send confirmation code. This account may not have an email address configured.\\n\"\n                    f\"Please contact support for assistance.\",\n                    completed=False,\n                )\n\n        # Step 3: Verify confirmation code and link account\n        from communication.command_handlers.account_handler import (\n            _pending_link_operations,\n        )\n\n        pending = _pending_link_operations.get(channel_identifier)\n\n        if not pending or pending[\"operation_type\"] != \"link\":\n            return InteractionResponse(\n                \"\u274c No pending account linking operation found. Please start over.\",\n                completed=False,\n            )\n\n        if confirmation_code.strip() != pending[\"confirmation_code\"]:\n            return InteractionResponse(\n                f\"\u274c Invalid confirmation code. Please check your {channel_type} and try again.\",\n                completed=False,\n            )\n\n        # Link the account\n        try:\n            updates = {}\n            if channel_type == \"discord\":\n                updates[\"discord_user_id\"] = channel_identifier\n            elif channel_type == \"email\":\n                updates[\"email\"] = channel_identifier\n\n            success = update_user_account(pending[\"user_id\"], updates)\n\n            if not success:\n                return InteractionResponse(\n                    \"\u274c Failed to link account. Please try again or contact support.\",\n                    completed=False,\n                )\n\n            try:\n                update_user_index(pending[\"user_id\"])\n            except Exception as index_error:\n                logger.warning(\n                    f\"Failed to update user index after linking account: {index_error}\"\n                )\n\n            # Clear pending operation\n            del _pending_link_operations[channel_identifier]\n\n            return InteractionResponse(\n                f\"\u2705 **Account linked successfully!**\\n\\n\"\n                f\"Your MHM account (`{username}`) is now linked to your {channel_type} account.\\n\\n\"\n                f\"You can now use commands like:\\n\"\n                f\"\u2022 `/help` - See all available commands\\n\"\n                f\"\u2022 `/profile` - View your profile\\n\"\n                f\"\u2022 `create task [description]` - Create a new task\\n\"\n                f\"\u2022 `show my tasks` - View your tasks\\n\\n\"\n                f\"Welcome back! \ud83d\ude80\",\n                completed=True,\n                rich_data={\n                    \"type\": \"account_linked\",\n                    \"username\": username,\n                    \"user_id\": pending[\"user_id\"],\n                },\n            )\n        except Exception as e:\n            logger.error(f\"Error linking account: {e}\")\n            return InteractionResponse(\n                \"\u274c An error occurred while linking your account. Please try again.\",\n                completed=False,\n            )\n\n    @handle_errors(\"checking account status\")\n    def _handle_check_account_status(self, user_id: str) -> InteractionResponse:\n        \"\"\"Check if user has an account linked\"\"\"\n        internal_user_id = get_user_id_by_identifier(user_id)\n\n        if internal_user_id:\n            user_data_result = get_user_data(internal_user_id, \"account\")\n            account_data = user_data_result.get(\"account\", {})\n            username = account_data.get(\"internal_username\", \"Unknown\")\n\n            return InteractionResponse(\n                f\"\u2705 You have a MHM account linked!\\n\"\n                f\"Your account username: `{username}`\\n\"\n                f\"Use `/profile` to view your profile or `/help` to see available commands.\",\n                completed=True,\n                rich_data={\n                    \"type\": \"account_status\",\n                    \"has_account\": True,\n                    \"username\": username,\n                },\n            )\n        else:\n            return InteractionResponse(\n                \"\u274c No MHM account found linked to this account.\\n\"\n                \"Use the account creation or linking options to get started.\",\n                completed=False,\n                rich_data={\"type\": \"account_status\", \"has_account\": False},\n                suggestions=[\"Create account\", \"Link account\"],\n            )\n\n    @handle_errors(\"checking if username exists\", default_return=False)\n    def _username_exists(self, username: str) -> bool:\n        \"\"\"Check if a username already exists in the system\"\"\"\n        user_ids = get_all_user_ids()\n        for user_id in user_ids:\n            user_data_result = get_user_data(user_id, \"account\")\n            account_data = user_data_result.get(\"account\", {})\n            if account_data.get(\"internal_username\", \"\").lower() == username.lower():\n                return True\n        return False\n\n    @handle_errors(\"getting user ID by username\", default_return=None)\n    def _get_user_id_by_username(self, username: str) -> str | None:\n        \"\"\"Get user ID by username\"\"\"\n        user_ids = get_all_user_ids()\n        for user_id in user_ids:\n            user_data_result = get_user_data(user_id, \"account\")\n            account_data = user_data_result.get(\"account\", {})\n            if account_data.get(\"internal_username\", \"\").lower() == username.lower():\n                return user_id\n        return None\n\n    @handle_errors(\"getting account handler help\")\n    def get_help(self) -> str:\n        \"\"\"Get help text for account management commands.\"\"\"\n        return \"Account management - create or link your MHM account\"\n\n    @handle_errors(\"getting account handler examples\")\n    def get_examples(self) -> list[str]:\n        \"\"\"Get example commands for account management.\"\"\"\n        return [\"create account\", \"link account\", \"check account status\"]\n\n\n# Module-level utilities for account management\nimport secrets\nimport string\n\n# Store pending account operations (confirmation codes, etc.)\n# Format: {channel_identifier: {operation_type: str, username: str, user_id: str, confirmation_code: str, timestamp: float}}\n_pending_link_operations: dict[str, dict[str, Any]] = {}\n\n\n@handle_errors(\"generating confirmation code\", default_return=\"000000\")\n@handle_errors(\"generating confirmation code\", default_return=\"000000\")\ndef _generate_confirmation_code() -> str:\n    \"\"\"Generate a 6-digit confirmation code\"\"\"\n    return \"\".join(secrets.choice(string.digits) for _ in range(6))\n\n\n@handle_errors(\"sending confirmation code\", default_return=False)\ndef _send_confirmation_code(\n    user_id: str,\n    confirmation_code: str,\n    channel_type: str,\n    channel_identifier: str | None = None,\n) -> bool:\n    \"\"\"\n    Send confirmation code via email (for account linking security).\n\n    When linking a Discord account to an existing MHM account, the confirmation code\n    is sent to the email address associated with the MHM account for security verification.\n\n    Args:\n        user_id: Internal user ID of the existing MHM account\n        confirmation_code: The 6-digit confirmation code\n        channel_type: The channel being linked ('discord', 'email', etc.) - used for message context\n        channel_identifier: Channel-specific identifier (Discord user ID, etc.) - used for message context only\n    \"\"\"\n    try:\n        user_data_result = get_user_data(user_id, \"account\")\n        account_data = user_data_result.get(\"account\", {})\n\n        recipient = account_data.get(\"email\", \"\")\n        if not recipient:\n            logger.warning(\n                f\"User {user_id} does not have an email address configured - cannot send confirmation code\"\n            )\n            return False\n\n        # Always send confirmation codes via email for security\n        # channel_type is used for message context (which channel is being linked)\n\n        # Send via communication manager\n        from communication.core.channel_orchestrator import CommunicationManager\n\n        comm_manager = CommunicationManager()\n\n        if comm_manager:\n            message = f\"\"\"Hello!\n\nYou requested to link your MHM account to {channel_type}.\n\nYour confirmation code is: {confirmation_code}\n\nEnter this code in {channel_type} to complete the linking process.\n\nIf you didn't request this, please ignore this message.\n\nThank you,\nMHM Team\"\"\"\n\n            # Send via email channel (confirmation codes are always sent via email for security)\n            # Use send_message_sync which handles async/sync conversion internally\n            # This avoids creating unawaited coroutines and RuntimeWarnings\n            success = comm_manager.send_message_sync(\n                channel_name=\"email\",\n                recipient=recipient,\n                message=message,\n                message_type=\"account_linking\",\n                channel_preference=\"email\",\n            )\n\n            if success:\n                logger.info(f\"Sent confirmation code to {recipient} for user {user_id}\")\n                return True\n            else:\n                logger.warning(\n                    f\"Failed to send confirmation code to {recipient} for user {user_id}\"\n                )\n                return False\n        else:\n            logger.warning(\n                \"Communication manager not available for sending confirmation code\"\n            )\n            return False\n\n    except Exception as e:\n        logger.error(f\"Error sending confirmation code for user {user_id}: {e}\")\n        return False\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 98,
                    "line_content": "# Extract feature selection (default to True for backward compatibility)",
                    "start": 3692,
                    "end": 3714
                  }
                ]
              ],
              [
                "communication\\core\\channel_orchestrator.py",
                "# channel_orchestrator.py\n\nimport asyncio\nimport threading\nimport time\nimport random\nimport re\nfrom typing import Dict, List, Optional, Any\n\nfrom core.logger import get_component_logger\nfrom core.error_handling import handle_errors\nfrom communication.communication_channels.base.base_channel import BaseChannel, ChannelConfig, ChannelStatus\nfrom communication.core.factory import ChannelFactory\nfrom communication.core.retry_manager import RetryManager\nfrom communication.core.channel_monitor import ChannelMonitor\nfrom core.user_data_handlers import get_user_data\nfrom core.message_management import store_sent_message\nfrom core.schedule_management import get_current_time_periods_with_validation, get_current_day_names\nfrom core.file_operations import determine_file_path, load_json_data  # determine_file_path needed for test mocking\nfrom core.config import EMAIL_SMTP_SERVER, DISCORD_BOT_TOKEN, get_user_data_dir\nfrom core.service_utilities import wait_for_network\n\n# Route orchestration logs to channels component; keep module logger for local debug if needed\ncomm_logger = get_component_logger('channel_orchestrator')\nlogger = comm_logger\n\nclass BotInitializationError(Exception):\n    \"\"\"Custom exception for bot initialization failures.\"\"\"\n    pass\n\nclass MessageSendError(Exception):\n    \"\"\"Custom exception for message sending failures.\"\"\"\n    pass\n\nclass CommunicationManager:\n    \"\"\"Manages all communication channels with improved modularity\"\"\"\n    \n    _instance = None\n    _lock = threading.Lock()\n    \n    @handle_errors(\"creating channel orchestrator instance\", default_return=None)\n    def __new__(cls, *args, **kwargs):\n        \"\"\"Ensure that only one instance of the CommunicationManager exists (Singleton pattern).\"\"\"\n        with cls._lock:\n            if cls._instance is None:\n                cls._instance = super(CommunicationManager, cls).__new__(cls)\n                cls._instance._initialized = False\n            return cls._instance\n\n    @handle_errors(\"initializing channel orchestrator\", default_return=None)\n    def __init__(self):\n        \"\"\"Initialize the CommunicationManager singleton\"\"\"\n        # Check if already initialized\n        if hasattr(self, '_initialized') and self._initialized:\n            return\n        \n        # Initialize basic attributes\n        self._channels_dict = {}\n        self.channel_configs = {}\n        self._running = False\n        self._main_loop = None\n        self._loop_thread = None\n        self.scheduler_manager = None\n        self._last_task_reminders = {}  # Track last task reminder per user: {user_id: task_id}\n        self._email_polling_thread = None\n        self._email_polling_stop_event = threading.Event()\n        self._processed_email_ids = set()  # Track processed emails to avoid duplicates\n        \n        # Initialize extracted modules\n        # Pass send_message_sync as callback for retry manager\n        self.retry_manager = RetryManager(send_callback=self.send_message_sync)\n        self.channel_monitor = ChannelMonitor()\n        \n        # Set up event loop for async operations\n        self.__init____setup_event_loop()\n        \n        # Mark as initialized\n        self._initialized = True\n            \n    @handle_errors(\"setting up event loop\", default_return=None)\n    def __init____setup_event_loop(self):\n        \"\"\"Set up a dedicated event loop for async operations\"\"\"\n        try:\n            # Try to get existing loop - use get_running_loop() to avoid deprecation warning\n            try:\n                self._main_loop = asyncio.get_running_loop()\n                self._event_loop = self._main_loop\n                # If we get here, there's a running loop, so create a new one for our operations\n                import threading\n                @handle_errors(\"running event loop\", default_return=None)\n                def run_event_loop():\n                    \"\"\"\n                    Run the event loop in a separate thread for async operations.\n                    \n                    This nested function is used to manage the event loop for async channel operations.\n                    \"\"\"\n                    self._main_loop = asyncio.new_event_loop()\n                    self._event_loop = self._main_loop\n                    asyncio.set_event_loop(self._main_loop)\n                    self._main_loop.run_forever()\n                \n                self._loop_thread = threading.Thread(target=run_event_loop, daemon=True)\n                self._loop_thread.start()\n                \n                # Wait a moment for loop to start\n                import time\n                time.sleep(0.1)\n            except RuntimeError:\n                # No running loop, try to get current loop\n                try:\n                    # Use get_running_loop() first, fallback to new_event_loop()\n                    try:\n                        self._main_loop = asyncio.get_running_loop()\n                    except RuntimeError:\n                        self._main_loop = asyncio.new_event_loop()\n                        asyncio.set_event_loop(self._main_loop)\n                    self._event_loop = self._main_loop\n                except RuntimeError:\n                    # No event loop exists, create new one\n                    self._main_loop = asyncio.new_event_loop()\n                    self._event_loop = self._main_loop\n                    asyncio.set_event_loop(self._main_loop)\n        except Exception as e:\n            # Fallback: create new loop\n            self._main_loop = asyncio.new_event_loop()\n            self._event_loop = self._main_loop\n            asyncio.set_event_loop(self._main_loop)\n    \n    @handle_errors(\"running async sync\", default_return=None)\n    def send_message_sync__run_async_sync(self, coro):\n        \"\"\"Run async function synchronously using our managed loop\"\"\"\n        if not self._main_loop:\n            self.__init____setup_event_loop()\n        if not self._main_loop:\n            raise RuntimeError(\"Event loop not initialized for sync execution\")\n        if self._loop_thread:\n            # Submit to running loop\n            future = asyncio.run_coroutine_threadsafe(coro, self._main_loop)\n            return future.result(timeout=30)  # 30 second timeout\n        else:\n            # Use current loop\n            return self._main_loop.run_until_complete(coro)\n\n    @handle_errors(\"setting scheduler manager\", default_return=None)\n    def set_scheduler_manager(self, scheduler_manager):\n        \"\"\"Set the scheduler manager for the communication manager.\"\"\"\n        self.scheduler_manager = scheduler_manager\n        logger.debug(\"Scheduler manager set in CommunicationManager.\")\n\n    @handle_errors(\"queueing failed message\", default_return=None)\n    def send_message_sync__queue_failed_message(self, user_id: str, category: str, message: str, recipient: str, channel_name: str):\n        \"\"\"Queue a failed message for retry\"\"\"\n        self.retry_manager.queue_failed_message(user_id, category, message, recipient, channel_name)\n\n    @handle_errors(\"starting retry thread\", default_return=None)\n    def start_all__start_retry_thread(self):\n        \"\"\"Start the retry thread for failed messages\"\"\"\n        self.retry_manager.start_retry_thread()\n\n    @handle_errors(\"stopping retry thread\", default_return=None)\n    def stop_all__stop_retry_thread(self):\n        \"\"\"Stop the retry thread\"\"\"\n        self.retry_manager.stop_retry_thread()\n\n    @handle_errors(\"starting restart monitor\", default_return=None)\n    def start_all__start_restart_monitor(self):\n        \"\"\"Start the automatic restart monitor thread\"\"\"\n        self.channel_monitor.set_channels(self._channels_dict)\n        self.channel_monitor.start_restart_monitor()\n\n    @handle_errors(\"stopping restart monitor\", default_return=None)\n    def stop_all__stop_restart_monitor(self):\n        \"\"\"Stop the automatic restart monitor thread\"\"\"\n        self.channel_monitor.stop_restart_monitor()\n\n    @handle_errors(\"starting email polling thread\", default_return=None)\n    def start_all__start_email_polling(self):\n        \"\"\"Start the email polling thread to process incoming emails\"\"\"\n        if self._email_polling_thread is not None and self._email_polling_thread.is_alive():\n            logger.debug(\"Email polling thread already running\")\n            return\n        \n        self._email_polling_stop_event.clear()\n        self._email_polling_thread = threading.Thread(target=self._email_polling_loop, daemon=True)\n        self._email_polling_thread.start()\n        logger.info(\"Email polling thread started\")\n\n    @handle_errors(\"stopping email polling thread\", default_return=None)\n    def stop_all__stop_email_polling(self):\n        \"\"\"Stop the email polling thread\"\"\"\n        if self._email_polling_thread is not None:\n            logger.info(\"Stopping email polling thread...\")\n            self._email_polling_stop_event.set()\n            self._email_polling_thread.join(timeout=5)\n            if self._email_polling_thread.is_alive():\n                logger.warning(\"Email polling thread didn't stop within timeout\")\n            else:\n                logger.info(\"Email polling thread stopped\")\n            self._email_polling_thread = None\n\n    @handle_errors(\"email polling loop\", default_return=None)\n    def _email_polling_loop(self):\n        \"\"\"Background thread that periodically polls for incoming emails and processes them\"\"\"\n        logger.info(\"Email polling loop started\")\n        poll_interval = 30  # Check for emails every 30 seconds\n        \n        while not self._email_polling_stop_event.is_set():\n            try:\n                # Only poll if email channel is available and ready\n                email_channel = self._channels_dict.get('email')\n                if email_channel and email_channel.is_ready() and self._running:\n                    # Poll for new emails\n                    try:\n                        # Use the event loop to run async receive_messages\n                        # Check if we have a running loop thread - if not, use temporary loop\n                        if self._loop_thread and self._loop_thread.is_alive() and self._main_loop and not self._main_loop.is_closed():\n                            try:\n                                logger.debug(\"Using main loop thread for email polling\")\n                                future = asyncio.run_coroutine_threadsafe(\n                                    email_channel.receive_messages(),\n                                    self._main_loop\n                                )\n                                emails = future.result(timeout=10)\n                            except RuntimeError as loop_error:\n                                # Event loop may have been closed between check and use\n                                logger.warning(f\"Event loop became invalid during email polling, using fallback: {loop_error}\")\n                                # Fallback: create temporary loop\n                                loop = asyncio.new_event_loop()\n                                asyncio.set_event_loop(loop)\n                                try:\n                                    emails = loop.run_until_complete(email_channel.receive_messages())\n                                finally:\n                                    loop.close()\n                        else:\n                            # Fallback: create temporary loop (main loop not running or not available)\n                            logger.debug(\"Using temporary event loop for email polling (main loop not running)\")\n                            loop = asyncio.new_event_loop()\n                            asyncio.set_event_loop(loop)\n                            try:\n                                emails = loop.run_until_complete(email_channel.receive_messages())\n                            finally:\n                                loop.close()\n                        \n                        # Process each email\n                        for email_msg in emails:\n                            email_id = email_msg.get('message_id')\n                            if email_id and email_id not in self._processed_email_ids:\n                                self._process_incoming_email(email_msg)\n                                self._processed_email_ids.add(email_id)\n                                # Limit processed IDs set size to prevent memory growth\n                                if len(self._processed_email_ids) > 1000:\n                                    # Keep only the most recent 500\n                                    self._processed_email_ids = set(list(self._processed_email_ids)[-500:])\n                    except asyncio.TimeoutError:\n                        logger.error(\"Error polling for emails: Timeout waiting for receive_messages() to complete (10s timeout exceeded)\", exc_info=True)\n                    except RuntimeError as e:\n                        logger.error(f\"Error polling for emails: RuntimeError - {e} (event loop may be closed or invalid)\", exc_info=True)\n                    except Exception as e:\n                        # Enhanced error logging with full exception details\n                        error_type = type(e).__name__\n                        error_msg = str(e) if str(e) else f\"Exception of type {error_type} with no message\"\n                        logger.error(f\"Error polling for emails: {error_type} - {error_msg}\", exc_info=True)\n                else:\n                    logger.debug(\"Email channel not available or not ready, skipping poll\")\n                \n            except Exception as e:\n                # Enhanced error logging for outer exception handler\n                error_type = type(e).__name__\n                error_msg = str(e) if str(e) else f\"Exception of type {error_type} with no message\"\n                logger.error(f\"Error in email polling loop: {error_type} - {error_msg}\", exc_info=True)\n            \n            # Wait for poll interval or stop event\n            if self._email_polling_stop_event.wait(timeout=poll_interval):\n                break\n        \n        logger.info(\"Email polling loop stopped\")\n\n    @handle_errors(\"processing incoming email\", default_return=None)\n    def _process_incoming_email(self, email_msg: dict[str, Any]):\n        \"\"\"Process an incoming email message and send response\"\"\"\n        try:\n            email_from = email_msg.get('from', '')\n            email_body = email_msg.get('body', '')\n            email_subject = email_msg.get('subject', '')\n            \n            if not email_from or not email_body:\n                logger.debug(f\"Skipping email with missing from or body: {email_msg}\")\n                return\n            \n            # Extract email address from \"from\" field (may be \"Name <email@example.com>\" or just \"email@example.com\")\n            email_match = re.search(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', email_from)\n            if not email_match:\n                logger.warning(f\"Could not extract email address from 'from' field: {email_from}\")\n                return\n            \n            sender_email = email_match.group(0)\n            \n            # Map email to user ID\n            from core.user_data_handlers import get_user_id_by_identifier\n            user_id = get_user_id_by_identifier(sender_email)\n            \n            if not user_id:\n                logger.info(f\"Email from unregistered user: {sender_email}\")\n                # Send registration prompt\n                response_text = (\n                    f\"I don't recognize you yet! Please register first using the MHM application. \"\n                    f\"Your email is: {sender_email}\"\n                )\n                self._send_email_response(sender_email, response_text, email_subject)\n                return\n            \n            logger.info(f\"Processing email from registered user: {sender_email} (user_id: {user_id})\")\n            \n            # Route message to InteractionManager\n            from communication.message_processing.interaction_manager import handle_user_message\n            response = handle_user_message(user_id, email_body, \"email\")\n            \n            if response and response.message:\n                # Send response back via email\n                self._send_email_response(sender_email, response.message, f\"Re: {email_subject}\")\n            else:\n                logger.warning(f\"No response generated for email from user {user_id}\")\n                \n        except Exception as e:\n            logger.error(f\"Error processing incoming email: {e}\", exc_info=True)\n\n    @handle_errors(\"sending email response\", default_return=None)\n    def _send_email_response(self, recipient_email: str, response_text: str, subject: str = \"Re: Your Message\"):\n        \"\"\"Send an email response to a user\"\"\"\n        try:\n            email_channel = self._channels_dict.get('email')\n            if not email_channel or not email_channel.is_ready():\n                logger.error(\"Email channel not available for sending response\")\n                return\n            \n            # Use the event loop to send email\n            if self._main_loop and not self._main_loop.is_closed():\n                future = asyncio.run_coroutine_threadsafe(\n                    email_channel.send_message(recipient_email, response_text, subject=subject),\n                    self._main_loop\n                )\n                future.result(timeout=10)\n            else:\n                # Fallback: create temporary loop\n                loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(loop)\n                loop.run_until_complete(email_channel.send_message(recipient_email, response_text, subject=subject))\n                loop.close()\n            \n            logger.info(f\"Email response sent to {recipient_email}\")\n        except Exception as e:\n            logger.error(f\"Error sending email response to {recipient_email}: {e}\")\n\n    @handle_errors(\"initializing channels from config\", default_return=False)\n    def initialize_channels_from_config(self, channel_configs: dict[str, ChannelConfig] | None = None):\n        \"\"\"\n        Initialize channels from configuration with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        # Validate channel_configs\n        if channel_configs is not None and not isinstance(channel_configs, dict):\n            logger.error(f\"Invalid channel_configs: {type(channel_configs)}\")\n            return False\n        \"\"\"Initialize channels from configuration\"\"\"\n        if channel_configs is None:\n            # Use default configurations\n            channel_configs = self._get_default_channel_configs()\n        \n        self.channel_configs = channel_configs\n        \n        # Create event loop for async operations if we're not in one\n        try:\n            # Use get_running_loop() first, fallback to new_event_loop()\n            try:\n                loop = asyncio.get_running_loop()\n            except RuntimeError:\n                loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(loop)\n        except RuntimeError:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n        \n        # Run the async initialization\n        return loop.run_until_complete(self.initialize_channels_from_config__initialize_channels_async())\n\n    @handle_errors(\"initializing channels from config async\", default_return=None)\n    async def initialize_channels_from_config__initialize_channels_async(self):\n        \"\"\"Async method to initialize all configured channels\"\"\"\n        logger.debug(\"Starting async channel initialization.\")\n        if not self.channel_configs:\n            logger.error(\"Channel configs not initialized for async initialization\")\n            return False\n        \n        for name, config in self.channel_configs.items():\n            if not config.enabled:\n                logger.info(f\"Channel {name} is disabled, skipping\")\n                continue\n                \n            channel = ChannelFactory.create_channel(name, config)\n            if not channel:\n                logger.error(f\"Failed to create channel: {name}\")\n                continue\n                \n            # Initialize with retry logic\n            success = await self._initialize_channel_with_retry(channel, config)\n            if success:\n                # Store the channel\n                self._channels_dict[name] = channel\n                logger.debug(f\"Channel {name} ready\")\n            else:\n                logger.error(f\"Failed to initialize channel {name} after retries\")\n        \n        return len(self._channels_dict) > 0\n\n    @handle_errors(\"initializing channel with retry\", user_friendly=False, default_return=False)\n    async def _initialize_channel_with_retry(self, channel: BaseChannel, config: ChannelConfig) -> bool:\n        \"\"\"Initialize a channel with retry logic\"\"\"\n        retry_delay = config.retry_delay\n        \n        for attempt in range(config.max_retries):\n            try:\n                logger.debug(f\"Initializing {channel.config.name}, attempt {attempt + 1}/{config.max_retries}\")\n                \n                # Add timeout to prevent hanging\n                try:\n                    success = await asyncio.wait_for(\n                        channel.initialize(), \n                        timeout=30.0  # 30 second timeout per attempt\n                    )\n                    if success:\n                        logger.info(f\"Channel {channel.config.name} initialized on attempt {attempt + 1}\")\n                        return True\n                    else:\n                        logger.warning(f\"Channel {channel.config.name} initialization returned False on attempt {attempt + 1}\")\n                        \n                        # Special handling for Discord bot - check if it's actually connected\n                        if channel.config.name == 'discord' and hasattr(channel, 'is_actually_connected'):\n                            if channel.is_actually_connected():\n                                logger.info(f\"Discord bot is actually connected despite initialization returning False\")\n                                return True\n                        \n                except asyncio.TimeoutError:\n                    logger.warning(f\"Channel {channel.config.name} initialization timed out on attempt {attempt + 1}\")\n                    \n                    # Special handling for Discord bot - check if it's actually connected after timeout\n                    if channel.config.name == 'discord' and hasattr(channel, 'is_actually_connected'):\n                        if channel.is_actually_connected():\n                            logger.info(f\"Discord bot is actually connected despite initialization timeout\")\n                            return True\n                \n            except Exception as e:\n                logger.warning(f\"Channel {channel.config.name} initialization attempt {attempt + 1} failed: {e}\")\n            \n            if attempt < config.max_retries - 1:\n                logger.debug(f\"Waiting {retry_delay} seconds before next attempt for {channel.config.name}\")\n                await asyncio.sleep(retry_delay)\n                retry_delay *= config.backoff_multiplier\n        \n        logger.error(f\"Failed to initialize {channel.config.name} after {config.max_retries} attempts\")\n        return False\n\n    @handle_errors(\"getting default channel configs\", default_return={})\n    def _get_default_channel_configs(self) -> dict[str, ChannelConfig]:\n        \"\"\"Get default channel configurations\"\"\"\n        configs = {}\n        \n        # Only create configs for channels that have tokens/configs available\n        \n        #         enabled=True,\n        #         max_retries=5,\n        #         retry_delay=2.0,\n        #         backoff_multiplier=2.0\n        #     )\n        \n        if EMAIL_SMTP_SERVER:\n            configs['email'] = ChannelConfig(\n                name='email',\n                enabled=True,\n                max_retries=3,\n                retry_delay=1.0,\n                backoff_multiplier=2.0\n            )\n        \n        if DISCORD_BOT_TOKEN:\n            configs['discord'] = ChannelConfig(\n                name='discord',\n                enabled=True,\n                max_retries=3,\n                retry_delay=1.5,\n                backoff_multiplier=2.0\n            )\n        \n        return configs\n\n    @handle_errors(\"starting all channels\", default_return=False)\n    def start_all(self):\n        \"\"\"\n        Start all communication channels with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        \"\"\"Start all communication channels\"\"\"\n        logger.debug(\"Starting all communication channels.\")\n        \n        try:\n            # If no channel configs set, load defaults now\n            if not self.channel_configs:\n                self.channel_configs = self._get_default_channel_configs()\n                logger.debug(\"Loaded default channel configurations for startup\")\n            # Start the retry thread for failed messages\n            self.start_all__start_retry_thread()\n            \n            # Start the restart monitor\n            self.start_all__start_restart_monitor()\n            \n            # Start email polling if email channel is configured\n            if 'email' in self.channel_configs and self.channel_configs['email'].enabled:\n                self.start_all__start_email_polling()\n            \n            # Try async startup first\n            try:\n                loop = asyncio.get_running_loop()\n                # If there's a running loop, just do sync startup\n                self._start_sync()\n            except RuntimeError:\n                # No running loop, safe to create one\n                try:\n                    loop = asyncio.new_event_loop()\n                    asyncio.set_event_loop(loop)\n                    loop.run_until_complete(self._start_all_async())\n                    loop.close()\n                except Exception:\n                    # Fallback to sync startup\n                    self._start_sync()\n            \n            self._running = True\n            logger.info(\"All channels started successfully\")\n            \n        except Exception as e:\n            logger.error(f\"Error during startup: {e}\")\n            # Final fallback\n            self._start_sync()\n\n    @handle_errors(\"starting all async channels\", default_return=None)\n    async def _start_all_async(self):\n        \"\"\"Async method to start all configured channels\"\"\"\n        logger.debug(\"Starting async channel startup.\")\n        if not self.channel_configs:\n            logger.error(\"Channel configs not initialized for async startup\")\n            return False\n        \n        for name, config in self.channel_configs.items():\n            if not config.enabled:\n                logger.info(f\"Channel {name} is disabled, skipping\")\n                continue\n            # Skip if channel already exists to avoid duplicate instances\n            if name in self._channels_dict and self._channels_dict[name] is not None:\n                logger.debug(f\"Channel {name} already exists, skipping re-creation\")\n                continue\n                \n            channel = ChannelFactory.create_channel(name, config)\n            if not channel:\n                logger.error(f\"Failed to create channel: {name}\")\n                continue\n                \n            # Initialize with retry logic\n            success = await self._initialize_channel_with_retry(channel, config)\n            if success:\n                # Store the channel\n                self._channels_dict[name] = channel\n                logger.debug(f\"Channel {name} ready\")\n            else:\n                logger.error(f\"Failed to initialize channel {name} after retries\")\n        \n        return len(self._channels_dict) > 0\n\n    @handle_errors(\"starting sync channels\", default_return=None)\n    def _start_sync(self):\n        \"\"\"Synchronous method to start all configured channels\"\"\"\n        logger.debug(\"Starting sync channel startup.\")\n        if not self.channel_configs:\n            logger.error(\"Channel configs not initialized for sync startup\")\n            return False\n        \n        for name, config in self.channel_configs.items():\n            if not config.enabled:\n                logger.info(f\"Channel {name} is disabled, skipping\")\n                continue\n            # Skip if channel already exists to avoid duplicate instances\n            if name in self._channels_dict and self._channels_dict[name] is not None:\n                logger.debug(f\"Channel {name} already exists, skipping re-creation\")\n                continue\n                \n            channel = ChannelFactory.create_channel(name, config)\n            if not channel:\n                logger.error(f\"Failed to create channel: {name}\")\n                continue\n                \n            # Initialize with retry logic (sync version)\n            success = self._initialize_channel_with_retry_sync(channel, config)\n            if success:\n                # Store the channel\n                self._channels_dict[name] = channel\n                logger.debug(f\"Channel {name} ready\")\n            else:\n                logger.error(f\"Failed to initialize channel {name} after retries\")\n        \n        return len(self._channels_dict) > 0\n\n    @handle_errors(\"initializing channel with retry (sync)\", user_friendly=False, default_return=False)\n    def _initialize_channel_with_retry_sync(self, channel: BaseChannel, config: ChannelConfig) -> bool:\n        \"\"\"Synchronous version of channel initialization with retry logic\"\"\"\n        retry_delay = config.retry_delay\n        \n        for attempt in range(config.max_retries):\n            try:\n                logger.debug(f\"Initializing {channel.config.name}, attempt {attempt + 1}/{config.max_retries}\")\n                \n                # Try to initialize the channel\n                if hasattr(channel, 'initialize'):\n                    # If it's an async method, run it in a new event loop\n                    try:\n                        loop = asyncio.new_event_loop()\n                        asyncio.set_event_loop(loop)\n                        success = loop.run_until_complete(asyncio.wait_for(\n                            channel.initialize(), \n                            timeout=30.0  # 30 second timeout per attempt\n                        ))\n                        loop.close()\n                    except asyncio.TimeoutError:\n                        logger.warning(f\"Channel {channel.config.name} initialization timed out on attempt {attempt + 1}\")\n                        success = False\n                    except Exception as e:\n                        logger.warning(f\"Channel {channel.config.name} initialization attempt {attempt + 1} failed: {e}\")\n                        success = False\n                else:\n                    # If no initialize method, assume it's ready\n                    success = True\n                \n                if success:\n                    logger.info(f\"Channel {channel.config.name} initialized on attempt {attempt + 1}\")\n                    return True\n                else:\n                    logger.warning(f\"Channel {channel.config.name} initialization returned False on attempt {attempt + 1}\")\n                    \n                    # Special handling for Discord bot - check if it's actually connected\n                    if channel.config.name == 'discord' and hasattr(channel, 'is_actually_connected'):\n                        if channel.is_actually_connected():\n                            logger.info(f\"Discord bot is actually connected despite initialization returning False\")\n                            return True\n                \n            except Exception as e:\n                logger.warning(f\"Channel {channel.config.name} initialization attempt {attempt + 1} failed: {e}\")\n            \n            if attempt < config.max_retries - 1:\n                logger.debug(f\"Waiting {retry_delay} seconds before next attempt for {channel.config.name}\")\n                time.sleep(retry_delay)\n                retry_delay *= config.backoff_multiplier\n        \n        logger.error(f\"Failed to initialize {channel.config.name} after {config.max_retries} attempts\")\n        return False\n\n    @handle_errors(\"sending message\", default_return=False)\n    async def send_message(self, channel_name: str, recipient: str, message: str, **kwargs) -> bool:\n        \"\"\"Send message via specified channel using unified interface\"\"\"\n        logger.debug(f\"Preparing to send message to {recipient} via {channel_name}\")\n        \n        channel = self._channels_dict.get(channel_name)\n        if not channel:\n            logger.error(f\"Channel {channel_name} not found\")\n            return False\n        \n        # Special handling for Discord bot - check if it can actually send messages\n        if channel_name == 'discord' and hasattr(channel, 'can_send_messages'):\n            if not channel.can_send_messages():\n                logger.error(f\"Channel {channel_name} not ready - cannot send messages\")\n                # Queue for retry after reconnection\n                try:\n                    self.send_message_sync__queue_failed_message(\n                        kwargs.get('user_id', ''),\n                        kwargs.get('category', 'unknown'),\n                        message,\n                        recipient,\n                        channel_name\n                    )\n                except Exception:\n                    pass\n                return False\n        elif not channel.is_ready():\n            # Don't call async get_status() in sync context - just log the issue\n            logger.error(f\"Channel {channel_name} not ready\")\n            # Queue for retry after reconnection\n            try:\n                self.send_message_sync__queue_failed_message(\n                    kwargs.get('user_id', ''),\n                    kwargs.get('category', 'unknown'),\n                    message,\n                    recipient,\n                    channel_name\n                )\n            except Exception:\n                pass\n            return False\n\n        # Check network connectivity with proper error handling\n        try:\n            if not wait_for_network():\n                from core.error_handling import handle_network_error\n                handle_network_error(\n                    ConnectionError(\"Network not available\"), \n                    \"message send\", \n                    kwargs.get('user_id')\n                )\n                return False\n        except Exception as network_error:\n            from core.error_handling import handle_network_error\n            handle_network_error(network_error, \"network check\", kwargs.get('user_id'))\n            return False\n        \n        try:\n            # FIXED: Ensure we're actually awaiting a coroutine\n            success = await channel.send_message(recipient, message, **kwargs)\n            \n            # FIXED: Better return value validation\n            # Use == instead of is to handle mock return values correctly\n            if success is True or success == True:\n                # Enhanced logging with message content and time period\n                message_preview = message[:50] + \"...\" if len(message) > 50 else message\n                time_period = kwargs.get('time_period', 'unknown')\n                user_id = kwargs.get('user_id', 'unknown')\n                category = kwargs.get('category', 'unknown')\n                # Log will be handled by the deduplication logic below\n                return True\n            elif success is False or success == False:\n                logger.warning(f\"Channel {channel_name} returned False for message send to {recipient}\")\n                return False\n            else:\n                # Handle unexpected return values\n                logger.warning(f\"Channel {channel_name} returned unexpected value: {success} (type: {type(success)})\")\n                # If it's not explicitly False, assume success if no exception was raised\n                return True\n            \n        except ConnectionError as e:\n            from core.error_handling import handle_network_error\n            handle_network_error(e, f\"message send via {channel_name}\", kwargs.get('user_id'))\n            return False\n        except TimeoutError as e:\n            from core.error_handling import handle_network_error\n            handle_network_error(e, f\"message send via {channel_name}\", kwargs.get('user_id'))\n            return False\n        except Exception as e:\n            from core.error_handling import handle_communication_error\n            handle_communication_error(e, channel_name, f\"message send to {recipient}\", kwargs.get('user_id'))\n            return False\n\n    @handle_errors(\"checking logging health\", user_friendly=False, default_return=False)\n    def _check_logging_health(self):\n        \"\"\"\n        Check if logging is still working and recover if needed.\n        \n        Verifies that the logging system is functional and attempts to restart it if issues are detected.\n        \"\"\"\n        try:\n            # Test logging\n            test_message = f\"Logging health check - {time.time()}\"\n            logger.debug(test_message)\n            \n            # Force flush via component logger handlers only (avoid direct logging import in app code)\n            for handler in logger.logger.handlers if hasattr(logger, 'logger') else []:\n                if hasattr(handler, 'flush'):\n                    handler.flush()\n            \n            logger.debug(\"Logging health check passed\")\n            return True\n        except Exception as e:\n            logger.error(f\"Logging health check failed: {e}\")\n            # Try to restart logging\n            from core.logger import force_restart_logging\n            if force_restart_logging():\n                logger.info(\"Logging system restarted successfully\")\n                return True\n            return False\n\n    @handle_errors(\"sending message (sync)\", default_return=False)\n    def send_message_sync(self, channel_name: str, recipient: str, message: str, **kwargs) -> bool:\n        \"\"\"Synchronous wrapper with logging health check\"\"\"\n        # Check logging health periodically\n        self._check_logging_health()\n        \n        # Queue immediately if channel is not ready\n        channel = self._channels_dict.get(channel_name)\n        not_ready = False\n        if not channel:\n            not_ready = True\n        elif channel_name == 'discord' and hasattr(channel, 'can_send_messages') and not channel.can_send_messages():\n            not_ready = True\n        elif channel and not channel.is_ready():\n            not_ready = True\n        if not_ready:\n            logger.error(f\"Channel {channel_name} not ready - queuing message for retry\")\n            try:\n                self.send_message_sync__queue_failed_message(\n                    kwargs.get('user_id', ''),\n                    kwargs.get('category', 'unknown'),\n                    message,\n                    recipient,\n                    channel_name\n                )\n            except Exception as e:\n                logger.warning(f\"Failed to queue message for retry: {e}\")\n            return False\n        \n        try:\n            # Double-check channel exists before attempting to send\n            # This is a defensive check in case we somehow got past the earlier check\n            if channel_name not in self._channels_dict:\n                logger.error(f\"Channel {channel_name} not found in channels_dict - cannot send\")\n                return False\n            \n            # Get the underlying bot directly\n            if channel_name == 'discord':\n                discord_channel = self._channels_dict.get('discord')\n                if discord_channel and hasattr(discord_channel, 'bot'):\n                    # Use the discord.py sync methods if available\n                    bot = discord_channel.bot\n                    \n                    # Try to send via the bot's sync methods\n                    try:\n                        # Handle special Discord user format - skip sync method for discord_user: format\n                        if recipient.startswith(\"discord_user:\"):\n                            # This format is handled by the async send_message method\n                            # Skip the sync channel lookup for discord_user: format\n                            pass\n                        else:\n                            # Try to convert to integer for channel ID\n                            channel = bot.get_channel(int(recipient))\n                            if channel:\n                                # This is hacky but might work\n                                import asyncio\n                                loop = asyncio.new_event_loop()\n                                asyncio.set_event_loop(loop)\n                                loop.run_until_complete(channel.send(message))\n                                loop.close()\n                                logger.info(f\"Direct Discord message sent to channel {recipient}\")\n                                return True\n                    except Exception as e:\n                        logger.error(f\"Direct Discord send failed: {e}\")\n                        \n            # Fallback to async send\n            # Note: This should only be reached if channel exists and is ready\n            # If we get here with a non-existent channel, something went wrong\n            try:\n                result = self.send_message_sync__run_async_sync(\n                    self.send_message(channel_name, recipient, message, **kwargs)\n                )\n                # Ensure we return False if result is None or unexpected\n                if result is None:\n                    logger.warning(f\"Async send_message returned None for channel {channel_name}\")\n                    return False\n                return result\n            except Exception as e:\n                logger.error(f\"Fallback async send failed: {e}\")\n                return False\n            \n        except Exception as e:\n            logger.error(f\"Error in simplified send_message_sync: {e}\")\n            # Queue for retry if this is a scheduled message\n            if 'user_id' in kwargs and 'category' in kwargs:\n                self.send_message_sync__queue_failed_message(\n                    kwargs['user_id'],\n                    kwargs['category'],\n                    message,\n                    recipient,\n                    channel_name\n                )\n            return False\n\n    @handle_errors(\"broadcasting message\", default_return={})\n    async def broadcast_message(self, recipients: dict[str, str], message: str) -> dict[str, bool]:\n        \"\"\"Send message to multiple channels\"\"\"\n        logger.debug(f\"Broadcasting message to {len(recipients)} channels\")\n        results = {}\n        \n        # Create tasks for all sends\n        tasks = []\n        for channel_name, recipient in recipients.items():\n            if channel_name in self._channels_dict:\n                task = asyncio.create_task(\n                    self.send_message(channel_name, recipient, message),\n                )\n                tasks.append((channel_name, task))\n            else:\n                logger.warning(f\"Channel {channel_name} not available for broadcast\")\n                results[channel_name] = False\n        \n        # Wait for all tasks to complete\n        for channel_name, task in tasks:\n            try:\n                results[channel_name] = await task\n            except Exception as e:\n                logger.error(f\"Error in broadcast to {channel_name}: {e}\")\n                results[channel_name] = False\n        \n        successful = sum(results.values())\n        logger.info(f\"Broadcast completed: {successful}/{len(recipients)} channels successful\")\n        return results\n\n    @handle_errors(\"getting channel status\", user_friendly=False, default_return=None)\n    async def get_channel_status(self, channel_name: str) -> ChannelStatus | None:\n        \"\"\"Get status of a specific channel\"\"\"\n        channel = self._channels_dict.get(channel_name)\n        if channel:\n            return await channel.get_status()\n        return None\n\n    @handle_errors(\"getting all channel statuses\", default_return={})\n    async def get_all_statuses(self) -> dict[str, ChannelStatus]:\n        \"\"\"Get status of all channels\"\"\"\n        statuses = {}\n        for name, channel in self._channels_dict.items():\n            if channel:\n                statuses[name] = await channel.get_status()\n        return statuses\n\n    @handle_errors(\"performing health check on all channels\", user_friendly=False, default_return={})\n    async def health_check_all(self) -> dict[str, Any]:\n        \"\"\"Perform health check on all channels\"\"\"\n        health_results = {}\n        \n        for name, channel in self._channels_dict.items():\n            if channel:\n                # Per-item error handling: continue processing other channels even if one fails\n                try:\n                    health_results[name] = await channel.get_health_status()\n                except Exception as e:\n                    health_results[name] = {\n                        'status': 'error',\n                        'error': str(e)\n                    }\n        \n        return health_results\n\n    @handle_errors(\"getting Discord connectivity status\", default_return=None)\n    def get_discord_connectivity_status(self) -> dict[str, Any] | None:\n        \"\"\"Get detailed Discord connectivity status if available\"\"\"\n        if 'discord' in self._channels_dict:\n            discord_channel = self._channels_dict['discord']\n            if hasattr(discord_channel, 'get_health_status'):\n                return discord_channel.get_health_status()\n        return None\n\n    @handle_errors(\"shutting down all channels (async)\", user_friendly=False, default_return=None)\n    async def _shutdown_all_async(self):\n        \"\"\"Async method to shutdown all channels\"\"\"\n        for name, channel in list(self._channels_dict.items()):\n            if channel is not None:\n                try:\n                    await channel.shutdown()\n                    logger.info(f\"Channel {name} shutdown successfully\")\n                except Exception as e:\n                    logger.error(f\"Error shutting down {name}: {e}\")\n        \n        self._channels_dict.clear()\n\n    @handle_errors(\"stopping all channels\", default_return=False)\n    def stop_all(self):\n        \"\"\"\n        Stop all communication channels with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        \"\"\"Stop all communication channels\"\"\"\n        logger.debug(\"Stopping all communication channels.\")\n        \n        try:\n            # Stop the retry thread first\n            self.stop_all__stop_retry_thread()\n            \n            # Stop the restart monitor\n            self.stop_all__stop_restart_monitor()\n            \n            # Try async shutdown first\n            try:\n                loop = asyncio.get_running_loop()\n                # If there's a running loop, just do sync shutdown\n                self._shutdown_sync()\n            except RuntimeError:\n                # No running loop, safe to create one\n                try:\n                    loop = asyncio.new_event_loop()\n                    asyncio.set_event_loop(loop)\n                    # Add timeout to prevent hanging\n                    try:\n                        loop.run_until_complete(asyncio.wait_for(self._shutdown_all_async(), timeout=15.0))\n                    except asyncio.TimeoutError:\n                        logger.warning(\"Channel shutdown timed out after 15 seconds - forcing sync shutdown\")\n                        self._shutdown_sync()\n                    loop.close()\n                except Exception as e:\n                    logger.error(f\"Error in async shutdown: {e}\")\n                    # Fallback to sync shutdown\n                    self._shutdown_sync()\n            \n            self._running = False\n            logger.info(\"All channels stopped successfully\")\n            \n        except Exception as e:\n            logger.error(f\"Error during shutdown: {e}\")\n            # Final fallback\n            self._shutdown_sync()\n\n    @handle_errors(\"shutting down all channels (sync)\", user_friendly=False, default_return=None)\n    def _shutdown_sync(self):\n        \"\"\"\n        Synchronous shutdown method for all channels.\n        \n        Stops all communication channels and cleans up resources.\n        \"\"\"\n        logger.info(\"Shutting down CommunicationManager...\")\n        self._running = False\n        \n        # Stop email polling\n        self.stop_all__stop_email_polling()\n        \n        # Stop all channels\n        for name, channel in self._channels_dict.items():\n            try:\n                if hasattr(channel, 'shutdown'):\n                    # Add timeout to prevent hanging\n                    try:\n                        asyncio.run(asyncio.wait_for(channel.shutdown(), timeout=10.0))\n                    except asyncio.TimeoutError:\n                        logger.warning(f\"Channel {name} shutdown timed out after 10 seconds\")\n                    except RuntimeError:\n                        # Event loop already running - try sync approach\n                        logger.warning(f\"Could not run async shutdown for {name} - event loop conflict\")\n                elif hasattr(channel, 'stop'):\n                    channel.stop()  # Fallback for any remaining legacy channels\n                logger.debug(f\"Channel {name} stopped\")\n            except Exception as e:\n                logger.error(f\"Error stopping channel {name}: {e}\")\n        \n        # Stop event loop\n        if self._main_loop and not self._main_loop.is_closed():\n            try:\n                self._main_loop.call_soon_threadsafe(self._main_loop.stop)\n                if self._loop_thread and self._loop_thread.is_alive():\n                    self._loop_thread.join(timeout=5)\n            except Exception as e:\n                logger.error(f\"Error stopping event loop: {e}\")\n        \n        logger.info(\"CommunicationManager shutdown complete\")\n\n    @handle_errors(\"receiving messages\", default_return=[])\n    async def receive_messages(self) -> list[dict[str, Any]]:\n        \"\"\"Receive messages from all communication channels\"\"\"\n        logger.debug(\"Receiving messages from all communication channels.\")\n        all_messages = []\n        \n        for channel_name, channel in self._channels_dict.items():\n            if not channel.is_ready():\n                continue\n                \n            try:\n                messages = await channel.receive_messages()\n                # Add channel info to each message\n                for msg in messages:\n                    msg['channel'] = channel_name\n                all_messages.extend(messages)\n            except Exception as e:\n                logger.error(f\"Failed to receive messages from {channel_name}: {e}\")\n\n        logger.info(f\"Total messages received: {len(all_messages)}\")\n        return all_messages\n\n    @handle_errors(\"handling message sending\", default_return=None)\n    def handle_message_sending(self, user_id: str, category: str):\n        \"\"\"\n        Handle sending messages for a user and category with improved recipient resolution.\n        Now uses scheduled check-ins instead of random replacement.\n        \"\"\"\n        logger.debug(f\"Handling message sending for user_id: {user_id}, category: {category}\")\n        \n        if not user_id:\n            logger.error(\"User ID is not provided.\")\n            return\n\n        # Get user preferences\n        prefs_result = get_user_data(user_id, 'preferences', normalize_on_read=True)\n        preferences = prefs_result.get('preferences')\n        if not preferences:\n            logger.error(f\"User preferences not found for user {user_id}.\")\n            return\n\n        messaging_service = preferences.get('channel', {}).get('type')\n        if not messaging_service:\n            logger.error(f\"No messaging service configured for user {user_id}\")\n            return\n\n        # Get the appropriate recipient ID for the messaging service\n        recipient = self._get_recipient_for_service(user_id, messaging_service, preferences)\n        if not recipient:\n            logger.error(f\"No valid recipient found for user {user_id} with service {messaging_service}\")\n            return\n\n        # Handle check-in category specially\n        if category == \"checkin\":\n            self._handle_scheduled_checkin(user_id, messaging_service, recipient)\n            return\n\n        # Handle AI-generated messages and track if message was actually sent\n        message_sent = False\n        sent_message_content = None\n        if category in [\"personalized\", \"ai_personalized\"]:\n            message_sent, sent_message_content = self._send_ai_generated_message(user_id, category, messaging_service, recipient)\n        else:\n            message_sent, sent_message_content = self._send_predefined_message(user_id, category, messaging_service, recipient)\n        \n        # Store sent message content for response file (if this is a test message request)\n        if sent_message_content:\n            self._last_sent_message = {\n                'user_id': user_id,\n                'category': category,\n                'message': sent_message_content\n            }\n        \n        # CRITICAL: Only expire check-in flows if a message was actually sent and delivered\n        # Don't expire flows for failed sends or when no message was available to send\n        if message_sent:\n            # Expire active check-in flows ONLY for non-scheduled messages to avoid user confusion\n            # Scheduled messages (motivational, health, etc.) are expected and shouldn't cancel active check-ins\n            # Only cancel check-in flows when we're responding to user input with unrelated content\n            self._expire_checkin_flow_if_needed(user_id, category)\n            \n            # Message sending completion already logged above with full details\n        else:\n            logger.debug(f\"No message sent for user {user_id}, category {category} - preserving any active check-in flow\")\n\n    @handle_errors(\"expiring check-in flow if needed\", user_friendly=False, default_return=None)\n    def _expire_checkin_flow_if_needed(self, user_id: str, category: str):\n        \"\"\"Expire check-in flow if this is a non-scheduled message.\"\"\"\n        # Determine if this is a scheduled message or a response to user input\n        is_scheduled_message = category in ['motivational', 'health', 'checkin', 'task_reminders']\n        \n        if not is_scheduled_message:\n            # This is a response to user input - expire any active check-in flow\n            from communication.message_processing.conversation_flow_manager import conversation_manager\n            conversation_manager.expire_checkin_flow_due_to_unrelated_outbound(user_id)\n            logger.debug(f\"Expired check-in flow for user {user_id} due to unrelated response message\")\n        else:\n            logger.debug(f\"Skipping check-in flow expiration for scheduled {category} message to user {user_id}\")\n\n    @handle_errors(\"getting recipient for service\", default_return=None)\n    def _get_recipient_for_service(self, user_id: str, messaging_service: str, preferences: dict) -> str | None:\n        \"\"\"\n        Get recipient for service with validation.\n        \n        Returns:\n            Optional[str]: Recipient ID, None if failed\n        \"\"\"\n        # Validate user_id\n        if not user_id or not isinstance(user_id, str):\n            logger.error(f\"Invalid user_id: {user_id}\")\n            return None\n            \n        if not user_id.strip():\n            logger.error(\"Empty user_id provided\")\n            return None\n            \n        # Validate messaging_service\n        if not messaging_service or not isinstance(messaging_service, str):\n            logger.error(f\"Invalid messaging_service: {messaging_service}\")\n            return None\n            \n        if not messaging_service.strip():\n            logger.error(\"Empty messaging_service provided\")\n            return None\n            \n        # Validate preferences\n        if not preferences or not isinstance(preferences, dict):\n            logger.error(f\"Invalid preferences: {preferences}\")\n            return None\n        \"\"\"Get the appropriate recipient ID for the messaging service\"\"\"\n        if messaging_service == \"discord\":\n            # For Discord, we need to get the channel ID, not the user ID\n            # The Discord bot handles user ID mapping internally\n            # We'll use a special marker that the Discord bot can recognize\n            return f\"discord_user:{user_id}\"\n\n            return None\n        elif messaging_service == \"email\":\n            # Get email from account.json, not preferences\n            # Get user account data\n            user_data_result = get_user_data(user_id, 'account', normalize_on_read=True)\n            account_data = user_data_result.get('account')\n            if not account_data:\n                logger.error(f\"User account not found for {user_id}\")\n                return False\n            return account_data.get('email', '') if account_data else None\n        else:\n            logger.error(f\"Unknown messaging service: {messaging_service}\")\n            return None\n\n    @handle_errors(\"determining if check-in prompt should be sent\", default_return=True)\n    def _should_send_checkin_prompt(self, user_id: str, checkin_prefs: dict) -> bool:\n        \"\"\"\n        Determine if it's time to send a check-in prompt based on user preferences.\n        For check-ins, we respect the schedule-based approach - if the scheduler\n        triggered this function, it means it's time for a check-in during the\n        scheduled period.\n        \"\"\"\n        try:\n            frequency = checkin_prefs.get('frequency', 'daily')\n            \n            # If frequency is set to \"none\" or \"manual\", never auto-prompt\n            if frequency in ['none', 'manual']:\n                logger.debug(f\"User {user_id} has check-in frequency set to '{frequency}', skipping auto-prompt\")\n                return False\n            \n            # For check-ins, we trust the scheduler to determine timing\n            # The scheduler only calls this during the scheduled time period\n            # So if we get here, it's time for a check-in\n            logger.debug(f\"Check-in scheduled for user {user_id} during scheduled time period\")\n            return True\n                \n        except Exception as e:\n            logger.error(f\"Error determining if check-in prompt should be sent for user {user_id}: {e}\")\n            # Default to sending check-in if there's an error\n            return True\n\n    @handle_errors(\"handling scheduled check-in\", user_friendly=False, default_return=None)\n    def _handle_scheduled_checkin(self, user_id: str, messaging_service: str, recipient: str):\n        \"\"\"\n        Handle scheduled check-in messages based on user preferences and frequency.\n        \"\"\"\n        prefs_result = get_user_data(user_id, 'preferences', normalize_on_read=True)\n        preferences = prefs_result.get('preferences')\n        if not preferences:\n            logger.error(f\"User preferences not found for user {user_id}\")\n            return\n        \n        # Check if check-ins are enabled in account features\n        # Get user account data\n        user_data_result = get_user_data(user_id, 'account', normalize_on_read=True)\n        account_data = user_data_result.get('account')\n        if not account_data or account_data.get('features', {}).get('checkins') != 'enabled':\n            logger.debug(f\"Check-ins disabled for user {user_id}\")\n            return\n        \n        checkin_prefs = preferences.get('checkin_settings', {})\n        \n        # Check frequency and last check-in time\n        frequency = checkin_prefs.get('frequency', 'daily')\n        \n        # If frequency is \"none\", don't send scheduled check-ins\n        if frequency == 'none':\n            logger.debug(f\"Check-in frequency set to 'none' for user {user_id}, skipping scheduled check-in\")\n            return\n        \n        # Check if it's time for a check-in based on frequency\n        if self._should_send_checkin_prompt(user_id, checkin_prefs):\n            self._send_checkin_prompt(user_id, messaging_service, recipient)\n            logger.info(f\"Sent scheduled check-in prompt to user {user_id}\")\n        else:\n            logger.debug(f\"Check-in not due yet for user {user_id}\")\n\n    @handle_errors(\"sending check-in prompt\", default_return=None)\n    def _send_checkin_prompt(self, user_id: str, messaging_service: str, recipient: str):\n        \"\"\"\n        Send a check-in prompt message to start the check-in flow.\n        \"\"\"\n        try:\n            # Initialize the dynamic check-in flow properly\n            from communication.message_processing.conversation_flow_manager import conversation_manager\n            \n            # Initialize flow without logging start yet; log only after successful send\n            reply_text, completed = conversation_manager._start_dynamic_checkin(user_id)\n            \n            # Create custom view for check-in buttons (Discord only)\n            # Note: Views are created lazily within the Discord async context to avoid event loop errors\n            custom_view = None\n            if messaging_service == 'discord':\n                try:\n                    # Import here to avoid circular dependencies\n                    from communication.communication_channels.discord.checkin_view import get_checkin_view\n                    # Try to create view, but handle event loop errors gracefully\n                    import asyncio\n                    try:\n                        # Check if we're in an async context\n                        asyncio.get_running_loop()\n                        # We're in async context, safe to create view\n                        custom_view = get_checkin_view(user_id)\n                    except RuntimeError:\n                        # No running event loop - view will be created lazily in Discord thread\n                        # Pass a factory function instead\n                        @handle_errors(\"creating check-in view\", default_return=None)\n                        def create_view():\n                            return get_checkin_view(user_id)\n                        custom_view = create_view\n                except Exception as e:\n                    logger.warning(f\"Could not create check-in view for Discord: {e}\")\n            \n            # Send the initial message to the user with retry support\n            success = self.send_message_sync(messaging_service, recipient, reply_text, \n                                           user_id=user_id, category=\"checkin\", view=custom_view)\n            \n            if success:\n                logger.info(f\"Successfully sent check-in prompt to user {user_id} and initialized flow\")\n                # Now record the check-in start in user activity\n                try:\n                    user_logger = get_component_logger('user_activity')\n                    user_logger.info(\"User check-in started\", user_id=user_id, checkin_type=\"daily\")\n                except Exception:\n                    pass\n            else:\n                logger.warning(f\"Failed to send check-in prompt to user {user_id}\")\n                # Preserve conversation state so we can retry sending when channel recovers\n                \n        except Exception as e:\n            logger.error(f\"Error sending check-in prompt to user {user_id}: {e}\")\n            # Preserve conversation state to allow retry after recovery\n\n    @handle_errors(\"sending AI-generated message\", default_return=(False, None))\n    def _send_ai_generated_message(self, user_id: str, category: str, messaging_service: str, recipient: str) -> tuple[bool, str | None]:\n        \"\"\"\n        Send an AI-generated personalized message using contextual AI.\n        \n        Returns:\n            tuple[bool, str | None]: (success, message_content) - True if sent successfully, and the message content that was sent\n        \"\"\"\n        try:\n            from ai.chatbot import get_ai_chatbot\n            ai_bot = get_ai_chatbot()\n            \n            # Use contextual AI for richer, more personalized messages\n            if category in [\"personalized\", \"ai_personalized\"]:\n                # Create a contextual prompt for personalized message generation\n                context_prompt = \"Generate a supportive, personalized message based on my recent activity and mood.\"\n                message_to_send = ai_bot.generate_contextual_response(user_id, context_prompt, timeout=15)\n            else:\n                # Fallback to standard personalized message\n                message_to_send = ai_bot.generate_personalized_message(user_id)\n            \n            import uuid\n            message_id = str(uuid.uuid4())\n\n            success = self.send_message_sync(messaging_service, recipient, message_to_send, \n                                           user_id=user_id, category=category)\n            if success:\n                # Get current time period for storage\n                matching_periods, valid_periods = get_current_time_periods_with_validation(user_id, category)\n                current_time_period = matching_periods[0] if matching_periods else None\n                store_sent_message(user_id, category, message_id, message_to_send, time_period=current_time_period)\n                # Enhanced logging with message content\n                message_preview = message_to_send[:50] + \"...\" if len(message_to_send) > 50 else message_to_send\n                logger.info(f\"Sent contextual AI-generated message for user {user_id}, category {category} | Content: '{message_preview}'\")\n                return True, message_to_send\n            else:\n                logger.error(f\"Failed to send AI-generated message for user {user_id}\")\n                return False, None\n                \n        except Exception as e:\n            logger.error(f\"Error sending AI-generated message for user {user_id}: {e}\")\n            return False, None\n\n    @handle_errors(\"sending predefined message\", default_return=(False, None))\n    def _send_predefined_message(self, user_id: str, category: str, messaging_service: str, recipient: str) -> tuple[bool, str | None]:\n        \"\"\"\n        Send a pre-defined message from the user's message library with deduplication.\n        \n        Returns:\n            tuple[bool, str | None]: (success, message_content) - True if sent successfully, and the message content that was sent\n        \"\"\"\n        try:\n            matching_periods, valid_periods = get_current_time_periods_with_validation(user_id, category)\n            logger.debug(f\"MESSAGE_SELECTION: User {user_id}, category {category} | Matching periods: {matching_periods}, Valid periods: {valid_periods}\")\n            \n            # Remove 'ALL' from matching_periods if there are other periods\n            if 'ALL' in matching_periods and len(matching_periods) > 1:\n                matching_periods = [p for p in matching_periods if p != 'ALL']\n                logger.debug(f\"MESSAGE_SELECTION: Removed 'ALL' from matching_periods, now: {matching_periods}\")\n            # If no periods match (other than ALL), use ALL as fallback\n            if not matching_periods and 'ALL' in valid_periods:\n                matching_periods = ['ALL']\n                logger.debug(f\"MESSAGE_SELECTION: Using 'ALL' as fallback period\")\n\n            # Use new user-specific message file structure\n            from pathlib import Path\n            user_messages_dir = Path(get_user_data_dir(user_id)) / 'messages'\n            file_path = user_messages_dir / f\"{category}.json\"\n            data = load_json_data(str(file_path))  # Convert Path to string\n            # Normalize messages file shape for robust selection\n            try:\n                from core.schemas import validate_messages_file_dict\n                if isinstance(data, dict):\n                    data, _ = validate_messages_file_dict(data)\n            except Exception:\n                pass\n\n            if not data or 'messages' not in data:\n                logger.error(f\"MESSAGE_SELECTION_ERROR: No messages found for category {category} and user {user_id}.\")\n                return False, None\n\n            # Get current day for filtering\n            current_days = get_current_day_names()\n            logger.debug(f\"MESSAGE_SELECTION: Current days: {current_days}\")\n            logger.debug(f\"MESSAGE_SELECTION: Total messages in library: {len(data['messages'])}\")\n\n            # Get all available messages for the current time period\n            all_messages = [\n                msg for msg in data['messages']\n                if any(day in msg['days'] for day in current_days)\n                and any(period in msg['time_periods'] for period in matching_periods)\n            ]\n            \n            if not all_messages:\n                logger.warning(f\"MESSAGE_SELECTION_NO_MATCH: No messages found for user {user_id}, category {category} | Current days: {current_days}, Matching periods: {matching_periods}, Total messages: {len(data['messages'])}\")\n                # Sample first 3 messages to show what we're looking for\n                sample_messages = data['messages'][:3]\n                for i, msg in enumerate(sample_messages):\n                    logger.debug(f\"MESSAGE_SELECTION_SAMPLE_{i}: days={msg.get('days')}, time_periods={msg.get('time_periods')}, message_preview='{msg.get('message', '')[:50]}'\")\n                return False, None\n\n            # ENHANCED: Apply deduplication logic to time-period-filtered messages\n            from core.message_management import get_recent_messages\n            \n            # Get recent messages to check for duplicates\n            recent_messages = get_recent_messages(user_id, category=category, limit=50, days_back=60)\n            recent_content = {msg.get('message', '').strip().lower() for msg in recent_messages if msg.get('message')}\n            \n            # Filter out recent duplicates from time-period-filtered messages\n            available_messages = []\n            for msg in all_messages:\n                message_content = msg.get('message', '').strip()\n                if message_content and message_content.lower() not in recent_content:\n                    available_messages.append(msg)\n            \n            if not available_messages:\n                logger.info(f\"No messages available after deduplication for user {user_id}, category {category}. All time-period messages were sent recently.\")\n                # Fallback: if all time-period messages are recent, select from all time-period messages\n                available_messages = all_messages\n                logger.info(f\"Using fallback: selecting from all {len(available_messages)} time-period messages\")\n            \n            # Select a message using weighted selection (prioritizes specific time periods over 'ALL')\n            message_to_send = self._select_weighted_message(available_messages, matching_periods)\n            logger.debug(f\"Selected message for user {user_id}, category {category} from {len(available_messages)} available messages\")\n            \n            # IMPROVED: Better success/failure tracking\n            try:\n                success = self.send_message_sync(messaging_service, recipient, message_to_send['message'], \n                                               user_id=user_id, category=category)\n                \n                if success:\n                    from core.message_management import store_sent_message\n                    # Get the current time period for storage\n                    current_time_period = matching_periods[0] if matching_periods else None\n                    store_sent_message(user_id, category, message_to_send['message_id'], message_to_send['message'], time_period=current_time_period)\n                    # Enhanced logging with message content and time period\n                    message_preview = message_to_send['message'][:50] + \"...\" if len(message_to_send['message']) > 50 else message_to_send['message']\n                    logger.info(f\"Message sent successfully via {messaging_service} to {recipient} | User: {user_id}, Category: {category}, Period: {current_time_period} | Content: '{message_preview}'\")\n                    return True, message_to_send['message']\n                else:\n                    # Enhanced logging with message content and time period\n                    current_time_period = matching_periods[0] if matching_periods else None\n                    message_preview = message_to_send['message'][:50] + \"...\" if len(message_to_send['message']) > 50 else message_to_send['message']\n                    logger.warning(f\"Message send returned False but may have still been delivered for user {user_id}, category {category} | Period: {current_time_period} | Content: '{message_preview}'\")\n                    # Still store it since the message might have gone through\n                    from core.message_management import store_sent_message\n                    store_sent_message(user_id, category, message_to_send['message_id'], message_to_send['message'], time_period=current_time_period)\n                    return True, message_to_send['message']  # Message was attempted and likely delivered\n                    \n            except Exception as send_error:\n                logger.error(f\"Exception during message send for user {user_id}, category {category}: {send_error}\")\n                # Don't store the message if there was an exception\n                return False, None\n\n        except Exception as e:\n            logger.error(f\"Error in predefined message handling for user {user_id}, category {category}: {e}\")\n            return False, None\n\n\n    # NEW METHODS: More specific channel management methods\n    @handle_errors(\"getting active channels\", default_return=[])\n    def get_active_channels(self) -> list[str]:\n        \"\"\"\n        Get active channels with validation.\n        \n        Returns:\n            List[str]: List of active channels, empty list if failed\n        \"\"\"\n        \"\"\"Get list of currently active/running channels\"\"\"\n        return list(self._channels_dict.keys())\n    \n    @handle_errors(\"getting configured channels\", default_return=[])\n    def get_configured_channels(self) -> list[str]:\n        \"\"\"\n        Get configured channels with validation.\n        \n        Returns:\n            List[str]: List of configured channels, empty list if failed\n        \"\"\"\n        \"\"\"Get list of channels that are configured (from config)\"\"\"\n        from core.config import get_available_channels\n        return get_available_channels()\n    \n    @handle_errors(\"getting registered channels\", default_return=[])\n    def get_registered_channels(self) -> list[str]:\n        \"\"\"\n        Get registered channels with validation.\n        \n        Returns:\n            List[str]: List of registered channels, empty list if failed\n        \"\"\"\n        \"\"\"Get list of channels that are registered in the factory\"\"\"\n        from communication.core.factory import ChannelFactory\n        return ChannelFactory.get_registered_channels()\n\n    @handle_errors(\"handling task reminder\", default_return=None)\n    def handle_task_reminder(self, user_id: str, task_id: str):\n        \"\"\"\n        Handle task reminder with validation.\n        \n        Returns:\n            None: Always returns None\n        \"\"\"\n        # Validate user_id\n        if not user_id or not isinstance(user_id, str):\n            logger.error(f\"Invalid user_id: {user_id}\")\n            return None\n            \n        if not user_id.strip():\n            logger.error(\"Empty user_id provided\")\n            return None\n            \n        # Validate task_id\n        if not task_id or not isinstance(task_id, str):\n            logger.error(f\"Invalid task_id: {task_id}\")\n            return None\n            \n        if not task_id.strip():\n            logger.error(\"Empty task_id provided\")\n            return None\n        \"\"\"\n        Handle sending task reminders for a user.\n        \"\"\"\n        logger.debug(f\"Handling task reminder for user_id: {user_id}, task_id: {task_id}\")\n        \n        if not user_id or not task_id:\n            logger.error(\"User ID and task ID are required for task reminder.\")\n            return\n\n        # Import task management functions\n        from tasks.task_management import get_task_by_id, are_tasks_enabled\n        \n        # Check if tasks are enabled for this user\n        if not are_tasks_enabled(user_id):\n            logger.debug(f\"Tasks not enabled for user {user_id}\")\n            return\n\n        # Get the task details\n        task = get_task_by_id(user_id, task_id)\n        if not task:\n            logger.error(f\"Task {task_id} not found for user {user_id}\")\n            return\n\n        # Check if task is still active\n        if task.get('completed', False):\n            logger.debug(f\"Task {task_id} is already completed, skipping reminder\")\n            return\n\n        # Get user preferences\n        prefs_result = get_user_data(user_id, 'preferences')\n        preferences = prefs_result.get('preferences')\n        if not preferences:\n            logger.error(f\"User preferences not found for user {user_id}.\")\n            return\n\n        messaging_service = preferences.get('channel', {}).get('type')\n        if not messaging_service:\n            logger.error(f\"No messaging service configured for user {user_id}\")\n            return\n\n        # Get the appropriate recipient ID for the messaging service\n        recipient = self._get_recipient_for_service(user_id, messaging_service, preferences)\n        if not recipient:\n            logger.error(f\"No valid recipient found for user {user_id} with service {messaging_service}\")\n            return\n\n        # Create the task reminder message\n        reminder_message = self._create_task_reminder_message(task)\n        \n        # Create custom view for task reminder buttons (Discord only)\n        # Note: Views are created lazily within the Discord async context to avoid event loop errors\n        custom_view = None\n        if messaging_service == 'discord':\n            try:\n                from communication.communication_channels.discord.task_reminder_view import get_task_reminder_view\n                task_title = task.get('title', 'Untitled Task')\n                # Try to create view, but handle event loop errors gracefully\n                import asyncio\n                try:\n                    # Check if we're in an async context\n                    asyncio.get_running_loop()\n                    # We're in async context, safe to create view\n                    custom_view = get_task_reminder_view(user_id, task_id, task_title)\n                except RuntimeError:\n                    # No running event loop - view will be created lazily in Discord thread\n                    # Pass a factory function instead\n                    @handle_errors(\"creating task reminder view\", default_return=None)\n                    def create_view():\n                        return get_task_reminder_view(user_id, task_id, task_title)\n                    custom_view = create_view\n            except Exception as e:\n                logger.warning(f\"Could not create task reminder view for Discord: {e}\")\n        \n        # Send the reminder\n        success = self.send_message_sync(messaging_service, recipient, reminder_message, view=custom_view)\n        \n        if success:\n            logger.info(f\"Task reminder sent successfully for user {user_id}, task {task_id}\")\n            # Track the last task reminder for this user\n            self._last_task_reminders[user_id] = task_id\n        else:\n            logger.error(f\"Failed to send task reminder for user {user_id}, task {task_id}\")\n\n    @handle_errors(\"getting last task reminder\", default_return=None)\n    def get_last_task_reminder(self, user_id: str) -> str | None:\n        \"\"\"\n        Get last task reminder with validation.\n        \n        Returns:\n            Optional[str]: Last task reminder, None if failed\n        \"\"\"\n        # Validate user_id\n        if not user_id or not isinstance(user_id, str):\n            logger.error(f\"Invalid user_id: {user_id}\")\n            return None\n            \n        if not user_id.strip():\n            logger.error(\"Empty user_id provided\")\n            return None\n        \"\"\"\n        Get the task ID of the last task reminder sent to a user.\n        \n        Args:\n            user_id: The user's ID\n            \n        Returns:\n            The task ID of the last reminder, or None if no reminder was sent\n        \"\"\"\n        return self._last_task_reminders.get(user_id)\n\n    @handle_errors(\"creating task reminder message\", default_return=\"Task reminder\")\n    def _create_task_reminder_message(self, task: dict) -> str:\n        \"\"\"\n        Create task reminder message with validation.\n        \n        Returns:\n            str: Task reminder message, default if failed\n        \"\"\"\n        # Validate task\n        if not task or not isinstance(task, dict):\n            logger.error(f\"Invalid task: {task}\")\n            return \"Task reminder\"\n        \"\"\"\n        Create a formatted task reminder message.\n        \"\"\"\n        title = task.get('title', 'Untitled Task')\n        description = task.get('description', '')\n        due_date = task.get('due_date', '')\n        priority = task.get('priority', 'medium')\n        task_id = task.get('task_id', '')\n        # Tasks now use tags instead of categories\n        \n        # Create priority emoji\n        priority_emoji = {\n            'low': '\ud83d\udfe2',\n            'medium': '\ud83d\udfe1', \n            'high': '\ud83d\udd34',\n            'critical': '\ud83d\udea8'\n        }.get(priority, '\ud83d\udfe1')\n        \n        # Build the message\n        message = f\"\ud83d\udca1 **Task Reminder:** {priority_emoji}\\n\\n\"\n        message += f\"**{title}**\\n\"\n        \n        if description:\n            message += f\"{description}\\n\\n\"\n        \n        if due_date:\n            message += f\"\ud83d\udcc5 **Due:** {due_date}\\n\"\n        \n        message += f\"\u26a1 **Priority:** {priority.title()}\"\n        \n        return message\n\n    @handle_errors(\"selecting weighted message\", default_return=\"\")\n    def _select_weighted_message(self, available_messages, matching_periods):\n        \"\"\"\n        Select weighted message with validation.\n        \n        Returns:\n            str: Selected message, empty string if failed\n        \"\"\"\n        # Validate available_messages\n        if not available_messages or not isinstance(available_messages, list):\n            logger.error(f\"Invalid available_messages: {available_messages}\")\n            return \"\"\n            \n        # Validate matching_periods\n        if not matching_periods or not isinstance(matching_periods, list):\n            logger.error(f\"Invalid matching_periods: {matching_periods}\")\n            return \"\"\n        \"\"\"\n        Select a message using a weighting system that prioritizes\n        messages with specific time periods over 'ALL' time periods.\n        \n        Args:\n            available_messages: List of available messages\n            matching_periods: List of current matching time periods\n            \n        Returns:\n            Selected message\n        \"\"\"\n        import random\n        \n        if not available_messages:\n            return None\n        \n        # Separate messages into two groups:\n        # 1. Messages with specific time periods (higher priority)\n        # 2. Messages with only 'ALL' time periods (lower priority)\n        \n        specific_period_messages = []\n        all_period_messages = []\n        \n        for msg in available_messages:\n            time_periods = msg.get('time_periods', [])\n            # Check if message has any specific time periods (not just 'ALL')\n            has_specific_periods = any(period != 'ALL' for period in time_periods)\n            \n            if has_specific_periods:\n                specific_period_messages.append(msg)\n            else:\n                all_period_messages.append(msg)\n        \n        # Weighted selection: 70% chance for specific periods, 30% chance for 'ALL' periods\n        if specific_period_messages and random.random() < 0.7:\n            # Select from specific period messages\n            selected_message = random.choice(specific_period_messages)\n            logger.debug(f\"Selected message with specific time periods (weighted selection)\")\n        elif all_period_messages:\n            # Select from 'ALL' period messages\n            selected_message = random.choice(all_period_messages)\n            logger.debug(f\"Selected message with 'ALL' time periods (weighted selection)\")\n        else:\n            # Fallback to any available message\n            selected_message = random.choice(available_messages)\n            logger.debug(f\"Selected message (fallback selection)\")\n        \n        return selected_message\n\n",
                [
                  {
                    "pattern": "(?i)legacy channels",
                    "match": "legacy channels",
                    "line": 1042,
                    "line_content": "channel.stop()  # Fallback for any remaining legacy channels",
                    "start": 49609,
                    "end": 49624
                  }
                ]
              ],
              [
                "communication\\communication_channels\\discord\\bot.py",
                "# communication/communication_channels/discord/bot.py\n\nimport discord\nfrom discord import app_commands\nimport asyncio\nimport threading\nfrom discord.ext import commands\nfrom typing import List, Dict, Any, Optional\nimport queue\nimport time\nimport socket\nimport enum\nimport contextlib\nimport subprocess\nimport shutil\nimport os\nimport psutil\n\nfrom core.config import DISCORD_BOT_TOKEN, DISCORD_APPLICATION_ID\nfrom core.logger import get_component_logger\nfrom communication.communication_channels.base.base_channel import BaseChannel, ChannelType, ChannelStatus, ChannelConfig\nfrom core.user_data_handlers import get_user_id_by_identifier\nfrom core.error_handling import handle_errors\n\n# Route all Discord module logs to the Discord component logger so they appear in logs/discord.log\ndiscord_logger = get_component_logger('discord')\nlogger = discord_logger\n\nintents = discord.Intents.default()\nintents.messages = True\nintents.message_content = True\n\nclass DiscordConnectionStatus(enum.Enum):\n    \"\"\"Detailed Discord connection status for better error reporting\"\"\"\n    UNINITIALIZED = \"uninitialized\"\n    INITIALIZING = \"initializing\"\n    CONNECTED = \"connected\"\n    DISCONNECTED = \"disconnected\"\n    DNS_FAILURE = \"dns_failure\"\n    NETWORK_FAILURE = \"network_failure\"\n    AUTH_FAILURE = \"authentication_failure\"\n    RATE_LIMITED = \"rate_limited\"\n    GATEWAY_ERROR = \"gateway_error\"\n    UNKNOWN_ERROR = \"unknown_error\"\n\nclass DiscordBot(BaseChannel):\n    @handle_errors(\"initializing Discord bot\", default_return=None)\n    def __init__(self, config: ChannelConfig | None = None):\n        # Initialize BaseChannel\n        \"\"\"Initialize the object.\"\"\"\n        if config is None:\n            config = ChannelConfig(\n                name='discord',\n                max_retries=5,  # Increased from 3\n                retry_delay=2.0,  # Increased from 1.5\n                backoff_multiplier=2.0\n            )\n        super().__init__(config)\n        \n        self.bot = None\n        self.discord_thread = None\n        self._loop = None\n        self._starting = False\n        self._command_queue = queue.Queue()  # For sending commands to Discord thread\n        self._result_queue = queue.Queue()   # For getting results back\n        self._reconnect_attempts = 0\n        self._max_reconnect_attempts = 10\n        self._last_reconnect_time = 0\n        self._reconnect_cooldown = 60  # Increased to 60 seconds to reduce rapid reconnection attempts\n        self._connection_status = DiscordConnectionStatus.UNINITIALIZED\n        self._last_health_check = 0\n        self._health_check_interval = 30  # Check health every 30 seconds\n        self._detailed_error_info = {}\n        # Idempotency flags\n        self._events_registered = False\n        self._commands_registered = False\n        # Session management\n        self._sessions_to_cleanup = []\n        # Task management for proper cleanup\n        self._sync_task = None\n        # Webhook server for receiving installation events\n        self._webhook_server = None\n        # Ngrok process for webhook tunneling (auto-launched if enabled)\n        self._ngrok_process = None\n        self._ngrok_pid = None  # Store PID separately in case process reference is lost\n        self._on_ready_fired = False  # Track if on_ready() event has fired\n        # Ensure BaseChannel logs for this instance also go to the Discord component log\n        self.logger = discord_logger\n\n    @property\n    @handle_errors(\"getting Discord channel type\", default_return=ChannelType.ASYNC)\n    def channel_type(self) -> ChannelType:\n        \"\"\"\n        Get the channel type for Discord bot.\n        \n        Returns:\n            ChannelType.ASYNC: Discord bot operates asynchronously\n        \"\"\"\n        return ChannelType.ASYNC\n\n    @handle_errors(\"checking DNS resolution\", default_return=False)\n    def _check_dns_resolution(self, hostname: str = \"discord.com\") -> bool:\n        \"\"\"\n        Check DNS resolution with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        # Validate hostname\n        if not hostname or not isinstance(hostname, str):\n            logger.error(f\"Invalid hostname: {hostname}\")\n            return False\n            \n        if not hostname.strip():\n            logger.error(\"Empty hostname provided\")\n            return False\n        \"\"\"Check DNS resolution for a hostname with enhanced fallback and error reporting\"\"\"\n        # Alternative DNS servers to try if primary fails\n        alternative_dns_servers = [\n            \"8.8.8.8\",      # Google DNS\n            \"1.1.1.1\",      # Cloudflare DNS\n            \"208.67.222.222\", # OpenDNS\n            \"9.9.9.9\"       # Quad9 DNS\n        ]\n        \n        # Try primary DNS first (system default)\n        try:\n            socket.gethostbyname(hostname)\n            # Only log DNS success occasionally to reduce log noise\n            if hasattr(self, '_dns_success_count'):\n                self._dns_success_count += 1\n            else:\n                self._dns_success_count = 1\n            \n            # Log DNS success every 60th check to reduce noise\n            if self._dns_success_count % 60 == 0:\n                logger.debug(f\"Primary DNS resolution successful for {hostname} (check #{self._dns_success_count})\")\n            return True\n        except socket.gaierror as e:\n            primary_error = {\n                'hostname': hostname,\n                'error_code': e.errno,\n                'error_message': str(e),\n                'timestamp': time.time(),\n                'dns_server': 'system_default'\n            }\n            \n            logger.warning(f\"Primary DNS failed for {hostname}: {e}\")\n            \n            # Try alternative DNS servers\n            for dns_server in alternative_dns_servers:\n                try:\n                    logger.info(f\"Trying alternative DNS server {dns_server} for {hostname}\")\n                    \n                    # Create a custom resolver using the alternative DNS server\n                    import dns.resolver\n                    resolver = dns.resolver.Resolver()\n                    resolver.nameservers = [dns_server]\n                    resolver.timeout = 5\n                    resolver.lifetime = 10\n                    \n                    # Try to resolve the hostname\n                    answers = resolver.resolve(hostname, 'A')\n                    if answers:\n                        ip_address = str(answers[0])\n                        logger.info(f\"Successfully resolved {hostname} to {ip_address} using {dns_server}\")\n                        \n                        # Update error info to show which DNS server worked\n                        self._detailed_error_info['dns_error'] = {\n                            'hostname': hostname,\n                            'primary_error': primary_error,\n                            'resolved_with': dns_server,\n                            'resolved_ip': ip_address,\n                            'timestamp': time.time()\n                        }\n                        return True\n                        \n                except Exception as alt_e:\n                    logger.debug(f\"Alternative DNS {dns_server} also failed: {alt_e}\")\n                    continue\n            \n            # All DNS servers failed\n            self._detailed_error_info['dns_error'] = {\n                'hostname': hostname,\n                'primary_error': primary_error,\n                'alternative_dns_failed': alternative_dns_servers,\n                'timestamp': time.time()\n            }\n            logger.error(f\"All DNS servers failed for {hostname}\")\n            self._connection_status = DiscordConnectionStatus.DNS_FAILURE\n            return False\n\n    @handle_errors(\"checking network connectivity\", default_return=False)\n    def _check_network_connectivity(self, hostname: str = \"discord.com\", port: int = 443) -> bool:\n        \"\"\"\n        Check network connectivity with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        # Validate hostname\n        if not hostname or not isinstance(hostname, str):\n            logger.error(f\"Invalid hostname: {hostname}\")\n            return False\n            \n        if not hostname.strip():\n            logger.error(\"Empty hostname provided\")\n            return False\n            \n        # Validate port\n        if not isinstance(port, int) or port < 1 or port > 65535:\n            logger.error(f\"Invalid port: {port}\")\n            return False\n        \"\"\"Check if network connectivity is available to Discord servers with enhanced fallback and timeout handling\"\"\"\n        # Discord endpoints to try in order of preference\n        discord_endpoints = [\n            (\"discord.com\", 443),\n            (\"gateway.discord.gg\", 443),\n            (\"gateway-us-east1-b.discord.gg\", 443),\n            (\"gateway-us-east1-c.discord.gg\", 443),\n            (\"gateway-us-east1-d.discord.gg\", 443),\n            (\"gateway-us-east1-a.discord.gg\", 443)\n        ]\n        \n        # If a specific hostname was requested, try it first\n        if hostname != \"discord.com\":\n            discord_endpoints.insert(0, (hostname, port))\n        \n        for endpoint_hostname, endpoint_port in discord_endpoints:\n            try:\n                # Use a shorter timeout for faster failure detection\n                socket.create_connection((endpoint_hostname, endpoint_port), timeout=5)\n                # Only log network success occasionally to reduce log noise\n                if hasattr(self, '_network_success_count'):\n                    self._network_success_count += 1\n                else:\n                    self._network_success_count = 1\n                \n                # Log network success every 60th check to reduce noise\n                if self._network_success_count % 60 == 0:\n                    logger.debug(f\"Network connectivity successful to {endpoint_hostname}:{endpoint_port} (check #{self._network_success_count})\")\n                return True\n            except (socket.gaierror, socket.timeout, OSError) as e:\n                logger.debug(f\"Network connectivity failed to {endpoint_hostname}:{endpoint_port} - {e}\")\n                continue\n        \n        # All endpoints failed\n        self._detailed_error_info['network_error'] = {\n            'hostname': hostname,\n            'port': port,\n            'endpoints_tried': discord_endpoints,\n            'error_type': 'all_endpoints_failed',\n            'error_message': f\"All Discord endpoints failed connectivity test\",\n            'timestamp': time.time()\n        }\n        logger.error(f\"All Discord endpoints failed network connectivity test\")\n        self._connection_status = DiscordConnectionStatus.NETWORK_FAILURE\n        return False\n\n    @handle_errors(\"waiting for network recovery\", default_return=False)\n    def _wait_for_network_recovery(self, max_wait: int = 300) -> bool:\n        \"\"\"\n        Wait for network recovery with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        # Validate max_wait\n        if not isinstance(max_wait, int) or max_wait < 0:\n            logger.error(f\"Invalid max_wait: {max_wait}\")\n            return False\n        \"\"\"Wait for network connectivity to recover with enhanced monitoring and early exit\"\"\"\n        logger.info(f\"Waiting for network connectivity to recover (max {max_wait}s)...\")\n        start_time = time.time()\n        check_interval = 10  # Check every 10 seconds\n        \n        while time.time() - start_time < max_wait:\n            # Check DNS resolution first\n            if self._check_dns_resolution():\n                # Then check network connectivity\n                if self._check_network_connectivity():\n                    logger.info(\"Network connectivity recovered successfully\")\n                    self._connection_status = DiscordConnectionStatus.INITIALIZING\n                    return True\n            \n            # Wait before next check\n            time.sleep(check_interval)\n        \n        logger.error(f\"Network connectivity did not recover within {max_wait} seconds\")\n        return False\n\n    @contextlib.asynccontextmanager\n    @handle_errors(\"cleaning up session context\", default_return=None)\n    async def shutdown__session_cleanup_context(self):\n        \"\"\"Context manager for proper session cleanup with timeout handling\"\"\"\n        sessions_to_cleanup = []\n        try:\n            yield sessions_to_cleanup\n        finally:\n            # Clean up all sessions with timeout\n            cleanup_tasks = []\n            for session in sessions_to_cleanup:\n                if hasattr(session, 'close') and not session.closed:\n                    cleanup_tasks.append(self._cleanup_session_with_timeout(session))\n            \n            if cleanup_tasks:\n                try:\n                    await asyncio.wait_for(\n                        asyncio.gather(*cleanup_tasks, return_exceptions=True),\n                        timeout=10.0\n                    )\n                    logger.info(f\"Successfully cleaned up {len(cleanup_tasks)} sessions\")\n                except asyncio.TimeoutError:\n                    logger.warning(\"Session cleanup timed out, some sessions may not be properly closed\")\n                except Exception as e:\n                    logger.error(f\"Error during session cleanup: {e}\")\n\n    @handle_errors(\"cleaning up session with timeout\", user_friendly=False, default_return=False)\n    async def _cleanup_session_with_timeout(self, session) -> bool:\n        \"\"\"Clean up a single session with timeout handling\"\"\"\n        try:\n            await asyncio.wait_for(session.close(), timeout=5.0)\n            return True\n        except asyncio.TimeoutError:\n            logger.warning(f\"Session cleanup timed out for {type(session).__name__}\")\n            return False\n        except Exception as e:\n            logger.debug(f\"Error closing session {type(session).__name__}: {e}\")\n            return False\n\n    @handle_errors(\"cleaning up event loop safely\", user_friendly=False, default_return=False)\n    async def _cleanup_event_loop_safely(self, loop: asyncio.AbstractEventLoop) -> bool:\n        \"\"\"Safely clean up event loop with proper task cancellation and error handling\"\"\"\n        if not loop or loop.is_closed():\n            return True\n        \n        # Get all tasks in this specific loop\n        tasks = [task for task in asyncio.all_tasks(loop) if not task.done()]\n        \n        if not tasks:\n            logger.debug(\"No pending tasks to cancel\")\n            return True\n        \n        logger.info(f\"Cancelling {len(tasks)} pending tasks\")\n        \n        # Cancel all tasks\n        for task in tasks:\n            if not task.done():\n                task.cancel()\n        \n        # Wait for tasks to be cancelled with timeout\n        try:\n            await asyncio.wait_for(\n                asyncio.gather(*tasks, return_exceptions=True),\n                timeout=5.0\n            )\n            logger.info(\"All tasks cancelled successfully\")\n        except asyncio.TimeoutError:\n            logger.warning(\"Task cancellation timed out, some tasks may still be running\")\n        \n        # Close the loop if it's not already closed\n        if not loop.is_closed():\n            loop.close()\n            logger.info(\"Event loop closed successfully\")\n        \n        return True\n\n    @handle_errors(\"cleaning up aiohttp sessions\", user_friendly=False, default_return=False)\n    async def _cleanup_aiohttp_sessions(self) -> bool:\n        \"\"\"Clean up any remaining aiohttp sessions to prevent warnings\"\"\"\n        # Get current event loop\n        loop = asyncio.get_running_loop()\n        \n        # Find and close any aiohttp sessions\n        import gc\n        import aiohttp\n        \n        # Force garbage collection to find any orphaned sessions\n        gc.collect()\n        \n        # Look for aiohttp ClientSession objects in memory\n        for obj in gc.get_objects():\n            if isinstance(obj, aiohttp.ClientSession) and not obj.closed:\n                try:\n                    await obj.close()\n                    logger.debug(\"Closed orphaned aiohttp session\")\n                except Exception as e:\n                    logger.debug(f\"Error closing aiohttp session: {e}\")\n        \n        return True\n\n    @handle_errors(\"getting detailed connection status\", default_return={})\n    def _get_detailed_connection_status(self) -> dict[str, Any]:\n        \"\"\"Get detailed connection status information\"\"\"\n        status_info = {\n            'connection_status': self._connection_status.value,\n            'bot_initialized': self.bot is not None,\n            'bot_ready': self.bot.is_ready() if self.bot else False,\n            'bot_closed': self.bot.is_closed() if self.bot else True,\n            'reconnect_attempts': self._reconnect_attempts,\n            'max_reconnect_attempts': self._max_reconnect_attempts,\n            'last_reconnect_time': self._last_reconnect_time,\n            'dns_resolution': self._check_dns_resolution(),\n            'network_connectivity': self._check_network_connectivity(),\n            'detailed_errors': self._detailed_error_info.copy(),\n            'timestamp': time.time()\n        }\n        \n        # Add Discord-specific status if available\n        if self.bot:\n            try:\n                status_info['latency'] = self.bot.latency\n                status_info['guild_count'] = len(self.bot.guilds)\n            except Exception as e:\n                status_info['discord_status_error'] = str(e)\n        \n        return status_info\n\n    @handle_errors(\"updating connection status\", default_return=None)\n    def _shared__update_connection_status(self, status: DiscordConnectionStatus, error_info: dict[str, Any] | None = None):\n        \"\"\"Update connection status with detailed error information\"\"\"\n        # Only log if status actually changed\n        if self._connection_status != status:\n            self._connection_status = status\n            if error_info:\n                self._detailed_error_info.update(error_info)\n            \n            # Log status change with details (single log message)\n            logger.info(f\"Discord connection status changed to: {status.value}\")\n            if error_info:\n                logger.debug(f\"Connection error details: {error_info}\")\n        else:\n            # Status didn't change, just update error info if provided\n            if error_info:\n                self._detailed_error_info.update(error_info)\n\n    @handle_errors(\"checking network health\", default_return=False)\n    def _check_network_health(self) -> bool:\n        \"\"\"Comprehensive network health check with detailed reporting\"\"\"\n        logger.debug(\"Performing network health check...\")\n        \n        # Check DNS resolution first\n        if not self._check_dns_resolution():\n            logger.warning(\"DNS resolution failed during health check\")\n            return False\n        \n        # Check network connectivity\n        if not self._check_network_connectivity():\n            logger.warning(\"Network connectivity failed during health check\")\n            return False\n        \n        # If we have a bot instance, check its health\n        if self.bot and hasattr(self.bot, 'latency'):\n            try:\n                latency = self.bot.latency\n                if latency > 1.0:  # More than 1 second latency\n                    logger.warning(f\"High Discord latency detected: {latency:.3f}s\")\n                    return False\n                logger.debug(f\"Discord latency: {latency:.3f}s\")\n            except Exception as e:\n                logger.debug(f\"Could not check Discord latency: {e}\")\n        \n        logger.debug(\"Network health check passed\")\n        return True\n\n    @handle_errors(\"checking if should attempt reconnection\", default_return=False)\n    def _should_attempt_reconnection(self) -> bool:\n        \"\"\"Determine if reconnection should be attempted based on various factors\"\"\"\n        current_time = time.time()\n        \n        # Check if we've exceeded max attempts\n        if self._reconnect_attempts >= self._max_reconnect_attempts:\n            logger.warning(f\"Maximum reconnection attempts ({self._max_reconnect_attempts}) exceeded\")\n            return False\n        \n        # Check cooldown period\n        if current_time - self._last_reconnect_time < self._reconnect_cooldown:\n            remaining_cooldown = self._reconnect_cooldown - (current_time - self._last_reconnect_time)\n            logger.debug(f\"Reconnection cooldown active, {remaining_cooldown:.1f}s remaining\")\n            return False\n        \n        # Check network health before attempting reconnection\n        if not self._check_network_health():\n            logger.info(\"Network health check failed, skipping reconnection attempt\")\n            return False\n        \n        return True\n\n    @handle_errors(\"initializing Discord bot\", default_return=False)\n    async def initialize(self) -> bool:\n        \"\"\"\n        Initialize Discord bot with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        \"\"\"Initialize Discord bot with enhanced network resilience\"\"\"\n        if self._starting:\n            logger.info(\"Discord bot already initializing\")\n            return False\n        \n        if self.is_ready():\n            logger.info(\"Discord bot already initialized\")\n            return True\n        \n        self._starting = True\n        self._set_status(ChannelStatus.INITIALIZING)\n        self._shared__update_connection_status(DiscordConnectionStatus.INITIALIZING)\n        \n        try:\n            if not DISCORD_BOT_TOKEN:\n                error_msg = \"Discord bot token not configured.\"\n                self._set_status(ChannelStatus.ERROR, error_msg)\n                logger.error(error_msg)\n                return False\n\n            # Pre-flight network check\n            logger.info(\"Performing pre-flight network check...\")\n            if not self._check_network_health():\n                logger.warning(\"Pre-flight network check failed, but continuing with initialization\")\n                # Don't fail immediately, let Discord.py handle the connection\n            \n            # Create bot instance with automatic command processing disabled\n            self.bot = commands.Bot(\n                command_prefix='!',\n                intents=intents,\n                application_id=DISCORD_APPLICATION_ID,\n                help_command=None  # Disable default help command\n            )\n            \n            # Register events and commands\n            if not self._events_registered:\n                self.initialize__register_events()\n            \n            if not self._commands_registered:\n                self.initialize__register_commands()\n            \n            # Start bot in a separate thread\n            self.discord_thread = threading.Thread(\n                target=self.initialize__run_bot_in_thread, \n                daemon=True\n            )\n            self.discord_thread.start()\n            \n            # Wait for the bot to be ready with enhanced monitoring\n            max_wait = 60  # 60 seconds for network issues\n            wait_interval = 0.5  # Check every 0.5 seconds\n            total_waited = 0\n            \n            while total_waited < max_wait:\n                await asyncio.sleep(wait_interval)\n                total_waited += wait_interval\n                \n                if self.bot and self.bot.is_ready():\n                    self._set_status(ChannelStatus.READY)\n                    self._reconnect_attempts = 0  # Reset reconnect attempts on successful connection\n                    self._starting = False  # Reset starting flag\n                    self._shared__update_connection_status(DiscordConnectionStatus.CONNECTED)\n                    logger.info(\"Discord bot initialized successfully\")\n                    \n                    # If on_ready() hasn't fired yet (or won't fire), manually trigger webhook server startup\n                    # This can happen if the bot becomes ready before on_ready() fires, or if on_ready() fails silently\n                    # Wait a moment to see if on_ready() fires naturally\n                    await asyncio.sleep(0.5)  # Give on_ready() a chance to fire\n                    \n                    if hasattr(self, '_on_ready_handler') and self._on_ready_handler:\n                        if not self._on_ready_fired:\n                            discord_logger.warning(\"Bot is ready but on_ready() hasn't fired - manually triggering webhook server startup\")\n                            try:\n                                # Only create task if loop is running and not closed\n                                if hasattr(self.bot, 'loop') and self.bot.loop and not self.bot.loop.is_closed():\n                                    self.bot.loop.create_task(self._on_ready_handler())\n                                else:\n                                    discord_logger.debug(\"Bot loop not available for manual on_ready trigger\")\n                            except Exception as e:\n                                discord_logger.warning(f\"Failed to manually trigger webhook server startup: {e}\", exc_info=True)\n                    \n                    return True\n                \n                # Log progress for longer waits\n                if total_waited % 10 == 0:  # Every 10 seconds\n                    logger.info(f\"Waiting for Discord bot to be ready... ({total_waited}s/{max_wait}s)\")\n                    \n                    # Perform periodic network health check\n                    if total_waited % 20 == 0:  # Every 20 seconds\n                        if not self._check_network_health():\n                            logger.warning(\"Network health check failed during initialization\")\n            \n            # If we get here, the bot didn't become ready in time\n            error_msg = f\"Discord bot failed to become ready within {max_wait} seconds\"\n            self._set_status(ChannelStatus.ERROR, error_msg)\n            self._shared__update_connection_status(DiscordConnectionStatus.GATEWAY_ERROR, {\n                'error': error_msg,\n                'timeout_seconds': max_wait,\n                'timestamp': time.time()\n            })\n            logger.error(error_msg)\n            return False\n        finally:\n            # Always reset the starting flag, even if initialization fails\n            self._starting = False\n\n    @handle_errors(\"running Discord bot in thread\")\n    def initialize__run_bot_in_thread(self):\n        \"\"\"Run Discord bot in completely isolated thread with its own event loop\"\"\"\n        # Create completely new event loop for this thread\n        self._loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self._loop)\n        \n        # Start the bot and process commands\n        self._loop.run_until_complete(self.initialize__bot_main_loop())\n\n    @handle_errors(\"running Discord bot main loop\")\n    async def initialize__bot_main_loop(self):\n        \"\"\"Main bot loop that handles both Discord and command queue\"\"\"\n        bot = self.bot\n        if not bot or not DISCORD_BOT_TOKEN:\n            logger.error(\"Discord bot not initialized or token missing\")\n            return\n        # Start the Discord bot\n        bot_task = asyncio.create_task(bot.start(DISCORD_BOT_TOKEN))\n        \n        # Process command queue concurrently with bot\n        command_task = asyncio.create_task(self.initialize__process_command_queue())\n        \n        try:\n            # Wait for either the bot to finish or a stop command\n            done, pending = await asyncio.wait(\n                [bot_task, command_task],\n                return_when=asyncio.FIRST_COMPLETED\n            )\n            \n            # Cancel any remaining tasks\n            for task in pending:\n                task.cancel()\n                try:\n                    await task\n                except asyncio.CancelledError:\n                    pass\n        finally:\n            # Ensure bot is properly closed\n            if not bot.is_closed():\n                await bot.close()\n            \n            # Clean up HTTP session to prevent \"Unclosed client session\" errors\n            try:\n                if hasattr(bot, '_HTTP') and bot._HTTP:\n                    if hasattr(bot._HTTP, '_HTTPClient') and bot._HTTP._HTTPClient:\n                        if hasattr(bot._HTTP._HTTPClient, '_session') and bot._HTTP._HTTPClient._session:\n                            await bot._HTTP._HTTPClient._session.close()\n                            logger.info(\"Discord bot HTTP session closed in main loop\")\n            except Exception as e:\n                logger.debug(f\"Error closing HTTP session in main loop (may already be closed): {e}\")\n\n    @handle_errors(\"processing Discord command queue\")\n    async def initialize__process_command_queue(self):\n        \"\"\"Process command queue without blocking Discord bot heartbeat\"\"\"\n        while True:\n            try:\n                # Check for commands from main thread (non-blocking)\n                try:\n                    command, args = self._command_queue.get_nowait()\n                    \n                    if command == \"send_message\":\n                        if len(args) == 2:\n                            # Backward compatibility: just message\n                            recipient, message = args\n                            result = await self._send_message_internal(recipient, message)\n                        elif len(args) == 4:\n                            # New format: message with rich data and suggestions\n                            recipient, message, rich_data, suggestions = args\n                            result = await self._send_message_internal(recipient, message, rich_data, suggestions)\n                        elif len(args) == 5:\n                            # New format: message with rich data, suggestions, and custom view\n                            recipient, message, rich_data, suggestions, custom_view = args\n                            result = await self._send_message_internal(recipient, message, rich_data, suggestions, custom_view)\n                        else:\n                            logger.error(f\"Invalid send_message args: {args}\")\n                            result = False\n                        self._result_queue.put(result)\n                    elif command == \"stop\":\n                        logger.info(\"Discord bot received stop command\")\n                        return  # Exit the command processing loop\n                        \n                except queue.Empty:\n                    pass\n                \n                # Give Discord bot time to process heartbeat and other events\n                await asyncio.sleep(0.1)\n                \n            except Exception as e:\n                logger.error(f\"Error in Discord command processing: {e}\")\n                # Continue processing even if one command fails\n                await asyncio.sleep(0.1)\n\n    @handle_errors(\"registering Discord events\")\n    def initialize__register_events(self):\n        \"\"\"Register Discord event handlers\"\"\"\n        if self._events_registered or not self.bot:\n            return\n        \n        # Create the on_ready handler function\n        @handle_errors(\"Discord bot on_ready internal handler\", user_friendly=False, default_return=None)\n        async def _on_ready_internal():\n            # Prevent duplicate execution if already called\n            if self._on_ready_fired:\n                return\n            \n            self._on_ready_fired = True\n            bot = self.bot\n            if not bot:\n                return\n            # Single consolidated log message\n            logger.info(f\"Discord Bot logged in as {bot.user}\")\n            print(f\"Discord Bot is online as {bot.user}\")\n            \n            # Reset reconnect attempts on successful connection\n            self._reconnect_attempts = 0\n            self._set_status(ChannelStatus.READY)\n            self._shared__update_connection_status(DiscordConnectionStatus.CONNECTED)\n\n            # Sync application (slash) commands\n            @handle_errors(\"syncing Discord application commands\", user_friendly=False, default_return=None)\n            async def _sync_app_cmds():\n                await bot.tree.sync()\n                logger.info(\"Discord application commands synced\")\n            # Schedule on the bot's loop to ensure proper task context\n            # Store task reference for proper cleanup during shutdown\n            self._sync_task = bot.loop.create_task(_sync_app_cmds())\n            \n            # Check for new users who have authorized the app (can now DM us)\n            # This runs periodically to catch users who authorized while bot was offline\n            @handle_errors(\"checking for new authorized Discord users\", user_friendly=False, default_return=None)\n            async def _check_new_authorized_users():\n                from communication.communication_channels.discord.welcome_handler import (\n                    has_been_welcomed,\n                    mark_as_welcomed,\n                    get_welcome_message\n                )\n                \n                # Get all users who can DM us (have authorized the app)\n                # Note: We can't directly query this, but we can check when they first DM us\n                # This is handled in on_message for DMs\n                discord_logger.debug(\"Bot ready - will welcome users on Discord app authorization (via webhook) or first interaction\")\n            \n            # Schedule the check (non-blocking)\n            bot.loop.create_task(_check_new_authorized_users())\n            \n            # Start webhook server for receiving installation events\n            try:\n                from communication.communication_channels.discord.webhook_server import WebhookServer\n                from core.config import DISCORD_WEBHOOK_PORT, DISCORD_AUTO_NGROK\n                \n                # Auto-launch ngrok if enabled\n                if DISCORD_AUTO_NGROK:\n                    self._start_ngrok_tunnel(DISCORD_WEBHOOK_PORT)\n                \n                self._webhook_server = WebhookServer(port=DISCORD_WEBHOOK_PORT, bot_instance=self)\n                if self._webhook_server.start():\n                    # Log message is handled by WebhookServer.start() - don't duplicate\n                    if self._ngrok_process:\n                        discord_logger.info(f\"ngrok tunnel active - check ngrok web interface at http://127.0.0.1:4040 for public URL\")\n                    else:\n                        # Check if ngrok is running externally\n                        ngrok_running = False\n                        try:\n                            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n                                try:\n                                    if not proc.info['name']:\n                                        continue\n                                    proc_name = proc.info['name'].lower()\n                                    if 'ngrok' in proc_name:\n                                        cmdline = proc.info.get('cmdline', [])\n                                        if cmdline and 'http' in ' '.join(cmdline).lower():\n                                            if proc.is_running():\n                                                ngrok_running = True\n                                                discord_logger.info(f\"ngrok tunnel detected (external) - check http://127.0.0.1:4040 for public URL\")\n                                                break\n                                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n                                    continue\n                        except Exception:\n                            pass\n                        \n                        if not ngrok_running:\n                            discord_logger.info(f\"Webhook server ready on port {DISCORD_WEBHOOK_PORT} - configure webhook URL in Discord Developer Portal\")\n                else:\n                    discord_logger.warning(\"Failed to start Discord webhook server\")\n            except Exception as e:\n                discord_logger.warning(f\"Could not start webhook server: {e}\")\n                # Non-critical - bot will still work, just won't receive installation events\n        \n        # Wrap with error handling decorator\n        @self.bot.event\n        @handle_errors(\"Discord bot ready event\", user_friendly=False, default_return=None)\n        async def on_ready():\n            await _on_ready_internal()\n\n        @self.bot.event\n        @handle_errors(\"handling Discord disconnect event\", default_return=None)\n        async def on_disconnect():\n            logger.warning(\"Discord bot disconnected\")\n            # Use component logger for Discord disconnection events\n            discord_logger.warning(\"Discord bot disconnected\", \n                                 bot_name=str(self.bot.user) if self.bot and self.bot.user else \"unknown\",\n                                 reconnect_attempts=self._reconnect_attempts)\n            # Don't change status to INITIALIZING - keep it as READY or ERROR\n            # This prevents the bot from getting stuck in INITIALIZING state\n            current_status = self.get_status()\n            if current_status == ChannelStatus.READY:\n                # Only change to ERROR if we were previously ready\n                self._set_status(ChannelStatus.ERROR, \"Disconnected\")\n            self._shared__update_connection_status(DiscordConnectionStatus.DISCONNECTED)\n            \n            # Let Discord.py handle reconnection, but log our status\n            logger.info(\"Discord.py will handle automatic reconnection\")\n\n        @self.bot.event\n        @handle_errors(\"handling Discord error event\", default_return=None)\n        async def on_error(event, *args, **kwargs):\n            logger.error(f\"Discord bot error in event {event}: {args} {kwargs}\")\n            \n            # Use component logger for Discord error events\n            error_str = str(args) + str(kwargs)\n            discord_logger.error(\"Discord bot error\", \n                               event=event, \n                               error_details=error_str[:200],  # Truncate long errors\n                               bot_name=str(self.bot.user) if self.bot and self.bot.user else \"unknown\")\n            \n            # Check if this is a connection-related error\n            if any(keyword in error_str.lower() for keyword in ['connection', 'dns', 'timeout', 'network']):\n                logger.warning(\"Connection-related error detected - checking network status\")\n                discord_logger.warning(\"Connection-related error detected\", event=event)\n                if not self._check_dns_resolution():\n                    logger.error(\"DNS resolution failed during error recovery\")\n                    discord_logger.error(\"DNS resolution failed during error recovery\")\n                    self._shared__update_connection_status(DiscordConnectionStatus.DNS_FAILURE)\n                if not self._check_network_connectivity():\n                    logger.error(\"Network connectivity failed during error recovery\")\n                    discord_logger.error(\"Network connectivity failed during error recovery\")\n                    self._shared__update_connection_status(DiscordConnectionStatus.NETWORK_FAILURE)\n            \n            # Let discord.py handle reconnection for most errors\n\n        @self.bot.event\n        @handle_errors(\"handling Discord interaction event\", default_return=None)\n        async def on_interaction(interaction: discord.Interaction):\n            \"\"\"Handle Discord interactions (slash commands, buttons, etc.)\"\"\"\n            # Handle button clicks (component interactions)\n            if interaction.type == discord.InteractionType.component:\n                # Check if this is a welcome message button\n                if interaction.data and 'custom_id' in interaction.data:\n                    custom_id = interaction.data['custom_id']\n                    if custom_id.startswith('welcome_create_') or custom_id.startswith('welcome_link_'):\n                        # Extract Discord user ID from custom_id\n                        # Format: welcome_create_<discord_user_id> or welcome_link_<discord_user_id>\n                        parts = custom_id.split('_', 2)\n                        if len(parts) >= 3:\n                            discord_user_id = parts[2]\n                            \n                            from communication.communication_channels.discord.account_flow_handler import (\n                                start_account_creation_flow,\n                                start_account_linking_flow\n                            )\n                            \n                            # Get Discord username if available\n                            discord_username = interaction.user.name if interaction.user else None\n                            \n                            # Validate interaction type before proceeding\n                            if not isinstance(interaction, discord.Interaction):\n                                discord_logger.error(f\"Invalid interaction type for welcome button: {type(interaction)}\")\n                                await interaction.response.send_message(\n                                    \"\u274c An error occurred. Please try again.\",\n                                    ephemeral=True\n                                )\n                                return\n                            \n                            if custom_id.startswith('welcome_create_'):\n                                await start_account_creation_flow(interaction, discord_user_id, discord_username)\n                            elif custom_id.startswith('welcome_link_'):\n                                await start_account_linking_flow(interaction, discord_user_id)\n                            return\n                    \n                    # Handle check-in buttons\n                    elif custom_id.startswith('checkin_'):\n                        # Check-in buttons are handled by the view's callback methods\n                        # The view is attached to the message, so discord.py will handle it automatically\n                        # We just need to let it pass through\n                        return\n                    \n                    # Handle task reminder buttons\n                    elif custom_id.startswith('task_'):\n                        # Task reminder buttons are handled by the view's callback methods\n                        # The view is attached to the message, so discord.py will handle it automatically\n                        # We just need to let it pass through\n                        return\n                    \n                    # Handle suggestion buttons (from InteractionResponse suggestions)\n                    elif custom_id.startswith('suggestion_'):\n                        try:\n                            # Acknowledge the interaction first (required by Discord)\n                            await interaction.response.defer()\n                            \n                            # Get the button label from the interaction component\n                            button_label = None\n                            if hasattr(interaction, 'component') and interaction.component:\n                                button_label = getattr(interaction.component, 'label', None)\n                            \n                            # Fallback: try to get label from interaction data\n                            if not button_label and interaction.data:\n                                button_label = interaction.data.get('label')\n                            \n                            # If we still don't have the label, try to get it from the message components\n                            if not button_label and interaction.message:\n                                for component in interaction.message.components:\n                                    if hasattr(component, 'children'):\n                                        for child in component.children:\n                                            if hasattr(child, 'custom_id') and child.custom_id == custom_id:\n                                                button_label = getattr(child, 'label', None)\n                                                break\n                                    if button_label:\n                                        break\n                            \n                            discord_logger.info(f\"Processing suggestion button '{button_label}' (custom_id: {custom_id})\")\n                            \n                            if button_label:\n                                # Get internal user ID\n                                discord_user_id = str(interaction.user.id)\n                                from core.user_data_handlers import get_user_id_by_identifier\n                                internal_user_id = get_user_id_by_identifier(discord_user_id)\n                                \n                                if internal_user_id:\n                                    # Process the button label as a user message\n                                    # Use the label as-is (lowercase) - flow handlers will handle skip/cancel\n                                    button_message = button_label.lower().strip()\n                                    discord_logger.info(f\"Processing button click as message '{button_message}' for user {internal_user_id}\")\n                                    \n                                    from communication.message_processing.interaction_manager import handle_user_message\n                                    response = handle_user_message(internal_user_id, button_message, \"discord\")\n                                    \n                                    # Send the response using followup (since we already deferred)\n                                    # Create embed if rich_data is provided\n                                    embed = None\n                                    if response.rich_data:\n                                        embed = self._create_discord_embed(response.message, response.rich_data)\n                                    \n                                    # Create view with buttons if suggestions are provided\n                                    view = None\n                                    if response.suggestions:\n                                        view = self._create_action_row(response.suggestions)\n                                    \n                                    # Send response via followup\n                                    if embed and view:\n                                        await interaction.followup.send(content=response.message or \"\", embed=embed, view=view)\n                                    elif embed:\n                                        await interaction.followup.send(content=response.message or \"\", embed=embed)\n                                    elif view:\n                                        await interaction.followup.send(content=response.message, view=view)\n                                    else:\n                                        await interaction.followup.send(content=response.message)\n                                else:\n                                    discord_logger.warning(f\"No internal user ID found for Discord user {discord_user_id}\")\n                                    await interaction.followup.send(\"\u274c User account not found. Please try again.\", ephemeral=True)\n                            else:\n                                discord_logger.warning(f\"Could not extract button label from suggestion button {custom_id}\")\n                                await interaction.followup.send(\"\u274c Could not process button click. Please try typing the command instead.\", ephemeral=True)\n                        except Exception as e:\n                            discord_logger.error(f\"Error handling suggestion button (custom_id: {custom_id}): {e}\", exc_info=True)\n                            try:\n                                await interaction.followup.send(\"\u274c An error occurred processing your button click. Please try typing the command instead.\", ephemeral=True)\n                            except Exception as followup_error:\n                                discord_logger.error(f\"Error sending followup message: {followup_error}\", exc_info=True)\n                        return\n                \n                # Let other component interactions fall through to default handling\n                # (buttons from other parts of the system)\n                return\n            \n            # Handle application commands (slash commands)\n            if interaction.type == discord.InteractionType.application_command:\n                discord_user_id = str(interaction.user.id)\n                command_name = interaction.command.name if hasattr(interaction, 'command') and interaction.command else 'unknown'\n                discord_logger.debug(f\"DISCORD_INTERACTION: user_id={discord_user_id}, command={command_name}\")\n                \n                # Check if this is a new user who hasn't been welcomed\n                from communication.communication_channels.discord.welcome_handler import (\n                    has_been_welcomed,\n                    mark_as_welcomed,\n                    get_welcome_message\n                )\n                from core.user_data_handlers import get_user_id_by_identifier\n                \n                internal_user_id = get_user_id_by_identifier(discord_user_id)\n                \n                # Special handling for /start command (explicit welcome trigger)\n                if command_name == 'start' and not internal_user_id:\n                    welcome_msg = get_welcome_message(discord_user_id, is_authorization=True)\n                    try:\n                        # Send welcome DM\n                        await interaction.user.send(welcome_msg)\n                        mark_as_welcomed(discord_user_id)\n                        if not interaction.response.is_done():\n                            await interaction.response.send_message(\n                                \"\ud83d\udc4b Welcome! I've sent you setup instructions via DM. Check your direct messages!\",\n                                ephemeral=True\n                            )\n                        discord_logger.info(f\"Sent welcome DM to newly authorized Discord user via /start: {discord_user_id}\")\n                        return  # Don't process the command further\n                    except discord.Forbidden:\n                        # User has DMs disabled - respond in interaction instead\n                        mark_as_welcomed(discord_user_id)\n                        if not interaction.response.is_done():\n                            await interaction.response.send_message(\n                                f\"\ud83d\udc4b Welcome! I see you've authorized MHM. However, I can't send you a direct message.\\n\\n\"\n                                f\"**Your Discord ID:** `{discord_user_id}`\\n\\n\"\n                                f\"**To get started:**\\n\"\n                                f\"- Create a new account via the MHM application UI with your Discord ID, or\\n\"\n                                f\"- Ask an administrator to link your Discord ID to an existing account\",\n                                ephemeral=True\n                            )\n                        discord_logger.info(f\"Welcomed user {discord_user_id} via /start (DM blocked)\")\n                        return\n                    except Exception as e:\n                        discord_logger.warning(f\"Error sending welcome DM to {discord_user_id}: {e}\")\n                \n                # For any other command, check if user needs welcome\n                if not internal_user_id and not has_been_welcomed(discord_user_id):\n                    # User has authorized the app (they can use slash commands) but hasn't been welcomed\n                    welcome_msg = get_welcome_message(discord_user_id, is_authorization=True)\n                    \n                    try:\n                        # Send welcome DM (non-blocking - don't interfere with command processing)\n                        await interaction.user.send(welcome_msg)\n                        mark_as_welcomed(discord_user_id)\n                        discord_logger.info(f\"Sent welcome DM to newly authorized Discord user via interaction: {discord_user_id}\")\n                    except discord.Forbidden:\n                        # User has DMs disabled - mark as welcomed but don't interrupt command flow\n                        mark_as_welcomed(discord_user_id)\n                        discord_logger.info(f\"User {discord_user_id} has DMs disabled, marked as welcomed\")\n                    except Exception as e:\n                        discord_logger.warning(f\"Error sending welcome DM to {discord_user_id}: {e}\")\n\n        @self.bot.event\n        @handle_errors(\"handling Discord guild join event\", default_return=None)\n        async def on_guild_join(guild):\n            \"\"\"Handle when the bot is added to a new Discord server\"\"\"\n            discord_logger.info(f\"Bot added to server: {guild.name} (ID: {guild.id})\")\n            \n            # Try to find a suitable channel to send welcome message\n            # Prefer system channel, then first text channel the bot can send to\n            welcome_channel = None\n            \n            # Try system channel first (where Discord sends system messages)\n            if guild.system_channel and guild.system_channel.permissions_for(guild.me).send_messages:\n                welcome_channel = guild.system_channel\n                discord_logger.debug(f\"Using system channel: {guild.system_channel.name}\")\n            else:\n                # Find first text channel the bot can send to\n                for channel in guild.text_channels:\n                    if channel.permissions_for(guild.me).send_messages:\n                        welcome_channel = channel\n                        discord_logger.debug(f\"Using text channel: {channel.name}\")\n                        break\n            \n            if welcome_channel:\n                welcome_msg = (\n                    f\"\ud83d\udc4b **Hello {guild.name}!**\\n\\n\"\n                    f\"I'm **MHM (Mental Health Manager)**, your mental health assistant bot. \"\n                    f\"I'm here to help you manage tasks, check-ins, reminders, and provide personalized support.\\n\\n\"\n                    f\"**To get started:**\\n\"\n                    f\"1. Send me a message to get your Discord ID\\n\"\n                    f\"2. Create or link a MHM account with that Discord ID\\n\"\n                    f\"3. Start using commands like `/help` to see what I can do!\\n\\n\"\n                    f\"**Quick Commands:**\\n\"\n                    f\"- `/help` - See all available commands\\n\"\n                    f\"- `create task [description]` - Create a new task\\n\"\n                    f\"- `show my tasks` - View your tasks\\n\"\n                    f\"- `show my profile` - View your profile (once linked)\\n\\n\"\n                    f\"Feel free to ask me anything! I'm here to help. \ud83d\ude80\"\n                )\n                \n                try:\n                    await welcome_channel.send(welcome_msg)\n                    discord_logger.info(f\"Sent welcome message to {guild.name} in channel {welcome_channel.name}\")\n                except Exception as e:\n                    discord_logger.warning(f\"Could not send welcome message to {guild.name}: {e}\")\n            else:\n                discord_logger.warning(f\"Could not find a suitable channel to send welcome message in {guild.name}\")\n\n        @self.bot.event\n        @handle_errors(\"handling Discord message\", default_return=None)\n        async def on_message(message):\n            # COMPREHENSIVE LOGGING: Log ALL messages received\n            discord_logger.debug(f\"DISCORD_MESSAGE_RECEIVED: author_id={message.author.id}, content='{message.content[:100]}', channel={message.channel.id}, guild={message.guild.id if message.guild else 'DM'}\")\n            \n            # Don't respond to ourselves\n            if self.bot and message.author == self.bot.user:\n                discord_logger.debug(f\"DISCORD_MESSAGE_IGNORED: Message from bot itself, ignoring\")\n                return\n\n            # Process explicit commands (starting with \"!\" or \"/\") through our interaction manager\n            # Note: We don't call bot.process_commands() because we handle commands through our own system\n\n            # Process regular messages (not commands) through the interaction manager\n            # This prevents Discord from automatically trying to match commands to regular messages\n            # Map Discord user ID to internal user ID\n            discord_user_id = str(message.author.id)\n            discord_logger.debug(f\"DISCORD_MESSAGE_PROCESS: Looking up user for Discord ID {discord_user_id}\")\n            internal_user_id = get_user_id_by_identifier(discord_user_id)\n            \n            if not internal_user_id:\n                discord_logger.warning(f\"DISCORD_MESSAGE_UNRECOGNIZED: No internal user found for Discord ID {discord_user_id}\")\n                \n                # Send welcome message if this is the first time\n                from communication.communication_channels.discord.welcome_handler import (\n                    has_been_welcomed,\n                    mark_as_welcomed,\n                    get_welcome_message\n                )\n                \n                # Check if this is a DM (user-installable app authorization)\n                is_dm = isinstance(message.channel, discord.DMChannel)\n                \n                if not has_been_welcomed(discord_user_id):\n                    # Determine if this is an app authorization (DM) or server message\n                    welcome_msg = get_welcome_message(discord_user_id, is_authorization=is_dm)\n                    \n                    if is_dm:\n                        # User-installable app: user has authorized and sent first DM\n                        # Send welcome message as DM proactively\n                        try:\n                            await message.author.send(welcome_msg)\n                            mark_as_welcomed(discord_user_id)\n                            discord_logger.info(f\"Sent welcome DM to newly authorized Discord user: {discord_user_id}\")\n                        except discord.Forbidden:\n                            # User has DMs disabled or blocked bot - send in the channel instead\n                            await message.channel.send(\n                                f\"I see you've authorized MHM! However, I can't send you a direct message. \"\n                                f\"Here's your Discord ID to get started: `{discord_user_id}`\\n\\n\"\n                                f\"**To get started:**\\n\"\n                                f\"- Create a new account via the MHM application UI with your Discord ID, or\\n\"\n                                f\"- Ask an administrator to link your Discord ID to an existing account\"\n                            )\n                            mark_as_welcomed(discord_user_id)\n                            discord_logger.info(f\"Welcomed user {discord_user_id} via channel (DM blocked)\")\n                    else:\n                        # Server message - but user might have authorized the app\n                        # Try to send welcome DM first, fallback to channel\n                        try:\n                            await message.author.send(welcome_msg)\n                            mark_as_welcomed(discord_user_id)\n                            discord_logger.info(f\"Sent welcome DM to new Discord user (from server message): {discord_user_id}\")\n                        except discord.Forbidden:\n                            # Can't DM - send in channel instead\n                            await message.channel.send(welcome_msg)\n                            mark_as_welcomed(discord_user_id)\n                            discord_logger.info(f\"Sent welcome message to new Discord user in channel: {discord_user_id}\")\n                else:\n                    # User has been welcomed before\n                    if is_dm:\n                        # DM reminder - send as DM\n                        try:\n                            await message.author.send(\n                                f\"I don't recognize you yet! To use MHM, you need to create or link a MHM account.\\n\\n\"\n                                f\"**Your Discord ID:** `{discord_user_id}`\\n\\n\"\n                                f\"**To get started:**\\n\"\n                                f\"- Create a new account via the MHM application UI, or\\n\"\n                                f\"- Ask an administrator to link your Discord ID to an existing account\\n\\n\"\n                                f\"Once your account is linked, I'll be able to help you with tasks, reminders, and more!\"\n                            )\n                        except discord.Forbidden:\n                            # Fallback to channel if DM blocked\n                            await message.channel.send(\n                                f\"Your Discord ID: `{discord_user_id}` - Please create or link a MHM account to get started!\"\n                            )\n                    else:\n                        # Server reminder - send in channel\n                        await message.channel.send(\n                            f\"I don't recognize you yet! To use MHM, you need to create or link a MHM account.\\n\\n\"\n                            f\"**Your Discord ID:** `{discord_user_id}`\\n\\n\"\n                            f\"**To get started:**\\n\"\n                            f\"- Create a new account via the MHM application UI, or\\n\"\n                            f\"- Ask an administrator to link your Discord ID to an existing account\\n\\n\"\n                            f\"Once your account is linked, I'll be able to help you with tasks, reminders, and more!\"\n                        )\n                \n                return\n            \n            discord_logger.info(f\"DISCORD_MESSAGE_USER_IDENTIFIED: Discord ID {discord_user_id} \u2192 Internal user {internal_user_id}\")\n\n            # Validate that the stored Discord user ID is still accessible\n            # This helps catch cases where the user's Discord account was deleted or they blocked the bot\n            stored_discord_id = None\n            try:\n                from core.user_data_handlers import get_user_data\n                user_data_result = get_user_data(internal_user_id, 'account')\n                account_data = user_data_result.get('account', {})\n                stored_discord_id = account_data.get('discord_user_id')\n                \n                if stored_discord_id and str(stored_discord_id) != discord_user_id:\n                    # The user's Discord ID has changed - update it\n                    logger.info(f\"Updating Discord user ID for user {internal_user_id} from {stored_discord_id} to {discord_user_id}\")\n                    account_data['discord_user_id'] = discord_user_id\n                    from core.user_data_handlers import save_user_data\n                    save_user_data(internal_user_id, 'account', account_data)\n                    \n            except Exception as e:\n                logger.warning(f\"Error validating Discord user ID for {internal_user_id}: {e}\")\n\n            # Use the new interaction manager for enhanced user interactions\n            try:\n                from communication.message_processing.interaction_manager import handle_user_message\n                discord_logger.info(f\"DISCORD_BOT: Calling handle_user_message for user {internal_user_id} with message: '{message.content[:50]}...'\")\n                response = handle_user_message(internal_user_id, message.content, \"discord\")\n                \n                if response.message:\n                    # Send response directly to the channel (bypass _send_message_internal to avoid DM attempts)\n                    send_success = await self._send_to_channel(message.channel, response.message, response.rich_data, response.suggestions)\n                    \n                    if send_success:\n                        # Log successful message handling\n                        discord_logger.info(\"Discord message handled successfully\", \n                                          user_id=internal_user_id, \n                                          message_length=len(message.content),\n                                          response_length=len(response.message),\n                                          suggestions_count=len(response.suggestions) if response.suggestions else 0,\n                                          has_rich_data=bool(response.rich_data))\n                    else:\n                        # Message send failed - could be due to user not being accessible\n                        discord_logger.warning(\"Discord message send failed for user\", \n                                             user_id=internal_user_id,\n                                             discord_user_id=discord_user_id)\n                        \n            except Exception as e:\n                logger.error(f\"Error in enhanced interaction for user {internal_user_id}: {e}\")\n                discord_logger.error(\"Discord message handling failed\", \n                                   user_id=internal_user_id, \n                                   error=str(e),\n                                   fallback_used=True)\n                # Don't fall back to conversation manager - interaction manager handles this internally\n                # Just send a generic error message\n                await message.channel.send(\"I'm having trouble processing your message right now. Please try again in a moment.\")\n\n        # Store reference to the handler so we can call it manually if on_ready() doesn't fire\n        # Store as a callable that creates the coroutine, not the coroutine itself\n        # This prevents unawaited coroutine warnings in test environments\n        self._on_ready_handler = _on_ready_internal\n        \n        self._events_registered = True\n        \n        # If bot is already ready when we register events, on_ready won't fire\n        # So we need to manually trigger the webhook server startup\n        if self.bot and self.bot.is_ready():\n            try:\n                # Only create task if loop is running and not closed\n                if hasattr(self.bot, 'loop') and self.bot.loop and not self.bot.loop.is_closed():\n                    self.bot.loop.create_task(_on_ready_internal())\n                else:\n                    discord_logger.debug(\"Bot loop not available for manual on_ready trigger\")\n            except Exception as e:\n                discord_logger.warning(f\"Failed to manually trigger webhook server startup: {e}\", exc_info=True)\n\n    @handle_errors(\"registering Discord commands\")\n    def initialize__register_commands(self):\n        \"\"\"Register Discord commands\"\"\"\n        if self._commands_registered or not self.bot:\n            return\n\n        # Register dynamic application (slash) commands from the channel-agnostic map\n        from communication.message_processing.interaction_manager import get_interaction_manager, handle_user_message\n        im = get_interaction_manager()\n        cmd_defs = im.get_command_definitions()\n\n        for cmd in cmd_defs:\n            name = cmd[\"name\"]\n            mapped = cmd[\"mapped_message\"]\n            description = cmd[\"description\"]\n\n            @handle_errors(\"handling Discord app command\", context={\"command\": name}, default_return=None)\n            async def _app_cb(interaction: discord.Interaction, _mapped=mapped, _name=name):\n                discord_user_id = str(interaction.user.id)\n                internal_user_id = get_user_id_by_identifier(discord_user_id)\n                if not internal_user_id:\n                    # Welcome message should have been sent by on_interaction handler\n                    # But provide helpful response if they try to use a command\n                    await interaction.response.send_message(\n                        f\"Please create or link a MHM account to use this feature. Your Discord ID: `{discord_user_id}`\",\n                        ephemeral=True\n                    )\n                    return\n                response = handle_user_message(internal_user_id, _mapped, \"discord\")\n                \n                # Create embed if rich_data is provided\n                embed = None\n                if response.rich_data:\n                    embed = self._create_discord_embed(response.message, response.rich_data)\n                \n                # Create view with buttons if suggestions are provided\n                view = None\n                if response.suggestions:\n                    view = self._create_action_row(response.suggestions)\n                \n                # Send response with embed and/or view\n                if embed and view:\n                    await interaction.response.send_message(embed=embed, view=view)\n                elif embed:\n                    await interaction.response.send_message(embed=embed)\n                elif view:\n                    await interaction.response.send_message(response.message, view=view)\n                else:\n                    await interaction.response.send_message(response.message)\n\n            try:\n                app_cmd = app_commands.Command(name=name, description=(description or f\"{name} command\"), callback=_app_cb)\n                self.bot.tree.add_command(app_cmd)\n            except Exception:\n                # If already exists, skip silently\n                pass\n\n        # Dynamically expose a set of native-style classic commands based on the central slash map.\n        from communication.message_processing.interaction_manager import get_interaction_manager\n        im = get_interaction_manager()\n        cmd_defs = im.get_command_definitions()\n\n        for cmd in cmd_defs:\n            name = cmd[\"name\"]\n            mapped = cmd[\"mapped_message\"]\n            # Skip Discord's native classic commands to avoid duplication\n            if name in [\"help\"]:\n                continue\n\n            @handle_errors(\"handling Discord dynamic command\", context={\"command\": name}, default_return=None)\n            async def _dynamic(ctx, _mapped=mapped, _name=name):\n                discord_user_id = str(ctx.author.id)\n                internal_user_id = get_user_id_by_identifier(discord_user_id)\n                if not internal_user_id:\n                    await ctx.send(\"Please register first to use this feature.\")\n                    return\n                from communication.message_processing.interaction_manager import handle_user_message\n                response = handle_user_message(internal_user_id, _mapped, \"discord\")\n                await ctx.send(response.message)\n\n            # Register as a classic command: users can type !tasks, !profile, etc.\n            try:\n                self.bot.command(name=name)(_dynamic)\n            except Exception:\n                # Ignore duplicates if any\n                pass\n\n        self._commands_registered = True\n\n    @handle_errors(\"shutting down Discord bot\", default_return=False)\n    async def shutdown(self) -> bool:\n        \"\"\"Shutdown Discord bot safely with improved session and event loop management\"\"\"\n        logger.info(\"Starting Discord bot shutdown...\")\n        \n        try:\n            # Stop ngrok FIRST - don't wait for full bot shutdown\n            # This ensures ngrok stops even if shutdown hangs\n            self._stop_ngrok_tunnel()\n            \n            # Send stop command to Discord thread\n            try:\n                self._command_queue.put((\"stop\", None))\n            except Exception as e:\n                logger.warning(f\"Error sending stop command: {e}\")\n            \n            # Wait for thread to finish\n            if self.discord_thread and self.discord_thread.is_alive():\n                self.discord_thread.join(timeout=10)\n                if self.discord_thread.is_alive():\n                    logger.warning(\"Discord thread did not stop gracefully\")\n            \n            # Properly close the bot and event loop with enhanced cleanup\n            if self.bot:\n                async with self.shutdown__session_cleanup_context() as sessions_to_cleanup:\n                    # Cancel any pending sync task first\n                    if hasattr(self, '_sync_task') and self._sync_task and not self._sync_task.done():\n                        self._sync_task.cancel()\n                        try:\n                            await asyncio.wait_for(self._sync_task, timeout=2.0)\n                        except (asyncio.CancelledError, asyncio.TimeoutError):\n                            logger.debug(\"Sync task cancelled or timed out during shutdown\")\n                        except Exception as e:\n                            logger.debug(f\"Error waiting for sync task cancellation: {e}\")\n                    \n                    # Close the bot first\n                    if not self.bot.is_closed():\n                        await self.bot.close()\n                        logger.info(\"Discord bot closed successfully\")\n                    \n                    # Collect all sessions that need cleanup\n                    if hasattr(self.bot, '_HTTP') and self.bot._HTTP:\n                        if hasattr(self.bot._HTTP, '_HTTPClient') and self.bot._HTTP._HTTPClient:\n                            if hasattr(self.bot._HTTP._HTTPClient, '_session') and self.bot._HTTP._HTTPClient._session:\n                                sessions_to_cleanup.append(self.bot._HTTP._HTTPClient._session)\n                    \n                    # Clean up the event loop if it exists\n                    if hasattr(self, '_loop') and self._loop:\n                        await self._cleanup_event_loop_safely(self._loop)\n                    \n                    # Additional cleanup for aiohttp sessions\n                    await self._cleanup_aiohttp_sessions()\n                    \n                    # Stop webhook server\n                    if self._webhook_server:\n                        try:\n                            self._webhook_server.stop()\n                        except Exception as e:\n                            logger.debug(f\"Error stopping webhook server: {e}\")\n                    \n                    # Stop ngrok tunnel if running\n                    self._stop_ngrok_tunnel()\n            \n            # Ensure ngrok is stopped even if shutdown had errors\n            self._stop_ngrok_tunnel()\n            \n            return True\n        finally:\n            # Always set status to STOPPED, even if shutdown encountered errors\n            # This ensures tests can verify shutdown was attempted\n            self._set_status(ChannelStatus.STOPPED)\n            logger.info(\"Discord bot shutdown completed\")\n\n    @handle_errors(\"sending Discord message\", default_return=False)\n    async def send_message(self, recipient: str, message: str, **kwargs) -> bool:\n        \"\"\"\n        Send Discord message with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        # Validate recipient\n        if not recipient or not isinstance(recipient, str):\n            logger.error(f\"Invalid recipient: {recipient}\")\n            return False\n            \n        if not recipient.strip():\n            logger.error(\"Empty recipient provided\")\n            return False\n            \n        # Validate message\n        if not message or not isinstance(message, str):\n            logger.error(f\"Invalid message: {message}\")\n            return False\n            \n        if not message.strip():\n            logger.error(\"Empty message provided\")\n            return False\n        \"\"\"Send message via Discord using thread-safe queue communication with rich response support\"\"\"\n        if not self.is_ready():\n            logger.error(\"Discord bot is not ready to send messages\")\n            return False\n\n        # Check for rich response data\n        rich_data = kwargs.get('rich_data', {})\n        suggestions = kwargs.get('suggestions', [])\n        custom_view = kwargs.get('view', None)\n        \n        # Send command to Discord thread with rich data and optional custom view\n        if custom_view:\n            self._command_queue.put((\"send_message\", (recipient, message, rich_data, suggestions, custom_view)))\n        else:\n            self._command_queue.put((\"send_message\", (recipient, message, rich_data, suggestions)))\n        \n        # Wait for result with timeout\n        timeout = 10  # 10 seconds\n        start_time = time.time()\n        \n        while time.time() - start_time < timeout:\n            try:\n                result = self._result_queue.get_nowait()\n                return result\n            except queue.Empty:\n                time.sleep(0.1)\n        \n        logger.error(f\"Timeout waiting for Discord message send to {recipient}\")\n        return False\n\n    @handle_errors(\"validating Discord user accessibility\", user_friendly=False, default_return=False)\n    async def _validate_discord_user_accessibility(self, user_id: str) -> bool:\n        \"\"\"Validate if a Discord user ID is still accessible\"\"\"\n        bot = self.bot\n        if not bot:\n            logger.error(\"Discord bot not initialized\")\n            return False\n        user_id_int = int(user_id)\n        user = bot.get_user(user_id_int)\n        if not user:\n            try:\n                user = await bot.fetch_user(user_id_int)\n                return True\n            except discord.NotFound:\n                logger.warning(f\"Discord user {user_id} not found (404)\")\n                return False\n            except discord.Forbidden:\n                logger.warning(f\"Bot forbidden from accessing Discord user {user_id} (403)\")\n                return False\n        return True\n\n    @handle_errors(\"sending message to Discord channel\", user_friendly=False, default_return=False)\n    async def _send_to_channel(self, channel, message: str, rich_data: dict[str, Any] | None = None, suggestions: list[str] | None = None) -> bool:\n        \"\"\"Send message directly to a Discord channel (for regular message responses)\"\"\"\n        rich_data = rich_data or {}\n        suggestions = suggestions or []\n        \n        # Create Discord embed if rich data is provided\n        embed = None\n        if rich_data:\n            embed = self._create_discord_embed(message, rich_data)\n        \n        # Create view with buttons if suggestions are provided\n        view = None\n        if suggestions:\n            view = self._create_action_row(suggestions)\n        \n        # Send to the channel\n        if embed and view:\n            await channel.send(content=message or None, embed=embed, view=view)\n        elif embed:\n            await channel.send(content=message or None, embed=embed)\n        elif view:\n            await channel.send(content=message, view=view)\n        else:\n            await channel.send(content=message)\n        \n        logger.info(f\"Message sent to Discord channel {channel.id}\")\n        discord_logger.info(\"Discord channel message sent\", \n                          channel_id=str(channel.id), \n                          message_length=len(message),\n                          has_embed=bool(embed),\n                          has_components=bool(view))\n        return True\n\n    @handle_errors(\"sending Discord message internally\", default_return=False)\n    async def _send_message_internal(self, recipient: str, message: str, rich_data: dict[str, Any] | None = None, suggestions: list[str] | None = None, custom_view: Any | None = None) -> bool:\n        \"\"\"\n        Send Discord message internally with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        bot = self.bot\n        if not bot:\n            logger.error(\"Discord bot not initialized\")\n            return False\n        # Validate recipient\n        if not recipient or not isinstance(recipient, str):\n            logger.error(f\"Invalid recipient: {recipient}\")\n            return False\n            \n        if not recipient.strip():\n            logger.error(\"Empty recipient provided\")\n            return False\n            \n        # Validate message\n        if not message or not isinstance(message, str):\n            logger.error(f\"Invalid message: {message}\")\n            return False\n            \n        if not message.strip():\n            logger.error(\"Empty message provided\")\n            return False\n            \n        # Validate rich_data\n        if rich_data is not None and not isinstance(rich_data, dict):\n            logger.error(f\"Invalid rich_data: {type(rich_data)}\")\n            return False\n            \n        # Validate suggestions\n        if suggestions is not None and not isinstance(suggestions, list):\n            logger.error(f\"Invalid suggestions: {type(suggestions)}\")\n            return False\n        \"\"\"Send message safely within async context with rich response support\"\"\"\n        rich_data = rich_data or {}\n        suggestions = suggestions or []\n        \n        # Create Discord embed if rich data is provided\n        embed = None\n        if rich_data:\n            embed = self._create_discord_embed(message, rich_data)\n        \n        # Create view with buttons - prefer custom_view, then suggestions\n        view = None\n        if custom_view:\n            # Handle factory functions (callables) - create view within async context\n            if callable(custom_view) and not isinstance(custom_view, type):\n                try:\n                    view = custom_view()\n                except Exception as e:\n                    logger.error(f\"Error creating view from factory function: {e}\")\n                    view = None\n            else:\n                view = custom_view\n        elif suggestions:\n            view = self._create_action_row(suggestions)\n        \n        # Handle special Discord user marker first\n        if recipient.startswith(\"discord_user:\"):\n            internal_user_id = recipient.split(\":\", 1)[1]\n            # Get the user's Discord user ID and send a DM\n            try:\n                from core.user_data_handlers import get_user_data\n                user_data_result = get_user_data(internal_user_id, 'account')\n                account_data = user_data_result.get('account', {})\n                discord_user_id = account_data.get('discord_user_id')\n                \n                if discord_user_id:\n                    user_id_int = int(discord_user_id)\n                    user = bot.get_user(user_id_int)\n                    if not user:\n                        user = await bot.fetch_user(user_id_int)\n                    \n                    if user:\n                        send_kwargs: dict[str, Any] = {\"content\": message}\n                        if embed:\n                            send_kwargs[\"embed\"] = embed\n                        if view:\n                            send_kwargs[\"view\"] = view\n                        await user.send(**send_kwargs)\n                        # Log detailed message information (consolidated from two separate logs)\n                        logger.info(f\"Discord DM sent | {{\\\"user_id\\\": \\\"{discord_user_id}\\\", \\\"message_length\\\": {len(message)}, \\\"has_embed\\\": {bool(embed)}, \\\"has_components\\\": {bool(view)}, \\\"message_preview\\\": \\\"{message[:50]}...\\\"}}\")\n                        return True\n                    else:\n                        logger.warning(f\"Could not find Discord user {discord_user_id} for internal user {internal_user_id}\")\n                        return False\n                else:\n                    logger.warning(f\"No Discord user ID found for internal user {internal_user_id}\")\n                    return False\n            except Exception as e:\n                logger.error(f\"Error sending DM to Discord user {internal_user_id}: {e}\")\n                return False\n        \n        # Handle direct Discord user ID (for account linking when user doesn't have internal ID yet)\n        if recipient.startswith(\"discord_direct:\"):\n            discord_user_id = recipient.split(\":\", 1)[1]\n            # Send DM directly to Discord user ID (no internal user lookup needed)\n            try:\n                user_id_int = int(discord_user_id)\n                user = bot.get_user(user_id_int)\n                if not user:\n                    user = await bot.fetch_user(user_id_int)\n                \n                if user:\n                    send_kwargs: dict[str, Any] = {\"content\": message}\n                    if embed:\n                        send_kwargs[\"embed\"] = embed\n                    if view:\n                        send_kwargs[\"view\"] = view\n                    await user.send(**send_kwargs)\n                    logger.info(f\"Discord DM sent directly | {{\\\"discord_user_id\\\": \\\"{discord_user_id}\\\", \\\"message_length\\\": {len(message)}, \\\"has_embed\\\": {bool(embed)}, \\\"has_components\\\": {bool(view)}, \\\"message_preview\\\": \\\"{message[:50]}...\\\"}}\")\n                    return True\n                else:\n                    logger.warning(f\"Could not find Discord user {discord_user_id}\")\n                    return False\n            except Exception as e:\n                logger.error(f\"Error sending DM directly to Discord user {discord_user_id}: {e}\")\n                return False\n        \n        # Try as a channel first (preferred method)\n        try:\n            channel_id = int(recipient)\n            channel = bot.get_channel(channel_id)\n            if channel:\n                send_kwargs: dict[str, Any] = {\"content\": message}\n                if embed:\n                    send_kwargs[\"embed\"] = embed\n                if view:\n                    send_kwargs[\"view\"] = view\n                await channel.send(**send_kwargs)\n                logger.info(f\"Message sent to Discord channel {recipient}\")\n                discord_logger.info(\"Discord channel message sent\", \n                                  channel_id=recipient, \n                                  message_length=len(message),\n                                  has_embed=bool(embed),\n                                  has_components=bool(view))\n                return True\n            else:\n                logger.warning(f\"Could not find Discord channel with ID {recipient}\")\n        except (ValueError, TypeError):\n            logger.warning(f\"Invalid channel ID format: {recipient}\")\n            pass  # Not a valid channel ID\n        \n        \n        # If we get here, we couldn't send the message\n        logger.error(f\"Could not find Discord channel or user with ID {recipient}\")\n        discord_logger.error(\"Discord message send failed - recipient not found\", \n                           recipient=recipient)\n        return False\n        \n        logger.error(f\"Could not find Discord channel or user with ID {recipient}\")\n        discord_logger.error(\"Discord message send failed - recipient not found\", \n                           recipient=recipient)\n        return False\n    \n    @handle_errors(\"creating Discord embed\", default_return=None)\n    def _create_discord_embed(self, message: str, rich_data: dict[str, Any]) -> discord.Embed:\n        \"\"\"\n        Create Discord embed with validation.\n        \n        Returns:\n            discord.Embed: Created embed, None if failed\n        \"\"\"\n        # Validate message\n        if not message or not isinstance(message, str):\n            logger.error(f\"Invalid message: {message}\")\n            return None\n            \n        if not message.strip():\n            logger.error(\"Empty message provided\")\n            return None\n            \n        # Validate rich_data\n        if not rich_data or not isinstance(rich_data, dict):\n            logger.error(f\"Invalid rich_data: {rich_data}\")\n            return None\n        \"\"\"Create a Discord embed from rich data\"\"\"\n        embed = discord.Embed()\n        \n        # Set title\n        if 'title' in rich_data:\n            embed.title = rich_data['title']\n        else:\n            # Extract title from message if it starts with **\n            if message.startswith('**') and '**' in message[2:]:\n                title_end = message.find('**', 2)\n                embed.title = message[2:title_end]\n                message = message[title_end + 2:].strip()\n        \n        # Set description\n        if 'description' in rich_data:\n            embed.description = rich_data['description']\n        else:\n            embed.description = message\n        \n        # Set color based on type or use default\n        color_map = {\n            'success': discord.Color.green(),\n            'error': discord.Color.red(),\n            'warning': discord.Color.yellow(),\n            'info': discord.Color.blue(),\n            'task': discord.Color.purple(),\n            'profile': discord.Color.orange(),\n            'schedule': discord.Color.blue(),\n            'analytics': discord.Color.green()\n        }\n        \n        embed_type = rich_data.get('type', 'info')\n        embed.color = color_map.get(embed_type, discord.Color.blue())\n        \n        # Add fields\n        if 'fields' in rich_data:\n            for field in rich_data['fields']:\n                name = field.get('name', '')\n                value = field.get('value', '')\n                inline = field.get('inline', False)\n                embed.add_field(name=name, value=value, inline=inline)\n        \n        # Add footer\n        if 'footer' in rich_data:\n            embed.set_footer(text=rich_data['footer'])\n        \n        # Add timestamp\n        if 'timestamp' in rich_data:\n            embed.timestamp = rich_data['timestamp']\n        \n        return embed\n    \n    @handle_errors(\"creating Discord action row\", default_return=None)\n    def _create_action_row(self, suggestions: list[str]) -> discord.ui.View:\n        \"\"\"\n        Create Discord action row with validation.\n        \n        Returns:\n            discord.ui.View: Created view, None if failed\n        \"\"\"\n        # Validate suggestions\n        if not suggestions or not isinstance(suggestions, list):\n            logger.error(f\"Invalid suggestions: {suggestions}\")\n            return None\n            \n        if not suggestions:\n            logger.error(\"Empty suggestions provided\")\n            return None\n        \"\"\"Create a Discord view with buttons from suggestions\"\"\"\n        # Use discord.ui.View instead of ActionRow for discord.py v2.x compatibility\n        view = discord.ui.View()\n        \n        # Limit to 5 buttons (Discord limit)\n        for i, suggestion in enumerate(suggestions[:5]):\n            # Create a button with a unique custom_id\n            button = discord.ui.Button(\n                style=discord.ButtonStyle.primary,\n                label=suggestion[:80],  # Discord button label limit\n                custom_id=f\"suggestion_{i}_{hash(suggestion) % 10000}\"\n            )\n            view.add_item(button)\n        \n        return view\n\n    @handle_errors(\"receiving Discord messages\", default_return=[])\n    async def receive_messages(self) -> list[dict[str, Any]]:\n        \"\"\"Receive messages from Discord\"\"\"\n        # Discord messages are handled via events, not polling\n        # Return empty list as messages are processed via event handlers\n        return []\n\n    @handle_errors(\"performing Discord health check\", default_return=False)\n    async def health_check(self) -> bool:\n        \"\"\"Perform comprehensive health check on Discord bot with detailed status\"\"\"\n        current_time = time.time()\n        \n        # Rate limit health checks to avoid spam\n        if current_time - self._last_health_check < self._health_check_interval:\n            return self._connection_status == DiscordConnectionStatus.CONNECTED\n        \n        self._last_health_check = current_time\n        \n        # Get detailed status information\n        status_info = self._get_detailed_connection_status()\n        \n        # Check basic bot state\n        if not self.bot:\n            logger.warning(\"Discord bot not initialized\")\n            self._shared__update_connection_status(DiscordConnectionStatus.UNINITIALIZED)\n            return False\n        \n        if self.bot.is_closed():\n            logger.warning(\"Discord bot is closed\")\n            self._shared__update_connection_status(DiscordConnectionStatus.DISCONNECTED)\n            return False\n        \n        if not self.bot.is_ready():\n            logger.warning(\"Discord bot is not ready\")\n            self._shared__update_connection_status(DiscordConnectionStatus.DISCONNECTED)\n            return False\n        \n        # Enhanced network connectivity checks\n        dns_ok = self._check_dns_resolution()\n        network_ok = self._check_network_connectivity()\n        \n        if not dns_ok:\n            logger.warning(\"DNS resolution failed during health check\")\n            self._shared__update_connection_status(DiscordConnectionStatus.DNS_FAILURE)\n            return False\n        \n        if not network_ok:\n            logger.warning(\"Network connectivity failed during health check\")\n            self._shared__update_connection_status(DiscordConnectionStatus.NETWORK_FAILURE)\n            return False\n        \n        # Check Discord-specific metrics\n        try:\n            latency = self.bot.latency\n            if latency > 1.0:  # High latency warning\n                logger.warning(f\"Discord latency is high: {latency:.2f}s\")\n                status_info['high_latency'] = True\n                status_info['latency'] = latency\n        except Exception as e:\n            logger.warning(f\"Could not check Discord latency: {e}\")\n        \n        # Update status to connected if all checks pass\n        self._shared__update_connection_status(DiscordConnectionStatus.CONNECTED)\n        logger.debug(\"Discord health check passed\")\n        return True\n\n    @handle_errors(\"getting Discord health status\", default_return={})\n    def get_health_status(self) -> dict[str, Any]:\n        \"\"\"Get comprehensive health status information\"\"\"\n        return self._get_detailed_connection_status()\n\n    @handle_errors(\"getting connection status summary\", default_return=\"Unknown\")\n    def get_connection_status_summary(self) -> str:\n        \"\"\"Get a human-readable connection status summary\"\"\"\n        status_info = self._get_detailed_connection_status()\n        \n        if status_info['connection_status'] == 'connected':\n            latency = status_info.get('latency', 'unknown')\n            guild_count = status_info.get('guild_count', 'unknown')\n            return f\"Connected (Latency: {latency}s, Guilds: {guild_count})\"\n        elif status_info['connection_status'] == 'dns_failure':\n            error = status_info.get('detailed_errors', {}).get('dns_error', {})\n            return f\"DNS Failure: {error.get('error_message', 'Unknown DNS error')}\"\n        elif status_info['connection_status'] == 'network_failure':\n            error = status_info.get('detailed_errors', {}).get('network_error', {})\n            return f\"Network Failure: {error.get('error_message', 'Unknown network error')}\"\n        elif status_info['connection_status'] == 'gateway_error':\n            return \"Gateway Error: Unable to connect to Discord servers\"\n        elif status_info['connection_status'] == 'disconnected':\n            return \"Disconnected: Bot is not ready or closed\"\n        else:\n            return f\"Status: {status_info['connection_status']}\"\n\n    @handle_errors(\"checking if actually connected\", default_return=False)\n    def is_actually_connected(self) -> bool:\n        \"\"\"Check if the Discord bot is actually connected, regardless of initialization status\"\"\"\n        if not self.bot:\n            return False\n        \n        # Check if the bot is ready and not closed\n        if self.bot.is_ready() and not self.bot.is_closed():\n            # If we're actually connected but our status is wrong, fix it\n            if self.get_status() != ChannelStatus.READY:\n                logger.info(\"Discord bot is actually connected - fixing status\")\n                self._set_status(ChannelStatus.READY)\n                self._starting = False\n            return True\n        \n        # If bot exists but not ready, check if it's in a recoverable state\n        if self.bot and not self.bot.is_closed():\n            # Bot exists and not closed, but not ready - might be reconnecting\n            return False\n        \n        return False\n\n    @handle_errors(\"checking if can send messages\", default_return=False)\n    def can_send_messages(self) -> bool:\n        \"\"\"Check if the Discord bot can actually send messages\"\"\"\n        if not self.is_actually_connected():\n            return False\n        \n        # Additional checks for message sending capability\n        try:\n            bot = self.bot\n            if not bot:\n                return False\n            # Check if we have the bot user (means we're logged in)\n            if not bot.user:\n                return False\n            \n            # Check if we have any guilds (servers) we're connected to\n            if not bot.guilds:\n                return False\n            \n            return True\n        except Exception as e:\n            logger.warning(f\"Error checking message sending capability: {e}\")\n            return False\n\n    @handle_errors(\"manually reconnecting Discord bot\", default_return=False)\n    async def manual_reconnect(self) -> bool:\n        \"\"\"Manually trigger a reconnection attempt\"\"\"\n        if not self.bot:\n            logger.error(\"Cannot reconnect - bot not initialized\")\n            return False\n        if not DISCORD_BOT_TOKEN:\n            logger.error(\"Cannot reconnect - Discord bot token not configured\")\n            return False\n        \n        logger.info(\"Manual reconnection requested\")\n        \n        # Check network connectivity first\n        if not self._check_dns_resolution():\n            logger.error(\"DNS resolution failed - cannot reconnect\")\n            return False\n        \n        try:\n            # Close the current connection\n            await self.bot.close()\n            \n            # Wait a moment\n            await asyncio.sleep(2)\n            \n            # Attempt to reconnect\n            await self.bot.start(DISCORD_BOT_TOKEN)\n            \n            logger.info(\"Manual reconnection successful\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"Manual reconnection failed: {e}\")\n            return False\n\n\n    @handle_errors(\"starting ngrok tunnel\", default_return=None)\n    def _start_ngrok_tunnel(self, port: int):\n        \"\"\"\n        Start ngrok tunnel for webhook server (development only).\n        \n        Args:\n            port: Local port to tunnel (e.g., 8080)\n        \"\"\"\n        try:\n            # Check if ngrok is available\n            ngrok_path = shutil.which('ngrok')\n            if not ngrok_path:\n                discord_logger.warning(\"ngrok not found in PATH - auto-launch disabled. Install ngrok or set DISCORD_AUTO_NGROK=false\")\n                return\n            \n            # Check if ngrok is already running (avoid duplicates)\n            if self._ngrok_process and self._ngrok_process.poll() is None:\n                discord_logger.info(\"ngrok tunnel already running (managed by this bot)\")\n                return\n            \n            # Check if ngrok is already running from another process\n            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n                try:\n                    if not proc.info['name']:\n                        continue\n                    proc_name = proc.info['name'].lower()\n                    if 'ngrok' in proc_name:\n                        cmdline = proc.info.get('cmdline', [])\n                        if cmdline and 'http' in ' '.join(cmdline).lower():\n                            if proc.is_running():\n                                discord_logger.info(f\"ngrok tunnel already running externally (PID: {proc.info['pid']}) - skipping auto-launch\")\n                                return\n                except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n                    continue\n            \n            # Start ngrok process\n            discord_logger.info(f\"Starting ngrok tunnel for port {port}...\")\n            try:\n                # Windows: use CREATE_NO_WINDOW to hide console\n                # Unix: redirect output to avoid cluttering logs\n                if os.name == 'nt':  # Windows\n                    self._ngrok_process = subprocess.Popen(\n                        ['ngrok', 'http', str(port)],\n                        stdout=subprocess.DEVNULL,\n                        stderr=subprocess.DEVNULL,\n                        creationflags=subprocess.CREATE_NO_WINDOW\n                    )\n                else:  # Unix/Linux/Mac\n                    self._ngrok_process = subprocess.Popen(\n                        ['ngrok', 'http', str(port)],\n                        stdout=subprocess.DEVNULL,\n                        stderr=subprocess.DEVNULL\n                    )\n                \n                # Give ngrok a moment to start\n                time.sleep(2)\n                \n                # Check if process started successfully\n                if self._ngrok_process.poll() is None:\n                    self._ngrok_pid = self._ngrok_process.pid  # Store PID for fallback cleanup\n                    discord_logger.info(f\"ngrok tunnel started successfully (PID: {self._ngrok_process.pid})\")\n                    discord_logger.info(f\"ngrok web interface: http://127.0.0.1:4040\")\n                    discord_logger.info(f\"Check ngrok web interface for public URL to configure in Discord Developer Portal\")\n                else:\n                    discord_logger.warning(f\"ngrok process exited immediately (exit code: {self._ngrok_process.poll()})\")\n                    self._ngrok_process = None\n                    self._ngrok_pid = None\n                    \n            except FileNotFoundError:\n                discord_logger.warning(\"ngrok executable not found - auto-launch disabled\")\n                self._ngrok_process = None\n                self._ngrok_pid = None\n            except Exception as e:\n                discord_logger.warning(f\"Failed to start ngrok: {e}\")\n                self._ngrok_process = None\n                self._ngrok_pid = None\n                \n        except Exception as e:\n            discord_logger.warning(f\"Error starting ngrok tunnel: {e}\")\n            self._ngrok_process = None\n            self._ngrok_pid = None\n    \n    @handle_errors(\"stopping ngrok tunnel\", default_return=None)\n    def _stop_ngrok_tunnel(self):\n        \"\"\"Stop ngrok tunnel if running.\"\"\"\n        # Check if we've already stopped ngrok (avoid duplicate stop attempts)\n        if not self._ngrok_process and not self._ngrok_pid:\n            # Already stopped or never started - skip silently\n            return\n        \n        stopped = False\n        \n        # Try to stop using process reference first\n        if self._ngrok_process:\n            try:\n                if self._ngrok_process.poll() is None:\n                    # Process is still running - terminate it\n                    pid = self._ngrok_process.pid\n                    discord_logger.info(f\"Stopping ngrok tunnel (PID: {pid})...\")\n                    self._ngrok_process.terminate()\n                    \n                    # Wait up to 5 seconds for graceful shutdown\n                    try:\n                        self._ngrok_process.wait(timeout=5)\n                        discord_logger.info(\"ngrok tunnel stopped\")\n                        stopped = True\n                    except subprocess.TimeoutExpired:\n                        # Force kill if it doesn't stop gracefully\n                        discord_logger.warning(\"ngrok did not stop gracefully - forcing termination\")\n                        self._ngrok_process.kill()\n                        self._ngrok_process.wait()\n                        discord_logger.info(\"ngrok tunnel force-stopped\")\n                        stopped = True\n                else:\n                    # Process already exited\n                    exit_code = self._ngrok_process.poll()\n                    discord_logger.debug(f\"ngrok tunnel already exited (exit code: {exit_code})\")\n                    stopped = True\n                        \n                self._ngrok_process = None\n            except Exception as e:\n                discord_logger.warning(f\"Error stopping ngrok tunnel via process reference: {e}\")\n                self._ngrok_process = None\n        \n        # Fallback: Try to stop by PID if process reference was lost\n        if not stopped and self._ngrok_pid:\n            try:\n                discord_logger.info(f\"Attempting to stop ngrok tunnel by PID (PID: {self._ngrok_pid})...\")\n                proc = psutil.Process(self._ngrok_pid)\n                if proc.is_running():\n                    proc.terminate()\n                    try:\n                        proc.wait(timeout=5)\n                        discord_logger.info(\"ngrok tunnel stopped (via PID)\")\n                        stopped = True\n                    except psutil.TimeoutExpired:\n                        proc.kill()\n                        proc.wait()\n                        discord_logger.info(\"ngrok tunnel force-stopped (via PID)\")\n                        stopped = True\n                else:\n                    discord_logger.debug(f\"ngrok process {self._ngrok_pid} already exited\")\n                    stopped = True\n            except psutil.NoSuchProcess:\n                discord_logger.debug(f\"ngrok process {self._ngrok_pid} not found (already stopped)\")\n                stopped = True\n            except Exception as e:\n                discord_logger.warning(f\"Error stopping ngrok tunnel by PID: {e}\")\n        \n        # Clear references only after successful stop\n        if stopped:\n            self._ngrok_process = None\n            self._ngrok_pid = None\n\n    # Keep the existing send_dm method for specific Discord functionality\n    @handle_errors(\"sending Discord DM\", default_return=False)\n    async def send_dm(self, user_id: str, message: str) -> bool:\n        \"\"\"\n        Send Discord DM with validation.\n        \n        Returns:\n            bool: True if successful, False if failed\n        \"\"\"\n        # Validate user_id\n        if not user_id or not isinstance(user_id, str):\n            logger.error(f\"Invalid user_id: {user_id}\")\n            return False\n            \n        if not user_id.strip():\n            logger.error(\"Empty user_id provided\")\n            return False\n            \n        # Validate message\n        if not message or not isinstance(message, str):\n            logger.error(f\"Invalid message: {message}\")\n            return False\n            \n        if not message.strip():\n            logger.error(\"Empty message provided\")\n            return False\n        \"\"\"Send a direct message to a Discord user\"\"\"\n        return await self.send_message(user_id, message)\n\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "Backward compatibility",
                    "line": 666,
                    "line_content": "# Backward compatibility: just message",
                    "start": 29333,
                    "end": 29355
                  }
                ]
              ],
              [
                "development_tools\\ai_work\\analyze_ai_work.py",
                "#!/usr/bin/env python3\n# TOOL_TIER: supporting\n\n\"\"\"\nLightweight structural validator for AI-generated work.\nHelps verify basic completeness and consistency before presenting to user.\n\nNote: This tool performs lightweight structural validation only. For comprehensive\nanalysis, use the domain-specific tools:\n- Documentation: docs/analyze_documentation_sync.py\n- Error handling: error_handling/analyze_error_handling.py\n- Test coverage: tests/analyze_test_coverage.py\n- Changelog sync: docs/fix_version_sync.py\n\"\"\"\n\nimport ast\nimport re\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\n\n# YAML support removed - not needed for lightweight structural validation\n\n# Add project root to path for core module imports\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\n# Handle both relative and absolute imports\ntry:\n    from . import config\nexcept ImportError:\n    import sys\n    from pathlib import Path\n\n    project_root = Path(__file__).parent.parent.parent\n    if str(project_root) not in sys.path:\n        sys.path.insert(0, str(project_root))\n    from development_tools import config\n\nfrom core.logger import get_component_logger\n\nlogger = get_component_logger(\"development_tools\")\n\n# Load config at module level\nVALIDATE_AI_WORK_CONFIG = config.get_analyze_ai_work_config()\n\n\ndef validate_documentation_completeness(doc_file: str, code_files: List[str]) -> Dict:\n    \"\"\"Validate that documentation covers all relevant code.\"\"\"\n    results = {\n        \"file_exists\": False,\n        \"coverage\": 100.0,  # Default to 100% for high-level docs like README.md\n        \"missing_items\": [],\n        \"extra_items\": [],\n        \"warnings\": [],\n    }\n\n    # Check if documentation file exists\n    doc_path = Path(doc_file)\n    results[\"file_exists\"] = doc_path.exists()\n\n    if not results[\"file_exists\"]:\n        results[\"warnings\"].append(f\"Documentation file {doc_file} does not exist\")\n        return results\n\n    # Read documentation content\n    try:\n        with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n            doc_content = f.read()\n    except Exception as e:\n        results[\"warnings\"].append(f\"Error reading {doc_file}: {e}\")\n        return results\n\n    # Extract mentioned items from documentation\n    mentioned_items = set()\n\n    # Look for function/class names in backticks\n    function_matches = re.findall(r\"`([a-zA-Z_][a-zA-Z0-9_]*)`\", doc_content)\n    mentioned_items.update(function_matches)\n\n    # Look for file paths\n    file_matches = re.findall(r\"`([^`]+\\.py)`\", doc_content)\n    mentioned_items.update(file_matches)\n\n    # Extract actual items from code files\n    actual_items = set()\n    for code_file in code_files:\n        code_path = Path(code_file)\n        if code_path.exists():\n            try:\n                with open(code_path, \"r\", encoding=\"utf-8\") as f:\n                    content = f.read()\n\n                tree = ast.parse(content)\n\n                # Extract function names\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.FunctionDef):\n                        actual_items.add(node.name)\n                    elif isinstance(node, ast.ClassDef):\n                        actual_items.add(node.name)\n\n                # Add file name\n                actual_items.add(code_path.name)\n\n            except Exception as e:\n                results[\"warnings\"].append(f\"Error parsing {code_file}: {e}\")\n\n    # Calculate coverage - for README.md, we don't expect it to mention every function\n    if doc_file.endswith(\"README.md\"):\n        # README.md is a high-level overview, not comprehensive API docs\n        results[\"coverage\"] = 100.0\n        results[\"missing_items\"] = []\n        results[\"extra_items\"] = []\n        results[\"warnings\"].append(\n            \"README.md validation: High-level docs don't need to mention every function\"\n        )\n    elif actual_items:\n        covered_items = mentioned_items.intersection(actual_items)\n        results[\"coverage\"] = len(covered_items) / len(actual_items) * 100\n        results[\"missing_items\"] = list(actual_items - mentioned_items)\n        results[\"extra_items\"] = list(mentioned_items - actual_items)\n\n    return results\n\n\ndef validate_code_consistency(changed_files: List[str]) -> Dict:\n    \"\"\"Validate that code changes are consistent across files.\"\"\"\n    results = {\n        \"import_consistency\": True,\n        \"naming_consistency\": True,\n        \"function_signatures\": [],\n        \"warnings\": [],\n    }\n\n    # Check for consistent imports\n    import_patterns = {}\n    for file_path in changed_files:\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n\n            tree = ast.parse(content)\n\n            # Extract imports\n            imports = []\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        imports.append(alias.name)\n                elif isinstance(node, ast.ImportFrom):\n                    module = node.module or \"\"\n                    for alias in node.names:\n                        imports.append(f\"{module}.{alias.name}\")\n\n            import_patterns[file_path] = imports\n\n            # Extract function signatures\n            for node in ast.walk(tree):\n                if isinstance(node, ast.FunctionDef):\n                    args = [arg.arg for arg in node.args.args]\n                    results[\"function_signatures\"].append(\n                        {\"file\": file_path, \"name\": node.name, \"args\": args}\n                    )\n\n        except Exception as e:\n            results[\"warnings\"].append(f\"Error parsing {file_path}: {e}\")\n\n    # Check for naming consistency\n    function_names = {}\n    for sig in results[\"function_signatures\"]:\n        name = sig[\"name\"]\n        if name not in function_names:\n            function_names[name] = []\n        function_names[name].append(sig[\"file\"])\n\n    # Find duplicate function names\n    duplicates = {\n        name: files for name, files in function_names.items() if len(files) > 1\n    }\n    if duplicates:\n        results[\"naming_consistency\"] = False\n        results[\"warnings\"].append(f\"Duplicate function names found: {duplicates}\")\n\n    return results\n\n\ndef validate_file_structure(\n    created_files: List[str], modified_files: List[str]\n) -> Dict:\n    \"\"\"Validate that file structure changes are appropriate.\"\"\"\n    results = {\n        \"appropriate_locations\": True,\n        \"naming_conventions\": True,\n        \"warnings\": [],\n    }\n\n    # Check file locations\n    for file_path in created_files + modified_files:\n        path = Path(file_path)\n\n        # Check if file is in appropriate directory\n        if path.suffix == \".py\":\n            if \"test\" in path.name.lower() and \"tests\" not in str(path):\n                results[\"warnings\"].append(\n                    f\"Test file {file_path} not in tests/ directory\"\n                )\n                results[\"appropriate_locations\"] = False\n\n            if path.name.startswith(\"ui_\") and \"ui\" not in str(path):\n                results[\"warnings\"].append(f\"UI file {file_path} not in ui/ directory\")\n                results[\"appropriate_locations\"] = False\n\n    # Check naming conventions\n    for file_path in created_files + modified_files:\n        path = Path(file_path)\n\n        if path.suffix == \".py\":\n            # Check for snake_case\n            if \"_\" not in path.stem and path.stem.lower() != path.stem:\n                results[\"warnings\"].append(\n                    f\"Python file {file_path} should use snake_case\"\n                )\n                results[\"naming_conventions\"] = False\n\n    return results\n\n\ndef generate_validation_report(validation_type: str, **kwargs) -> str:\n    \"\"\"Generate a comprehensive validation report.\"\"\"\n    report = []\n    report.append(f\"Validation Type: {validation_type}\")\n    report.append(\"\")\n\n    if validation_type == \"documentation\":\n        results = validate_documentation_completeness(\n            kwargs.get(\"doc_file\", \"\"), kwargs.get(\"code_files\", [])\n        )\n\n        report.append(f\"Documentation File: {kwargs.get('doc_file', 'N/A')}\")\n        report.append(f\"File Exists: {results['file_exists']}\")\n        report.append(f\"Coverage: {results['coverage']:.1f}%\")\n\n        if results[\"missing_items\"]:\n            report.append(f\"Missing Items: {len(results['missing_items'])}\")\n            for item in results[\"missing_items\"][:5]:\n                report.append(f\"   - {item}\")\n\n        if results[\"extra_items\"]:\n            report.append(f\"Extra Items: {len(results['extra_items'])}\")\n            for item in results[\"extra_items\"][:5]:\n                report.append(f\"   - {item}\")\n\n    elif validation_type == \"code_consistency\":\n        results = validate_code_consistency(kwargs.get(\"changed_files\", []))\n\n        report.append(f\"Code Consistency Check\")\n        report.append(f\"Import Consistency: {results['import_consistency']}\")\n        report.append(f\"Naming Consistency: {results['naming_consistency']}\")\n        report.append(f\"Functions Found: {len(results['function_signatures'])}\")\n\n        if results[\"warnings\"]:\n            report.append(f\"Warnings: {len(results['warnings'])}\")\n            for warning in results[\"warnings\"][:3]:\n                report.append(f\"   - {warning}\")\n\n    elif validation_type == \"file_structure\":\n        results = validate_file_structure(\n            kwargs.get(\"created_files\", []), kwargs.get(\"modified_files\", [])\n        )\n\n        report.append(f\"File Structure Validation\")\n        report.append(f\"Appropriate Locations: {results['appropriate_locations']}\")\n        report.append(f\"Naming Conventions: {results['naming_conventions']}\")\n\n        if results[\"warnings\"]:\n            report.append(f\"Warnings: {len(results['warnings'])}\")\n            for warning in results[\"warnings\"][:3]:\n                report.append(f\"   - {warning}\")\n\n    # Overall assessment\n    report.append(\"\")\n    report.append(\"OVERALL ASSESSMENT:\")\n\n    if validation_type == \"documentation\":\n        completeness_threshold = VALIDATE_AI_WORK_CONFIG.get(\n            \"completeness_threshold\", 90.0\n        )\n        if results[\"coverage\"] >= completeness_threshold:\n            report.append(\"GOOD - Documentation covers most items\")\n        elif results[\"coverage\"] >= completeness_threshold * 0.5:\n            report.append(\"FAIR - Documentation needs improvement\")\n        else:\n            report.append(\"POOR - Documentation is incomplete\")\n\n    elif validation_type == \"code_consistency\":\n        if results[\"naming_consistency\"] and not results[\"warnings\"]:\n            report.append(\"GOOD - Code is consistent\")\n        else:\n            report.append(\"NEEDS ATTENTION - Inconsistencies found\")\n\n    elif validation_type == \"file_structure\":\n        if results[\"appropriate_locations\"] and results[\"naming_conventions\"]:\n            report.append(\"GOOD - File structure is appropriate\")\n        else:\n            report.append(\"NEEDS ATTENTION - File structure issues found\")\n\n    return \"\\n\".join(report)\n\n\ndef analyze_ai_work(\n    work_type: str,\n    project_root: Optional[str] = None,\n    config_path: Optional[str] = None,\n    **kwargs,\n) -> Dict:\n    \"\"\"\n    Main validation function for AI work.\n\n    Args:\n        work_type: Type of validation to perform (\"documentation\", \"code_changes\", \"file_creation\")\n        project_root: Optional project root path (for config loading)\n        config_path: Optional path to config file (for config loading)\n        **kwargs: Additional arguments for validation\n    \"\"\"\n    global VALIDATE_AI_WORK_CONFIG\n\n    # Load config if project_root or config_path provided\n    if project_root or config_path:\n        if config_path:\n            config.load_external_config(config_path)\n        # Reload config (project_root is handled via config file)\n        VALIDATE_AI_WORK_CONFIG = config.get_analyze_ai_work_config()\n\n        # Load rule sets if configured\n        rule_set_paths = VALIDATE_AI_WORK_CONFIG.get(\"rule_set_paths\", [])\n        rule_sets = VALIDATE_AI_WORK_CONFIG.get(\"rule_sets\", {})\n\n        # Load rule sets from files if paths provided\n        if rule_set_paths:\n            for rule_path in rule_set_paths:\n                rule_path_obj = Path(rule_path)\n                if not rule_path_obj.is_absolute() and project_root:\n                    rule_path_obj = Path(project_root) / rule_path_obj\n\n                if rule_path_obj.exists():\n                    try:\n                        with open(rule_path_obj, \"r\", encoding=\"utf-8\") as f:\n                            if rule_path_obj.suffix in (\".yaml\", \".yml\"):\n                                logger.warning(\n                                    f\"YAML rule files not supported, skipping {rule_path}\"\n                                )\n                                continue\n                            else:\n                                import json\n\n                                loaded_rules = json.load(f)\n                            if isinstance(loaded_rules, dict):\n                                rule_sets.update(loaded_rules)\n                    except Exception as e:\n                        logger.warning(f\"Failed to load rule set from {rule_path}: {e}\")\n\n    # Generate report text\n    if work_type == \"documentation\":\n        output = generate_validation_report(\"documentation\", **kwargs)\n    elif work_type == \"code_changes\":\n        output = generate_validation_report(\"code_consistency\", **kwargs)\n    elif work_type == \"file_creation\":\n        output = generate_validation_report(\"file_structure\", **kwargs)\n    else:\n        output = \"Unknown validation type\"\n\n    # Extract status from output\n    status = \"UNKNOWN\"\n    if isinstance(output, str):\n        if \"POOR\" in output or \"FAIL\" in output:\n            status = \"POOR\"\n        elif \"GOOD\" in output or \"PASS\" in output:\n            status = \"GOOD\"\n        elif \"NEEDS ATTENTION\" in output or \"FAIR\" in output or \"WARNING\" in output:\n            status = \"NEEDS_ATTENTION\"\n\n    # Return standard format\n    return {\n        \"summary\": {\n            \"total_issues\": 0 if status in (\"GOOD\", \"UNKNOWN\") else 1,\n            \"files_affected\": 0,\n            \"status\": status,\n        },\n        \"details\": {\"output\": output, \"work_type\": work_type},\n    }\n\n\ndef execute(\n    project_root: Optional[str] = None, config_path: Optional[str] = None, **kwargs\n) -> Dict:\n    \"\"\"Execute validation (for use by run_development_tools).\"\"\"\n    work_type = kwargs.pop(\"work_type\", \"documentation\")\n    return analyze_ai_work(\n        work_type, project_root=project_root, config_path=config_path, **kwargs\n    )\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        description=\"Lightweight structural validator for AI-generated work\"\n    )\n    parser.add_argument(\n        \"--work-type\",\n        default=\"documentation\",\n        choices=[\"documentation\", \"code_changes\", \"file_creation\"],\n        help=\"Type of validation to perform\",\n    )\n    parser.add_argument(\n        \"--doc-file\", help=\"Documentation file to validate (for documentation type)\"\n    )\n    parser.add_argument(\n        \"--code-files\",\n        nargs=\"*\",\n        help=\"Code files to validate (for documentation type)\",\n    )\n    parser.add_argument(\n        \"--changed-files\",\n        nargs=\"*\",\n        help=\"Changed files to validate (for code_changes type)\",\n    )\n    parser.add_argument(\n        \"--created-files\",\n        nargs=\"*\",\n        help=\"Created files to validate (for file_creation type)\",\n    )\n    parser.add_argument(\n        \"--modified-files\",\n        nargs=\"*\",\n        help=\"Modified files to validate (for file_creation type)\",\n    )\n\n    parser.add_argument(\n        \"--json\", action=\"store_true\", help=\"Output results as JSON in standard format\"\n    )\n    args = parser.parse_args()\n\n    kwargs = {}\n    if args.doc_file:\n        kwargs[\"doc_file\"] = args.doc_file\n    if args.code_files:\n        kwargs[\"code_files\"] = args.code_files\n    if args.changed_files:\n        kwargs[\"changed_files\"] = args.changed_files\n    if args.created_files:\n        kwargs[\"created_files\"] = args.created_files\n    if args.modified_files:\n        kwargs[\"modified_files\"] = args.modified_files\n\n    result = analyze_ai_work(args.work_type, **kwargs)\n\n    if args.json:\n        # Output JSON in standard format\n        import json\n\n        print(json.dumps(result, indent=2))\n    else:\n        # Print text output for backward compatibility\n        output = result.get(\"details\", {}).get(\"output\", \"\")\n        print(output)\n        # Also log for debugging (but not the full output)\n        if len(output) < 500:  # Only log short outputs\n            logger.info(output)\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 466,
                    "line_content": "# Print text output for backward compatibility",
                    "start": 16549,
                    "end": 16571
                  }
                ]
              ],
              [
                "development_tools\\config\\config.py",
                "# TOOL_TIER: core\n\n# AI Tools Configuration\n\"\"\"\nConfiguration settings for AI collaboration tools.\nOptimized for AI assistants to get concise, actionable information about the codebase.\n\nNOTE: This module contains MHM-specific default values. For other projects, override these\nvia development_tools_config.json in the project root. All getter functions check external\nconfig first, then fall back to these defaults.\n\"\"\"\n\nfrom pathlib import Path\nimport json\nimport os\nfrom typing import Optional, Dict, Any\n\n# External config cache (loaded from file if provided)\n_external_config: Optional[Dict[str, Any]] = None\n_config_file_path: Optional[Path] = None\n\n\ndef load_external_config(config_path: Optional[str] = None) -> bool:\n    \"\"\"\n    Load configuration from an external JSON file.\n\n    Args:\n        config_path: Path to config file (JSON format). If None, looks for\n                     'development_tools_config.json' in project root.\n\n    Returns:\n        True if config was loaded successfully, False otherwise.\n    \"\"\"\n    global _external_config, _config_file_path\n\n    if config_path:\n        config_file = Path(config_path).resolve()\n    else:\n        # Try to find config in development_tools/config/ first, then project root\n        project_root = _get_default_project_root()\n        # First check in development_tools/config/\n        config_file = (\n            project_root\n            / \"development_tools\"\n            / \"config\"\n            / \"development_tools_config.json\"\n        )\n        if not config_file.exists():\n            # Fallback to project root for backward compatibility\n            config_file = project_root / \"development_tools_config.json\"\n\n    if not config_file.exists():\n        _external_config = None\n        _config_file_path = None\n        return False\n\n    try:\n        with open(config_file, \"r\", encoding=\"utf-8\") as f:\n            _external_config = json.load(f)\n        _config_file_path = config_file\n        return True\n    except Exception as e:\n        # Log error but don't fail - fall back to defaults\n        _external_config = None\n        _config_file_path = None\n        return False\n\n\ndef _get_external_value(key: str, default: Any) -> Any:\n    \"\"\"Get value from external config if available, otherwise return default.\"\"\"\n    if _external_config is None:\n        return default\n    # Support nested keys like \"paths.project_root\"\n    keys = key.split(\".\")\n    value = _external_config\n    for k in keys:\n        if isinstance(value, dict) and k in value:\n            value = value[k]\n        else:\n            return default\n    return value\n\n\ndef get_external_value(key: str, default: Any) -> Any:\n    \"\"\"Public API to get value from external config.\"\"\"\n    return _get_external_value(key, default)\n\n\ndef _get_default_project_root() -> Path:\n    \"\"\"Get default project root based on current working directory.\"\"\"\n    if os.path.basename(os.getcwd()) == \"development_tools\":\n        return Path(\"..\")\n    else:\n        return Path(\".\")\n\n\n# Project-specific settings\n# Handle both direct execution and runner execution\n# NOTE: Hardcoded defaults are minimal/generic. Projects should provide development_tools_config.json\n# for complete configuration. These defaults are fallbacks only.\n_DEFAULT_PROJECT_ROOT = _get_default_project_root()\nPROJECT_ROOT = str(_DEFAULT_PROJECT_ROOT)\n# Generic default scan directories (should be overridden via config file)\nSCAN_DIRECTORIES = []  # Empty by default - requires config file\n\n# AI Collaboration Optimization\nAI_COLLABORATION = {\n    \"concise_output\": True,  # Generate concise summaries for AI context\n    \"detailed_files\": True,  # Save detailed results to files for reference\n    \"actionable_insights\": True,  # Focus on actionable recommendations\n    \"context_aware\": True,  # Adapt output based on what AI needs to know\n    \"priority_issues\": True,  # Highlight critical issues first\n    \"integration_mode\": True,  # Enable tool integration and data sharing\n}\n\n# Function discovery settings\nFUNCTION_DISCOVERY = {\n    \"moderate_complexity_threshold\": 50,  # Functions above this complexity need review\n    \"high_complexity_threshold\": 100,  # Functions above this complexity need refactoring\n    \"critical_complexity_threshold\": 200,  # Functions above this complexity are critical\n    \"min_docstring_length\": 10,  # Minimum docstring length to be considered documented\n    \"handler_keywords\": [  # Keywords that indicate handler/utility functions\n        \"handle\",\n        \"process\",\n        \"validate\",\n        \"check\",\n        \"get\",\n        \"set\",\n        \"save\",\n        \"load\",\n        \"create\",\n        \"update\",\n        \"delete\",\n        \"manage\",\n        \"configure\",\n        \"setup\",\n    ],\n    \"test_keywords\": [  # Keywords that indicate test functions\n        \"test_\",\n        \"test\",\n        \"check_\",\n        \"verify_\",\n        \"assert_\",\n    ],\n    \"critical_functions\": [  # Functions that are critical for system operation\n        \"main\",\n        \"run\",\n        \"start\",\n        \"stop\",\n        \"init\",\n        \"setup\",\n        \"validate\",\n    ],\n}\n\n# Validation settings\nVALIDATION = {\n    \"documentation_coverage_threshold\": 80.0,  # Minimum acceptable documentation coverage\n    \"moderate_complexity_warning\": 50,  # Functions above this complexity get warnings\n    \"high_complexity_warning\": 100,  # Functions above this complexity need attention\n    \"critical_complexity_warning\": 200,  # Functions above this complexity are critical\n    \"duplicate_function_warning\": True,  # Warn about duplicate function names\n    \"missing_docstring_warning\": True,  # Warn about functions without docstrings\n    \"critical_issues_first\": True,  # Show critical issues before minor ones\n}\n\n# Error handling settings (for analyze_error_handling.py)\n# NOTE: Defaults are generic. Projects should provide error_handling section in config file.\nERROR_HANDLING = {\n    \"decorator_names\": [\"@handle_errors\", \"handle_errors\", \"error_handler\"],\n    \"exception_base_classes\": [\n        \"BaseError\",\n        \"DataError\",\n        \"ConfigurationError\",\n        \"CommunicationError\",\n        \"ValidationError\",\n        \"AIError\",\n    ],\n    \"error_handler_functions\": [\n        \"handle_file_error\",\n        \"handle_network_error\",\n        \"handle_communication_error\",\n        \"handle_configuration_error\",\n        \"handle_validation_error\",\n        \"handle_ai_error\",\n        \"safe_file_operation\",\n    ],\n    \"generic_exceptions\": {\n        \"Exception\": \"BaseError\",\n        \"ValueError\": \"ValidationError or DataError\",\n        \"KeyError\": \"DataError or ConfigurationError\",\n        \"TypeError\": \"ValidationError or DataError\",\n    },\n    \"critical_function_keywords\": {\n        \"file_operations\": [\"open\", \"read\", \"write\", \"save\", \"load\"],\n        \"network_operations\": [\"send\", \"receive\", \"connect\", \"request\"],\n        \"data_operations\": [\"parse\", \"serialize\", \"deserialize\", \"validate\"],\n        \"user_operations\": [\"create\", \"update\", \"delete\", \"authenticate\"],\n        \"ai_operations\": [\"generate\", \"process\", \"analyze\", \"classify\"],\n    },\n}\n\n# Audit settings\nAUDIT = {\n    \"include_generated_files\": False,  # Include auto-generated UI files\n    \"include_test_files\": True,  # Include test files in analysis\n    \"include_legacy_files\": False,  # Include deprecated/legacy files\n    \"max_output_lines\": 20,  # Maximum lines to show in concise reports\n    \"save_detailed_results\": True,  # Save detailed results to files\n    \"generate_summary\": True,  # Generate executive summary\n    \"highlight_issues\": True,  # Highlight issues that need attention\n}\n\n# Output formatting for AI collaboration\nOUTPUT = {\n    \"use_emojis\": False,  # No emojis for cleaner AI consumption\n    \"show_progress\": False,  # Minimal progress indicators\n    \"color_output\": False,  # No colors for AI consumption\n    \"detailed_reports\": False,  # Save detailed reports to files instead\n    \"concise_format\": True,  # Use concise, scannable format\n    \"priority_indicators\": True,  # Use indicators like [CRITICAL], [WARN], [INFO]\n    \"action_items\": True,  # Clearly mark actionable items\n}\n\n# File patterns\nFILE_PATTERNS = {\n    \"python_files\": \"*.py\",\n    \"documentation_files\": \"*.md\",\n    \"ui_files\": \"*.ui\",\n    \"config_files\": \"*.json\",\n    \"exclude_patterns\": [\n        \"__pycache__\",\n        \".git\",\n        \"venv\",\n        \"env\",\n        \"node_modules\",\n        \"*.pyc\",\n        \"*.pyo\",\n    ],\n}\n\n# Quick audit settings optimized for AI\n# NOTE: Paths are relative to project root. Projects should provide config file.\nQUICK_AUDIT = {\n    \"run_function_audit\": True,\n    \"run_dependency_audit\": True,\n    \"run_documentation_audit\": True,\n    \"run_validation\": True,\n    \"save_results\": True,\n    \"results_file\": \"development_tools/reports/analysis_detailed_results.json\",  # Generic path - override via config\n    \"issues_file\": \"development_tools/critical_issues.txt\",  # Generic path - override via config\n    \"audit_scripts\": [],  # Empty by default - requires config file\n    \"concise_output\": True,\n    \"prioritize_issues\": True,\n}\n\n# Audit tier configuration\n# Defines which tools run in each audit tier\nAUDIT_TIERS = {\n    \"quick\": {\n        \"tools\": [\n            \"analyze_functions\",\n            \"analyze_documentation_sync\",\n            \"system_signals\",\n            \"quick_status\",\n        ],\n        \"description\": \"Lightweight analysis - core metrics only\",\n    },\n    \"standard\": {\n        \"tools\": [\n            \"analyze_documentation\",\n            \"analyze_error_handling\",\n            \"decision_support\",\n            \"analyze_config\",\n            \"analyze_ai_work\",\n        ],\n        \"description\": \"Standard analysis - includes quality checks\",\n    },\n    \"full\": {\n        \"analyze_tools\": [\n            \"generate_test_coverage\",\n            \"analyze_unused_imports\",\n            \"analyze_legacy_references\",\n            \"analyze_module_dependencies\",\n        ],\n        \"report_generators\": [\n            \"generate_legacy_reference_report\",  # \u2192 LEGACY_REFERENCE_REPORT.md\n            \"generate_test_coverage_reports\",  # \u2192 TEST_COVERAGE_EXPANSION_PLAN.md\n            \"analyze_unused_imports\",  # \u2192 UNUSED_IMPORTS_REPORT.md (generates report)\n        ],\n        \"description\": \"Comprehensive analysis - includes coverage, dependencies, and improvement reports\",\n    },\n}\n\n# Version sync settings\n# NOTE: Projects should provide config file. All paths are relative to project root.\nVERSION_SYNC = {\n    \"ai_docs\": [],  # Empty by default - requires config file\n    \"docs\": [],  # Empty by default - requires config file\n    \"cursor_rules\": [\".cursor/rules/*.mdc\"],  # Generic pattern\n    \"communication_docs\": [],  # Empty by default - requires config file\n    \"core_docs\": [],  # Empty by default - requires config file\n    \"logs_docs\": [],  # Empty by default - requires config file\n    \"scripts_docs\": [],  # Empty by default - requires config file\n    \"tests_docs\": [],  # Empty by default - requires config file\n    \"documentation_patterns\": [\"*.md\"],  # Generic pattern\n    \"exclude_patterns\": [\"*.pyc\", \"__pycache__\", \".git\", \".venv\"],  # Generic patterns\n}\n\n# Workflow configuration\nWORKFLOW = {\n    \"audit_first\": True,  # Always run audit before other operations\n    \"validate_results\": True,  # Validate results before presenting\n    \"require_user_approval\": False,  # AI tools don't need user approval\n    \"save_intermediate_results\": True,  # Save intermediate results for debugging\n    \"generate_action_items\": True,  # Generate clear action items\n    \"prioritize_by_impact\": True,  # Prioritize issues by potential impact\n}\n\n# Documentation configuration\nDOCUMENTATION = {\n    \"auto_generate\": True,  # Auto-generate documentation\n    \"preserve_manual_content\": True,  # Preserve manual enhancements\n    \"update_frequency\": \"on_demand\",  # Update when requested\n    \"coverage_target\": 95.0,  # Target documentation coverage\n    \"quality_threshold\": 80.0,  # Minimum quality score\n}\n\n# Auto-documentation settings\nAUTO_DOCUMENT = {\n    \"enabled\": True,  # Enable auto-documentation\n    \"template_generation\": True,  # Generate templates for missing docs\n    \"quality_check\": True,  # Check documentation quality\n    \"suggest_improvements\": True,  # Suggest improvements\n    \"preserve_manual_work\": True,  # Preserve manual enhancements\n}\n\n# AI validation configuration\nAI_VALIDATION = {\n    \"completeness_threshold\": 90.0,  # Minimum completeness score\n    \"accuracy_threshold\": 85.0,  # Minimum accuracy score\n    \"consistency_threshold\": 80.0,  # Minimum consistency score\n    \"actionable_threshold\": 75.0,  # Minimum actionable score\n    \"critical_issues_weight\": 3.0,  # Weight for critical issues\n    \"warning_issues_weight\": 1.5,  # Weight for warning issues\n    \"info_issues_weight\": 1.0,  # Weight for info issues\n}\n\n\n# Helper functions\ndef get_project_root():\n    \"\"\"Get the project root directory (from external config if available, otherwise default).\"\"\"\n    external_root = _get_external_value(\"paths.project_root\", None)\n    if external_root:\n        return Path(external_root)\n    # Fall back to default logic\n    if _external_config is None:\n        # Use default detection\n        if os.path.basename(os.getcwd()) == \"development_tools\":\n            return Path(\"..\")\n        else:\n            return Path(\".\")\n    return Path(PROJECT_ROOT)\n\n\ndef get_scan_directories():\n    \"\"\"\n    Get the directories to scan for analysis (from external config if available, otherwise default).\n\n    NOTE: Default is empty list. Projects must provide scan_directories in config file.\n    \"\"\"\n    external_dirs = _get_external_value(\"paths.scan_directories\", None)\n    if external_dirs:\n        return external_dirs\n    return SCAN_DIRECTORIES  # Empty by default - requires config file\n\n\ndef get_project_name(default: str = \"Project\") -> str:\n    \"\"\"Get project name from config (from external config if available, otherwise default).\"\"\"\n    return _get_external_value(\"project.name\", default)\n\n\ndef get_project_key_files(default: Optional[list] = None) -> list:\n    \"\"\"Get project key files from config (from external config if available, otherwise default).\"\"\"\n    if default is None:\n        default = [\"requirements.txt\"]\n    key_files = _get_external_value(\"project.key_files\", default)\n    return key_files if isinstance(key_files, list) else default\n\n\ndef get_project_core_system_files(default: Optional[list] = None) -> list:\n    \"\"\"Get project core system files from config (from external config if available, otherwise default).\n\n    Checks project.core_system_files first, then falls back to project.key_files if not found.\n    \"\"\"\n    if default is None:\n        default = [\n            \"run_mhm.py\",\n            \"core/service.py\",\n            \"core/config.py\",\n            \"requirements.txt\",\n            \".gitignore\",\n        ]\n    core_files = _get_external_value(\"project.core_system_files\", None)\n    # If core_system_files is empty or invalid, fall back to key_files\n    if not core_files or not isinstance(core_files, list) or len(core_files) == 0:\n        core_files = _get_external_value(\"project.key_files\", default)\n    return core_files if isinstance(core_files, list) else default\n\n\ndef get_analyze_functions_config():\n    \"\"\"Get analyze functions configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"analyze_functions\", None)\n    if external_config:\n        result = FUNCTION_DISCOVERY.copy()\n        result.update(external_config)\n        return result\n    return FUNCTION_DISCOVERY\n\n\ndef get_validation_config():\n    \"\"\"Get validation configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"validation\", None)\n    if external_config:\n        result = VALIDATION.copy()\n        result.update(external_config)\n        return result\n    return VALIDATION\n\n\ndef get_error_handling_config():\n    \"\"\"Get error handling configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"error_handling\", None)\n    if external_config:\n        result = ERROR_HANDLING.copy()\n        # Deep merge for nested dicts\n        if \"generic_exceptions\" in external_config and \"generic_exceptions\" in result:\n            result[\"generic_exceptions\"].update(\n                external_config.get(\"generic_exceptions\", {})\n            )\n        if (\n            \"critical_function_keywords\" in external_config\n            and \"critical_function_keywords\" in result\n        ):\n            result[\"critical_function_keywords\"].update(\n                external_config.get(\"critical_function_keywords\", {})\n            )\n        # Update other keys\n        for key, value in external_config.items():\n            if key not in (\"generic_exceptions\", \"critical_function_keywords\"):\n                result[key] = value\n        return result\n    return ERROR_HANDLING\n\n\ndef get_audit_config():\n    \"\"\"Get audit configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"audit\", None)\n    if external_config:\n        result = AUDIT.copy()\n        result.update(external_config)\n        return result\n    return AUDIT\n\n\ndef get_output_config():\n    \"\"\"Get output formatting configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"output\", None)\n    if external_config:\n        result = OUTPUT.copy()\n        result.update(external_config)\n        return result\n    return OUTPUT\n\n\ndef get_workflow_config():\n    \"\"\"Get workflow configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"workflow\", None)\n    if external_config:\n        result = WORKFLOW.copy()\n        result.update(external_config)\n        return result\n    return WORKFLOW\n\n\ndef get_documentation_config():\n    \"\"\"Get documentation configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"documentation\", None)\n    if external_config:\n        result = DOCUMENTATION.copy()\n        result.update(external_config)\n        return result\n    return DOCUMENTATION\n\n\ndef get_auto_document_config():\n    \"\"\"Get auto-documentation configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"auto_document\", None)\n    if external_config:\n        result = AUTO_DOCUMENT.copy()\n        result.update(external_config)\n        return result\n    return AUTO_DOCUMENT\n\n\ndef get_ai_validation_config():\n    \"\"\"Get AI validation configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"ai_validation\", None)\n    if external_config:\n        result = AI_VALIDATION.copy()\n        result.update(external_config)\n        return result\n    return AI_VALIDATION\n\n\ndef get_ai_collaboration_config():\n    \"\"\"Get AI collaboration optimization configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"ai_collaboration\", None)\n    if external_config:\n        result = AI_COLLABORATION.copy()\n        result.update(external_config)\n        return result\n    return AI_COLLABORATION\n\n\ndef get_fix_version_sync_config():\n    \"\"\"Get fix version sync configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"fix_version_sync\", None)\n    if external_config:\n        # Merge external config with defaults (external takes precedence)\n        result = VERSION_SYNC.copy()\n        result.update(external_config)\n        return result\n    return VERSION_SYNC\n\n\ndef get_quick_audit_config():\n    \"\"\"Get quick audit configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"quick_audit\", None)\n    if external_config:\n        # Merge external config with defaults (external takes precedence)\n        result = QUICK_AUDIT.copy()\n        result.update(external_config)\n        return result\n    return QUICK_AUDIT\n\n\ndef get_audit_tiers_config():\n    \"\"\"Get audit tier configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"audit_tiers\", None)\n    if external_config:\n        # Merge external config with defaults (external takes precedence)\n        result = AUDIT_TIERS.copy()\n        # Deep merge for nested dicts\n        for tier in [\"quick\", \"standard\", \"full\"]:\n            if tier in external_config:\n                if tier in result:\n                    result[tier] = {**result[tier], **external_config[tier]}\n                else:\n                    result[tier] = external_config[tier]\n        return result\n    return AUDIT_TIERS\n\n\ndef get_paths_config():\n    \"\"\"\n    Get paths configuration (docs, logs, data directories).\n    Returns dict with keys: docs_dir, logs_dir, data_dir, ai_docs_dir, development_docs_dir\n\n    NOTE: Defaults are generic. Projects should provide config file for proper paths.\n    \"\"\"\n    paths = {\n        \"docs_dir\": _get_external_value(\"paths.docs_dir\", \"docs\"),  # Generic default\n        \"logs_dir\": _get_external_value(\"paths.logs_dir\", \"logs\"),\n        \"data_dir\": _get_external_value(\"paths.data_dir\", \"data\"),\n        \"ai_docs_dir\": _get_external_value(\n            \"paths.ai_docs_dir\", \"docs/ai\"\n        ),  # Generic default\n        \"development_docs_dir\": _get_external_value(\n            \"paths.development_docs_dir\", \"docs\"\n        ),  # Generic default\n    }\n    return paths\n\n\ndef get_exclusions_config():\n    \"\"\"\n    Get exclusions configuration from external config.\n    Returns dict with exclusion patterns organized by category.\n\n    NOTE: Returns empty dict if no external config. standard_exclusions.py will use\n    its internal defaults as fallback.\n    \"\"\"\n    exclusions = _get_external_value(\"exclusions\", {})\n    return exclusions if isinstance(exclusions, dict) else {}\n\n\ndef get_constants_config():\n    \"\"\"\n    Get constants configuration from external config.\n    Returns dict with project-specific constants (default_docs, paired_docs, etc.).\n\n    NOTE: Returns empty dict if no external config. constants.py will use\n    its internal defaults as fallback.\n    \"\"\"\n    constants = _get_external_value(\"constants\", {})\n    return constants if isinstance(constants, dict) else {}\n\n\n# Documentation analysis configuration\n# NOTE: Defaults are minimal. See development_tools_config.json.example for full examples.\nDOCUMENTATION_ANALYSIS = {\n    \"heading_patterns\": [\"## \", \"### \"],  # Generic markdown patterns\n    \"placeholder_patterns\": [r\"TBD\", r\"TODO\"],  # Minimal generic patterns\n    \"placeholder_flags\": [\"IGNORECASE\"],\n    \"topic_keywords\": {},  # Empty - projects should define their own\n    \"ignore_rules\": [],\n}\n\n\ndef get_documentation_analysis_config():\n    \"\"\"Get documentation analysis configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"documentation_analysis\", None)\n    if external_config:\n        result = DOCUMENTATION_ANALYSIS.copy()\n        # Deep merge for nested dicts\n        if \"topic_keywords\" in external_config and \"topic_keywords\" in result:\n            result[\"topic_keywords\"].update(external_config.get(\"topic_keywords\", {}))\n        # Update other keys\n        for key, value in external_config.items():\n            if key != \"topic_keywords\":\n                result[key] = value\n        return result\n    return DOCUMENTATION_ANALYSIS\n\n\n# Audit function registry configuration\n# NOTE: Defaults are minimal. See development_tools_config.json.example for full examples.\nAUDIT_FUNCTION_REGISTRY = {\n    \"registry_path\": \"development_docs/FUNCTION_REGISTRY_DETAIL.md\",  # Generic default path\n    \"high_complexity_min\": 50,  # Generic threshold\n    \"top_complexity\": 10,\n    \"top_undocumented\": 5,\n    \"top_duplicates\": 5,\n    \"error_sample_limit\": 5,\n    \"max_complexity_json\": 200,\n    \"max_undocumented_json\": 200,\n    \"max_duplicates_json\": 200,\n}\n\n\ndef get_analyze_function_registry_config():\n    \"\"\"Get audit function registry configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"analyze_function_registry\", None)\n    if external_config:\n        result = AUDIT_FUNCTION_REGISTRY.copy()\n        result.update(external_config)\n        return result\n    return AUDIT_FUNCTION_REGISTRY\n\n\n# Audit module dependencies configuration\nAUDIT_MODULE_DEPENDENCIES = {\n    \"dependency_doc_path\": \"development_docs/MODULE_DEPENDENCIES_DETAIL.md\",  # Generic default\n}\n\n\ndef get_analyze_module_dependencies_config():\n    \"\"\"Get audit module dependencies configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"analyze_module_dependencies\", None)\n    if external_config:\n        result = AUDIT_MODULE_DEPENDENCIES.copy()\n        result.update(external_config)\n        return result\n    return AUDIT_MODULE_DEPENDENCIES\n\n\n# Audit package exports configuration\nAUDIT_PACKAGE_EXPORTS = {\n    \"export_patterns\": [],  # Patterns to identify exports\n    \"expected_exports\": {},  # Expected exports by module\n}\n\n\ndef get_analyze_package_exports_config():\n    \"\"\"Get audit package exports configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"analyze_package_exports\", None)\n    if external_config:\n        result = AUDIT_PACKAGE_EXPORTS.copy()\n        # Deep merge for nested dicts\n        if \"expected_exports\" in external_config and \"expected_exports\" in result:\n            result[\"expected_exports\"].update(\n                external_config.get(\"expected_exports\", {})\n            )\n        # Update other keys\n        for key, value in external_config.items():\n            if key != \"expected_exports\":\n                result[key] = value\n        return result\n    return AUDIT_PACKAGE_EXPORTS\n\n\n# Config validator configuration\nCONFIG_VALIDATOR = {\n    \"config_schema\": {},  # Expected config schema structure (empty = auto-detect)\n    \"validation_rules\": {},  # Custom validation rules\n}\n\n\ndef get_analyze_config_config():\n    \"\"\"Get config validator configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"analyze_config\", None)\n    if external_config:\n        result = CONFIG_VALIDATOR.copy()\n        # Deep merge for nested dicts\n        if \"validation_rules\" in external_config and \"validation_rules\" in result:\n            result[\"validation_rules\"].update(\n                external_config.get(\"validation_rules\", {})\n            )\n        # Update other keys\n        for key, value in external_config.items():\n            if key != \"validation_rules\":\n                result[key] = value\n        return result\n    return CONFIG_VALIDATOR\n\n\n# Validate AI work configuration\n# NOTE: Defaults are minimal. See development_tools_config.json.example for full examples.\nVALIDATE_AI_WORK = {\n    \"completeness_threshold\": 90.0,  # Generic thresholds\n    \"accuracy_threshold\": 85.0,\n    \"consistency_threshold\": 80.0,\n    \"actionable_threshold\": 75.0,\n    \"rule_sets\": {},  # Empty - projects should define their own\n    \"rule_set_paths\": [],\n}\n\n\ndef get_analyze_ai_work_config():\n    \"\"\"Get validate AI work configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"analyze_ai_work\", None)\n    if external_config:\n        result = VALIDATE_AI_WORK.copy()\n        # Deep merge for nested dicts\n        if \"rule_sets\" in external_config and \"rule_sets\" in result:\n            result[\"rule_sets\"].update(external_config.get(\"rule_sets\", {}))\n        # Update other keys\n        for key, value in external_config.items():\n            if key not in (\"rule_sets\",):\n                result[key] = value\n        return result\n    return VALIDATE_AI_WORK\n\n\n# Unused imports checker configuration\nUNUSED_IMPORTS = {\n    \"pylint_command\": [\"python\", \"-m\", \"pylint\"],  # Pylint command as list\n    \"ignore_patterns\": [],  # Patterns to ignore\n    \"type_stub_locations\": [],  # Locations for type stubs (.pyi files)\n    \"timeout_seconds\": 30,  # Timeout per file\n}\n\n\ndef get_unused_imports_config():\n    \"\"\"Get unused imports checker configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"unused_imports\", None)\n    if external_config:\n        result = UNUSED_IMPORTS.copy()\n        result.update(external_config)\n        return result\n    return UNUSED_IMPORTS\n\n\n# Quick status configuration\nQUICK_STATUS = {\n    \"core_files\": [],  # Will use project.key_files if empty\n    \"key_directories\": [],  # Will use paths.scan_directories if empty\n    \"data_source_plugins\": {},  # Plugin hooks for data sources (Discord, schedulers, etc.)\n}\n\n\ndef get_quick_status_config():\n    \"\"\"Get quick status configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"quick_status\", None)\n    if external_config:\n        result = QUICK_STATUS.copy()\n        # Deep merge for nested dicts\n        if \"data_source_plugins\" in external_config and \"data_source_plugins\" in result:\n            result[\"data_source_plugins\"].update(\n                external_config.get(\"data_source_plugins\", {})\n            )\n        # Update other keys\n        for key, value in external_config.items():\n            if key != \"data_source_plugins\":\n                result[key] = value\n        return result\n    return QUICK_STATUS\n\n\n# Status configuration\n# NOTE: Defaults are minimal. See development_tools_config.json.example for full examples.\nSTATUS = {\n    \"check_key_files\": True,\n    \"check_audit_results\": True,\n    \"generate_status_files\": True,\n    \"status_files\": {\n        # Default paths include development_tools/ for backward compatibility\n        # Projects can override via development_tools_config.json\n        \"ai_status\": \"development_tools/AI_STATUS.md\",\n        \"ai_priorities\": \"development_tools/AI_PRIORITIES.md\",\n        \"consolidated_report\": \"development_tools/consolidated_report.txt\",\n    },\n}\n\n\ndef get_status_config():\n    \"\"\"Get status configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"status\", None)\n    if external_config:\n        result = STATUS.copy()\n        # Deep merge for nested dicts\n        if \"status_files\" in external_config and \"status_files\" in result:\n            result[\"status_files\"].update(external_config.get(\"status_files\", {}))\n        # Update other keys\n        for key, value in external_config.items():\n            if key != \"status_files\":\n                result[key] = value\n        return result\n    return STATUS\n\n\n# System signals configuration\nSYSTEM_SIGNALS = {\n    \"core_files\": [],  # Will use project.key_files if empty\n    \"data_source_plugins\": {},  # Plugin hooks for data sources\n}\n\n\ndef get_system_signals_config():\n    \"\"\"Get system signals configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"system_signals\", None)\n    if external_config:\n        result = SYSTEM_SIGNALS.copy()\n        # Deep merge for nested dicts\n        if \"data_source_plugins\" in external_config and \"data_source_plugins\" in result:\n            result[\"data_source_plugins\"].update(\n                external_config.get(\"data_source_plugins\", {})\n            )\n        # Update other keys\n        for key, value in external_config.items():\n            if key != \"data_source_plugins\":\n                result[key] = value\n        return result\n    return SYSTEM_SIGNALS\n\n\n# Auto document functions configuration\n# NOTE: Defaults are minimal. See development_tools_config.json.example for full examples.\nAUTO_DOCUMENT_FUNCTIONS = {\n    \"template_paths\": {},  # Empty - projects should define their own\n    \"doc_targets\": {},  # Empty - projects should define their own\n    \"formatting_rules\": {},  # Empty - projects should define their own\n    \"function_type_detection\": {  # Minimal generic patterns\n        \"test_function\": {\"name_patterns\": [\"test_\"]},\n        \"special_method\": {\"name_pattern\": \"__.*__\"},\n        \"constructor\": {\"name\": \"__init__\"},\n        \"main_function\": {\"name\": \"main\"},\n    },\n}\n\n\ndef get_generate_function_docstrings_config():\n    \"\"\"Get generate function docstrings configuration (from external config if available, otherwise default).\"\"\"\n    external_config = _get_external_value(\"generate_function_docstrings\", None)\n    if external_config:\n        result = AUTO_DOCUMENT_FUNCTIONS.copy()\n        # Deep merge for nested dicts\n        for key in [\n            \"template_paths\",\n            \"doc_targets\",\n            \"formatting_rules\",\n            \"function_type_detection\",\n        ]:\n            if key in external_config and key in result:\n                if isinstance(result[key], dict):\n                    result[key].update(external_config.get(key, {}))\n                else:\n                    result[key] = external_config.get(key, result[key])\n        # Update other keys\n        for key, value in external_config.items():\n            if key not in (\n                \"template_paths\",\n                \"doc_targets\",\n                \"formatting_rules\",\n                \"function_type_detection\",\n            ):\n                result[key] = value\n        return result\n    return AUTO_DOCUMENT_FUNCTIONS\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 49,
                    "line_content": "# Fallback to project root for backward compatibility",
                    "start": 1583,
                    "end": 1605
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 808,
                    "line_content": "# Default paths include development_tools/ for backward compatibility",
                    "start": 29863,
                    "end": 29885
                  }
                ]
              ],
              [
                "development_tools\\config\\__init__.py",
                "\"\"\"Configuration module for development tools.\"\"\"\n\n# Re-export config module for backward compatibility\n# Import everything from the actual config module\nfrom .config import *  # noqa: F401, F403\n\n# The above should work, but if it doesn't, we can explicitly import\n# For now, let's try the simpler approach and see if it works\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 3,
                    "line_content": "# Re-export config module for backward compatibility",
                    "start": 81,
                    "end": 103
                  }
                ]
              ],
              [
                "development_tools\\docs\\analyze_documentation_sync.py",
                "#!/usr/bin/env python3\n# TOOL_TIER: core\n# TOOL_PORTABILITY: portable\n\n\"\"\"\nDocumentation Synchronization Checker\n\nThis script checks for paired AI/human documentation consistency.\nOther documentation checks have been decomposed into separate tools:\n- Path drift: analyze_path_drift.py\n- ASCII compliance: analyze_ascii_compliance.py\n- Heading numbering: analyze_heading_numbering.py\n- Missing addresses: analyze_missing_addresses.py\n- Unconverted links: analyze_unconverted_links.py\n- Directory trees: generate_directory_tree.py\n\nConfiguration is loaded from external config file (development_tools_config.json)\nif available, making this tool portable across different projects.\n\nUsage:\n    python docs/analyze_documentation_sync.py [--check]\n\"\"\"\n\nimport re\nimport argparse\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional\nfrom collections import defaultdict\n\n# Add project root to path for core module imports\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nfrom core.logger import get_component_logger\n\n# Handle both relative and absolute imports\nif __name__ != \"__main__\" and __package__ and \".\" in __package__:\n    from .. import config\n    from ..shared.constants import PAIRED_DOCS\nelse:\n    from development_tools import config\n    from development_tools.shared.constants import PAIRED_DOCS\n\nlogger = get_component_logger(\"development_tools\")\n\n\nclass DocumentationSyncChecker:\n    \"\"\"Checks for paired AI/human documentation consistency.\"\"\"\n\n    def __init__(\n        self, project_root: Optional[str] = None, config_path: Optional[str] = None\n    ):\n        \"\"\"\n        Initialize documentation sync checker.\n\n        Args:\n            project_root: Root directory of the project\n            config_path: Optional path to external config file\n        \"\"\"\n        # Load external config if provided\n        if config_path:\n            config.load_external_config(config_path)\n        else:\n            config.load_external_config()\n\n        # Use provided project_root or get from config\n        if project_root:\n            self.project_root = Path(project_root).resolve()\n        else:\n            self.project_root = Path(config.get_project_root()).resolve()\n\n        # Load paired docs from constants (which loads from config)\n        self.paired_docs = dict(PAIRED_DOCS)\n\n    def check_paired_documentation(self) -> Dict[str, List[str]]:\n        \"\"\"Check for inconsistencies in paired AI/human documentation.\"\"\"\n        issues = defaultdict(list)\n\n        for human_doc, ai_doc in self.paired_docs.items():\n            human_path = self.project_root / human_doc\n            ai_path = self.project_root / ai_doc\n\n            if not human_path.exists():\n                issues[\"missing_human_docs\"].append(human_doc)\n            if not ai_path.exists():\n                issues[\"missing_ai_docs\"].append(ai_doc)\n\n            if human_path.exists() and ai_path.exists():\n                # Check for content synchronization issues\n                try:\n                    with open(human_path, \"r\", encoding=\"utf-8\") as f:\n                        human_content = f.read()\n                    with open(ai_path, \"r\", encoding=\"utf-8\") as f:\n                        ai_content = f.read()\n\n                    # Simple content comparison (could be enhanced)\n                    human_sections = set(\n                        re.findall(r\"^##\\s+(.+)$\", human_content, re.MULTILINE)\n                    )\n                    ai_sections = set(\n                        re.findall(r\"^##\\s+(.+)$\", ai_content, re.MULTILINE)\n                    )\n\n                    missing_in_ai = human_sections - ai_sections\n                    missing_in_human = ai_sections - human_sections\n\n                    if missing_in_ai:\n                        issues[\"content_sync\"].append(\n                            f\"{human_doc} has sections missing in {ai_doc}: {missing_in_ai}\"\n                        )\n                    if missing_in_human:\n                        issues[\"content_sync\"].append(\n                            f\"{ai_doc} has sections missing in {human_doc}: {missing_in_human}\"\n                        )\n\n                except Exception as e:\n                    issues[\"read_errors\"].append(\n                        f\"Error reading {human_doc} or {ai_doc}: {e}\"\n                    )\n\n        return issues\n\n    def run_checks(self) -> Dict[str, any]:\n        \"\"\"\n        Run paired documentation synchronization checks and return results in standard format.\n\n        Note: Other documentation checks (path drift, ASCII compliance, etc.)\n        have been decomposed into separate tools. This method only checks\n        paired documentation consistency.\n\n        Returns:\n            Dictionary with standard format: 'summary', and 'details' keys\n        \"\"\"\n        if logger:\n            logger.info(\"Analyzing documentation synchronization...\")\n\n        paired_docs = self.check_paired_documentation()\n\n        # Generate summary\n        total_issues = sum(len(issues) for issues in paired_docs.values())\n        status = \"PASS\" if total_issues == 0 else \"FAIL\"\n\n        # Return standard format\n        return {\n            \"summary\": {\n                \"total_issues\": total_issues,\n                \"files_affected\": 0,  # Not file-based\n                \"status\": status,\n            },\n            \"details\": {\"paired_doc_issues\": total_issues, \"paired_docs\": paired_docs},\n        }\n\n    def print_report(self, results: Dict[str, any]):\n        \"\"\"Print a formatted report of the results.\"\"\"\n        summary = results[\"summary\"]\n        print(f\"\\nSUMMARY:\")\n        print(f\"   Status: {summary['status']}\")\n        print(f\"   Total Issues: {summary['total_issues']}\")\n        print(f\"   Paired Doc Issues: {summary['paired_doc_issues']}\")\n\n        # Paired Documentation Issues\n        if results[\"paired_docs\"]:\n            print(f\"\\nPAIRED DOCUMENTATION ISSUES:\")\n            for issue_type, issues in results[\"paired_docs\"].items():\n                if issues:\n                    print(f\"   {issue_type}:\")\n                    for issue in issues:\n                        # Clean Unicode characters that cause encoding issues\n                        clean_issue = issue.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n                        print(f\"     - {clean_issue}\")\n\n        if summary[\"total_issues\"] == 0:\n            print(f\"\\nAll paired documentation synchronization checks passed!\")\n        else:\n            print(\n                f\"\\nFound {summary['total_issues']} paired documentation synchronization issues.\"\n            )\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    import json\n\n    parser = argparse.ArgumentParser(\n        description=\"Check paired documentation synchronization\"\n    )\n    parser.add_argument(\n        \"--check\",\n        action=\"store_true\",\n        help=\"Run paired documentation checks (default: always runs)\",\n    )\n    parser.add_argument(\n        \"--json\", action=\"store_true\", help=\"Output results as JSON in standard format\"\n    )\n\n    args = parser.parse_args()\n\n    checker = DocumentationSyncChecker()\n\n    # Always run checks by default (this is an analysis tool)\n    # The --check flag is maintained for backward compatibility but has no effect\n    results = checker.run_checks()\n\n    if args.json:\n        # Output JSON in standard format\n        print(json.dumps(results, indent=2))\n    else:\n        # Convert to old format for print_report compatibility\n        legacy_results = {\n            \"summary\": {\n                \"total_issues\": results[\"summary\"][\"total_issues\"],\n                \"paired_doc_issues\": results[\"details\"][\"paired_doc_issues\"],\n                \"status\": results[\"summary\"][\"status\"],\n            },\n            \"paired_docs\": results[\"details\"][\"paired_docs\"],\n        }\n        checker.print_report(legacy_results)\n\n\nif __name__ == \"__main__\":\n    main()\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 203,
                    "line_content": "# The --check flag is maintained for backward compatibility but has no effect",
                    "start": 7276,
                    "end": 7298
                  }
                ]
              ],
              [
                "development_tools\\imports\\analyze_dependency_patterns.py",
                "#!/usr/bin/env python3\n# TOOL_TIER: core\n# TOOL_PORTABILITY: portable\n\n\"\"\"\nDependency Pattern Analyzer\nAnalyzes dependency patterns, circular dependencies, and risk areas.\n\"\"\"\n\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Tuple\n\n# Add project root to path for core module imports\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nfrom core.logger import get_component_logger\n\n# Handle both relative and absolute imports\n# Note: ensure_ascii is imported but not used in this module\n\n# Load external config on module import\nif __name__ != \"__main__\" and __package__ and \".\" in __package__:\n    from .. import config\nelse:\n    from development_tools import config\n\nconfig.load_external_config()\n\nlogger = get_component_logger(\"development_tools\")\n\n\nclass DependencyPatternAnalyzer:\n    \"\"\"Analyzes dependency patterns, circular dependencies, and risk areas.\"\"\"\n\n    def analyze_dependency_patterns(\n        self, actual_imports: Dict[str, Dict]\n    ) -> Dict[str, Any]:\n        \"\"\"Analyze dependency patterns for AI consumption.\"\"\"\n        patterns = {\n            \"core_dependencies\": [],\n            \"communication_dependencies\": [],\n            \"ui_dependencies\": [],\n            \"third_party_dependencies\": [],\n            \"circular_dependencies\": [],\n            \"high_coupling\": [],\n        }\n\n        # Analyze dependency patterns\n        for file_path, data in actual_imports.items():\n            local_imports = data[\"imports\"].get(\"local\", [])\n            third_party_imports = data[\"imports\"].get(\"third_party\", [])\n\n            # Core dependencies\n            if file_path.startswith(\"core/\"):\n                patterns[\"core_dependencies\"].append(\n                    {\n                        \"file\": file_path,\n                        \"local_imports\": len(local_imports),\n                        \"third_party_imports\": len(third_party_imports),\n                        \"modules\": [imp[\"module\"] for imp in local_imports],\n                    }\n                )\n\n            # Communication/AI dependencies\n            elif file_path.startswith(\"communication/\") or file_path.startswith(\"ai/\"):\n                patterns[\"communication_dependencies\"].append(\n                    {\n                        \"file\": file_path,\n                        \"local_imports\": len(local_imports),\n                        \"third_party_imports\": len(third_party_imports),\n                        \"modules\": [imp[\"module\"] for imp in local_imports],\n                    }\n                )\n\n            # UI dependencies\n            elif file_path.startswith(\"ui/\"):\n                patterns[\"ui_dependencies\"].append(\n                    {\n                        \"file\": file_path,\n                        \"local_imports\": len(local_imports),\n                        \"third_party_imports\": len(third_party_imports),\n                        \"modules\": [imp[\"module\"] for imp in local_imports],\n                    }\n                )\n\n            # High coupling detection\n            if len(local_imports) > 5:\n                patterns[\"high_coupling\"].append(\n                    {\n                        \"file\": file_path,\n                        \"import_count\": len(local_imports),\n                        \"modules\": [imp[\"module\"] for imp in local_imports],\n                    }\n                )\n\n            # Third-party dependencies\n            if third_party_imports:\n                patterns[\"third_party_dependencies\"].append(\n                    {\n                        \"file\": file_path,\n                        \"dependencies\": [imp[\"module\"] for imp in third_party_imports],\n                    }\n                )\n\n        # Detect circular dependencies\n        patterns[\"circular_dependencies\"] = self.detect_circular_dependencies(\n            actual_imports\n        )\n\n        return patterns\n\n    def detect_circular_dependencies(\n        self, actual_imports: Dict[str, Dict]\n    ) -> List[Tuple[str, str]]:\n        \"\"\"Detect potential circular dependencies.\"\"\"\n        circular = []\n\n        for file_path, data in actual_imports.items():\n            local_imports = [imp[\"module\"] for imp in data[\"imports\"][\"local\"]]\n\n            # Check if imported modules also import this module\n            for imported_module in local_imports:\n                # Convert module name to file path\n                module_file = imported_module.replace(\".\", \"/\") + \".py\"\n\n                if module_file in actual_imports:\n                    imported_data = actual_imports[module_file]\n                    imported_local = [\n                        imp[\"module\"] for imp in imported_data[\"imports\"][\"local\"]\n                    ]\n\n                    # Convert current file to module name\n                    current_module = file_path.replace(\"/\", \".\").replace(\".py\", \"\")\n\n                    if current_module in imported_local:\n                        pair = tuple(sorted([file_path, module_file]))\n                        if pair not in circular:\n                            circular.append(pair)\n\n        return circular\n\n    def detect_risk_areas(\n        self, actual_imports: Dict[str, Dict], patterns: Dict[str, Any]\n    ) -> str:\n        \"\"\"Detect dependency risk areas dynamically.\"\"\"\n        lines = []\n\n        # High coupling\n        high_coupling = patterns.get(\"high_coupling\", [])\n        if high_coupling:\n            lines.append(\"### High Coupling\")\n            for item in sorted(\n                high_coupling, key=lambda x: x[\"import_count\"], reverse=True\n            )[:5]:\n                lines.append(\n                    f\"- `{item['file']}` -> {item['import_count']} local dependencies (heavy coupling)\"\n                )\n            lines.append(\"\")\n\n        # Third-party risks\n        third_party = patterns.get(\"third_party_dependencies\", [])\n        if third_party:\n            lines.append(\"### Third-Party Risks\")\n            third_party_map = {}\n            for item in third_party:\n                for dep in item[\"dependencies\"]:\n                    if dep not in third_party_map:\n                        third_party_map[dep] = []\n                    third_party_map[dep].append(item[\"file\"])\n\n            for dep, files in sorted(\n                third_party_map.items(), key=lambda x: len(x[1]), reverse=True\n            )[:5]:\n                lines.append(f\"- `{files[0]}` -> {dep} ({len(files)} modules use this)\")\n            lines.append(\"\")\n\n        # Circular dependencies\n        circular = patterns.get(\"circular_dependencies\", [])\n        if circular:\n            lines.append(\"### Circular Dependencies to Monitor\")\n            for pair in circular[:5]:\n                lines.append(f\"- `{pair[0]}` <-> `{pair[1]}`\")\n            lines.append(\"\")\n\n        return \"\\n\".join(lines) if lines else \"*No significant risk areas detected*\"\n\n    def find_critical_dependencies(self, actual_imports: Dict[str, Dict]) -> str:\n        \"\"\"Find critical dependencies dynamically.\"\"\"\n        lines = []\n\n        # Entry points\n        entry_points = [\n            f for f in actual_imports.keys() if \"run_\" in f or f.endswith(\"main.py\")\n        ]\n        if entry_points:\n            lines.append(\"### Entry Points\")\n            for ep in entry_points[:5]:\n                deps = self._format_module_dependencies(ep, actual_imports)\n                purpose = (\n                    \"main application entry\" if \"run_\" in ep else \"application entry\"\n                )\n                lines.append(f\"- `{ep}` -> {deps} ({purpose})\")\n            lines.append(\"\")\n\n        # Data flow\n        data_modules = [\n            f\n            for f in actual_imports.keys()\n            if \"user_data\" in f or \"file_operations\" in f\n        ]\n        if data_modules:\n            lines.append(\"### Data Flow\")\n            for mod in data_modules[:3]:\n                deps = self._format_module_dependencies(mod, actual_imports)\n                lines.append(f\"- {mod.split('/')[-1]}: {mod} <- {deps}\")\n            lines.append(\"\")\n\n        # Communication flow\n        comm_modules = [\n            f for f in actual_imports.keys() if f.startswith(\"communication/\")\n        ]\n        if comm_modules:\n            lines.append(\"### Communication Flow\")\n            for mod in comm_modules[:3]:\n                deps = self._format_module_dependencies(mod, actual_imports)\n                purpose = mod.split(\"/\")[-1].replace(\".py\", \"\")\n                lines.append(f\"- {purpose}: {mod} <- {deps}\")\n            lines.append(\"\")\n\n        return \"\\n\".join(lines) if lines else \"*No critical dependencies detected*\"\n\n    def _format_module_dependencies(\n        self, file_path: str, actual_imports: Dict[str, Dict], max_deps: int = 5\n    ) -> str:\n        \"\"\"Format module dependencies concisely - deduplicated and clean.\"\"\"\n        if file_path not in actual_imports:\n            return \"unknown\"\n\n        data = actual_imports[file_path]\n\n        # Get unique dependencies (deduplicate)\n        local_deps = list(\n            dict.fromkeys([imp[\"module\"] for imp in data[\"imports\"][\"local\"]])\n        )\n        stdlib_deps = list(\n            dict.fromkeys(\n                [imp[\"module\"] for imp in data[\"imports\"][\"standard_library\"]]\n            )\n        )\n        third_party_deps = list(\n            dict.fromkeys([imp[\"module\"] for imp in data[\"imports\"][\"third_party\"]])\n        )\n\n        parts = []\n\n        # Group dependencies (show top most common)\n        if stdlib_deps:\n            # Show most common stdlib modules\n            stdlib_names = \", \".join(sorted(stdlib_deps)[:4])\n            parts.append(f\"standard library ({stdlib_names})\")\n\n        if third_party_deps:\n            # Show most common third-party modules\n            third_party_names = \", \".join(sorted(third_party_deps)[:3])\n            parts.append(f\"third-party ({third_party_names})\")\n\n        if local_deps:\n            # Extract module names (last part of dotted path) and deduplicate\n            local_names = list(dict.fromkeys([d.split(\".\")[-1] for d in local_deps]))\n            # Show most relevant local dependencies\n            local_display = \", \".join(local_names[:max_deps])\n            if len(local_names) > max_deps:\n                local_display += f\" (+{len(local_names) - max_deps} more)\"\n            parts.append(local_display)\n\n        return \", \".join(parts) if parts else \"none\"\n\n    def generate_dependency_patterns_section(\n        self, patterns: Dict[str, Any], actual_imports: Dict[str, Dict]\n    ) -> str:\n        \"\"\"Generate dependency patterns section dynamically.\"\"\"\n        lines = []\n\n        # Core -> Communication/AI pattern\n        comm_ai_deps = patterns.get(\"communication_dependencies\", [])\n        if comm_ai_deps:\n            lines.append(\"### Core -> Communication and AI (most common)\")\n            lines.append(\"Communication and AI modules depend on core system modules.\")\n            for item in comm_ai_deps[:3]:\n                core_deps = [m for m in item[\"modules\"] if m.startswith(\"core.\")]\n                if core_deps:\n                    lines.append(f\"- `{item['file']}` -> {', '.join(core_deps[:3])}\")\n            lines.append(\"\")\n\n        # UI -> Core pattern\n        ui_deps = patterns.get(\"ui_dependencies\", [])\n        if ui_deps:\n            lines.append(\"### UI -> Core\")\n            lines.append(\"UI modules rely on core configuration and data access.\")\n            for item in ui_deps[:3]:\n                core_deps = [m for m in item[\"modules\"] if m.startswith(\"core.\")]\n                if core_deps:\n                    lines.append(f\"- `{item['file']}` -> {', '.join(core_deps[:3])}\")\n            lines.append(\"\")\n\n        # Communication -> Communication pattern\n        if comm_ai_deps:\n            lines.append(\"### Communication -> Communication\")\n            lines.append(\n                \"Communication modules compose other communication utilities for complete flows.\"\n            )\n            for item in comm_ai_deps[:3]:\n                comm_deps = [\n                    m\n                    for m in item[\"modules\"]\n                    if m.startswith(\"communication.\") or m.startswith(\"ai.\")\n                ]\n                if comm_deps:\n                    lines.append(f\"- `{item['file']}` -> {', '.join(comm_deps[:3])}\")\n            lines.append(\"\")\n\n        # Third-party integration\n        third_party = patterns.get(\"third_party_dependencies\", [])\n        if third_party:\n            lines.append(\"### Third-Party Integration\")\n            lines.append(\"External libraries provide channel and UI support.\")\n            for item in third_party[:5]:\n                deps = \", \".join(item[\"dependencies\"][:2])\n                lines.append(f\"- `{item['file']}` -> {deps}\")\n            lines.append(\"\")\n\n        return \"\\n\".join(lines) if lines else \"*No patterns detected*\"\n\n    def generate_quick_reference(\n        self, actual_imports: Dict[str, Dict], patterns: Dict[str, Any]\n    ) -> str:\n        \"\"\"Generate quick reference section dynamically.\"\"\"\n        lines = []\n\n        lines.append(\"### Common Patterns\")\n        lines.append(\n            \"1. Core system modules expose utilities with minimal dependencies.\"\n        )\n        lines.append(\n            \"2. Communication and AI modules depend on core and peer communication modules.\"\n        )\n        lines.append(\"3. UI modules rely on the UI framework and core services.\")\n        lines.append(\"4. Data access modules rely on configuration plus logging.\")\n        lines.append(\"\")\n\n        lines.append(\"### Dependency Guidelines\")\n        lines.append(\n            \"- Prefer core modules for shared logic instead of duplicating functionality.\"\n        )\n        lines.append(\n            \"- Avoid circular dependencies; break them with interfaces or utility modules.\"\n        )\n        lines.append(\n            \"- Use dependency injection for testability when modules call into services.\"\n        )\n        lines.append(\"- Keep third-party usage wrapped by dedicated modules.\")\n        lines.append(\"\")\n\n        # Module organization\n        lines.append(\"### Module Organisation\")\n        directories = {}\n        for file_path in actual_imports.keys():\n            parts = file_path.split(\"/\")\n            if len(parts) > 1:\n                top_dir = parts[0]\n                if top_dir not in directories:\n                    directories[top_dir] = []\n                directories[top_dir].append(file_path)\n\n        descriptions = {\n            \"core\": \"System utilities (minimal dependencies)\",\n            \"communication\": \"Channels and message processing (depends on core)\",\n            \"ai\": \"Chatbot functionality (depends on core)\",\n            \"ui\": \"User interface (depends on core, limited communication dependencies)\",\n            \"user\": \"User context (depends on core)\",\n            \"tasks\": \"Task management (depends on core)\",\n        }\n\n        for dir_name in sorted(directories.keys()):\n            desc = descriptions.get(dir_name, \"Module directory\")\n            lines.append(f\"- `{dir_name}/` - {desc}\")\n\n        return \"\\n\".join(lines)\n\n    def build_dynamic_decision_trees(self, actual_imports: Dict[str, Dict]) -> str:\n        \"\"\"Build dynamic decision trees based on actual imports.\"\"\"\n        lines = []\n\n        # Core System Decision Tree\n        core_modules = [f for f in actual_imports.keys() if f.startswith(\"core/\")]\n        if core_modules:\n            lines.append(\"### Need Core System Access?\")\n            lines.append(\"Core System Dependencies:\")\n\n            # Group by common patterns\n            config_modules = [f for f in core_modules if \"config\" in f]\n            logger_modules = [f for f in core_modules if \"logger\" in f]\n            data_modules = [\n                f for f in core_modules if \"user_data\" in f or \"file_operations\" in f\n            ]\n            error_modules = [f for f in core_modules if \"error\" in f]\n\n            if config_modules or logger_modules:\n                lines.append(\"- Configuration and Setup\")\n                for mod in (config_modules + logger_modules)[:3]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n\n            if data_modules:\n                lines.append(\"- Data Management\")\n                for mod in data_modules[:3]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n\n            if error_modules:\n                lines.append(\"- Error Handling\")\n                for mod in error_modules[:2]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n            lines.append(\"\")\n\n        # AI/Chatbot Decision Tree\n        ai_modules = [\n            f\n            for f in actual_imports.keys()\n            if f.startswith(\"ai/\") or (f.startswith(\"user/\") and \"context\" in f)\n        ]\n        comm_modules = [\n            f for f in actual_imports.keys() if f.startswith(\"communication/\")\n        ]\n        if ai_modules or comm_modules:\n            lines.append(\"### Need AI or Chatbot Support?\")\n            lines.append(\"AI System Dependencies:\")\n\n            if ai_modules:\n                lines.append(\"- AI Core\")\n                for mod in ai_modules[:2]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n\n            command_modules = [\n                f for f in comm_modules if \"command\" in f or \"interaction\" in f\n            ]\n            if command_modules:\n                lines.append(\"- Command Processing\")\n                for mod in command_modules[:3]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n\n            if comm_modules:\n                lines.append(\"- Communication Integration\")\n                channel_modules = [\n                    f for f in comm_modules if \"channel\" in f or \"orchestrator\" in f\n                ]\n                for mod in channel_modules[:2]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n            lines.append(\"\")\n\n        # Communication Channel Decision Tree\n        if comm_modules:\n            lines.append(\"### Need Communication Channel Coverage?\")\n            lines.append(\"Communication Dependencies:\")\n\n            base_modules = [f for f in comm_modules if \"base\" in f or \"factory\" in f]\n            if base_modules:\n                lines.append(\"- Channel Infrastructure\")\n                for mod in base_modules[:3]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n\n            channel_impls = [f for f in comm_modules if \"discord\" in f or \"email\" in f]\n            if channel_impls:\n                lines.append(\"- Specific Channels\")\n                for mod in channel_impls[:2]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n\n            flow_modules = [\n                f for f in comm_modules if \"conversation\" in f or \"flow\" in f\n            ]\n            if flow_modules:\n                lines.append(\"- Conversation Flow\")\n                for mod in flow_modules[:2]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n            lines.append(\"\")\n\n        # UI Decision Tree\n        ui_modules = [f for f in actual_imports.keys() if f.startswith(\"ui/\")]\n        if ui_modules:\n            lines.append(\"### Need UI Dependencies?\")\n            lines.append(\"UI Dependencies:\")\n\n            main_ui = [f for f in ui_modules if \"ui_app\" in f]\n            if main_ui:\n                lines.append(\"- Main Application\")\n                for mod in main_ui[:1]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n\n            dialogs = [f for f in ui_modules if \"dialog\" in f]\n            if dialogs:\n                lines.append(\"- Dialogs\")\n                for mod in dialogs[:3]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n\n            widgets = [f for f in ui_modules if \"widget\" in f]\n            if widgets:\n                lines.append(\"- Widgets\")\n                for mod in widgets[:3]:\n                    deps = self._format_module_dependencies(mod, actual_imports)\n                    lines.append(f\"  - {mod} <- {deps}\")\n            lines.append(\"\")\n\n        return (\n            \"\\n\".join(lines)\n            if lines\n            else \"*No modules detected - patterns may need updating*\"\n        )\n\n\n# Convenience functions for backward compatibility\ndef analyze_dependency_patterns(actual_imports: Dict[str, Dict]) -> Dict[str, Any]:\n    \"\"\"Analyze dependency patterns for AI consumption.\"\"\"\n    analyzer = DependencyPatternAnalyzer()\n    return analyzer.analyze_dependency_patterns(actual_imports)\n\n\ndef detect_circular_dependencies(\n    actual_imports: Dict[str, Dict],\n) -> List[Tuple[str, str]]:\n    \"\"\"Detect potential circular dependencies.\"\"\"\n    analyzer = DependencyPatternAnalyzer()\n    return analyzer.detect_circular_dependencies(actual_imports)\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Analyze dependency patterns\")\n    parser.add_argument(\n        \"--circular\", action=\"store_true\", help=\"Detect circular dependencies\"\n    )\n    args = parser.parse_args()\n\n    # This would need actual_imports data - typically called from generate_module_dependencies.py\n    print(\n        \"This tool is typically used as a library. Use generate_module_dependencies.py for full analysis.\"\n    )\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 532,
                    "line_content": "# Convenience functions for backward compatibility",
                    "start": 21070,
                    "end": 21092
                  }
                ]
              ],
              [
                "development_tools\\imports\\analyze_module_imports.py",
                "#!/usr/bin/env python3\n# TOOL_TIER: core\n# TOOL_PORTABILITY: portable\n\n\"\"\"\nModule Import Analyzer\nExtracts and analyzes imports from Python files.\n\"\"\"\n\nimport ast\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\n# Add project root to path for core module imports\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nfrom core.logger import get_component_logger\n\n# Handle both relative and absolute imports\nif __name__ != \"__main__\" and __package__ and \".\" in __package__:\n    from .. import config\n    from ..shared.common import ensure_ascii\nelse:\n    from development_tools import config\n    from development_tools.shared.common import ensure_ascii\n\nfrom development_tools.shared.constants import (\n    is_local_module as _is_local_module,\n    is_standard_library_module as _is_stdlib_module,\n)\n\n# Load external config on module import\nconfig.load_external_config()\n\nlogger = get_component_logger(\"development_tools\")\n\n\nclass ModuleImportAnalyzer:\n    \"\"\"Analyzes imports from Python files.\"\"\"\n\n    def __init__(\n        self,\n        project_root: Optional[str] = None,\n        local_prefixes: Optional[Tuple[str, ...]] = None,\n    ):\n        \"\"\"Initialize the module import analyzer.\"\"\"\n        if project_root:\n            self.project_root = Path(project_root).resolve()\n        else:\n            self.project_root = Path(config.get_project_root()).resolve()\n        self.local_prefixes = local_prefixes\n\n    def extract_imports_from_file(self, file_path: str) -> Dict[str, List[Dict]]:\n        \"\"\"Extract all imports from a Python file with detailed information.\"\"\"\n        # Note: Exclusion logic is handled in scan_all_python_files() before calling this function.\n        # This function processes any file passed to it (including test fixtures and files in tests/),\n        # so we don't apply exclusions here. This allows tests to pass file paths directly.\n\n        # Normalize path to handle both string and Path objects, and Windows/Unix path separators\n        from pathlib import Path\n\n        file_path_obj = Path(file_path)\n        # Resolve the path (handles relative paths, symlinks, etc.)\n        # This is safe even if the file doesn't exist yet - resolve() just normalizes the path\n        try:\n            file_path = str(file_path_obj.resolve())\n        except (OSError, RuntimeError):\n            # If resolve fails (e.g., broken symlink), use the path as-is\n            file_path = str(file_path_obj)\n\n        imports = {\"standard_library\": [], \"third_party\": [], \"local\": []}\n\n        try:\n            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n\n            tree = ast.parse(content)\n\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        module_name = alias.name\n                        import_info = {\n                            \"module\": module_name,\n                            \"as_name\": alias.asname,\n                            \"imported_items\": [\n                                module_name\n                            ],  # For direct imports, the module itself\n                        }\n\n                        if self.is_standard_library(module_name):\n                            imports[\"standard_library\"].append(import_info)\n                        elif self.is_local_import(module_name):\n                            imports[\"local\"].append(import_info)\n                        else:\n                            imports[\"third_party\"].append(import_info)\n\n                elif isinstance(node, ast.ImportFrom):\n                    module_name = node.module\n                    if module_name:\n                        # Extract specific imported items\n                        imported_items = []\n                        for alias in node.names:\n                            if alias.name == \"*\":\n                                imported_items.append(\"*\")\n                            else:\n                                imported_items.append(alias.name)\n\n                        import_info = {\n                            \"module\": module_name,\n                            \"as_name\": None,  # ImportFrom doesn't have module-level aliases\n                            \"imported_items\": imported_items,\n                        }\n\n                        if self.is_standard_library(module_name):\n                            imports[\"standard_library\"].append(import_info)\n                        elif self.is_local_import(module_name):\n                            imports[\"local\"].append(import_info)\n                        else:\n                            imports[\"third_party\"].append(import_info)\n\n        except Exception as e:\n            # Only log errors for non-excluded files (excluded files are skipped above)\n            logger.error(ensure_ascii(f\"Error parsing {file_path}: {e}\"))\n\n        return imports\n\n    def is_standard_library(self, module_name: str) -> bool:\n        \"\"\"Check if a module is part of the Python standard library.\"\"\"\n        return _is_stdlib_module(module_name)\n\n    def is_local_import(self, module_name: str) -> bool:\n        \"\"\"\n        Check if a module is a local import (part of our project).\n\n        Args:\n            module_name: The module name to check\n\n        Returns:\n            True if the module is local, False otherwise.\n        \"\"\"\n        if self.local_prefixes is not None:\n            # Use provided prefixes\n            if not module_name:\n                return False\n            base = module_name.split(\".\", 1)[0]\n            return base in self.local_prefixes\n        # Use default from constants (which loads from config)\n        return _is_local_module(module_name)\n\n    def format_import_details(self, import_info: Dict) -> str:\n        \"\"\"Format import information for display - handles both list and set formats.\"\"\"\n        module = import_info[\"module\"]\n        imported_items = import_info.get(\"imported_items\", [])\n\n        # Handle both list and set formats\n        if isinstance(imported_items, set):\n            imported_items = sorted(list(imported_items))\n        elif not isinstance(imported_items, list):\n            imported_items = [imported_items] if imported_items else []\n\n        if imported_items == [module] or \"*\" in imported_items or not imported_items:\n            # Direct import or wildcard import or no specific items\n            return module\n        else:\n            # Specific items imported - deduplicate and sort\n            unique_items = sorted(\n                list(dict.fromkeys(imported_items))\n            )  # Preserve order while deduplicating\n            items_str = \", \".join(unique_items)\n            return ensure_ascii(f\"{module} ({items_str})\")\n\n    def scan_all_python_files(self) -> Dict[str, Dict]:\n        \"\"\"Scan all Python files in the project and extract import information.\"\"\"\n        results = {}\n\n        # Import exclusion utilities (import at module level for testability)\n        from ..shared import standard_exclusions\n\n        # Directories to scan from configuration\n        scan_dirs = config.get_scan_directories()\n\n        for scan_dir in scan_dirs:\n            dir_path = self.project_root / scan_dir\n            if not dir_path.exists():\n                continue\n\n            for py_file in dir_path.rglob(\"*.py\"):\n                # Use production context exclusions to match audit behavior\n                if standard_exclusions.should_exclude_file(\n                    str(py_file), \"analysis\", \"production\"\n                ):\n                    continue\n\n                relative_path = py_file.relative_to(self.project_root)\n                file_key = str(relative_path).replace(\"\\\\\", \"/\")\n\n                imports = self.extract_imports_from_file(str(py_file))\n\n                results[file_key] = {\n                    \"imports\": imports,\n                    \"total_imports\": sum(\n                        len(imp_list) for imp_list in imports.values()\n                    ),\n                }\n\n        # Also scan root directory for .py files\n        for py_file in self.project_root.glob(\"*.py\"):\n            if py_file.name not in [\n                \"generate_function_registry.py\",\n                \"generate_module_dependencies.py\",\n            ]:\n                # Use production context exclusions to match audit behavior\n                if standard_exclusions.should_exclude_file(\n                    str(py_file), \"analysis\", \"production\"\n                ):\n                    continue\n\n                file_key = py_file.name\n\n                imports = self.extract_imports_from_file(str(py_file))\n\n                results[file_key] = {\n                    \"imports\": imports,\n                    \"total_imports\": sum(\n                        len(imp_list) for imp_list in imports.values()\n                    ),\n                }\n\n        return results\n\n    def find_usage_of_module(\n        self, target_module: str, all_modules: Dict[str, Dict]\n    ) -> List[str]:\n        \"\"\"Find all modules that use the target module.\"\"\"\n        users = []\n        for file_path, data in all_modules.items():\n            if target_module in data[\"imports\"][\"local\"]:\n                users.append(file_path)\n        return sorted(users)\n\n    def find_reverse_dependencies(\n        self, target_module: str, all_modules: Dict[str, Dict]\n    ) -> List[str]:\n        \"\"\"Find all modules that import the target module.\"\"\"\n        users = []\n\n        # Convert file path to module name (e.g., 'core/config.py' -> 'core.config')\n        module_name = target_module.replace(\"/\", \".\").replace(\".py\", \"\")\n\n        for file_path, data in all_modules.items():\n            # Check local imports for the target module\n            for import_info in data[\"imports\"][\"local\"]:\n                if import_info[\"module\"] == module_name:\n                    users.append(file_path)\n                    break\n\n        return sorted(users)\n\n    def analyze_dependency_changes(\n        self, file_path: str, data: Dict, existing_content: str\n    ) -> Dict:\n        \"\"\"Analyze if dependencies have changed since last generation.\"\"\"\n        changes = {\"added\": [], \"removed\": [], \"unchanged\": True}\n\n        # Extract current dependencies from the new structure\n        current_deps = set()\n        for import_info in data[\"imports\"][\"local\"]:\n            current_deps.add(import_info[\"module\"])\n\n        # Extract existing dependencies from content\n        existing_deps = set()\n        if existing_content:\n            section_start = existing_content.find(ensure_ascii(f\"#### `{file_path}`\"))\n            if section_start != -1:\n                next_section = existing_content.find(\"#### `\", section_start + 1)\n                if next_section == -1:\n                    section_end = len(existing_content)\n                else:\n                    section_end = next_section\n\n                section_content = existing_content[section_start:section_end]\n\n                # Extract dependencies from the section\n                for line in section_content.split(\"\\n\"):\n                    if line.strip().startswith(\"- `\") and line.strip().endswith(\"`\"):\n                        dep = line.strip()[3:-1]  # Remove '- `' and '`'\n                        if \".\" in dep:  # Only count local dependencies\n                            # Extract module name from formatted string (e.g., \"core.error_handling (func1, func2)\" -> \"core.error_handling\")\n                            module_name = dep.split(\" (\")[0]\n                            existing_deps.add(module_name)\n\n        # Calculate changes\n        added = current_deps - existing_deps\n        removed = existing_deps - current_deps\n\n        if added or removed:\n            changes[\"unchanged\"] = False\n            changes[\"added\"] = sorted(added)\n            changes[\"removed\"] = sorted(removed)\n\n        return changes\n\n    def infer_module_purpose(\n        self, file_path: str, data: Dict, all_modules: Dict[str, Dict]\n    ) -> str:\n        \"\"\"Infer a more detailed purpose based on dependencies and usage patterns.\"\"\"\n        # Extract local dependencies from the new structure\n        local_deps = [import_info[\"module\"] for import_info in data[\"imports\"][\"local\"]]\n        reverse_deps = self.find_reverse_dependencies(file_path, all_modules)\n\n        # Analyze dependency patterns\n        core_deps = [d for d in local_deps if d.startswith(\"core.\")]\n        bot_deps = [d for d in local_deps if d.startswith(\"bot.\")]\n        ui_deps = [d for d in local_deps if d.startswith(\"ui.\")]\n        test_deps = [d for d in local_deps if \"test\" in d]\n\n        # Infer purpose based on patterns\n        if file_path.startswith(\"communication/\") or file_path.startswith(\"ai/\"):\n            if \"ai_chatbot\" in file_path:\n                return \"AI chatbot implementation using LM Studio API\"\n            elif \"base_channel\" in file_path:\n                return \"Abstract base class for communication channels\"\n            elif \"channel_factory\" in file_path:\n                return \"Factory for creating communication channels\"\n            elif \"channel_registry\" in file_path:\n                return \"Registry for all available communication channels\"\n            elif \"communication_manager\" in file_path:\n                return \"Manages communication across all channels\"\n            elif \"conversation_manager\" in file_path:\n                return \"Manages conversation flows and check-ins\"\n            elif \"discord_bot\" in file_path:\n                return \"Discord bot implementation\"\n            elif \"email_bot\" in file_path:\n                return \"Email bot implementation\"\n            elif \"user_context_manager\" in file_path:\n                return \"Manages user context for AI conversations\"\n            else:\n                return ensure_ascii(\n                    f\"Communication channel implementation for {file_path.split('/')[-1].replace('.py', '')}\"\n                )\n\n        elif file_path.startswith(\"core/\"):\n            if \"config\" in file_path:\n                return \"Configuration management and validation\"\n            elif \"error_handling\" in file_path:\n                return \"Centralized error handling and recovery\"\n            elif \"file_operations\" in file_path:\n                return \"File operations and data management\"\n            elif \"logger\" in file_path:\n                return \"Logging system configuration and management\"\n            elif \"message_management\" in file_path:\n                return \"Message management and storage\"\n            elif \"response_tracking\" in file_path:\n                return \"Tracks user responses and interactions\"\n            elif \"schedule_management\" in file_path:\n                return \"Schedule management and time period handling\"\n            elif \"scheduler\" in file_path:\n                return \"Task scheduling and job management\"\n            elif \"service\" in file_path:\n                return \"Main service orchestration and management\"\n            elif \"service_utilities\" in file_path:\n                return \"Utility functions for service operations\"\n            elif \"ui_management\" in file_path:\n                return \"UI management and widget utilities\"\n            elif \"user_data\" in file_path:\n                if \"handlers\" in file_path:\n                    return \"User data handlers with caching and validation\"\n                elif \"manager\" in file_path:\n                    return \"Enhanced user data management with references\"\n                elif \"validation\" in file_path:\n                    return \"User data validation and integrity checks\"\n            elif \"user_data_handlers\" in file_path:\n                return \"Centralized user data access and management\"\n            elif \"validation\" in file_path:\n                return \"Data validation utilities\"\n            elif \"auto_cleanup\" in file_path:\n                return \"Automatic cache cleanup and maintenance\"\n            elif \"backup_manager\" in file_path:\n                return \"Manages automatic backups and rollback operations\"\n            elif \"checkin_analytics\" in file_path:\n                return \"Analyzes check-in data and provides insights\"\n            else:\n                return ensure_ascii(\n                    f\"Core system module for {file_path.split('/')[-1].replace('.py', '')}\"\n                )\n\n        elif file_path.startswith(\"ui/\"):\n            if \"dialogs\" in file_path:\n                return ensure_ascii(\n                    f\"Dialog component for {file_path.split('/')[-1].replace('.py', '').replace('_', ' ')}\"\n                )\n            elif \"widgets\" in file_path:\n                return ensure_ascii(\n                    f\"UI widget component for {file_path.split('/')[-1].replace('.py', '').replace('_', ' ')}\"\n                )\n            elif \"generated\" in file_path:\n                return ensure_ascii(\n                    f\"Auto-generated UI component for {file_path.split('/')[-1].replace('.py', '').replace('_', ' ')}\"\n                )\n            elif \"ui_app\" in file_path:\n                return \"Main UI application (PyQt6)\"\n            else:\n                return ensure_ascii(\n                    f\"User interface component for {file_path.split('/')[-1].replace('.py', '')}\"\n                )\n\n        elif file_path.startswith(\"tests/\"):\n            if \"behavior\" in file_path:\n                return ensure_ascii(\n                    f\"Behavior tests for {file_path.split('/')[-1].replace('.py', '').replace('test_', '').replace('_', ' ')}\"\n                )\n            elif \"integration\" in file_path:\n                return ensure_ascii(\n                    f\"Integration tests for {file_path.split('/')[-1].replace('.py', '').replace('test_', '').replace('_', ' ')}\"\n                )\n            elif \"unit\" in file_path:\n                return ensure_ascii(\n                    f\"Unit tests for {file_path.split('/')[-1].replace('.py', '').replace('test_', '').replace('_', ' ')}\"\n                )\n            elif \"ui\" in file_path:\n                return ensure_ascii(\n                    f\"UI tests for {file_path.split('/')[-1].replace('.py', '').replace('test_', '').replace('_', ' ')}\"\n                )\n            else:\n                return ensure_ascii(\n                    f\"Test file for {file_path.split('/')[-1].replace('.py', '').replace('test_', '').replace('_', ' ')}\"\n                )\n\n        elif file_path.startswith(\"user/\"):\n            if \"user_context\" in file_path:\n                return \"User context management\"\n            elif \"user_preferences\" in file_path:\n                return \"User preferences management\"\n            else:\n                return ensure_ascii(\n                    f\"User data module for {file_path.split('/')[-1].replace('.py', '')}\"\n                )\n\n        elif file_path.startswith(\"tasks/\"):\n            return \"Task management and scheduling\"\n\n        elif file_path.endswith(\"run_tests.py\") or \"test\" in file_path.lower():\n            return \"Test runner for the application\"\n        elif any(main_file in file_path for main_file in [\"run_\", \"main.py\"]):\n            return \"Main entry point for the application\"\n\n        # Fallback based on dependency patterns\n        if len(core_deps) > len(local_deps) * 0.7:\n            return f\"Core system module with heavy core dependencies\"\n        elif len(bot_deps) > 0:\n            return f\"Bot-related module with communication dependencies\"\n        elif len(ui_deps) > 0:\n            return f\"UI-related module with interface dependencies\"\n        elif len(test_deps) > 0:\n            return ensure_ascii(f\"Test-related module\")\n        else:\n            return ensure_ascii(f\"Module for {file_path}\")\n\n\n# Convenience functions for backward compatibility\ndef extract_imports_from_file(\n    file_path: str, local_prefixes: Optional[Tuple[str, ...]] = None\n) -> Dict[str, List[Dict]]:\n    \"\"\"Extract all imports from a Python file with detailed information.\"\"\"\n    analyzer = ModuleImportAnalyzer(local_prefixes=local_prefixes)\n    return analyzer.extract_imports_from_file(file_path)\n\n\ndef scan_all_python_files(\n    local_prefixes: Optional[Tuple[str, ...]] = None,\n) -> Dict[str, Dict]:\n    \"\"\"Scan all Python files in the project and extract import information.\"\"\"\n    analyzer = ModuleImportAnalyzer(local_prefixes=local_prefixes)\n    return analyzer.scan_all_python_files()\n\n\ndef find_reverse_dependencies(\n    target_module: str, all_modules: Dict[str, Dict]\n) -> List[str]:\n    \"\"\"Find all modules that import the target module.\"\"\"\n    analyzer = ModuleImportAnalyzer()\n    return analyzer.find_reverse_dependencies(target_module, all_modules)\n\n\ndef analyze_dependency_changes(\n    file_path: str, data: Dict, existing_content: str\n) -> Dict:\n    \"\"\"Analyze if dependencies have changed since last generation.\"\"\"\n    analyzer = ModuleImportAnalyzer()\n    return analyzer.analyze_dependency_changes(file_path, data, existing_content)\n\n\ndef infer_module_purpose(\n    file_path: str, data: Dict, all_modules: Dict[str, Dict]\n) -> str:\n    \"\"\"Infer a more detailed purpose based on dependencies and usage patterns.\"\"\"\n    analyzer = ModuleImportAnalyzer()\n    return analyzer.infer_module_purpose(file_path, data, all_modules)\n\n\ndef format_import_details(import_info: Dict) -> str:\n    \"\"\"Format import information for display - handles both list and set formats.\"\"\"\n    analyzer = ModuleImportAnalyzer()\n    return analyzer.format_import_details(import_info)\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Analyze module imports\")\n    parser.add_argument(\"--file\", type=str, help=\"Analyze a specific file\")\n    parser.add_argument(\"--scan\", action=\"store_true\", help=\"Scan all Python files\")\n    args = parser.parse_args()\n\n    analyzer = ModuleImportAnalyzer()\n\n    if args.file:\n        imports = analyzer.extract_imports_from_file(args.file)\n        print(f\"Imports from {args.file}:\")\n        print(f\"  Standard library: {len(imports['standard_library'])}\")\n        print(f\"  Third-party: {len(imports['third_party'])}\")\n        print(f\"  Local: {len(imports['local'])}\")\n    elif args.scan:\n        results = analyzer.scan_all_python_files()\n        print(f\"Scanned {len(results)} files\")\n        total_imports = sum(data[\"total_imports\"] for data in results.values())\n        print(f\"Total imports: {total_imports}\")\n    else:\n        parser.print_help()\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 464,
                    "line_content": "# Convenience functions for backward compatibility",
                    "start": 19846,
                    "end": 19868
                  }
                ]
              ],
              [
                "development_tools\\reports\\analyze_system_signals.py",
                "#!/usr/bin/env python3\n# TOOL_TIER: supporting\n\n\"\"\"\nSystem Signals Analysis Tool\n\nAnalyzes system health and status signals for the project.\nThis tool can be run independently or as part of audit workflows.\n\nConfiguration is loaded from external config file (development_tools_config.json)\nif available, making this tool portable across different projects.\n\"\"\"\n\nimport sys\nimport json\nimport subprocess\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Any\n\n# Add project root to path for core module imports\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\n# Handle both relative and absolute imports\n# Check if we're running as part of a package to avoid __package__ != __spec__.parent warnings\nif __name__ != \"__main__\" and __package__ and \".\" in __package__:\n    # Running as part of a package, use relative imports\n    from ..shared.standard_exclusions import should_exclude_file\n    from ..shared.constants import PROJECT_DIRECTORIES\nelse:\n    # Running directly or not as a package, use absolute imports\n    from development_tools.shared.standard_exclusions import should_exclude_file\n    from development_tools.shared.constants import PROJECT_DIRECTORIES\n\nfrom core.logger import get_component_logger\n\nlogger = get_component_logger(\"development_tools\")\n\n\nclass SystemSignalsAnalyzer:\n    \"\"\"Analyze system health and status signals\"\"\"\n\n    def __init__(\n        self, project_root: Optional[Path] = None, config_path: Optional[str] = None\n    ):\n        if project_root:\n            self.project_root = Path(project_root).resolve()\n        else:\n            # Correctly resolve to project root (parent.parent.parent from reports/)\n            self.project_root = Path(__file__).parent.parent.parent\n\n        # Load config if provided\n        # Check if we're running as part of a package to avoid __package__ != __spec__.parent warnings\n        if __name__ != \"__main__\" and __package__ and \".\" in __package__:\n            from . import config\n        else:\n            from development_tools import config\n\n        self.config = config  # Store reference for reuse\n\n        if config_path:\n            config.load_external_config(config_path)\n        else:\n            config.load_external_config()\n\n        self.system_signals_config = config.get_system_signals_config()\n\n    def analyze_system_signals(self) -> Dict[str, Any]:\n        \"\"\"Analyze comprehensive system signals\"\"\"\n        signals = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"system_health\": self._check_system_health(),\n            \"recent_activity\": self._get_recent_activity(),\n            \"critical_alerts\": self._identify_critical_alerts(),\n            \"performance_indicators\": self._assess_performance_indicators(),\n        }\n        return signals\n\n    def _check_system_health(self) -> Dict[str, Any]:\n        \"\"\"Check comprehensive system health indicators with actionable insights\"\"\"\n        health = {\n            \"overall_status\": \"OK\",\n            \"core_files\": {},\n            \"key_directories\": {},\n            \"last_audit\": None,\n            \"audit_freshness\": None,\n            \"test_coverage_status\": \"Unknown\",\n            \"documentation_sync_status\": \"Unknown\",\n            \"health_indicators\": [],\n            \"recommendations\": [],\n            \"warnings\": [],\n            \"errors\": [],\n            \"severity_levels\": {\"INFO\": [], \"WARNING\": [], \"CRITICAL\": []},\n        }\n\n        # Check core files - use config or project.key_files, fallback to generic defaults\n        core_files = self.system_signals_config.get(\"core_files\", [])\n        if not core_files:\n            # Try to get from project.key_files in config\n            core_files = self.config.get_project_key_files(\n                [\"requirements.txt\", \"pyproject.toml\"]\n            )\n\n        for file_path in core_files:\n            full_path = self.project_root / file_path\n            if full_path.exists():\n                health[\"core_files\"][file_path] = \"OK\"\n            else:\n                health[\"core_files\"][file_path] = \"MISSING\"\n                health[\"warnings\"].append(f\"Missing core file: {file_path}\")\n                health[\"severity_levels\"][\"CRITICAL\"].append(\n                    f\"Missing core file: {file_path}\"\n                )\n\n        # Check key directories\n        for dir_name in PROJECT_DIRECTORIES:\n            dir_path = self.project_root / dir_name\n            if dir_path.exists() and dir_path.is_dir():\n                health[\"key_directories\"][dir_name] = \"OK\"\n            else:\n                health[\"key_directories\"][dir_name] = \"MISSING\"\n                health[\"warnings\"].append(f\"Missing key directory: {dir_name}\")\n                health[\"severity_levels\"][\"WARNING\"].append(\n                    f\"Missing key directory: {dir_name}\"\n                )\n\n        # Check for recent audit and calculate freshness\n        audit_file = (\n            self.project_root\n            / \"development_tools\"\n            / \"reports\"\n            / \"analysis_detailed_results.json\"\n        )\n        if audit_file.exists():\n            try:\n                stat = audit_file.stat()\n                last_audit_time = datetime.fromtimestamp(stat.st_mtime)\n                health[\"last_audit\"] = last_audit_time.isoformat()\n\n                # Calculate audit freshness with specific time ranges\n                time_since_audit = datetime.now() - last_audit_time\n                days_since_audit = time_since_audit.days\n                hours_since_audit = time_since_audit.total_seconds() / 3600\n\n                # Format with specific time ranges\n                if hours_since_audit < 1:\n                    health[\"audit_freshness\"] = \"<1 hour\"\n                    health[\"health_indicators\"].append(\n                        \"Audit data is very recent (<1 hour)\"\n                    )\n                elif hours_since_audit < 24:\n                    health[\"audit_freshness\"] = \"<24 hours\"\n                    health[\"health_indicators\"].append(\n                        f\"Audit data is recent ({int(hours_since_audit)} hours ago)\"\n                    )\n                elif hours_since_audit <= 72:  # 25-72 hours (1-3 days)\n                    health[\"audit_freshness\"] = \"25-72 hours\"\n                    health[\"health_indicators\"].append(\n                        f\"Audit data is {days_since_audit} day(s) old\"\n                    )\n                elif days_since_audit <= 10:\n                    health[\"audit_freshness\"] = \"4-10 days\"\n                    health[\"severity_levels\"][\"WARNING\"].append(\n                        f\"Audit data is {days_since_audit} days old\"\n                    )\n                    health[\"recommendations\"].append(\n                        \"Run `python development_tools/run_development_tools.py audit` to refresh metrics\"\n                    )\n                elif days_since_audit <= 30:\n                    health[\"audit_freshness\"] = \"11-30 days\"\n                    health[\"severity_levels\"][\"WARNING\"].append(\n                        f\"Audit data is {days_since_audit} days old\"\n                    )\n                    health[\"recommendations\"].append(\n                        \"Run `python development_tools/run_development_tools.py audit --full` for comprehensive analysis\"\n                    )\n                else:\n                    health[\"audit_freshness\"] = f\">{days_since_audit} days\"\n                    health[\"severity_levels\"][\"WARNING\"].append(\n                        f\"Audit data is {days_since_audit} days old\"\n                    )\n                    health[\"recommendations\"].append(\n                        \"Run `python development_tools/run_development_tools.py audit --full` for comprehensive analysis\"\n                    )\n\n                # Try to extract test coverage and documentation sync status from audit results\n                try:\n                    import json\n\n                    with open(audit_file, \"r\", encoding=\"utf-8\") as f:\n                        audit_data = json.load(f)\n\n                    # Check test coverage status\n                    if \"results\" in audit_data:\n                        # Look for test coverage data\n                        coverage_keys = [\n                            \"run_test_coverage\",\n                            \"analyze_test_coverage\",\n                            \"generate_test_coverage_report\",\n                        ]\n                        for key in coverage_keys:\n                            if key in audit_data[\"results\"]:\n                                coverage_result = audit_data[\"results\"][key]\n                                if \"data\" in coverage_result:\n                                    coverage_data = coverage_result[\"data\"]\n                                    if isinstance(coverage_data, dict):\n                                        # Try to extract overall coverage\n                                        overall = coverage_data.get(\"overall\", {})\n                                        if isinstance(overall, dict):\n                                            coverage_pct = overall.get(\n                                                \"overall_coverage\", 0\n                                            )\n                                            if coverage_pct:\n                                                if coverage_pct >= 80:\n                                                    health[\"test_coverage_status\"] = (\n                                                        \"Good\"\n                                                    )\n                                                    health[\"health_indicators\"].append(\n                                                        f\"Test coverage: {coverage_pct}%\"\n                                                    )\n                                                elif coverage_pct >= 60:\n                                                    health[\"test_coverage_status\"] = (\n                                                        \"Moderate\"\n                                                    )\n                                                    health[\"severity_levels\"][\n                                                        \"WARNING\"\n                                                    ].append(\n                                                        f\"Test coverage is {coverage_pct}% (below 80% target)\"\n                                                    )\n                                                    health[\"recommendations\"].append(\n                                                        \"Consider expanding test coverage for better reliability\"\n                                                    )\n                                                else:\n                                                    health[\"test_coverage_status\"] = (\n                                                        \"Low\"\n                                                    )\n                                                    health[\"severity_levels\"][\n                                                        \"WARNING\"\n                                                    ].append(\n                                                        f\"Test coverage is {coverage_pct}% (below 60%)\"\n                                                    )\n                                                    health[\"recommendations\"].append(\n                                                        \"Test coverage is low - prioritize adding tests for critical paths\"\n                                                    )\n                                                break\n\n                    # Check documentation sync status\n                    if (\n                        \"results\" in audit_data\n                        and \"analyze_documentation_sync\" in audit_data[\"results\"]\n                    ):\n                        doc_sync_result = audit_data[\"results\"][\n                            \"analyze_documentation_sync\"\n                        ]\n                        if \"data\" in doc_sync_result:\n                            doc_sync_data = doc_sync_result[\"data\"]\n                            if isinstance(doc_sync_data, dict):\n                                summary = doc_sync_data.get(\"summary\", {})\n                                total_issues = summary.get(\"total_issues\", 0)\n                                status = summary.get(\"status\", \"UNKNOWN\")\n\n                                if total_issues == 0 and status in [\"PASS\", \"OK\"]:\n                                    health[\"documentation_sync_status\"] = \"Synchronized\"\n                                    health[\"health_indicators\"].append(\n                                        \"Documentation is synchronized\"\n                                    )\n                                elif total_issues <= 5:\n                                    health[\"documentation_sync_status\"] = \"Minor Issues\"\n                                    health[\"severity_levels\"][\"INFO\"].append(\n                                        f\"Documentation has {total_issues} minor sync issue(s)\"\n                                    )\n                                    health[\"recommendations\"].append(\n                                        \"Run `python development_tools/run_development_tools.py doc-sync` to check details\"\n                                    )\n                                else:\n                                    health[\"documentation_sync_status\"] = \"Issues\"\n                                    health[\"severity_levels\"][\"WARNING\"].append(\n                                        f\"Documentation has {total_issues} sync issue(s)\"\n                                    )\n                                    health[\"recommendations\"].append(\n                                        \"Run `python development_tools/run_development_tools.py doc-fix` to address documentation issues\"\n                                    )\n                except Exception as e:\n                    logger.debug(\n                        f\"Failed to parse audit data for health indicators: {e}\"\n                    )\n\n            except Exception:\n                health[\"last_audit\"] = \"Unknown\"\n        else:\n            health[\"warnings\"].append(\"No recent audit data found\")\n            health[\"severity_levels\"][\"WARNING\"].append(\"No recent audit data found\")\n            health[\"recommendations\"].append(\n                \"Run `python development_tools/run_development_tools.py audit --quick` for initial health check\"\n            )\n\n        # Enhanced error log analysis\n        error_log = self.project_root / \"logs\" / \"errors.log\"\n        if error_log.exists():\n            try:\n                stat = error_log.stat()\n                if stat.st_size > 0:\n                    # Check if log was modified recently\n                    time_since_error = datetime.now().timestamp() - stat.st_mtime\n                    if time_since_error < 3600:  # Last hour\n                        health[\"severity_levels\"][\"CRITICAL\"].append(\n                            \"Recent errors detected in logs/errors.log (last hour)\"\n                        )\n                        health[\"recommendations\"].append(\n                            \"Review logs/errors.log for recent error details\"\n                        )\n                    elif time_since_error < 86400:  # Last 24 hours\n                        health[\"severity_levels\"][\"WARNING\"].append(\n                            \"Errors detected in logs/errors.log (last 24 hours)\"\n                        )\n                        health[\"recommendations\"].append(\n                            \"Check logs/errors.log for recent errors\"\n                        )\n\n                    # Check log file size\n                    size_mb = stat.st_size / (1024 * 1024)\n                    if size_mb > 10:\n                        health[\"severity_levels\"][\"WARNING\"].append(\n                            f\"Error log file is large ({size_mb:.1f}MB) - consider rotation\"\n                        )\n            except Exception:\n                pass\n\n        # Determine overall status based on severity levels\n        if health[\"severity_levels\"][\"CRITICAL\"]:\n            health[\"overall_status\"] = \"CRITICAL\"\n        elif health[\"severity_levels\"][\"WARNING\"]:\n            health[\"overall_status\"] = \"ISSUES\"\n        elif health[\"warnings\"] or health[\"errors\"]:\n            health[\"overall_status\"] = \"ISSUES\"\n        else:\n            health[\"overall_status\"] = \"OK\"\n\n        return health\n\n    def _is_meaningful_change(self, file_path: str) -> bool:\n        \"\"\"\n        Determine if a file change is meaningful (code, docs, config) vs non-meaningful (cache, logs, data).\n\n        Returns True for meaningful changes, False for non-meaningful changes.\n        \"\"\"\n        file_path_lower = file_path.lower()\n        path_obj = Path(file_path)\n\n        # Exclude lock files\n        if file_path.endswith(\".lock\") or \".audit_in_progress.lock\" in file_path:\n            return False\n\n        # Exclude cache files and directories\n        if (\n            \".cache\" in file_path_lower\n            or \"/cache/\" in file_path_lower\n            or \"\\\\cache\\\\\" in file_path_lower\n        ):\n            return False\n\n        # Exclude log files (unless they indicate errors - but we'll exclude all for simplicity)\n        if file_path.endswith(\".log\") and \"logs/\" in file_path_lower:\n            return False\n\n        # Exclude data files (JSON in data directories, but allow config JSON files)\n        if file_path.endswith(\".json\"):\n            # Allow config files in root/config directories\n            if \"config/\" in file_path_lower or file_path_lower.startswith(\"config/\"):\n                return True\n            # Allow package.json, pyproject.toml companion files\n            if \"package.json\" in file_path_lower or \"tsconfig.json\" in file_path_lower:\n                return True\n            # Exclude data directory JSON files\n            if \"/data/\" in file_path_lower or \"\\\\data\\\\\" in file_path_lower:\n                return False\n            # Exclude JSON in user data directories\n            if \"/users/\" in file_path_lower or \"\\\\users\\\\\" in file_path_lower:\n                return False\n\n        # Exclude generated files (already handled by should_exclude_file, but be explicit)\n        if should_exclude_file(file_path, context=\"recent_changes\"):\n            return False\n\n        # Prioritize meaningful file types\n        meaningful_extensions = {\n            \".py\",\n            \".md\",\n            \".mdc\",\n            \".toml\",\n            \".ini\",\n            \".cfg\",\n            \".yaml\",\n            \".yml\",\n        }\n        if path_obj.suffix.lower() in meaningful_extensions:\n            return True\n\n        # Allow config files in root or config directories\n        if \"config\" in path_obj.parts or path_obj.name in [\n            \"requirements.txt\",\n            \"setup.py\",\n            \"pyproject.toml\",\n        ]:\n            return True\n\n        # Exclude test artifacts and temporary files\n        if path_obj.name.startswith(\".\") and path_obj.name != \".gitignore\":\n            return False\n\n        # Default: include other files (but they'll be lower priority)\n        return True\n\n    def _get_change_significance_score(self, file_path: str) -> int:\n        \"\"\"\n        Score change significance: higher = more significant.\n        Used for sorting: code > docs > config > other meaningful > non-meaningful.\n        \"\"\"\n        if not self._is_meaningful_change(file_path):\n            return 0\n\n        path_obj = Path(file_path)\n        ext = path_obj.suffix.lower()\n\n        # Code files: highest priority\n        if ext == \".py\":\n            return 100\n\n        # Documentation: high priority\n        if ext in {\".md\", \".mdc\"}:\n            return 80\n\n        # Config files: medium-high priority\n        if ext in {\".toml\", \".ini\", \".cfg\", \".yaml\", \".yml\"}:\n            return 60\n        if \"config\" in path_obj.parts or path_obj.name in [\n            \"requirements.txt\",\n            \"setup.py\",\n            \"pyproject.toml\",\n        ]:\n            return 60\n\n        # Other meaningful files: medium priority\n        return 40\n\n    def _get_recent_activity(self) -> Dict[str, Any]:\n        \"\"\"Get recent activity indicators using git to detect actual changes.\"\"\"\n        activity = {\n            \"recent_changes\": [],\n            \"git_status\": \"Unknown\",\n            \"last_commit\": None,\n            \"uncommitted_changes\": False,\n        }\n\n        # Check git status first\n        try:\n            result = subprocess.run(\n                [\"git\", \"status\", \"--porcelain\"],\n                capture_output=True,\n                text=True,\n                cwd=self.project_root,\n            )\n            if result.returncode == 0:\n                activity[\"git_status\"] = (\n                    \"Clean\" if not result.stdout.strip() else \"Modified\"\n                )\n                activity[\"uncommitted_changes\"] = bool(result.stdout.strip())\n        except Exception:\n            activity[\"git_status\"] = \"Unknown\"\n\n        # Get recent changes using git diff and git status\n        try:\n            changed_files = set()\n\n            # Get uncommitted changes (working directory)\n            try:\n                status_result = subprocess.run(\n                    [\"git\", \"status\", \"--porcelain\"],\n                    capture_output=True,\n                    text=True,\n                    cwd=self.project_root,\n                )\n                if status_result.returncode == 0:\n                    for line in status_result.stdout.strip().split(\"\\n\"):\n                        if line.strip():\n                            # Git status format: \"XY filename\" where XY is status code\n                            # X = staged status, Y = unstaged status\n                            # D = deleted, M = modified, A = added, etc.\n                            status_code = line.strip()[:2]\n                            # Skip deleted files (D in either position)\n                            if \"D\" in status_code:\n                                continue\n\n                            # Extract filename (skip status code)\n                            parts = line.strip().split(None, 1)\n                            if len(parts) >= 2:\n                                file_path = parts[1]\n                                # Handle renamed files (format: \"old -> new\")\n                                if \" -> \" in file_path:\n                                    file_path = file_path.split(\" -> \")[1]\n                                changed_files.add(file_path)\n            except Exception as e:\n                logger.debug(f\"Failed to get git status: {e}\")\n\n            # Get files changed in last commit (if any)\n            # Use --diff-filter to exclude deleted files\n            try:\n                diff_result = subprocess.run(\n                    [\"git\", \"diff\", \"--name-only\", \"--diff-filter=d\", \"HEAD~1\", \"HEAD\"],\n                    capture_output=True,\n                    text=True,\n                    cwd=self.project_root,\n                )\n                if diff_result.returncode == 0:\n                    for line in diff_result.stdout.strip().split(\"\\n\"):\n                        if line.strip():\n                            changed_files.add(line.strip())\n            except Exception:\n                # If HEAD~1 doesn't exist (new repo), just use HEAD\n                try:\n                    diff_result = subprocess.run(\n                        [\"git\", \"diff\", \"--name-only\", \"--diff-filter=d\", \"HEAD\"],\n                        capture_output=True,\n                        text=True,\n                        cwd=self.project_root,\n                    )\n                    if diff_result.returncode == 0:\n                        for line in diff_result.stdout.strip().split(\"\\n\"):\n                            if line.strip():\n                                changed_files.add(line.strip())\n                except Exception:\n                    pass\n\n            # Filter and prioritize meaningful changes\n            meaningful_changes = []\n            for file_path in changed_files:\n                # Normalize path separators\n                file_path_normalized = file_path.replace(\"\\\\\", \"/\")\n\n                # Skip if file doesn't exist (deleted files)\n                full_path = self.project_root / file_path_normalized\n                if not full_path.exists():\n                    continue\n\n                # Skip if excluded by standard exclusions\n                if should_exclude_file(file_path_normalized, context=\"recent_changes\"):\n                    continue\n\n                # Check if meaningful\n                if self._is_meaningful_change(file_path_normalized):\n                    score = self._get_change_significance_score(file_path_normalized)\n                    meaningful_changes.append((file_path_normalized, score))\n\n            # Sort by significance (highest first), then limit to 10\n            meaningful_changes.sort(key=lambda x: x[1], reverse=True)\n            activity[\"recent_changes\"] = [\n                file_path for file_path, _ in meaningful_changes[:10]\n            ]\n\n            # Fallback: If no git changes found, use mtime-based detection for last 24 hours\n            if not activity[\"recent_changes\"]:\n                recent_threshold = datetime.now() - timedelta(hours=24)\n                all_files_with_scores = []\n\n                for dir_name in PROJECT_DIRECTORIES:\n                    dir_path = self.project_root / dir_name\n                    if dir_path.exists():\n                        for file_path in dir_path.rglob(\"*\"):\n                            if file_path.is_file():\n                                rel_path = file_path.relative_to(self.project_root)\n                                rel_path_str = str(rel_path).replace(\"\\\\\", \"/\")\n\n                                if not should_exclude_file(\n                                    rel_path_str, context=\"recent_changes\"\n                                ):\n                                    if self._is_meaningful_change(rel_path_str):\n                                        try:\n                                            mtime = file_path.stat().st_mtime\n                                            mtime_dt = datetime.fromtimestamp(mtime)\n\n                                            if mtime_dt >= recent_threshold:\n                                                score = (\n                                                    self._get_change_significance_score(\n                                                        rel_path_str\n                                                    )\n                                                )\n                                                all_files_with_scores.append(\n                                                    (rel_path_str, score, mtime_dt)\n                                                )\n                                        except (OSError, ValueError):\n                                            continue\n\n                # Sort by score (highest first), then by mtime (most recent first), limit to 10\n                all_files_with_scores.sort(key=lambda x: (x[1], x[2]), reverse=True)\n                activity[\"recent_changes\"] = [\n                    file_path for file_path, _, _ in all_files_with_scores[:10]\n                ]\n\n        except Exception as e:\n            logger.debug(f\"Error getting recent activity: {e}\")\n            activity[\"recent_changes\"] = []\n\n        return activity\n\n    def _get_git_recent_threshold(self) -> datetime:\n        \"\"\"Get threshold for 'recent' based on git history\"\"\"\n        try:\n            import subprocess\n\n            result = subprocess.run(\n                [\"git\", \"log\", \"-1\", \"--format=%ct\"],\n                capture_output=True,\n                text=True,\n                cwd=self.project_root,\n            )\n            if result.returncode == 0:\n                last_commit_timestamp = int(result.stdout.strip())\n                last_commit_dt = datetime.fromtimestamp(last_commit_timestamp)\n                # 6 hours before last commit (more practical for active development)\n                threshold = last_commit_dt - timedelta(hours=6)\n                return threshold\n        except Exception:\n            pass\n\n        # Fallback to 24 hours ago if git is not available\n        return datetime.now() - timedelta(hours=24)\n\n    def _identify_critical_alerts(self) -> List[str]:\n        \"\"\"Identify critical system alerts\"\"\"\n        alerts = []\n\n        # Check for critical files\n        # Use core_files from config, or fallback to generic\n        critical_files = self.system_signals_config.get(\"core_files\", [])\n        if not critical_files:\n            critical_files = self.config.get_project_key_files([])\n        for file_path in critical_files:\n            if not (self.project_root / file_path).exists():\n                alerts.append(f\"CRITICAL: Missing {file_path}\")\n\n        # Check for error logs\n        error_log = self.project_root / \"logs\" / \"errors.log\"\n        if error_log.exists():\n            try:\n                stat = error_log.stat()\n                if stat.st_size > 0:\n                    # Check if log was modified recently (last hour)\n                    if datetime.now().timestamp() - stat.st_mtime < 3600:\n                        alerts.append(\"CRITICAL: Recent errors in logs/errors.log\")\n            except Exception:\n                pass\n\n        return alerts\n\n    def _assess_performance_indicators(self) -> Dict[str, Any]:\n        \"\"\"Assess system performance indicators\"\"\"\n        indicators = {\n            \"log_file_sizes\": {},\n            \"cache_status\": \"Unknown\",\n            \"memory_usage\": \"Unknown\",\n        }\n\n        # Check log file sizes\n        logs_dir = self.project_root / \"logs\"\n        if logs_dir.exists():\n            for log_file in logs_dir.glob(\"*.log\"):\n                try:\n                    size_mb = log_file.stat().st_size / (1024 * 1024)\n                    indicators[\"log_file_sizes\"][log_file.name] = f\"{size_mb:.1f}MB\"\n                except Exception:\n                    pass\n\n        # Check cache status\n        cache_dir = self.project_root / \"ai\" / \"cache\"\n        if cache_dir.exists():\n            try:\n                cache_files = list(cache_dir.rglob(\"*\"))\n                indicators[\"cache_status\"] = f\"{len(cache_files)} files\"\n            except Exception:\n                indicators[\"cache_status\"] = \"Error\"\n\n        return indicators\n\n\ndef main():\n    \"\"\"Main entry point\"\"\"\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        description=\"Analyze system signals for the project\"\n    )\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"Output as JSON\")\n    parser.add_argument(\"--output\", help=\"Output file path\")\n\n    args = parser.parse_args()\n\n    analyzer = SystemSignalsAnalyzer()\n    signals = analyzer.analyze_system_signals()\n\n    if args.json:\n        output = json.dumps(signals, indent=2)\n    else:\n        output = _format_human_readable(signals)\n\n    if args.output:\n        output_file = Path(args.output)\n        output_file.parent.mkdir(parents=True, exist_ok=True)\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(output)\n        logger.info(f\"System signals analysis written to: {output_file}\")\n        # User-facing confirmation stays as print() for immediate visibility\n        print(f\"System signals analysis written to: {output_file}\")\n    else:\n        # User-facing output stays as print() for immediate visibility\n        print(output)\n\n\ndef _format_human_readable(signals: Dict[str, Any]) -> str:\n    \"\"\"Format signals for human reading\"\"\"\n    lines = []\n    lines.append(\"SYSTEM SIGNALS ANALYSIS\")\n    lines.append(\"=\" * 50)\n    lines.append(f\"Generated: {signals['timestamp']}\")\n    lines.append(\"\")\n\n    # System Health\n    health = signals[\"system_health\"]\n    overall_status = health.get(\"overall_status\", \"Unknown\")\n    status_emoji = {\"OK\": \"\u2713\", \"ISSUES\": \"\u26a0\", \"CRITICAL\": \"\u2717\"}.get(overall_status, \"?\")\n    lines.append(f\"System Health: {status_emoji} {overall_status}\")\n    lines.append(\"\")\n\n    # Health Indicators\n    if health.get(\"health_indicators\"):\n        lines.append(\"Health Indicators:\")\n        for indicator in health[\"health_indicators\"]:\n            lines.append(f\"  \u2713 {indicator}\")\n        lines.append(\"\")\n\n    # Severity-based issues\n    severity_levels = health.get(\"severity_levels\", {})\n    if severity_levels.get(\"CRITICAL\"):\n        lines.append(\"CRITICAL ISSUES:\")\n        for issue in severity_levels[\"CRITICAL\"]:\n            lines.append(f\"  \u2717 {issue}\")\n        lines.append(\"\")\n\n    if severity_levels.get(\"WARNING\"):\n        lines.append(\"WARNINGS:\")\n        for warning in severity_levels[\"WARNING\"]:\n            lines.append(f\"  \u26a0 {warning}\")\n        lines.append(\"\")\n\n    if severity_levels.get(\"INFO\"):\n        lines.append(\"INFO:\")\n        for info in severity_levels[\"INFO\"]:\n            lines.append(f\"  \u2139 {info}\")\n        lines.append(\"\")\n\n    # Recommendations\n    if health.get(\"recommendations\"):\n        lines.append(\"RECOMMENDATIONS:\")\n        for i, rec in enumerate(health[\"recommendations\"], 1):\n            lines.append(f\"  {i}. {rec}\")\n        lines.append(\"\")\n\n    # Audit freshness\n    if health.get(\"audit_freshness\"):\n        lines.append(f\"Audit Freshness: {health['audit_freshness']}\")\n        if health.get(\"last_audit\"):\n            lines.append(f\"  Last Audit: {health['last_audit']}\")\n        lines.append(\"\")\n\n    # Test coverage status\n    if health.get(\"test_coverage_status\") != \"Unknown\":\n        lines.append(f\"Test Coverage Status: {health['test_coverage_status']}\")\n        lines.append(\"\")\n\n    # Documentation sync status\n    if health.get(\"documentation_sync_status\") != \"Unknown\":\n        lines.append(\n            f\"Documentation Sync Status: {health['documentation_sync_status']}\"\n        )\n        lines.append(\"\")\n\n    # Legacy warnings/errors (for backward compatibility)\n    if health.get(\"warnings\") and not severity_levels.get(\"WARNING\"):\n        for warning in health[\"warnings\"]:\n            lines.append(f\"  WARNING: {warning}\")\n        lines.append(\"\")\n    if health.get(\"errors\"):\n        for error in health[\"errors\"]:\n            lines.append(f\"  ERROR: {error}\")\n        lines.append(\"\")\n\n    # Recent Activity\n    activity = signals[\"recent_activity\"]\n    lines.append(f\"Git Status: {activity['git_status']}\")\n    if activity[\"recent_changes\"]:\n        lines.append(f\"Recent Changes ({len(activity['recent_changes'])} files):\")\n        for change in activity[\"recent_changes\"][:5]:  # Show first 5\n            lines.append(f\"  - {change}\")\n        if len(activity[\"recent_changes\"]) > 5:\n            lines.append(f\"  ... and {len(activity['recent_changes']) - 5} more\")\n    lines.append(\"\")\n\n    # Critical Alerts\n    if signals[\"critical_alerts\"]:\n        lines.append(\"CRITICAL ALERTS:\")\n        for alert in signals[\"critical_alerts\"]:\n            lines.append(f\"  - {alert}\")\n        lines.append(\"\")\n\n    # Performance Indicators\n    perf = signals[\"performance_indicators\"]\n    if perf[\"log_file_sizes\"]:\n        lines.append(\"Log File Sizes:\")\n        for log_name, size in perf[\"log_file_sizes\"].items():\n            lines.append(f\"  - {log_name}: {size}\")\n        lines.append(\"\")\n\n    return \"\\n\".join(lines)\n\n\nif __name__ == \"__main__\":\n    from datetime import timedelta\n\n    main()\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 788,
                    "line_content": "# Legacy warnings/errors (for backward compatibility)",
                    "start": 34148,
                    "end": 34170
                  }
                ]
              ],
              [
                "development_tools\\shared\\constants.py",
                "# TOOL_TIER: core\n\n\"\"\"\nShared constant data and helpers for AI development tools.\n\nConstants are loaded from external config file (development_tools_config.json)\nif available, otherwise fall back to generic defaults. This makes the module portable\nacross different projects.\n\"\"\"\nfrom __future__ import annotations\n\nimport re\nimport sys\nfrom typing import Dict, Tuple\n\n# Import config to load external constants\ntry:\n    from .. import config\nexcept ImportError:\n    # Fallback for when run as standalone script\n    import sys\n    from pathlib import Path\n\n    sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n    from development_tools import config\n\n# Load external config on module import (if not already loaded)\n# Load external config if available (safe to call multiple times)\ntry:\n    if hasattr(config, \"load_external_config\"):\n        config.load_external_config()\nexcept (AttributeError, ImportError):\n    pass\n\n# Default constants (generic fallbacks if config doesn't provide them)\n_DEFAULT_DEFAULT_DOCS: Tuple[str, ...] = (\n    \"README.md\",\n    \"TODO.md\",\n)\n\n_DEFAULT_PAIRED_DOCS: Dict[str, str] = {}\n\n_DEFAULT_LOCAL_MODULE_PREFIXES: Tuple[str, ...] = (\n    \"core\",\n    \"tests\",\n)\n\n\ndef _get_constants_config_safe():\n    \"\"\"Safely get constants config, returning None if config not available.\"\"\"\n    try:\n        if hasattr(config, \"get_constants_config\"):\n            return config.get_constants_config()\n    except (AttributeError, ImportError, TypeError):\n        pass\n    return None\n\n\ndef _load_default_docs() -> Tuple[str, ...]:\n    \"\"\"Load default docs list from config or return defaults.\"\"\"\n    constants_config = _get_constants_config_safe()\n    if constants_config and \"default_docs\" in constants_config:\n        return tuple(constants_config[\"default_docs\"])\n    return _DEFAULT_DEFAULT_DOCS\n\n\ndef _load_paired_docs() -> Dict[str, str]:\n    \"\"\"Load paired docs mapping from config or return defaults.\"\"\"\n    constants_config = _get_constants_config_safe()\n    if constants_config and \"paired_docs\" in constants_config:\n        return dict(constants_config[\"paired_docs\"])\n    return _DEFAULT_PAIRED_DOCS.copy()\n\n\ndef _load_local_module_prefixes() -> Tuple[str, ...]:\n    \"\"\"Load local module prefixes from config or return defaults.\"\"\"\n    constants_config = _get_constants_config_safe()\n    if constants_config and \"local_module_prefixes\" in constants_config:\n        return tuple(constants_config[\"local_module_prefixes\"])\n    return _DEFAULT_LOCAL_MODULE_PREFIXES\n\n\n# Load constants from config or use defaults\nDEFAULT_DOCS: Tuple[str, ...] = _load_default_docs()\nPAIRED_DOCS: Dict[str, str] = _load_paired_docs()\nLOCAL_MODULE_PREFIXES: Tuple[str, ...] = _load_local_module_prefixes()\n\nSTANDARD_LIBRARY_PREFIXES: Tuple[str, ...] = (\n    \"asyncio\",\n    \"concurrent\",\n    \"contextlib\",\n    \"email\",\n    \"http\",\n    \"importlib\",\n    \"logging\",\n    \"multiprocessing\",\n    \"pathlib\",\n    \"sqlite3\",\n    \"xml\",\n    \"xmlrpc\",\n    \"zipfile\",\n)\n\n_STD_EXTRA = {\n    \"argparse\",\n    \"atexit\",\n    \"base64\",\n    \"binascii\",\n    \"bz2\",\n    \"calendar\",\n    \"collections\",\n    \"configparser\",\n    \"copy\",\n    \"csv\",\n    \"dataclasses\",\n    \"datetime\",\n    \"decimal\",\n    \"difflib\",\n    \"functools\",\n    \"gc\",\n    \"glob\",\n    \"gzip\",\n    \"hashlib\",\n    \"inspect\",\n    \"io\",\n    \"itertools\",\n    \"json\",\n    \"math\",\n    \"os\",\n    \"pickle\",\n    \"pkgutil\",\n    \"platform\",\n    \"queue\",\n    \"random\",\n    \"re\",\n    \"runpy\",\n    \"shlex\",\n    \"shutil\",\n    \"signal\",\n    \"statistics\",\n    \"string\",\n    \"struct\",\n    \"subprocess\",\n    \"sys\",\n    \"tempfile\",\n    \"threading\",\n    \"time\",\n    \"timeit\",\n    \"typing\",\n    \"unicodedata\",\n    \"uuid\",\n    \"warnings\",\n    \"weakref\",\n    \"zipapp\",\n    \"zlib\",\n}\n\ntry:\n    _stdlib = set(sys.stdlib_module_names)\nexcept AttributeError:  # pragma: no cover - Python < 3.10\n    _stdlib = set()\n\n_stdlib.update(_STD_EXTRA)\n_stdlib.update(prefix.split(\".\", 1)[0] for prefix in STANDARD_LIBRARY_PREFIXES)\nSTANDARD_LIBRARY_MODULES = frozenset(_stdlib)\n\nCORRUPTED_ARTIFACT_PATTERNS = (\n    (\"replacement_character\", re.compile(r\"\\uFFFD\")),\n    (\"triple_question_marks\", re.compile(r\"\\?\\?\\?\")),\n)\n\n# Path drift detection constants\nTHIRD_PARTY_LIBRARIES: Tuple[str, ...] = (\n    \"PyQt5\",\n    \"PyQt6\",\n    \"PySide6\",\n    \"discord\",\n    \"requests\",\n    \"aiohttp\",\n    \"pandas\",\n    \"numpy\",\n    \"matplotlib\",\n    \"seaborn\",\n    \"scipy\",\n    \"sklearn\",\n    \"tensorflow\",\n    \"torch\",\n    \"pytorch\",\n    \"fastapi\",\n    \"sqlalchemy\",\n    \"alembic\",\n    \"psycopg2\",\n    \"pymongo\",\n    \"redis\",\n    \"celery\",\n    \"gunicorn\",\n    \"uvicorn\",\n    \"pytest\",\n    \"black\",\n    \"flake8\",\n    \"mypy\",\n)\n\nCOMMON_FUNCTION_NAMES: Tuple[str, ...] = (\n    \"get_logger\",\n    \"handle_errors\",\n    \"safe_file_operation\",\n    \"get_component_logger\",\n    \"cleanup_old_logs\",\n    \"functions\",\n    \"statements\",\n    \"handle_file_error\",\n    \"handle_network_error\",\n    \"handle_communication_error\",\n    \"handle_configuration_error\",\n    \"handle_validation_error\",\n    \"handle_ai_error\",\n    \"TestUserFactory\",\n    \"TestDataFactory\",\n    \"TestPathFactory\",\n    \"TestConfigFactory\",\n)\n\nCOMMON_CLASS_NAMES: Tuple[str, ...] = (\n    \"TestUserFactory\",\n    \"TestDataFactory\",\n    \"TestPathFactory\",\n    \"TestConfigFactory\",\n    \"BaseChannel\",\n    \"DiscordBot\",\n    \"EmailBot\",\n    \"TelegramBot\",\n    \"TaskManager\",\n    \"UserManager\",\n    \"ScheduleManager\",\n    \"CheckinManager\",\n    \"MessageRouter\",\n)\n\nCOMMON_VARIABLE_NAMES: Tuple[str, ...] = (\n    \"task\",\n    \"and\",\n    \"statements\",\n    \"from\",\n    \"in\",\n    \"to\",\n    \"for\",\n    \"with\",\n    \"as\",\n    \"if\",\n    \"else\",\n    \"elif\",\n    \"while\",\n    \"for\",\n    \"def\",\n    \"class\",\n    \"import\",\n    \"from\",\n    \"return\",\n    \"yield\",\n    \"try\",\n    \"except\",\n    \"finally\",\n    \"with\",\n    \"as\",\n    \"pass\",\n    \"break\",\n    \"continue\",\n)\n\nCOMMON_CODE_PATTERNS: Tuple[str, ...] = (\n    \"PyQt5.QtWidgets\",\n    \"PyQt5.QtCore\",\n    \"PyQt5.QtGui\",\n    \"PySide6.QtWidgets\",\n    \"ui.dialogs.task_crud_dialog\",\n    \"ui.dialogs.account_creator_dialog\",\n    \"ui.widgets.task_settings_widget\",\n    \"ui.widgets.channel_selection_widget\",\n    \"communication.discord.bot\",\n    \"communication.email.bot\",\n    \"core.logger\",\n    \"core.error_handling\",\n    \"core.config\",\n    \"core.scheduler\",\n    \"ai.chatbot\",\n)\n\n# Common patterns that should be ignored in path drift detection\nIGNORED_PATH_PATTERNS: Tuple[str, ...] = (\n    \"Python Official Tutorial\",\n    \"Real Python\",\n    \"Troubleshooting\",\n    \"README.md#troubleshooting\",\n    \"Navigation\",\n    \"Project Vision\",\n    \"Quick Start\",\n    \"Development Workflow\",\n    \"Documentation Guide\",\n    \"Development Plans\",\n    \"Recent Changes\",\n    \"Recent Changes (Most Recent First)\",\n)\n\n# Common command patterns that should be ignored\nCOMMAND_PATTERNS: Tuple[str, ...] = (\n    \"python \",\n    \"pip \",\n    \"git \",\n    \"npm \",\n    \"yarn \",\n    \"docker \",\n    \"kubectl \",\n)\n\n# Common template patterns that should be ignored\nTEMPLATE_PATTERNS: Tuple[str, ...] = (\"test_<\", \">.py\", \"{\", \"}\", \"*\", \"?\")\n\n# AI Development Tools Constants\n\n# =============================================================================\n# PROJECT STRUCTURE\n# =============================================================================\n\n\ndef _load_project_directories() -> Tuple[str, ...]:\n    \"\"\"Load project directories from config or return defaults.\"\"\"\n    constants_config = _get_constants_config_safe()\n    if constants_config and \"project_directories\" in constants_config:\n        return tuple(constants_config[\"project_directories\"])\n    # Default: just root\n    return (\".\",)\n\n\ndef _load_core_modules() -> Tuple[str, ...]:\n    \"\"\"Load core modules from config or return defaults.\"\"\"\n    constants_config = _get_constants_config_safe()\n    if constants_config and \"core_modules\" in constants_config:\n        return tuple(constants_config[\"core_modules\"])\n    # Default: empty\n    return ()\n\n\n# Core project directories (used by multiple tools)\nPROJECT_DIRECTORIES: Tuple[str, ...] = _load_project_directories()\n\n# Core modules for coverage and analysis (subset of PROJECT_DIRECTORIES)\nCORE_MODULES: Tuple[str, ...] = _load_core_modules()\n\n# =============================================================================\n# TOOL-SPECIFIC CONSTANTS\n# =============================================================================\n\n\ndef _load_ascii_compliance_files() -> Tuple[str, ...]:\n    \"\"\"Load ASCII compliance files list from config or return defaults.\n\n    ASCII compliance files are now the same as default_docs. If config has a note\n    or is empty, use DEFAULT_DOCS as the source of truth.\n    \"\"\"\n    constants_config = _get_constants_config_safe()\n    if constants_config and \"ascii_compliance_files\" in constants_config:\n        ascii_files = constants_config[\"ascii_compliance_files\"]\n        # Check if it's a note (single item that's a string containing \"Note:\")\n        if isinstance(ascii_files, list) and len(ascii_files) == 1:\n            if isinstance(ascii_files[0], str) and \"note\" in ascii_files[0].lower():\n                # Use DEFAULT_DOCS\n                return _load_default_docs()\n        # If it's a real list, use it (for backward compatibility)\n        if isinstance(ascii_files, list) and ascii_files:\n            return tuple(ascii_files)\n    # Default: use DEFAULT_DOCS\n    return _load_default_docs()\n\n\n# Files to check for ASCII compliance (AI collaborator facing docs)\nASCII_COMPLIANCE_FILES: Tuple[str, ...] = _load_ascii_compliance_files()\n\n\ndef _load_version_sync_directories() -> Dict[str, str]:\n    \"\"\"Load version sync directories from config or return defaults.\"\"\"\n    constants_config = _get_constants_config_safe()\n    if constants_config and \"fix_version_sync_directories\" in constants_config:\n        return dict(constants_config[\"fix_version_sync_directories\"])\n    # Default: empty (projects should define their own)\n    return {}\n\n\n# Version sync key directories\nVERSION_SYNC_DIRECTORIES: Dict[str, str] = _load_version_sync_directories()\n\n\ndef is_standard_library_module(module_name: str) -> bool:\n    \"\"\"Return True if *module_name* belongs to the Python standard library.\"\"\"\n    if not module_name:\n        return False\n    base = module_name.split(\".\", 1)[0]\n    if base in STANDARD_LIBRARY_MODULES:\n        return True\n    if base in sys.builtin_module_names:\n        return True\n    for prefix in STANDARD_LIBRARY_PREFIXES:\n        if module_name == prefix or module_name.startswith(prefix + \".\"):\n            return True\n    return False\n\n\ndef is_local_module(module_name: str) -> bool:\n    \"\"\"Return True if *module_name* is part of the local project namespace.\"\"\"\n    if not module_name:\n        return False\n    base = module_name.split(\".\", 1)[0]\n    return base in LOCAL_MODULE_PREFIXES\n\n\n__all__ = [\n    \"ASCII_COMPLIANCE_FILES\",\n    \"COMMAND_PATTERNS\",\n    \"COMMON_CLASS_NAMES\",\n    \"COMMON_CODE_PATTERNS\",\n    \"COMMON_FUNCTION_NAMES\",\n    \"COMMON_VARIABLE_NAMES\",\n    \"CORRUPTED_ARTIFACT_PATTERNS\",\n    \"CORE_MODULES\",\n    \"DEFAULT_DOCS\",\n    \"IGNORED_PATH_PATTERNS\",\n    \"LOCAL_MODULE_PREFIXES\",\n    \"PAIRED_DOCS\",\n    \"PROJECT_DIRECTORIES\",\n    \"STANDARD_LIBRARY_MODULES\",\n    \"STANDARD_LIBRARY_PREFIXES\",\n    \"TEMPLATE_PATTERNS\",\n    \"THIRD_PARTY_LIBRARIES\",\n    \"VERSION_SYNC_DIRECTORIES\",\n    \"is_local_module\",\n    \"is_standard_library_module\",\n]\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 369,
                    "line_content": "# If it's a real list, use it (for backward compatibility)",
                    "start": 9133,
                    "end": 9155
                  }
                ]
              ],
              [
                "development_tools\\shared\\mtime_cache.py",
                "#!/usr/bin/env python3\n\"\"\"\nMtime-based File Cache Utility\n\nProvides a reusable caching mechanism for file-based analyzers that checks\nfile modification times (mtime) to determine if cached results are still valid.\n\nUsage:\n    from development_tools.shared.mtime_cache import MtimeFileCache\n\n    cache = MtimeFileCache(\n        project_root=project_root,\n        use_cache=True,\n        tool_name='my_tool',\n        domain='docs'\n    )\n\n    # Check if file is cached\n    cached_results = cache.get_cached(file_path)\n    if cached_results is not None:\n        # Use cached results\n        return cached_results\n\n    # Process file and cache results\n    results = process_file(file_path)\n    cache.cache_results(file_path, results)\n    cache.save_cache()\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import Dict, Optional, Any, TypeVar\n\nT = TypeVar(\"T\")  # Generic type for cached results\n\ntry:\n    from core.logger import get_component_logger\n\n    logger = get_component_logger(\"development_tools\")\nexcept ImportError:\n    logger = None\n\n# Cache metadata key for config file mtime\n_CONFIG_MTIME_KEY = \"__config_mtime__\"\n\n\nclass MtimeFileCache:\n    \"\"\"\n    Mtime-based file cache for analyzer results.\n\n    Caches results keyed by file path, with validation based on file modification time.\n    Only re-processes files that have been modified since the last cache entry.\n    \"\"\"\n\n    def __init__(\n        self,\n        project_root: Path,\n        use_cache: bool = True,\n        tool_name: Optional[str] = None,\n        domain: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the cache.\n\n        Args:\n            project_root: Root directory of the project (for relative path generation)\n            use_cache: Whether to use caching (if False, all operations are no-ops)\n            tool_name: Name of the tool (e.g., 'analyze_ascii_compliance') - required for standardized storage\n            domain: Domain directory (e.g., 'docs') - required for standardized storage\n        \"\"\"\n        if tool_name is None or domain is None:\n            raise ValueError(\"tool_name and domain are required for MtimeFileCache\")\n\n        self.project_root = project_root.resolve()\n        self.use_cache = use_cache\n        self.cache_data: Dict[str, Dict[str, Any]] = {}\n        self.tool_name = tool_name\n        self.domain = domain\n        self.use_standardized_storage = True\n\n        # Get config file path for cache invalidation\n        self.config_file_path = self._get_config_file_path()\n\n        if self.use_cache:\n            self._load_cache()\n            # Check if config file changed and invalidate cache if needed\n            self._check_config_staleness()\n\n    def _get_config_file_path(self) -> Optional[Path]:\n        \"\"\"\n        Get the path to the development_tools_config.json file.\n\n        Returns:\n            Path to config file if found, None otherwise\n        \"\"\"\n        try:\n            # Try to get the config file path that was actually loaded\n            import development_tools.config.config as config_module\n\n            if (\n                hasattr(config_module, \"_config_file_path\")\n                and config_module._config_file_path\n            ):\n                return config_module._config_file_path\n        except Exception:\n            pass\n\n        # Fallback: try to find it using the same logic as config loading\n        try:\n            config_file = (\n                self.project_root\n                / \"development_tools\"\n                / \"config\"\n                / \"development_tools_config.json\"\n            )\n            if not config_file.exists():\n                config_file = self.project_root / \"development_tools_config.json\"\n            if config_file.exists():\n                return config_file\n        except Exception:\n            pass\n\n        return None\n\n    def _check_config_staleness(self) -> None:\n        \"\"\"\n        Check if config file has changed since cache was created.\n        If so, clear the cache to force regeneration with new config.\n        \"\"\"\n        if not self.config_file_path or not self.config_file_path.exists():\n            return\n\n        try:\n            # Get current config file mtime\n            current_config_mtime = self.config_file_path.stat().st_mtime\n\n            # Check cached config mtime\n            cached_config_mtime = None\n            if _CONFIG_MTIME_KEY in self.cache_data:\n                cached_config_mtime = self.cache_data[_CONFIG_MTIME_KEY].get(\"mtime\")\n\n            # If config file is newer than cached mtime, clear cache\n            if (\n                cached_config_mtime is not None\n                and current_config_mtime > cached_config_mtime\n            ):\n                if logger:\n                    logger.info(\n                        f\"Config file changed (mtime: {current_config_mtime} > {cached_config_mtime}), \"\n                        f\"invalidating cache for {self.tool_name or 'tool'}\"\n                    )\n                self.clear_cache()\n                # Update config mtime in cache immediately\n                self._update_config_mtime_in_cache()\n            elif cached_config_mtime is None:\n                # No cached config mtime (first run or cache was cleared), store current mtime\n                self._update_config_mtime_in_cache()\n        except Exception as e:\n            if logger:\n                logger.debug(f\"Error checking config file staleness: {e}\")\n\n    def _update_config_mtime_in_cache(self) -> None:\n        \"\"\"Store current config file mtime in cache metadata.\"\"\"\n        if not self.config_file_path or not self.config_file_path.exists():\n            return\n\n        try:\n            config_mtime = self.config_file_path.stat().st_mtime\n            self.cache_data[_CONFIG_MTIME_KEY] = {\n                \"mtime\": config_mtime,\n                \"results\": {},  # Empty results, just storing mtime\n            }\n        except Exception:\n            pass\n\n    def _load_cache(self) -> None:\n        \"\"\"Load cache from disk if it exists.\"\"\"\n        if self.use_standardized_storage:\n            # Use standardized storage\n            try:\n                from .output_storage import load_tool_cache\n\n                loaded_data = load_tool_cache(\n                    self.tool_name, self.domain, project_root=self.project_root\n                )\n                if loaded_data:\n                    # load_tool_cache already extracts data from metadata wrapper, so loaded_data is the cache content\n                    # Migrate old cache format (with 'issues' key) to new format (with 'results' key)\n                    migrated_data = {}\n                    for key, value in loaded_data.items():\n                        if isinstance(value, dict):\n                            # Check if it's old format with 'issues' key\n                            if \"issues\" in value and \"results\" not in value:\n                                migrated_data[key] = {\n                                    \"mtime\": value.get(\"mtime\"),\n                                    \"results\": value.get(\"issues\", []),\n                                }\n                            else:\n                                # Already in new format or has 'results' key\n                                migrated_data[key] = value\n                        else:\n                            # Invalid format, skip\n                            continue\n                    self.cache_data = migrated_data\n                    if logger:\n                        logger.debug(\n                            f\"Loaded cache from standardized storage ({self.tool_name}) with {len(self.cache_data)} entries\"\n                        )\n                    return\n            except Exception as e:\n                if logger:\n                    logger.warning(\n                        f\"Failed to load cache from standardized storage: {e}\"\n                    )\n                # If standardized storage fails, start with empty cache\n                # Tools will regenerate cache on next run\n                self.cache_data = {}\n\n    def save_cache(self) -> None:\n        \"\"\"Save cache to disk.\"\"\"\n        if not self.use_cache:\n            return\n\n        # Update config mtime in cache before saving\n        self._update_config_mtime_in_cache()\n\n        if self.use_standardized_storage:\n            # Use standardized storage\n            try:\n                from .output_storage import save_tool_cache\n\n                save_tool_cache(\n                    self.tool_name,\n                    self.domain,\n                    self.cache_data,\n                    project_root=self.project_root,\n                )\n                if logger:\n                    logger.debug(\n                        f\"Saved cache to standardized storage ({self.tool_name}) with {len(self.cache_data)} entries\"\n                    )\n                return\n            except Exception as e:\n                if logger:\n                    logger.warning(f\"Failed to save cache to standardized storage: {e}\")\n                # If standardized storage fails, log warning but don't fall back to legacy\n                # This ensures we fix standardized storage issues rather than silently using legacy paths\n\n    def _get_file_cache_key(self, file_path: Path) -> str:\n        \"\"\"Generate cache key for a file (relative path from project root).\"\"\"\n        try:\n            rel_path = file_path.resolve().relative_to(self.project_root)\n            return str(rel_path).replace(\"\\\\\", \"/\")\n        except ValueError:\n            # File is outside project root, use absolute path\n            return str(file_path.resolve())\n\n    def _is_file_cached(self, file_path: Path) -> bool:\n        \"\"\"Check if file results are cached and still valid (mtime matches).\"\"\"\n        if not self.use_cache:\n            return False\n\n        cache_key = self._get_file_cache_key(file_path)\n        if cache_key not in self.cache_data:\n            return False\n\n        cached_mtime = self.cache_data[cache_key].get(\"mtime\")\n        if cached_mtime is None:\n            return False\n\n        try:\n            current_mtime = file_path.stat().st_mtime\n            return current_mtime == cached_mtime\n        except OSError:\n            return False\n\n    def get_cached(self, file_path: Path) -> Optional[T]:\n        \"\"\"\n        Get cached results for a file if available and still valid.\n\n        Args:\n            file_path: Path to the file to check\n\n        Returns:\n            Cached results if available and valid, None otherwise\n        \"\"\"\n        if not self._is_file_cached(file_path):\n            return None\n\n        cache_key = self._get_file_cache_key(file_path)\n        cached_data = self.cache_data[cache_key].get(\"results\")\n        return cached_data\n\n    def cache_results(self, file_path: Path, results: T) -> None:\n        \"\"\"\n        Cache results for a file.\n\n        Args:\n            file_path: Path to the file being cached\n            results: Results to cache (must be JSON-serializable)\n        \"\"\"\n        if not self.use_cache:\n            return\n\n        try:\n            cache_key = self._get_file_cache_key(file_path)\n            mtime = file_path.stat().st_mtime\n            self.cache_data[cache_key] = {\"mtime\": mtime, \"results\": results}\n        except OSError:\n            # File doesn't exist or can't be accessed, skip caching\n            pass\n\n    def clear_cache(self) -> None:\n        \"\"\"Clear all cached data (in memory only, call save_cache() to persist).\"\"\"\n        self.cache_data = {}\n\n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Get statistics about the cache.\n\n        Returns:\n            Dictionary with cache statistics (total_entries, tool_name, domain)\n        \"\"\"\n        return {\n            \"total_entries\": len(self.cache_data),\n            \"tool_name\": self.tool_name,\n            \"domain\": self.domain,\n        }\n",
                [
                  {
                    "pattern": "(?i)legacy path",
                    "match": "legacy path",
                    "line": 246,
                    "line_content": "# This ensures we fix standardized storage issues rather than silently using legacy paths",
                    "start": 9195,
                    "end": 9206
                  }
                ]
              ],
              [
                "development_tools\\shared\\standard_exclusions.py",
                "#!/usr/bin/env python3\n# TOOL_TIER: core\n\n\"\"\"\nStandard Exclusion Patterns for Development Tools\n\nThis module provides standardized exclusion patterns that can be reused\nacross all development tools to ensure consistent file filtering.\n\nExclusion patterns are loaded from external config file (development_tools_config.json)\nif available, otherwise fall back to generic defaults. This makes the module portable\nacross different projects.\n\nUsage:\n    from development_tools.shared.standard_exclusions import get_exclusions\n\n    # Get exclusions for a specific tool type\n    exclusions = get_exclusions('coverage')\n    exclusions = get_exclusions('analysis')\n    exclusions = get_exclusions('documentation')\n\"\"\"\n\nfrom typing import Tuple, Dict, List\n\n# Import config to load external exclusions\ntry:\n    from .. import config\nexcept ImportError:\n    # Fallback for when run as standalone script\n    import sys\n    from pathlib import Path\n\n    sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n    from development_tools import config\n\n# Load external config on module import (if not already loaded)\n# load_external_config is safe to call multiple times\ntry:\n    if hasattr(config, \"load_external_config\"):\n        config.load_external_config()\nexcept (AttributeError, ImportError):\n    # Config may not be fully loaded yet, will be loaded when needed\n    pass\n\n# Default universal exclusions (generic patterns - should work for most projects)\n# These are fallbacks if external config doesn't provide exclusions\n_DEFAULT_BASE_EXCLUSIONS = [\n    # Python cache and compiled files\n    \"__pycache__\",\n    \"*.pyc\",\n    \"*.pyo\",\n    \"*.pyi\",\n    \".ruff_cache\",\n    # Virtual environments\n    \"venv\",\n    \".venv\",\n    \"env\",\n    \".env\",\n    \"node_modules\",\n    # Git and version control\n    \".git\",\n    \".gitignore\",\n    \".gitattributes\",\n    # IDE and editor files\n    \".vscode\",\n    \".idea\",\n    \"*.swp\",\n    \"*.swo\",\n    \"*~\",\n    \".cursorignore\",\n    # OS generated files\n    \".DS_Store\",\n    \"Thumbs.db\",\n    \"desktop.ini\",\n    # Archive and backup directories\n    \"archive\",\n    \"backup*\",\n    \"backups\",\n    # Scripts directory (should be excluded in all contexts)\n    \"scripts\",\n    \"scripts/*\",\n    \"*/scripts/*\",\n    # Test artifacts\n    \".pytest_cache\",\n    \"tests/__pycache__\",\n    \"tests/.pytest_cache\",\n    \"tests/logs/\",\n    \"tests/data/\",\n    \"tests/temp/\",\n    \"tests/fixtures/\",\n    \"tests/ai/results\",\n    \"tests/coverage_html\",\n    \"mhm.egg-info\",\n    # Pytest temporary directories (created during parallel test runs)\n    \"pytest-tmp-*\",\n    \"pytest-of-*\",\n    \"*/pytest-tmp-*\",\n    \"*/pytest-of-*\",\n    # Generated files (should be excluded everywhere)\n    \"*/generated/*\",\n    \"*/ui/generated/*\",\n    \"*/pyscript*\",\n    \"*/shibokensupport/*\",\n    \"*/signature_bootstrap.py\",\n]\n\n\ndef _get_exclusions_config_safe():\n    \"\"\"Safely get exclusions config, returning None if config not available.\"\"\"\n    try:\n        if hasattr(config, \"get_exclusions_config\"):\n            return config.get_exclusions_config()\n    except (AttributeError, ImportError, TypeError):\n        pass\n    return None\n\n\ndef _load_base_exclusions() -> List[str]:\n    \"\"\"Load universal exclusions from config or return defaults.\"\"\"\n    exclusions_config = _get_exclusions_config_safe()\n    if exclusions_config and \"base_exclusions\" in exclusions_config:\n        return exclusions_config[\"base_exclusions\"]\n    return _DEFAULT_BASE_EXCLUSIONS.copy()\n\n\n# Universal exclusions - start from config or fall back to defaults\nBASE_EXCLUSIONS = _load_base_exclusions()\n\n# Default tool-specific exclusions (empty by default - projects can override via config)\n_DEFAULT_TOOL_EXCLUSIONS = {}\n\n\ndef _load_tool_exclusions() -> Dict[str, List[str]]:\n    \"\"\"Load tool-specific exclusions from config or return defaults.\"\"\"\n    exclusions_config = _get_exclusions_config_safe()\n    if exclusions_config and \"tool_exclusions\" in exclusions_config:\n        return exclusions_config[\"tool_exclusions\"]\n    return _DEFAULT_TOOL_EXCLUSIONS.copy()\n\n\n# Tool-specific exclusions - load from config or fall back to defaults\nTOOL_EXCLUSIONS = _load_tool_exclusions()\n\n# Default context-specific exclusions (generic patterns)\n_DEFAULT_CONTEXT_EXCLUSIONS = {\n    \"recent_changes\": [],\n    \"production\": [\n        # Production should exclude development files (generic patterns)\n        \"development_tools/*\",\n        \"*/development_tools/*\",\n        \"tests/*\",\n        \"*/tests/*\",\n        \"*/test_*\",\n        \"scripts/*\",\n        \"*/scripts/*\",\n        \"archive/*\",\n        \"*/archive/*\",\n    ],\n    \"development\": [\n        # Development can include most files but exclude sensitive data\n        \"*/data/*\",\n        \"*/logs/*\",\n        \"*/backup*\",\n        \"*/backups/*\",\n    ],\n    \"testing\": [\n        # Testing should exclude generated files and data\n        \"*/generated/*\",\n        \"*/ui/generated/*\",\n        \"*/data/*\",\n        \"*/logs/*\",\n        \"*/backup*\",\n        \"*/backups/*\",\n    ],\n}\n\n\ndef _load_context_exclusions() -> Dict[str, List[str]]:\n    \"\"\"Load context-specific exclusions from config or return defaults.\n\n    recent_changes will be built from generated files after they're loaded.\n    \"\"\"\n    exclusions_config = _get_exclusions_config_safe()\n    result = _DEFAULT_CONTEXT_EXCLUSIONS.copy()\n\n    if exclusions_config and \"context_exclusions\" in exclusions_config:\n        context_exclusions = exclusions_config[\"context_exclusions\"]\n\n        # Update contexts (recent_changes will be built from generated files later)\n        for key, value in context_exclusions.items():\n            if key != \"recent_changes\":\n                result[key] = value\n\n    return result\n\n\n# Context-specific exclusions - will be finalized after generated files are defined\n_CONTEXT_EXCLUSIONS_TEMP = _load_context_exclusions()\n\n\ndef get_exclusions(tool_type: str = None, context: str = \"development\") -> list:\n    \"\"\"\n    Get exclusion patterns for a specific tool type and context.\n\n    Args:\n        tool_type: Type of tool ('coverage', 'analysis', 'documentation', 'fix_version_sync', 'file_operations')\n        context: Context ('production', 'development', 'testing')\n\n    Returns:\n        List of exclusion patterns\n    \"\"\"\n    exclusions = BASE_EXCLUSIONS.copy()\n\n    # Add tool-specific exclusions\n    if tool_type and tool_type in TOOL_EXCLUSIONS:\n        exclusions.extend(TOOL_EXCLUSIONS[tool_type])\n\n    # Add context-specific exclusions\n    if context and context in CONTEXT_EXCLUSIONS:\n        exclusions.extend(CONTEXT_EXCLUSIONS[context])\n\n    return exclusions\n\n\ndef should_exclude_file(\n    file_path, tool_type: str = None, context: str = \"development\"\n) -> bool:\n    \"\"\"\n    Check if a file should be excluded based on standard patterns.\n\n    Args:\n        file_path: Path to the file to check (str or Path object)\n        tool_type: Type of tool\n        context: Context\n\n    Returns:\n        True if file should be excluded\n    \"\"\"\n    import fnmatch\n    from pathlib import Path\n\n    exclusions = get_exclusions(tool_type, context)\n\n    # Convert Path object to string if needed\n    file_path_str = str(file_path)\n    normalized_path = file_path_str.replace(\"\\\\\", \"/\")\n\n    # Explicitly check for pytest temp directories first (most common exclusion during scanning)\n    # These are created during parallel test execution and should always be excluded\n    if \"pytest-tmp-\" in normalized_path or \"pytest-of-\" in normalized_path:\n        # Only exclude if the path is in tests/data/ (where pytest creates these during parallel execution)\n        # This prevents excluding files explicitly passed to extraction functions (like test fixtures)\n        if \"/tests/data/\" in normalized_path:\n            return True\n\n    # Check generated files patterns (ui/generated/*, etc.)\n    for pattern in GENERATED_FILE_PATTERNS:\n        if fnmatch.fnmatch(normalized_path, pattern) or fnmatch.fnmatch(\n            normalized_path, f\"*/{pattern}\"\n        ):\n            return True\n\n    # Check standard exclusions\n    for pattern in exclusions:\n        # Handle wildcard patterns with fnmatch\n        if fnmatch.fnmatch(normalized_path, pattern) or pattern in normalized_path:\n            return True\n\n    return False\n\n\ndef get_coverage_exclusions() -> list:\n    \"\"\"Get exclusions specifically for coverage analysis.\"\"\"\n    return get_exclusions(\"coverage\", \"development\")\n\n\ndef get_analysis_exclusions() -> list:\n    \"\"\"Get exclusions specifically for code analysis.\"\"\"\n    return get_exclusions(\"analysis\", \"development\")\n\n\ndef get_documentation_exclusions() -> list:\n    \"\"\"Get exclusions specifically for documentation tools.\"\"\"\n    return get_exclusions(\"documentation\", \"development\")\n\n\ndef get_version_sync_exclusions() -> list:\n    \"\"\"Get exclusions specifically for version synchronization.\"\"\"\n    return get_exclusions(\"fix_version_sync\", \"development\")\n\n\ndef get_file_operations_exclusions() -> list:\n    \"\"\"Get exclusions specifically for file operations.\"\"\"\n    return get_exclusions(\"file_operations\", \"development\")\n\n\n# =============================================================================\n# GENERATED FILES & EXCLUSIONS\n# =============================================================================\n\n\ndef _load_generated_files() -> Tuple[str, ...]:\n    \"\"\"Load generated files list from config or return defaults.\n\n    Generated files can include both specific file paths and glob patterns.\n    Patterns (containing *) are handled via pattern matching, while exact paths\n    are handled via direct comparison.\n    \"\"\"\n    exclusions_config = _get_exclusions_config_safe()\n    if exclusions_config and \"generated_files\" in exclusions_config:\n        return tuple(exclusions_config[\"generated_files\"])\n    # Default: empty (projects should define their own)\n    return ()\n\n\n# All generated files (loaded from config)\n# Contains both specific file paths and glob patterns (e.g., \"ui/generated/*\")\nGENERATED_FILES: Tuple[str, ...] = _load_generated_files()\n\n\n# Separate generated files into exact paths and patterns for different use cases\ndef _split_generated_files() -> Tuple[Tuple[str, ...], Tuple[str, ...]]:\n    \"\"\"Split generated files into exact paths and patterns.\"\"\"\n    exact_paths = []\n    patterns = []\n    for item in GENERATED_FILES:\n        if \"*\" in item or \"?\" in item or \"[\" in item:\n            patterns.append(item)\n        else:\n            exact_paths.append(item)\n    return tuple(exact_paths), tuple(patterns)\n\n\nGENERATED_FILE_PATHS, GENERATED_FILE_PATTERNS = _split_generated_files()\n\n# For backward compatibility: ALL_GENERATED_FILES contains only exact paths\n# (patterns are handled separately via should_exclude_file)\nALL_GENERATED_FILES: Tuple[str, ...] = GENERATED_FILE_PATHS\n\n# Finalize context exclusions now that generated files are defined\n# Build recent_changes from generated files (always, regardless of config)\n# Include both exact paths and patterns\n_CONTEXT_EXCLUSIONS_TEMP[\"recent_changes\"] = list(ALL_GENERATED_FILES) + list(\n    GENERATED_FILE_PATTERNS\n)\nCONTEXT_EXCLUSIONS = _CONTEXT_EXCLUSIONS_TEMP\n\n\ndef _load_base_exclusion_shortlist() -> Tuple[str, ...]:\n    \"\"\"Load standard exclusion patterns from config or return defaults.\"\"\"\n    exclusions_config = _get_exclusions_config_safe()\n    if exclusions_config and \"base_exclusion_shortlist\" in exclusions_config:\n        return tuple(exclusions_config[\"base_exclusion_shortlist\"])\n    # Default generic patterns\n    return (\n        \"logs/\",\n        \"data/\",\n        \"coverage_html/\",\n        \"__pycache__/\",\n        \".pytest_cache/\",\n        \".ruff_cache/\",\n        \"venv/\",\n        \".venv/\",\n        \"htmlcov/\",\n        \"archive/\",\n        \"ui/generated/\",\n        \"mhm.egg-info/\",\n        \"*.log\",\n        \".coverage\",\n        \"coverage.xml\",\n        \"*.html\",\n    )\n\n\n# Standard exclusion patterns (used by multiple tools)\nBASE_EXCLUSION_SHORTLIST: Tuple[str, ...] = _load_base_exclusion_shortlist()\n\n# =============================================================================\n# TOOL-SPECIFIC EXCLUSIONS\n# =============================================================================\n\n\ndef _load_historical_preserve_files() -> Tuple[str, ...]:\n    \"\"\"Load historical preserve files from config or return defaults.\"\"\"\n    exclusions_config = _get_exclusions_config_safe()\n    if exclusions_config and \"historical_preserve_files\" in exclusions_config:\n        return tuple(exclusions_config[\"historical_preserve_files\"])\n    # Default: empty (projects should define their own)\n    return ()\n\n\n# Legacy cleanup preserve files\nHISTORICAL_PRESERVE_FILES: Tuple[str, ...] = _load_historical_preserve_files()\n\n# Documentation sync checker placeholders\nDOC_SYNC_PLACEHOLDERS: Dict[str, str] = {\n    \"logs\": \"    (log files)\",\n    \"data\": \"    (data files)\",\n    \"htmlcov\": \"    (HTML coverage reports)\",\n    \"backups\": \"    (backup files)\",\n    \"archive\": \"    (archived files)\",\n    \"jsons\": \"    (JSON files created by development tools)\",\n    \"development_tools/tests/logs\": \"    (test coverage log files)\",\n}\n\n# Example usage\nif __name__ == \"__main__\":\n    print(\"Coverage exclusions:\")\n    for pattern in get_coverage_exclusions():\n        print(f\"  {pattern}\")\n\n    print(\"\\nAnalysis exclusions:\")\n    for pattern in get_analysis_exclusions():\n        print(f\"  {pattern}\")\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 336,
                    "line_content": "# For backward compatibility: ALL_GENERATED_FILES contains only exact paths",
                    "start": 10520,
                    "end": 10542
                  }
                ]
              ],
              [
                "development_tools\\tests\\analyze_test_markers.py",
                "#!/usr/bin/env python3\n# TOOL_TIER: supporting\n# TOOL_PORTABILITY: portable\n\n\"\"\"\nanalyze_test_markers.py\nAnalyze test files for pytest marker usage (read-only analysis).\nFor fixing operations, use fix_test_markers.py.\n\"\"\"\n\nimport sys\nimport re\nimport ast\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict, Optional\n\n# Add project root to path for core module imports\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\n# Handle both relative and absolute imports\ntry:\n    from . import config\nexcept ImportError:\n    from development_tools import config\n\nfrom core.logger import get_component_logger\n\n# Ensure external config is loaded\nconfig.load_external_config()\n\nlogger = get_component_logger(\"development_tools\")\n\nCATEGORY_MARKERS = {\"unit\", \"integration\", \"behavior\", \"ui\"}\n\n\nclass TestMarkerAnalyzer:\n    \"\"\"Analyze and manage pytest category markers in test files.\"\"\"\n\n    def __init__(self, project_root: Optional[Path] = None):\n        self.project_root = project_root or Path(config.get_project_root())\n        self.test_dir = self.project_root / \"tests\"\n\n    def find_test_files(self, exclude_ai: bool = True) -> List[Path]:\n        \"\"\"Find all test files in the tests directory.\"\"\"\n        test_files = []\n        if not self.test_dir.exists():\n            return test_files\n\n        for test_file in self.test_dir.rglob(\"test_*.py\"):\n            if test_file.is_file():\n                # Skip AI test files if requested\n                if exclude_ai and (\n                    \"ai/test_ai\" in str(test_file) or \"test_ai\" in test_file.name\n                ):\n                    continue\n\n                # Skip temporary test files in tests/data/ (pytest temporary directories)\n                file_str = str(test_file).replace(\"\\\\\", \"/\")\n                if \"/tests/data/\" in file_str or file_str.startswith(\"tests/data/\"):\n                    # Exclude pytest temporary directories\n                    if \"pytest-tmp-\" in file_str or \"pytest-of-\" in file_str:\n                        continue\n\n                test_files.append(test_file)\n\n        return sorted(test_files)\n\n    def has_category_marker(self, content: str) -> bool:\n        \"\"\"Check if content has any category marker.\"\"\"\n        patterns = [\n            r\"@pytest\\.mark\\.unit\\b\",\n            r\"@pytest\\.mark\\.integration\\b\",\n            r\"@pytest\\.mark\\.behavior\\b\",\n            r\"@pytest\\.mark\\.ui\\b\",\n        ]\n        return any(re.search(pattern, content) for pattern in patterns)\n\n    def get_expected_marker(self, file_path: Path) -> Optional[str]:\n        \"\"\"Determine expected marker based on directory structure.\"\"\"\n        f_str = str(file_path).replace(\"\\\\\", \"/\")\n        if \"/tests/unit/\" in f_str or f_str.startswith(\"tests/unit/\"):\n            return \"unit\"\n        elif \"/tests/integration/\" in f_str or f_str.startswith(\"tests/integration/\"):\n            return \"integration\"\n        elif \"/tests/behavior/\" in f_str or f_str.startswith(\"tests/behavior/\"):\n            return \"behavior\"\n        elif \"/tests/ui/\" in f_str or f_str.startswith(\"tests/ui/\"):\n            return \"ui\"\n        return None\n\n    def analyze_markers(self) -> Dict:\n        \"\"\"Analyze all test files for marker usage.\"\"\"\n        test_files = self.find_test_files()\n        files_needing_markers = []\n        files_by_dir = {\n            \"unit\": [],\n            \"integration\": [],\n            \"behavior\": [],\n            \"ui\": [],\n            \"other\": [],\n        }\n\n        for test_file in test_files:\n            try:\n                content = test_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n            except Exception as e:\n                logger.warning(f\"Failed to read {test_file}: {e}\")\n                continue\n\n            expected = self.get_expected_marker(test_file)\n            if expected:\n                files_by_dir[expected].append(str(test_file))\n            else:\n                files_by_dir[\"other\"].append(str(test_file))\n\n            if not self.has_category_marker(content):\n                files_needing_markers.append((str(test_file), expected))\n\n        return {\n            \"total_files\": len(test_files),\n            \"files_needing_markers\": len(files_needing_markers),\n            \"files_by_dir\": {k: len(v) for k, v in files_by_dir.items()},\n            \"missing_markers\": files_needing_markers,\n        }\n\n    def find_missing_markers_ast(self) -> List[Tuple[str, int, str, str]]:\n        \"\"\"Find missing markers using AST analysis (more accurate).\"\"\"\n        finder = MissingMarkerFinder()\n        test_files = self.find_test_files()\n\n        for file_path in test_files:\n            finder.analyze_file(file_path)\n\n        return finder.missing\n\n    def add_markers(self, dry_run: bool = False) -> Dict:\n        \"\"\"\n        Add missing markers to test files based on directory structure.\n\n        Note: This method is kept here for backward compatibility but the fixing\n        logic should be used via fix_test_markers.py module.\n        \"\"\"\n        test_files = self.find_test_files()\n        files_updated = []\n        files_skipped = []\n\n        for test_file in test_files:\n            try:\n                content = test_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n            except Exception as e:\n                logger.warning(f\"Failed to read {test_file}: {e}\")\n                continue\n\n            # Check if already has category marker\n            if self.has_category_marker(content):\n                files_skipped.append(str(test_file))\n                continue\n\n            # Determine expected category from directory\n            marker = self.get_expected_marker(test_file)\n            if not marker:\n                files_skipped.append(str(test_file))\n                continue\n\n            # Find all test classes\n            class_pattern = r\"^class\\s+(Test\\w+):\"\n            matches = list(re.finditer(class_pattern, content, re.MULTILINE))\n\n            if not matches:\n                files_skipped.append(str(test_file))\n                continue\n\n            # Add marker before first test class\n            first_match = matches[0]\n            insert_pos = first_match.start()\n\n            # Check if pytest is already imported\n            needs_pytest_import = (\n                \"import pytest\" not in content and \"from pytest\" not in content\n            )\n\n            # Build the marker line\n            marker_line = f\"@pytest.mark.{marker}\\n\"\n\n            # If we need to add pytest import, add it near the top\n            if needs_pytest_import:\n                # Find a good place to add import (after other imports)\n                import_pattern = r\"^(import\\s+\\w+|from\\s+\\w+.*import)\"\n                import_matches = list(\n                    re.finditer(import_pattern, content, re.MULTILINE)\n                )\n                if import_matches:\n                    last_import = import_matches[-1]\n                    import_end = last_import.end()\n                    # Add import after last import, before blank line if present\n                    next_line_start = content.find(\"\\n\", import_end) + 1\n                    if (\n                        next_line_start > 0\n                        and next_line_start < len(content)\n                        and content[next_line_start : next_line_start + 1] == \"\\n\"\n                    ):\n                        # Already a blank line, add before it\n                        content = (\n                            content[:next_line_start]\n                            + \"import pytest\\n\"\n                            + content[next_line_start:]\n                        )\n                    else:\n                        # Add import and blank line\n                        content = (\n                            content[:next_line_start]\n                            + \"import pytest\\n\\n\"\n                            + content[next_line_start:]\n                        )\n                else:\n                    # No imports found, add at top after docstring\n                    docstring_end = content.find('\"\"\"', content.find('\"\"\"') + 3) + 3\n                    if docstring_end > 2:\n                        next_line = content.find(\"\\n\", docstring_end) + 1\n                        content = (\n                            content[:next_line]\n                            + \"import pytest\\n\\n\"\n                            + content[next_line:]\n                        )\n                    else:\n                        # No docstring, add at very top\n                        content = \"import pytest\\n\\n\" + content\n\n            # Add marker before first class\n            # Find the line start for the class\n            line_start = content.rfind(\"\\n\", 0, insert_pos) + 1\n            # Check if there's already a marker on the previous line\n            prev_line_start = (\n                content.rfind(\"\\n\", 0, line_start - 1) + 1 if line_start > 0 else 0\n            )\n            prev_line = (\n                content[prev_line_start : line_start - 1].strip()\n                if line_start > 0\n                else \"\"\n            )\n\n            if prev_line.startswith(\"@pytest.mark.\"):\n                # Already has a marker, skip\n                files_skipped.append(str(test_file))\n                continue\n\n            # Insert marker before class\n            content = content[:line_start] + marker_line + content[line_start:]\n\n            # Add marker to remaining classes in the file\n            for match in matches[1:]:\n                class_line_start = content.rfind(\"\\n\", 0, match.start()) + 1\n                prev_class_line_start = (\n                    content.rfind(\"\\n\", 0, class_line_start - 1) + 1\n                    if class_line_start > 0\n                    else 0\n                )\n                prev_class_line = (\n                    content[prev_class_line_start : class_line_start - 1].strip()\n                    if class_line_start > 0\n                    else \"\"\n                )\n\n                if not prev_class_line.startswith(\"@pytest.mark.\"):\n                    content = (\n                        content[:class_line_start]\n                        + marker_line\n                        + content[class_line_start:]\n                    )\n\n            # Write updated content\n            if not dry_run:\n                test_file.write_text(content, encoding=\"utf-8\")\n            files_updated.append((str(test_file), marker))\n\n        return {\"updated\": files_updated, \"skipped\": files_skipped, \"dry_run\": dry_run}\n\n\nclass MissingMarkerFinder:\n    \"\"\"AST-based finder for missing markers (more accurate than regex).\"\"\"\n\n    def __init__(self):\n        self.missing = []\n\n    def _is_pytest_fixture(self, decorators):\n        for dec in decorators:\n            target = dec\n            if isinstance(dec, ast.Call):\n                target = dec.func\n            if isinstance(target, ast.Attribute):\n                if target.attr == \"fixture\":\n                    value = target.value\n                    if isinstance(value, ast.Name) and value.id == \"pytest\":\n                        return True\n                    if isinstance(value, ast.Attribute) and value.attr == \"fixture\":\n                        return True\n            elif isinstance(target, ast.Name):\n                if target.id == \"fixture\":\n                    return True\n        return False\n\n    def has_category_marker(self, decorators):\n        for dec in decorators:\n            target = dec\n            if isinstance(dec, ast.Call):\n                target = dec.func\n            if isinstance(target, ast.Attribute):\n                if target.attr in CATEGORY_MARKERS:\n                    value = target.value\n                    if isinstance(value, ast.Attribute) and value.attr == \"mark\":\n                        return True\n                    if isinstance(value, ast.Name) and value.id == \"mark\":\n                        # handles 'from pytest import mark'\n                        return True\n            elif isinstance(target, ast.Name):\n                if target.id in CATEGORY_MARKERS:\n                    return True\n        return False\n\n    def process_function(self, node, file_path, inherited_category=False):\n        if not node.name.startswith(\"test_\"):\n            return\n        if self._is_pytest_fixture(node.decorator_list):\n            return\n        if inherited_category or self.has_category_marker(node.decorator_list):\n            return\n        self.missing.append((str(file_path), node.lineno, node.name, \"function\"))\n\n    def _class_marked_not_test(self, node):\n        for stmt in node.body:\n            if isinstance(stmt, ast.Assign):\n                for target in stmt.targets:\n                    if isinstance(target, ast.Name) and target.id == \"__test__\":\n                        if (\n                            isinstance(stmt.value, ast.Constant)\n                            and stmt.value.value is False\n                        ):\n                            return True\n        return False\n\n    def process_class(self, node, file_path, inherited_category=False):\n        if self._class_marked_not_test(node):\n            return\n        if not node.name.startswith(\"Test\") and not inherited_category:\n            return\n        class_has = inherited_category or self.has_category_marker(node.decorator_list)\n        for stmt in node.body:\n            if isinstance(stmt, ast.FunctionDef):\n                self.process_function(stmt, file_path, inherited_category=class_has)\n            elif isinstance(stmt, ast.AsyncFunctionDef):\n                self.process_function(stmt, file_path, inherited_category=class_has)\n            elif isinstance(stmt, ast.ClassDef):\n                self.process_class(stmt, file_path, inherited_category=class_has)\n\n    def analyze_file(self, file_path):\n        try:\n            tree = ast.parse(file_path.read_text(encoding=\"utf-8\"))\n        except SyntaxError as exc:\n            logger.warning(f\"Skipping {file_path} due to syntax error: {exc}\")\n            return\n        for node in tree.body:\n            if isinstance(node, ast.FunctionDef):\n                self.process_function(node, file_path)\n            elif isinstance(node, ast.AsyncFunctionDef):\n                self.process_function(node, file_path)\n            elif isinstance(node, ast.ClassDef):\n                self.process_class(node, file_path)\n\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        description=\"Analyze and manage pytest test markers\"\n    )\n    parser.add_argument(\n        \"--check\", action=\"store_true\", help=\"Check for missing markers (default)\"\n    )\n    parser.add_argument(\n        \"--analyze\",\n        action=\"store_true\",\n        help=\"Analyze marker usage and provide detailed report\",\n    )\n    parser.add_argument(\"--json\", action=\"store_true\", help=\"Output results as JSON\")\n\n    args = parser.parse_args()\n\n    # Default to check if no action specified\n    if not (args.check or args.analyze):\n        args.check = True\n\n    analyzer = TestMarkerAnalyzer()\n\n    if args.analyze:\n        result = analyzer.analyze_markers()\n        if args.json:\n            import json\n\n            print(json.dumps(result, indent=2))\n        else:\n            print(f\"Total test files: {result['total_files']}\")\n            print(f\"Files needing category markers: {result['files_needing_markers']}\")\n            print(f\"\\nFiles by directory:\")\n            for dir_name, count in result[\"files_by_dir\"].items():\n                print(f\"  {dir_name}: {count}\")\n            if result[\"missing_markers\"]:\n                print(f\"\\nFiles needing markers (first 30):\")\n                for f, expected in result[\"missing_markers\"][:30]:\n                    print(f\"  {expected or 'unknown'}: {f}\")\n\n    else:  # args.check (default)\n        missing = analyzer.find_missing_markers_ast()\n        if args.json:\n            import json\n\n            print(\n                json.dumps(\n                    {\n                        \"missing_count\": len(missing),\n                        \"missing\": [\n                            {\"file\": f, \"line\": l, \"name\": n, \"type\": t}\n                            for f, l, n, t in missing\n                        ],\n                    },\n                    indent=2,\n                )\n            )\n        else:\n            if not missing:\n                print(\"All tests have category markers. Great job!\")\n                return 0\n\n            print(\"Tests missing category markers (unit/integration/behavior/ui):\")\n            for file_path, lineno, name, node_type in missing:\n                print(f\"  - {file_path}:{lineno} ({node_type} {name})\")\n\n            print(f\"\\nTotal missing markers: {len(missing)}\")\n            return 1 if missing else 0\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 142,
                    "line_content": "Note: This method is kept here for backward compatibility but the fixing",
                    "start": 4939,
                    "end": 4961
                  }
                ]
              ],
              [
                "development_tools\\tests\\generate_test_coverage_report.py",
                "#!/usr/bin/env python3\n# TOOL_TIER: core\n\n\"\"\"\ngenerate_test_coverage_report.py\nGenerates coverage reports (JSON, HTML, summary, TEST_COVERAGE_REPORT.md) from analysis results.\n\nConfiguration is loaded from external config file (development_tools_config.json)\nif available, making this tool portable across different projects.\n\"\"\"\n\nimport json\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, Optional\n\n# Add project root to path for core module imports\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nfrom core.logger import get_component_logger\nfrom core.time_utilities import now_timestamp_full\n\n# Import config module (absolute import for portability)\ntry:\n    from development_tools import config\nexcept ImportError:\n    # Fallback for when run as script\n    if str(project_root) not in sys.path:\n        sys.path.insert(0, str(project_root))\n    from development_tools import config\n\n# Load external config on module import (safe to call multiple times)\nconfig.load_external_config()\n\nlogger = get_component_logger(\"development_tools\")\n\n\nclass TestCoverageReportGenerator:\n    \"\"\"Generates coverage reports from analysis results.\"\"\"\n\n    def __init__(\n        self,\n        project_root: str = \".\",\n        coverage_config: Optional[str] = None,\n        artifact_directories: Optional[Dict[str, str]] = None,\n    ):\n        \"\"\"\n        Initialize coverage report generator.\n\n        Args:\n            project_root: Root directory of the project\n            coverage_config: Optional path to coverage config file (e.g., 'coverage.ini').\n                            If None, loads from config or uses default.\n            artifact_directories: Optional dict with keys: html_output, archive, logs.\n                                 If None, loads from config or uses defaults.\n        \"\"\"\n        self.project_root = Path(project_root).resolve()\n\n        # Load coverage configuration from external config\n        coverage_config_data = config.get_external_value(\"coverage\", {})\n\n        # Coverage config path (from parameter, config, or default)\n        if coverage_config is not None:\n            self.coverage_config_path = self.project_root / coverage_config\n        else:\n            # Check for coverage.ini in development_tools/tests first (new location), then root (legacy)\n            config_coverage_path = coverage_config_data.get(\n                \"coverage_config\", \"development_tools/tests/coverage.ini\"\n            )\n            self.coverage_config_path = self.project_root / config_coverage_path\n            if not self.coverage_config_path.exists():\n                # Fall back to root location for backward compatibility\n                self.coverage_config_path = self.project_root / \"coverage.ini\"\n\n        # Artifact directories (from parameter, config, or defaults)\n        if artifact_directories is not None:\n            self.coverage_html_dir = Path(\n                artifact_directories.get(\"html_output\", \"tests/coverage_html\")\n            )\n            self.archive_root = Path(\n                artifact_directories.get(\n                    \"archive\", \"development_tools/reports/archive/coverage_artifacts\"\n                )\n            )\n            self.coverage_logs_dir = Path(\n                artifact_directories.get(\"logs\", \"development_tools/tests/logs\")\n            )\n        else:\n            config_dirs = coverage_config_data.get(\"artifact_directories\", {})\n            self.coverage_html_dir = Path(\n                config_dirs.get(\"html_output\", \"tests/coverage_html\")\n            )\n            self.archive_root = Path(\n                config_dirs.get(\n                    \"archive\", \"development_tools/reports/archive/coverage_artifacts\"\n                )\n            )\n            self.coverage_logs_dir = Path(\n                config_dirs.get(\"logs\", \"development_tools/tests/logs\")\n            )\n\n        # Ensure directories exist\n        self.coverage_html_dir.parent.mkdir(parents=True, exist_ok=True)\n        self.archive_root.mkdir(parents=True, exist_ok=True)\n        self.coverage_logs_dir.mkdir(parents=True, exist_ok=True)\n\n        # Coverage data file\n        self.coverage_data_file = self.project_root / \".coverage\"\n\n        # Coverage plan file (TEST_COVERAGE_REPORT.md)\n        self.coverage_plan_file = (\n            self.project_root / \"development_docs\" / \"TEST_COVERAGE_REPORT.md\"\n        )\n\n    def generate_coverage_summary(\n        self, coverage_data: Dict[str, Dict[str, any]], overall_data: Dict[str, any]\n    ) -> str:\n        \"\"\"Generate a coverage summary for the plan.\"\"\"\n        summary_lines = []\n\n        # Overall coverage (format with 1 decimal place for accuracy)\n        coverage_value = overall_data[\"overall_coverage\"]\n        if isinstance(coverage_value, float):\n            coverage_str = f\"{coverage_value:.1f}\"\n        elif isinstance(coverage_value, int):\n            coverage_str = f\"{coverage_value:.0f}\"\n        else:\n            coverage_str = str(coverage_value)\n        summary_lines.append(f\"### **Overall Coverage: {coverage_str}%**\")\n        summary_lines.append(\n            f\"- **Total Statements**: {overall_data['total_statements']:,}\"\n        )\n        summary_lines.append(\n            f\"- **Covered Statements**: {overall_data['total_statements'] - overall_data['total_missed']:,}\"\n        )\n        summary_lines.append(\n            f\"- **Uncovered Statements**: {overall_data['total_missed']:,}\"\n        )\n        summary_lines.append(\n            f\"- **Goal**: Expand to **80%+ coverage** for comprehensive reliability\\n\"\n        )\n\n        # Coverage by category\n        try:\n            from .analyze_test_coverage import TestCoverageAnalyzer\n        except ImportError:\n            from development_tools.tests.analyze_test_coverage import (\n                TestCoverageAnalyzer,\n            )\n        analyzer = TestCoverageAnalyzer(str(self.project_root))\n        categories = analyzer.categorize_modules(coverage_data)\n\n        summary_lines.append(\"### **Coverage Summary by Category**\")\n\n        for category, modules in categories.items():\n            if modules:\n                avg_coverage = sum(coverage_data[m][\"coverage\"] for m in modules) / len(\n                    modules\n                )\n                summary_lines.append(\n                    f\"- **{category.title()} ({avg_coverage:.0f}% avg)**: {len(modules)} modules\"\n                )\n\n        summary_lines.append(\"\")\n\n        # Detailed module breakdown\n        summary_lines.append(\"### **Detailed Module Coverage**\")\n\n        # Sort modules by coverage (lowest first)\n        sorted_modules = sorted(coverage_data.items(), key=lambda x: x[1][\"coverage\"])\n\n        for module_name, data in sorted_modules:\n            status_emoji = (\n                \"*\"\n                if data[\"coverage\"] >= 80\n                else \"!\" if data[\"coverage\"] >= 60 else \"X\"\n            )\n            summary_lines.append(\n                f\"- **{status_emoji} {module_name}**: {data['coverage']}% ({data['covered']}/{data['statements']} lines)\"\n            )\n\n        return \"\\n\".join(summary_lines)\n\n    def update_coverage_plan(self, coverage_summary: str) -> bool:\n        \"\"\"Update the TEST_COVERAGE_REPORT.md with new metrics.\"\"\"\n        generated_timestamp = now_timestamp_full()\n\n        # Standard generated header\n        standard_header = f\"\"\"# Test Coverage Report\n\n> **File**: `development_docs/TEST_COVERAGE_REPORT.md`\n> **Generated**: This file is auto-generated. Do not edit manually.\n> **Last Generated**: {generated_timestamp}\n> **Source**: `python development_tools/tests/generate_test_coverage_report.py` - Test Coverage Report Generator\n\n\"\"\"\n\n        if not self.coverage_plan_file.exists():\n            # Create new file with standard header and rotation\n            from development_tools.shared.file_rotation import create_output_file\n\n            try:\n                content = (\n                    standard_header + \"## Current Status\\n\\n\" + coverage_summary + \"\\n\"\n                )\n                create_output_file(\n                    str(self.coverage_plan_file),\n                    content,\n                    rotate=True,\n                    max_versions=7,\n                    project_root=self.project_root,\n                )\n                if logger:\n                    logger.info(\n                        f\"Created coverage plan with standard header: {self.coverage_plan_file}\"\n                    )\n                return True\n            except Exception as e:\n                if logger:\n                    logger.error(f\"Error creating coverage plan: {e}\")\n                return False\n\n        try:\n            with open(self.coverage_plan_file, \"r\", encoding=\"utf-8\") as f:\n                content = f.read()\n\n            # Check if file has standard generated header\n            # LEGACY COMPATIBILITY\n            # Accept both TEST_COVERAGE_REPORT.md (new) and TEST_COVERAGE_EXPANSION_PLAN.md (old) in header check.\n            # New standardized filename: TEST_COVERAGE_REPORT.md\n            # Removal plan: After one release cycle, remove check for TEST_COVERAGE_EXPANSION_PLAN.md in header.\n            # Detection: Search for \"TEST_COVERAGE_EXPANSION_PLAN.md\" in header checks.\n            has_standard_header = (\n                (\n                    \"> **File**: `development_docs/TEST_COVERAGE_REPORT.md`\" in content\n                    or \"> **File**: `development_docs/TEST_COVERAGE_EXPANSION_PLAN.md`\"\n                    in content\n                )\n                and \"> **Generated**: This file is auto-generated\" in content\n                and (\n                    \"> **Source**:\" in content\n                    or \"> **Generated by**: generate_test_coverage.py\" in content\n                )\n            )\n            if (\n                \"TEST_COVERAGE_EXPANSION_PLAN.md\" in content\n                and \"TEST_COVERAGE_REPORT.md\" not in content\n            ):\n                logger.debug(\n                    \"LEGACY: Detected old filename TEST_COVERAGE_EXPANSION_PLAN.md in header (new name: TEST_COVERAGE_REPORT.md)\"\n                )\n\n            # Find and replace the current status section\n            section_header = \"## Current Status\"\n            current_status_pattern = r\"(## Current Status.*?)(?=\\n## |\\Z)\"\n\n            new_status_section = f\"{section_header}\\n\\n{coverage_summary}\\n\"\n\n            if has_standard_header:\n                # File has standard header - just update the status section and timestamp\n                if re.search(current_status_pattern, content, re.DOTALL):\n                    updated_content = re.sub(\n                        current_status_pattern,\n                        lambda _: new_status_section,\n                        content,\n                        flags=re.DOTALL,\n                    )\n                else:\n                    # Add status section after header\n                    header_end = content.find(\"\\n## \")\n                    if header_end == -1:\n                        header_end = len(content)\n                    updated_content = (\n                        content[:header_end]\n                        + \"\\n\\n\"\n                        + new_status_section\n                        + content[header_end:]\n                    )\n\n                # Update the last generated timestamp\n                timestamp_pattern = r\"(> \\*\\*Last Generated\\*\\*: ).*\"\n                if re.search(timestamp_pattern, updated_content):\n                    updated_content = re.sub(\n                        timestamp_pattern,\n                        lambda match: f\"{match.group(1)}{generated_timestamp}\",\n                        updated_content,\n                    )\n                else:\n                    # If timestamp not found, add it after the Source line\n                    source_pattern = r\"(> \\*\\*Source\\*\\*:.*\\n)\"\n                    if re.search(source_pattern, updated_content):\n                        updated_content = re.sub(\n                            source_pattern,\n                            lambda match: f\"{match.group(1)}> **Last Generated**: {generated_timestamp}\\n\",\n                            updated_content,\n                        )\n\n                # Remove duplicate headers - if we have multiple header sections, keep only the first one\n                # Pattern: header section starts with \"# Test Coverage Report\" and ends before \"## Current Status\"\n                header_pattern = (\n                    r\"(# Test Coverage Report.*?> \\*\\*Last Generated\\*\\*:.*?\\n\\n)\"\n                )\n                matches = list(re.finditer(header_pattern, updated_content, re.DOTALL))\n                if len(matches) > 1:\n                    # Keep only the first header, remove the rest\n                    first_header_end = matches[0].end()\n                    # Find where the second header starts\n                    second_header_start = matches[1].start()\n                    # Remove everything from first header end to second header start, but keep the content after\n                    updated_content = (\n                        updated_content[:first_header_end]\n                        + updated_content[second_header_start:]\n                    )\n                    # Now remove any remaining duplicate headers\n                    while True:\n                        new_matches = list(\n                            re.finditer(header_pattern, updated_content, re.DOTALL)\n                        )\n                        if len(new_matches) <= 1:\n                            break\n                        # Remove second header\n                        updated_content = (\n                            updated_content[: new_matches[0].end()]\n                            + updated_content[new_matches[1].end() :]\n                        )\n            else:\n                # File doesn't have standard header - replace with standard header\n                # Find the title\n                title_match = re.search(\n                    r\"^# Test Coverage Expansion Plan.*?\\n\", content, re.MULTILINE\n                )\n                if title_match:\n                    # Replace everything from title to first section with standard header\n                    title_end = title_match.end()\n                    first_section_match = re.search(r\"\\n## \", content[title_end:])\n                    if first_section_match:\n                        section_start = title_end + first_section_match.start()\n                        updated_content = standard_header + content[section_start:]\n                    else:\n                        updated_content = standard_header + content[title_end:]\n                else:\n                    # No title found, prepend standard header\n                    updated_content = standard_header + content\n\n                # Ensure status section exists\n                if \"## Current Status\" not in updated_content:\n                    updated_content = (\n                        updated_content.rstrip() + \"\\n\\n\" + new_status_section\n                    )\n                else:\n                    # Replace existing status section\n                    updated_content = re.sub(\n                        current_status_pattern,\n                        lambda _: new_status_section,\n                        updated_content,\n                        flags=re.DOTALL,\n                    )\n\n            # Write updated content\n            # Use rotation system for archiving\n            from development_tools.shared.file_rotation import create_output_file\n\n            create_output_file(\n                str(self.coverage_plan_file),\n                updated_content,\n                rotate=True,\n                max_versions=7,\n                project_root=self.project_root,\n            )\n\n            if logger:\n                logger.info(f\"Updated coverage plan: {self.coverage_plan_file}\")\n            return True\n\n        except Exception as e:\n            if logger:\n                logger.error(f\"Error updating coverage plan: {e}\")\n            return False\n\n    def finalize_coverage_outputs(self) -> None:\n        \"\"\"Finalize coverage outputs by combining shards and generating HTML/JSON reports.\"\"\"\n        # Set up coverage command and environment\n        coverage_cmd = [sys.executable, \"-m\", \"coverage\"]\n        env = os.environ.copy()\n        env[\"COVERAGE_FILE\"] = str(self.coverage_data_file)\n        if self.coverage_config_path.exists():\n            env[\"COVERAGE_RCFILE\"] = str(self.coverage_config_path)\n\n        # Combine coverage data files if they exist\n        coverage_data_files = list(self.project_root.glob(\".coverage.*\"))\n\n        if coverage_data_files:\n            if logger:\n                logger.info(\n                    f\"Combining {len(coverage_data_files)} coverage data files...\"\n                )\n\n            # Use coverage combine command\n            combine_cmd = coverage_cmd + [\"combine\"]\n\n            combine_result = subprocess.run(\n                combine_cmd,\n                capture_output=True,\n                text=True,\n                cwd=self.project_root,\n                env=env,\n            )\n\n            if combine_result.returncode != 0 and logger:\n                logger.warning(\n                    f\"coverage combine exited with {combine_result.returncode}: {combine_result.stderr.strip()}\"\n                )\n\n            # Clean up shard files after combining\n            for shard_file in coverage_data_files:\n                try:\n                    shard_file.unlink()\n                    if logger:\n                        logger.debug(f\"Removed coverage shard file: {shard_file}\")\n                except Exception as e:\n                    if logger:\n                        logger.warning(f\"Failed to remove shard file {shard_file}: {e}\")\n\n        # Generate HTML report\n        if logger:\n            logger.info(\"Generating HTML coverage report...\")\n\n        # Ensure HTML output directory exists\n        self.coverage_html_dir.mkdir(parents=True, exist_ok=True)\n\n        html_args = coverage_cmd + [\"html\", \"-d\", str(self.coverage_html_dir)]\n        # Add timeout for HTML generation (10 minutes should be plenty for most codebases)\n        # If it takes longer, there may be an issue with the coverage data\n        html_timeout = 600  # 10 minutes\n        try:\n            html_result = subprocess.run(\n                html_args,\n                capture_output=True,\n                text=True,\n                cwd=self.project_root,\n                env=env,\n                timeout=html_timeout,\n            )\n        except subprocess.TimeoutExpired:\n            if logger:\n                logger.warning(\n                    f\"coverage html timed out after {html_timeout} seconds - HTML generation may be incomplete\"\n                )\n            html_result = subprocess.CompletedProcess(\n                html_args,\n                returncode=1,\n                stdout=\"\",\n                stderr=f\"coverage html timed out after {html_timeout} seconds\",\n            )\n        # No longer creating coverage_html logs - user only uses stdout logs\n        # self._write_command_log('coverage_html', html_result)\n        if html_result.returncode != 0 and logger:\n            logger.warning(\n                f\"coverage html exited with {html_result.returncode}: {html_result.stderr.strip()}\"\n            )\n\n        # Regenerate JSON report after combine to ensure it reflects combined data\n        # Use the same location as generate_test_coverage.py (development_tools/tests/jsons/coverage.json)\n        jsons_dir = self.project_root / \"development_tools\" / \"tests\" / \"jsons\"\n        jsons_dir.mkdir(parents=True, exist_ok=True)\n        coverage_output = jsons_dir / \"coverage.json\"\n\n        # Archive the old coverage.json BEFORE creating the new one (to keep current file in main directory)\n        archive_dir = jsons_dir / \"archive\"\n        archive_dir.mkdir(parents=True, exist_ok=True)\n        if coverage_output.exists():\n            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n            archive_name = f\"coverage_{timestamp}.json\"\n            archive_path = archive_dir / archive_name\n            shutil.move(str(coverage_output), str(archive_path))\n            if logger:\n                logger.debug(f\"Archived old coverage.json to {archive_name}\")\n\n            # Clean up old archives to keep only 7 versions (current + 6 archived = 7 total)\n            from development_tools.shared.file_rotation import FileRotator\n\n            rotator = FileRotator(base_dir=str(archive_dir))\n            # Use _cleanup_old_versions with the base name (without extension)\n            rotator._cleanup_old_versions(\n                \"coverage\", max_versions=6\n            )  # Keep 6 archived (current is separate)\n\n        # Generate JSON report\n        json_args = coverage_cmd + [\"json\", \"-o\", str(coverage_output)]\n        json_result = subprocess.run(\n            json_args, capture_output=True, text=True, cwd=self.project_root, env=env\n        )\n\n        if json_result.returncode == 0:\n            if logger:\n                logger.info(f\"Coverage JSON report generated: {coverage_output}\")\n        else:\n            if logger:\n                logger.warning(\n                    f\"coverage json exited with {json_result.returncode}: {json_result.stderr.strip()}\"\n                )\n\n\ndef main():\n    \"\"\"Main entry point for standalone report generation.\"\"\"\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        description=\"Generate test coverage reports from existing coverage data\"\n    )\n    parser.add_argument(\n        \"--coverage-json\",\n        type=str,\n        default=\"development_tools/tests/jsons/coverage.json\",\n        help=\"Path to coverage.json file (default: development_tools/tests/jsons/coverage.json)\",\n    )\n    parser.add_argument(\n        \"--update-plan\",\n        action=\"store_true\",\n        help=\"Update TEST_COVERAGE_REPORT.md with current coverage metrics\",\n    )\n    parser.add_argument(\n        \"--summary\", action=\"store_true\", help=\"Print coverage summary to stdout\"\n    )\n\n    args = parser.parse_args()\n\n    # Load coverage data\n    # Resolve path relative to project root if it's a relative path\n    coverage_json_path = Path(args.coverage_json)\n    if not coverage_json_path.is_absolute():\n        # Assume relative to project root (where script is run from)\n        # When run via run_script, cwd is project_root, so relative paths work\n        # But we also need to handle when run directly, so resolve from script location\n        script_project_root = Path(__file__).parent.parent.parent\n        coverage_json_path = script_project_root / coverage_json_path\n\n    if not coverage_json_path.exists():\n        logger.error(f\"Coverage JSON file not found: {coverage_json_path}\")\n        logger.error(\"Run test coverage first to generate coverage data\")\n        return 1\n\n    # Determine project root from coverage.json path\n    # coverage.json is at: project_root/development_tools/tests/jsons/coverage.json\n    # So we need to go up 4 levels: coverage.json -> jsons -> tests -> development_tools -> project_root\n    project_root = coverage_json_path.parent.parent.parent.parent\n\n    # Load coverage data\n    try:\n        with open(coverage_json_path, \"r\", encoding=\"utf-8\") as f:\n            coverage_data = json.load(f)\n    except Exception as e:\n        logger.error(f\"Failed to load coverage JSON: {e}\")\n        return 1\n\n    # Parse coverage data (coverage.json format)\n    try:\n        from .analyze_test_coverage import TestCoverageAnalyzer\n    except ImportError:\n        from development_tools.tests.analyze_test_coverage import TestCoverageAnalyzer\n    analyzer = TestCoverageAnalyzer(str(project_root))\n    analysis_results = analyzer.analyze_coverage(coverage_json_path=coverage_json_path)\n\n    # Create report generator with project root\n    generator = TestCoverageReportGenerator(project_root=str(project_root))\n\n    # Generate summary\n    coverage_data_dict = analysis_results.get(\"modules\", {})\n    overall_data = analysis_results.get(\"overall\", {})\n    coverage_summary = generator.generate_coverage_summary(\n        coverage_data_dict, overall_data\n    )\n\n    # Update plan if requested\n    if args.update_plan:\n        success = generator.update_coverage_plan(coverage_summary)\n        if success:\n            print(\"\\n* Coverage plan updated successfully!\")\n        else:\n            print(\"\\n* Failed to update coverage plan\")\n            return 1\n\n    if args.summary:\n        print(coverage_summary)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 79,
                    "line_content": "# Fall back to root location for backward compatibility",
                    "start": 2796,
                    "end": 2818
                  }
                ]
              ],
              [
                "development_tools\\tests\\run_test_coverage.py",
                "#!/usr/bin/env python3\n# TOOL_TIER: core\n\n\"\"\"\nTest Coverage Execution Tool (Portable)\n\nThis script orchestrates pytest execution with coverage collection and generates\ncoverage data files. It runs the actual test suite and collects coverage metrics.\n\nNOTE: This tool EXECUTES tests and GENERATES coverage data. For pure analysis of\nexisting coverage data, use analyze_test_coverage.py instead.\n\nIt is configurable via development_tools_config.json to work with any project's\ntest setup and coverage configuration.\n\nUsage:\n    python tests/run_test_coverage.py [--output-file]\n\"\"\"\n\nimport argparse\nimport configparser\nimport json\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nfrom core.time_utilities import now_timestamp_full, now_timestamp_filename\n\n# Add project root to path for core module imports\n# Script is at: development_tools/tests/generate_test_coverage.py\n# So we need to go up 3 levels: tests -> development_tools -> project_root\nproject_root = Path(__file__).parent.parent.parent\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\n\nfrom core.logger import get_component_logger\n\n# Import config module (absolute import for portability)\ntry:\n    from development_tools import config\nexcept ImportError:\n    # Fallback for when run as script\n    # Already have project_root from above, just ensure it's in path\n    if str(project_root) not in sys.path:\n        sys.path.insert(0, str(project_root))\n    from development_tools import config\n\n# Import decomposed coverage analysis and report generation tools\ntry:\n    from development_tools.tests.analyze_test_coverage import TestCoverageAnalyzer\n    from development_tools.tests.generate_test_coverage_report import (\n        TestCoverageReportGenerator,\n    )\nexcept ImportError:\n    # Fallback for relative imports\n    try:\n        from .analyze_test_coverage import TestCoverageAnalyzer\n        from .generate_test_coverage_report import TestCoverageReportGenerator\n    except ImportError:\n        TestCoverageAnalyzer = None\n        TestCoverageReportGenerator = None\n\n# Load external config on module import (safe to call multiple times)\nconfig.load_external_config()\n\nlogger = get_component_logger(\"development_tools\")\n\n\nclass CoverageMetricsRegenerator:\n    \"\"\"\n    Executes test suite with coverage collection and regenerates coverage metrics.\n\n    This class orchestrates pytest execution to run tests and collect coverage data.\n    It does NOT analyze coverage data - for analysis, use TestCoverageAnalyzer\n    from analyze_test_coverage.py.\n\n    Portable across projects via external configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        project_root: str = \".\",\n        parallel: bool = True,\n        num_workers: Optional[str] = None,\n        pytest_command: Optional[List[str]] = None,\n        coverage_config: Optional[str] = None,\n        artifact_directories: Optional[Dict[str, str]] = None,\n        maxfail: Optional[int] = None,\n        use_domain_cache: bool = True,\n    ):\n        \"\"\"\n        Initialize coverage metrics regenerator.\n\n        Args:\n            project_root: Root directory of the project\n            parallel: Whether to run tests in parallel (default: True)\n            num_workers: Number of parallel workers or \"auto\" (default: \"auto\")\n            pytest_command: Optional pytest command as list (e.g., ['python', '-m', 'pytest']).\n                           If None, loads from config or uses default.\n            coverage_config: Optional path to coverage config file (e.g., 'coverage.ini').\n                            If None, loads from config or uses default.\n            artifact_directories: Optional dict with keys: html_output, archive, logs, dev_tools_html.\n                                If None, loads from config or uses defaults.\n            maxfail: Optional maximum number of test failures before stopping (default: 10).\n                    If None, loads from config or uses default of 10.\n            use_domain_cache: Whether to use test-file and dev tools coverage caching (default: True).\n                              When enabled, only runs test files for changed domains and merges cached data.\n                              Disable with --no-domain-cache flag.\n        \"\"\"\n        self.project_root = Path(project_root).resolve()\n\n        # Load coverage configuration from external config\n        coverage_config_data = config.get_external_value(\"coverage\", {})\n\n        # Get maxfail threshold (from parameter, config, or default)\n        if maxfail is not None:\n            self.maxfail = maxfail\n        else:\n            self.maxfail = coverage_config_data.get(\"maxfail\", 10)\n\n        # Pytest command (from parameter, config, or default)\n        if pytest_command is not None:\n            self.pytest_command = pytest_command\n        else:\n            config_pytest = coverage_config_data.get(\"pytest_command\", [])\n            if config_pytest:\n                self.pytest_command = (\n                    config_pytest\n                    if isinstance(config_pytest, list)\n                    else [config_pytest]\n                )\n            else:\n                # Default: use sys.executable with pytest module\n                self.pytest_command = [sys.executable, \"-m\", \"pytest\"]\n\n        # Pytest base arguments (from config or default)\n        # Replace --maxfail in base args if present, otherwise add it\n        base_args = coverage_config_data.get(\n            \"pytest_base_args\",\n            [\"--cov-report=term-missing\", \"--tb=line\", \"-q\", \"--maxfail=10\"],\n        )\n        # Update maxfail in base args if it exists, otherwise add it\n        self.pytest_base_args = []\n        maxfail_added = False\n        for arg in base_args:\n            if arg.startswith(\"--maxfail=\"):\n                self.pytest_base_args.append(f\"--maxfail={self.maxfail}\")\n                maxfail_added = True\n            else:\n                self.pytest_base_args.append(arg)\n        if not maxfail_added:\n            self.pytest_base_args.append(f\"--maxfail={self.maxfail}\")\n\n        # Test directory (from config or default)\n        self.test_directory = coverage_config_data.get(\"test_directory\", \"tests/\")\n\n        # Coverage config path (from parameter, config, or default)\n        if coverage_config is not None:\n            self.coverage_config_path = self.project_root / coverage_config\n        else:\n            # Check for coverage.ini in development_tools/tests first (new location), then root (legacy)\n            config_path = coverage_config_data.get(\n                \"coverage_config\", \"development_tools/tests/coverage.ini\"\n            )\n            self.coverage_config_path = self.project_root / config_path\n            if not self.coverage_config_path.exists():\n                # Fall back to root location for backward compatibility\n                self.coverage_config_path = self.project_root / \"coverage.ini\"\n\n        # Artifact directories (from parameter, config, or defaults)\n        if artifact_directories is not None:\n            self.artifact_dirs = artifact_directories\n        else:\n            config_artifacts = coverage_config_data.get(\"artifact_directories\", {})\n            if config_artifacts:\n                self.artifact_dirs = config_artifacts\n            else:\n                # Generic defaults\n                self.artifact_dirs = {\n                    \"html_output\": \"htmlcov\",\n                    \"archive\": \"development_tools/reports/archive/coverage_artifacts\",\n                    \"logs\": \"development_tools/tests/logs\",\n                    \"dev_tools_html\": None,  # Disabled - no longer generating dev tools HTML\n                }\n\n        # Set up paths from artifact directories\n        self.coverage_data_file: Path = self.project_root / \".coverage\"\n        self.coverage_html_dir: Path = self.project_root / self.artifact_dirs.get(\n            \"html_output\", \"htmlcov\"\n        )\n        self.coverage_logs_dir: Path = self.project_root / self.artifact_dirs.get(\n            \"logs\", \"development_tools/tests/logs\"\n        )\n        self.archive_root: Path = self.project_root / self.artifact_dirs.get(\n            \"archive\", \"development_tools/reports/archive/coverage_artifacts\"\n        )\n\n        # Dev tools specific coverage paths\n        self.dev_tools_coverage_config_path: Path = (\n            self.project_root / \"development_tools\" / \"tests\" / \"coverage_dev_tools.ini\"\n        )\n        self.dev_tools_coverage_data_file: Path = (\n            self.project_root / \"development_tools\" / \"tests\" / \".coverage_dev_tools\"\n        )\n        # Store coverage_dev_tools.json in development_tools/tests/jsons/\n        jsons_dir = self.project_root / \"development_tools\" / \"tests\" / \"jsons\"\n        jsons_dir.mkdir(parents=True, exist_ok=True)\n        self.dev_tools_coverage_json: Path = jsons_dir / \"coverage_dev_tools.json\"\n        self.dev_tools_coverage_html_dir: Optional[Path] = (\n            None  # Disabled - no longer generating dev tools HTML\n        )\n\n        self.pytest_stdout_log: Optional[Path] = None\n        self.pytest_stderr_log: Optional[Path] = None\n        self.archived_directories: List[Dict[str, str]] = []\n        self.command_logs: List[Path] = []\n        self.parallel = parallel\n        self.num_workers = (\n            num_workers or \"auto\"\n        )  # \"auto\" lets pytest-xdist decide, or specify a number\n        self._configure_coverage_paths()\n        self._migrate_legacy_logs()\n        self.coverage_logs_dir.mkdir(parents=True, exist_ok=True)\n\n        # Core modules to track coverage for\n        # Import constants from shared.constants\n        from development_tools.shared.constants import CORE_MODULES\n\n        self.core_modules = list(CORE_MODULES)\n\n        # Test-file-based caching (optional)\n        self.use_domain_cache = use_domain_cache  # Keep name for backward compatibility\n        self.test_file_cache = None\n        self.domain_mapper = None\n        self.dev_tools_cache = None\n        if self.use_domain_cache:\n            try:\n                from development_tools.tests.test_file_coverage_cache import (\n                    TestFileCoverageCache,\n                )\n                from development_tools.tests.domain_mapper import DomainMapper\n\n                self.test_file_cache = TestFileCoverageCache(self.project_root)\n                self.domain_mapper = DomainMapper(self.project_root)\n                if logger:\n                    logger.debug(\"Test-file-based coverage caching enabled\")\n            except ImportError as e:\n                if logger:\n                    logger.warning(\n                        f\"Failed to import test-file cache modules: {e}. Test-file caching disabled.\"\n                    )\n                self.use_domain_cache = False\n                self.test_file_cache = None\n                self.domain_mapper = None\n\n        if self.use_domain_cache:\n            try:\n                from development_tools.tests.dev_tools_coverage_cache import (\n                    DevToolsCoverageCache,\n                )\n\n                self.dev_tools_cache = DevToolsCoverageCache(self.project_root)\n                if logger:\n                    logger.debug(\"Dev tools coverage caching enabled\")\n            except ImportError as e:\n                if logger:\n                    logger.warning(\n                        f\"Failed to import dev tools coverage cache: {e}. Dev tools caching disabled.\"\n                    )\n                self.dev_tools_cache = None\n\n        # Initialize analyzer and report generator\n        # Try to import if not already available (handles cases where imports failed at module level)\n        analyzer_class = TestCoverageAnalyzer\n        if analyzer_class is None:\n            try:\n                from development_tools.tests.analyze_test_coverage import (\n                    TestCoverageAnalyzer as AnalyzerClass,\n                )\n\n                analyzer_class = AnalyzerClass\n            except ImportError:\n                try:\n                    from .analyze_test_coverage import (\n                        TestCoverageAnalyzer as AnalyzerClass,\n                    )\n\n                    analyzer_class = AnalyzerClass\n                except ImportError:\n                    analyzer_class = None\n\n        if analyzer_class is not None:\n            self.analyzer = analyzer_class(str(self.project_root))\n        else:\n            self.analyzer = None\n            if logger:\n                logger.warning(\n                    \"TestCoverageAnalyzer not available - coverage parsing may fail\"\n                )\n\n        report_generator_class = TestCoverageReportGenerator\n        if report_generator_class is None:\n            try:\n                from development_tools.tests.generate_test_coverage_report import (\n                    TestCoverageReportGenerator as ReportGeneratorClass,\n                )\n\n                report_generator_class = ReportGeneratorClass\n            except ImportError:\n                try:\n                    from .generate_test_coverage_report import (\n                        TestCoverageReportGenerator as ReportGeneratorClass,\n                    )\n\n                    report_generator_class = ReportGeneratorClass\n                except ImportError:\n                    report_generator_class = None\n\n        if report_generator_class is not None:\n            artifact_dirs = {\n                \"html_output\": str(\n                    self.coverage_html_dir.relative_to(self.project_root)\n                ),\n                \"archive\": str(self.archive_root.relative_to(self.project_root)),\n                \"logs\": str(self.coverage_logs_dir.relative_to(self.project_root)),\n            }\n            self.report_generator = report_generator_class(\n                project_root=str(self.project_root),\n                coverage_config=(\n                    str(self.coverage_config_path.relative_to(self.project_root))\n                    if self.coverage_config_path.exists()\n                    else None\n                ),\n                artifact_directories=artifact_dirs,\n            )\n            # Update coverage_data_file path in report generator to match ours\n            self.report_generator.coverage_data_file = self.coverage_data_file\n        else:\n            self.report_generator = None\n            if logger:\n                logger.warning(\n                    \"TestCoverageReportGenerator not available - report generation may fail\"\n                )\n\n    def _configure_coverage_paths(self) -> None:\n        \"\"\"Load coverage configuration paths from coverage.ini (if it exists and specifies paths).\"\"\"\n        if not self.coverage_config_path.exists():\n            # Fall back to defaults (already set in __init__)\n            return\n\n        coverage_ini = configparser.ConfigParser()\n        coverage_ini.read(self.coverage_config_path)\n\n        # Only override if coverage.ini explicitly specifies paths\n        # data_file paths in coverage.ini are relative to project root (where pytest runs from)\n        data_file = coverage_ini.get(\"run\", \"data_file\", fallback=\"\").strip()\n        if data_file:\n            # Resolve relative to project root, not config file location\n            self.coverage_data_file = (self.project_root / data_file).resolve()\n\n        html_directory = coverage_ini.get(\"html\", \"directory\", fallback=\"\").strip()\n        if html_directory:\n            # HTML directory paths are also relative to project root\n            self.coverage_html_dir = (self.project_root / html_directory).resolve()\n\n        # Ensure parent directories exist when we later write artefacts\n        self.coverage_data_file.parent.mkdir(parents=True, exist_ok=True)\n        self.coverage_html_dir.parent.mkdir(parents=True, exist_ok=True)\n\n    def _ensure_python_path_in_env(self, env: Dict[str, str]) -> Dict[str, str]:\n        \"\"\"Ensure PATH includes Python executable's directory for Windows DLL resolution.\n\n        On Windows, subprocesses may fail with STATUS_DLL_NOT_FOUND (0xC0000135) if PATH\n        doesn't include the Python executable's directory. This helper ensures PATH is\n        set correctly for subprocess execution.\n\n        Args:\n            env: Environment dictionary (typically from os.environ.copy())\n\n        Returns:\n            Modified environment dictionary with PATH updated if needed\n        \"\"\"\n        if sys.platform == \"win32\":\n            python_exe = Path(sys.executable)\n            python_dir = str(python_exe.parent)\n            current_path = env.get(\"PATH\", \"\")\n\n            # Add Python directory to PATH if not already present\n            if python_dir not in current_path:\n                # Prepend to ensure Python DLLs are found first\n                env[\"PATH\"] = (\n                    f\"{python_dir};{current_path}\" if current_path else python_dir\n                )\n\n        return env\n\n    def _migrate_legacy_logs(self) -> None:\n        \"\"\"Move legacy coverage logs from the old location into the new directory.\"\"\"\n        legacy_dir = self.project_root / \"logs\" / \"coverage_regeneration\"\n        if (\n            not legacy_dir.exists()\n            or legacy_dir.resolve() == self.coverage_logs_dir.resolve()\n        ):\n            return\n\n        self.coverage_logs_dir.mkdir(parents=True, exist_ok=True)\n        for item in legacy_dir.iterdir():\n            destination = self.coverage_logs_dir / item.name\n            try:\n                if destination.exists():\n                    # Preserve existing new-format logs by appending timestamp suffix\n                    suffix = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n                    destination = (\n                        self.coverage_logs_dir\n                        / f\"{destination.stem}_{suffix}{destination.suffix}\"\n                    )\n                shutil.move(str(item), str(destination))\n            except Exception as exc:\n                if logger:\n                    logger.warning(f\"Failed to migrate legacy log {item}: {exc}\")\n\n        try:\n            legacy_dir.rmdir()\n        except OSError:\n            # Directory not empty (maybe concurrent process); leave it alone\n            pass\n\n    def _merge_coverage_json(\n        self, coverage_json_1: Dict[str, Any], coverage_json_2: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Merge two coverage JSON dictionaries.\n\n        Args:\n            coverage_json_1: First coverage JSON (from cache or fresh run)\n            coverage_json_2: Second coverage JSON (from cache or fresh run)\n\n        Returns:\n            Merged coverage JSON with combined files and recalculated totals\n        \"\"\"\n        # NOTE:\n        # Coverage JSON contains per-file executed/missing line lists.\n        # For selective runs, we must UNION executed lines for unchanged domains,\n        # otherwise coverage will drop (because fewer tests ran in the selective run).\n        #\n        # For files belonging to changed domains, prefer fresh coverage to avoid mixing\n        # potentially stale line data.\n        merged = {\n            \"files\": {},\n            \"totals\": {\n                \"num_statements\": 0,\n                \"covered_lines\": 0,\n                \"missing_lines\": 0,\n                \"percent_covered\": 0.0,\n            },\n        }\n\n        # Validate input structures\n        if not isinstance(coverage_json_1, dict) or not isinstance(\n            coverage_json_2, dict\n        ):\n            if logger:\n                logger.warning(\n                    \"Invalid coverage JSON structure in merge - one or both inputs are not dictionaries\"\n                )\n            return merged\n\n        # Merge files from both coverage JSONs\n        files_1 = coverage_json_1.get(\"files\", {})\n        files_2 = coverage_json_2.get(\"files\", {})\n\n        if not isinstance(files_1, dict) or not isinstance(files_2, dict):\n            if logger:\n                logger.warning(\n                    \"Invalid 'files' structure in coverage JSON - expected dictionaries\"\n                )\n            return merged\n\n        # Add all files from first JSON\n        files_1_count = 0\n        for file_path, file_data in files_1.items():\n            if isinstance(file_data, dict):\n                merged[\"files\"][file_path] = file_data.copy()\n                files_1_count += 1\n\n        # Add or merge files from second JSON\n        duplicate_count = 0\n        files_2_unique_count = 0\n        for file_path, file_data in files_2.items():\n            if not isinstance(file_data, dict):\n                continue\n            if file_path in merged[\"files\"]:\n                duplicate_count += 1\n                existing = merged[\"files\"][file_path]\n                if not isinstance(existing, dict):\n                    merged[\"files\"][file_path] = file_data.copy()\n                    continue\n\n                # Determine source domain for this file (if any)\n                normalized_path = file_path.replace(\"\\\\\", \"/\")\n                file_domain = self.domain_mapper.get_source_domain(normalized_path)\n\n                # If the file is in a changed domain and the statement count changed, prefer fresh\n                # (line numbers likely shifted, making union potentially misleading).\n                # Otherwise, union executed lines so coverage doesn't drop on selective runs.\n                changed_domains = getattr(self, \"_merge_changed_domains\", None)\n                summary_1 = (\n                    existing.get(\"summary\", {})\n                    if isinstance(existing.get(\"summary\"), dict)\n                    else {}\n                )\n                summary_2 = (\n                    file_data.get(\"summary\", {})\n                    if isinstance(file_data.get(\"summary\"), dict)\n                    else {}\n                )\n                ns_1 = int(summary_1.get(\"num_statements\", 0) or 0)\n                ns_2 = int(summary_2.get(\"num_statements\", 0) or 0)\n\n                if (\n                    isinstance(changed_domains, set)\n                    and file_domain in changed_domains\n                    and ns_1 != ns_2\n                ):\n                    merged[\"files\"][file_path] = file_data.copy()\n                else:\n                    existing_executed = set(existing.get(\"executed_lines\", []) or [])\n                    fresh_executed = set(file_data.get(\"executed_lines\", []) or [])\n                    executed_union = sorted(existing_executed | fresh_executed)\n\n                    existing_missing = set(existing.get(\"missing_lines\", []) or [])\n                    fresh_missing = set(file_data.get(\"missing_lines\", []) or [])\n                    # Only lines missing in BOTH runs are still missing after combining\n                    missing_intersection = sorted(existing_missing & fresh_missing)\n\n                    existing_excluded = set(existing.get(\"excluded_lines\", []) or [])\n                    fresh_excluded = set(file_data.get(\"excluded_lines\", []) or [])\n                    excluded_union = sorted(existing_excluded | fresh_excluded)\n\n                    merged_file = existing.copy()\n                    merged_file[\"executed_lines\"] = executed_union\n                    merged_file[\"missing_lines\"] = missing_intersection\n                    merged_file[\"excluded_lines\"] = excluded_union\n\n                    covered_lines = len(executed_union)\n                    missing_lines = len(missing_intersection)\n                    num_statements = max(ns_1, ns_2, covered_lines + missing_lines)\n                    excluded_lines = len(excluded_union)\n\n                    percent = (\n                        round((covered_lines / num_statements) * 100, 2)\n                        if num_statements > 0\n                        else 0.0\n                    )\n\n                    merged_file[\"summary\"] = {\n                        \"num_statements\": num_statements,\n                        \"covered_lines\": covered_lines,\n                        \"missing_lines\": missing_lines,\n                        \"excluded_lines\": excluded_lines,\n                        \"percent_covered\": percent,\n                        \"percent_covered_display\": f\"{percent:.2f}\",\n                    }\n\n                    merged[\"files\"][file_path] = merged_file\n            else:\n                merged[\"files\"][file_path] = file_data.copy()\n                files_2_unique_count += 1\n\n        if logger:\n            logger.info(\n                f\"Merge details: {files_1_count} files from first JSON, {files_2_unique_count} unique files from second JSON, \"\n                f\"{duplicate_count} duplicates (line-union for unchanged domains; fresh for changed domains)\"\n            )\n\n        # Recalculate totals from merged files\n        total_statements = 0\n        total_covered = 0\n        total_missing = 0\n\n        for file_path, file_data in merged[\"files\"].items():\n            if not isinstance(file_data, dict):\n                continue\n            summary = file_data.get(\"summary\", {})\n            if isinstance(summary, dict):\n                total_statements += summary.get(\"num_statements\", 0)\n                total_covered += summary.get(\"covered_lines\", 0)\n                total_missing += summary.get(\"missing_lines\", 0)\n\n        merged[\"totals\"][\"num_statements\"] = total_statements\n        merged[\"totals\"][\"covered_lines\"] = total_covered\n        merged[\"totals\"][\"missing_lines\"] = total_missing\n\n        if total_statements > 0:\n            merged[\"totals\"][\"percent_covered\"] = round(\n                (total_covered / total_statements) * 100, 2\n            )\n\n        return merged\n\n    def run_coverage_analysis(self) -> Dict[str, Dict[str, any]]:\n        \"\"\"Run pytest coverage analysis and extract metrics.\"\"\"\n        if logger:\n            logger.info(\"Running pytest coverage analysis...\")\n\n        # Load coverage configuration from external config\n        coverage_config_data = config.get_external_value(\"coverage\", {})\n\n        # Test-file-based caching: check for changed domains and determine test files to run\n        changed_domains = set()\n        test_files_to_run = []\n        cached_coverage_json = None\n        merged_coverage_saved = False\n\n        if self.use_domain_cache and self.test_file_cache:\n            # Get changed domains\n            changed_domains = self.test_file_cache.get_changed_domains()\n\n            # Get test files that need to be re-run (those covering changed domains)\n            test_files_to_run = self.test_file_cache.get_test_files_to_run(\n                changed_domains\n            )\n\n            # Check for full coverage cache first (from previous full run when no domains changed)\n            full_coverage_cache = self.test_file_cache.get_full_coverage_cache()\n\n            # Get cached coverage from test files that don't need to run (for selective runs)\n            cached_test_file_coverage = self.test_file_cache.get_all_cached_coverage(\n                exclude_domains=changed_domains\n            )\n\n            if logger:\n                cache_stats = self.test_file_cache.get_cache_stats()\n                total_test_files = cache_stats[\"total_test_files\"]\n                unmapped_count = cache_stats.get(\"unmapped_test_files\", 0)\n                if changed_domains:\n                    logger.info(\n                        f\"Test-file cache: {len(changed_domains)} domain(s) changed: {sorted(changed_domains)}\"\n                    )\n                    # If we have a full coverage cache, any test file that isn't being re-run can \"use cache\"\n                    # (its coverage will come from the cached baseline during merge).\n                    full_coverage_cache_exists = bool(full_coverage_cache)\n                    can_use_cache = (\n                        (total_test_files - len(test_files_to_run))\n                        if full_coverage_cache_exists\n                        and len(test_files_to_run) < total_test_files\n                        else 0\n                    )\n                    # For full runs, test_files_to_run may include unmapped files; show the run count against itself.\n                    total_for_display = (\n                        len(test_files_to_run)\n                        if len(test_files_to_run) >= total_test_files\n                        else total_test_files\n                    )\n                    logger.info(\n                        f\"Test-file cache: {len(test_files_to_run)} of {total_for_display} test file(s) need to run, \"\n                        f\"{can_use_cache} of {total_test_files} test file(s) can use cache\"\n                    )\n                    if unmapped_count > 0:\n                        logger.warning(\n                            f\"Test-file cache: {unmapped_count} test file(s) not mapped to any domain (will be included in full runs)\"\n                        )\n                else:\n                    logger.info(\n                        f\"Test-file cache: No domains changed - {cache_stats['test_files_cached']} of {total_test_files} test file(s) can use cache\"\n                    )\n                    if unmapped_count > 0:\n                        logger.warning(\n                            f\"Test-file cache: {unmapped_count} test file(s) not mapped to any domain\"\n                        )\n                    if full_coverage_cache:\n                        logger.info(\n                            \"Test-file cache: Found full coverage cache from previous run\"\n                        )\n\n            # Use full coverage cache if available (for both no-change and selective runs)\n            # On selective runs, we'll merge this with fresh coverage from re-run tests\n            if full_coverage_cache:\n                cached_coverage_json = full_coverage_cache\n                if logger:\n                    if not changed_domains:\n                        logger.info(\n                            f\"Using full coverage cache ({len(full_coverage_cache.get('files', {}))} files)\"\n                        )\n                    else:\n                        logger.info(\n                            f\"Using full coverage cache as base for merge ({len(full_coverage_cache.get('files', {}))} files)\"\n                        )\n            # Otherwise, reconstruct cached coverage JSON from cached test file coverage (fallback)\n            elif cached_test_file_coverage:\n                cached_files = {}\n                for (\n                    test_file_path,\n                    test_file_coverage,\n                ) in cached_test_file_coverage.items():\n                    if isinstance(test_file_coverage, dict):\n                        test_file_files = test_file_coverage.get(\"files\", {})\n                        if isinstance(test_file_files, dict):\n                            # Merge files from this test file's coverage\n                            for file_path, file_data in test_file_files.items():\n                                if isinstance(file_data, dict):\n                                    # If file already exists, prefer the one with higher coverage\n                                    # (since multiple test files might cover the same source file)\n                                    if file_path in cached_files:\n                                        existing_summary = cached_files[file_path].get(\n                                            \"summary\", {}\n                                        )\n                                        new_summary = file_data.get(\"summary\", {})\n                                        if isinstance(\n                                            existing_summary, dict\n                                        ) and isinstance(new_summary, dict):\n                                            existing_covered = existing_summary.get(\n                                                \"covered_lines\", 0\n                                            )\n                                            new_covered = new_summary.get(\n                                                \"covered_lines\", 0\n                                            )\n                                            # Prefer higher coverage (more comprehensive)\n                                            if new_covered > existing_covered:\n                                                cached_files[file_path] = file_data\n                                    else:\n                                        cached_files[file_path] = file_data\n\n                if cached_files:\n                    cached_coverage_json = {\n                        \"files\": cached_files,\n                        \"totals\": {\n                            \"num_statements\": 0,\n                            \"covered_lines\": 0,\n                            \"missing_lines\": 0,\n                            \"percent_covered\": 0.0,\n                        },\n                    }\n                    # Recalculate totals\n                    total_statements = 0\n                    total_covered = 0\n                    total_missing = 0\n                    for file_path, file_data in cached_files.items():\n                        if isinstance(file_data, dict):\n                            summary = file_data.get(\"summary\", {})\n                            if isinstance(summary, dict):\n                                total_statements += summary.get(\"num_statements\", 0)\n                                total_covered += summary.get(\"covered_lines\", 0)\n                                total_missing += summary.get(\"missing_lines\", 0)\n\n                    cached_coverage_json[\"totals\"][\"num_statements\"] = total_statements\n                    cached_coverage_json[\"totals\"][\"covered_lines\"] = total_covered\n                    cached_coverage_json[\"totals\"][\"missing_lines\"] = total_missing\n                    if total_statements > 0:\n                        cached_coverage_json[\"totals\"][\"percent_covered\"] = round(\n                            (total_covered / total_statements) * 100, 2\n                        )\n\n                    if logger:\n                        logger.info(\n                            f\"Loaded cached coverage from {len(cached_test_file_coverage)} test file(s) ({len(cached_files)} source files)\"\n                        )\n\n        # Determine if we need to run tests\n        run_tests = True\n        test_filter_args = []\n\n        if self.use_domain_cache and self.test_file_cache:\n            if not test_files_to_run and cached_coverage_json:\n                # No test files need to run AND we have valid cached data - use cache only\n                run_tests = False\n                if logger:\n                    logger.info(\n                        \"All test files cached - using cached coverage data only (skipping test execution)\"\n                    )\n            elif not test_files_to_run and not cached_coverage_json:\n                # No test files need to run BUT no cached data available - must run all tests (first run or cache cleared)\n                run_tests = True\n                test_filter_args = []  # Run all tests\n                if logger:\n                    logger.info(\n                        \"No domains changed but cache is empty - running all tests to populate cache\"\n                    )\n            else:\n                # Some test files need to run - pass them to pytest\n                if logger:\n                    logger.info(\n                        f\"Running {len(test_files_to_run)} test file(s) that cover changed domain(s)\"\n                    )\n                # Convert Path objects to relative paths for pytest\n                test_filter_args = [\n                    str(tf.relative_to(self.project_root)) for tf in test_files_to_run\n                ]\n\n        # Store coverage.json in development_tools/tests/jsons/ instead of root\n        jsons_dir = self.project_root / \"development_tools\" / \"tests\" / \"jsons\"\n        jsons_dir.mkdir(parents=True, exist_ok=True)\n        coverage_output = jsons_dir / \"coverage.json\"\n\n        try:\n            self.archived_directories.clear()\n            self.command_logs.clear()\n            self.pytest_stdout_log = None\n            self.pytest_stderr_log = None\n            # Ensure directory exists\n            coverage_output.parent.mkdir(parents=True, exist_ok=True)\n            # Build coverage arguments dynamically from core modules\n            cov_args = []\n            for module in self.core_modules:\n                # Validate module name before adding to arguments\n                if not module or not module.strip():\n                    error_msg = f\"Invalid empty module name in core_modules: {self.core_modules}\"\n                    if logger:\n                        logger.error(error_msg)\n                    raise ValueError(error_msg)\n                cov_args.extend([\"--cov\", module.strip()])\n\n            # Validate no empty --cov arguments were created\n            if \"--cov\" in cov_args and cov_args.index(\"--cov\") < len(cov_args) - 1:\n                # Check if any --cov is followed by another --cov (empty argument)\n                for i in range(len(cov_args) - 1):\n                    if cov_args[i] == \"--cov\" and cov_args[i + 1] == \"--cov\":\n                        error_msg = f\"Detected empty --cov argument in command construction. cov_args: {cov_args}\"\n                        if logger:\n                            logger.error(error_msg)\n                        raise ValueError(error_msg)\n\n            # Skip test execution if all domains are unchanged (using cache only)\n            if not run_tests:\n                if logger:\n                    logger.info(\n                        \"Skipping test execution - using cached coverage data only\"\n                    )\n\n                # Load cached coverage JSON and return it\n                if cached_coverage_json:\n                    # Save cached JSON to coverage_output with timestamp metadata\n                    try:\n                        timestamp_str = now_timestamp_full()\n                        timestamp_iso = datetime.now().isoformat()\n                        cached_coverage_json[\"_metadata\"] = {\n                            \"generated_by\": \"test-file cache (no test execution) - Development Tools\",\n                            \"last_generated\": timestamp_str,\n                            \"timestamp\": timestamp_iso,\n                            \"note\": \"This file is auto-generated from cache. Do not edit manually.\",\n                        }\n                        with open(coverage_output, \"w\", encoding=\"utf-8\") as f:\n                            json.dump(cached_coverage_json, f, indent=2)\n                    except Exception as e:\n                        if logger:\n                            logger.warning(f\"Failed to save cached coverage JSON: {e}\")\n\n                    if self.analyzer and coverage_output.exists():\n                        coverage_data = self.analyzer.load_coverage_json(\n                            coverage_output\n                        )\n                        overall_coverage = self.analyzer.extract_overall_from_json(\n                            coverage_output\n                        )\n                    else:\n                        # Fallback: extract from cached JSON structure\n                        coverage_data = {}\n                        for file_path, file_data in cached_coverage_json.get(\n                            \"files\", {}\n                        ).items():\n                            summary = file_data.get(\"summary\", {})\n                            coverage_data[file_path] = {\n                                \"statements\": summary.get(\"num_statements\", 0),\n                                \"covered\": summary.get(\"covered_lines\", 0),\n                                \"missed\": summary.get(\"missing_lines\", 0),\n                                \"coverage\": summary.get(\"percent_covered\", 0.0),\n                            }\n                        overall_coverage = {\n                            \"overall_coverage\": cached_coverage_json[\"totals\"].get(\n                                \"percent_covered\", 0.0\n                            ),\n                            \"total_statements\": cached_coverage_json[\"totals\"].get(\n                                \"num_statements\", 0\n                            ),\n                            \"total_missed\": cached_coverage_json[\"totals\"].get(\n                                \"missing_lines\", 0\n                            ),\n                        }\n\n                    return {\n                        \"modules\": coverage_data,\n                        \"overall\": overall_coverage,\n                        \"coverage_collected\": True,\n                        \"from_cache\": True,\n                    }\n                else:\n                    # Fallback: return empty result\n                    if logger:\n                        logger.warning(\n                            \"No cached coverage data available - returning empty result\"\n                        )\n                    return {\n                        \"modules\": {},\n                        \"overall\": {},\n                        \"coverage_collected\": False,\n                        \"from_cache\": True,\n                    }\n\n            cmd = [\n                sys.executable,\n                \"-m\",\n                \"pytest\",\n            ]\n\n            # Add parallel execution if enabled\n            if self.parallel:\n                # Exclude no_parallel and e2e tests from parallel execution\n                # no_parallel tests run separately in serial mode\n                # e2e tests are slow and excluded from regular runs (per pytest.ini)\n                # This matches the behavior in run_tests.py to prevent flaky failures\n                cmd.extend([\"-m\", \"not (no_parallel or e2e)\"])\n                cmd.extend([\"-n\", self.num_workers])\n                # Use loadscope distribution to group tests by file/class for better isolation\n                # This reduces race conditions by keeping related tests together\n                cmd.extend([\"--dist=loadscope\"])\n                if logger:\n                    logger.info(\n                        f\"Using parallel execution with {self.num_workers} workers (loadscope distribution), excluding no_parallel tests\"\n                    )\n\n            # When running in parallel mode, we'll combine coverage later, so don't generate JSON yet\n            # When running in serial mode, generate JSON directly\n            if self.parallel:\n                # Don't generate JSON for parallel run - we'll combine coverage data files and regenerate JSON\n                cmd.extend(\n                    [\n                        *cov_args,\n                        \"--cov-report=term-missing\",\n                        f\"--cov-config={self.coverage_config_path.relative_to(self.project_root)}\",\n                        \"--tb=line\",  # Use line format for cleaner parallel output\n                        \"-q\",  # Quiet mode - reduces output noise\n                        f\"--maxfail={self.maxfail}\",\n                        # Ignore temp directories to prevent collecting tests from temp files\n                        \"--ignore=tests/data/pytest-tmp-*\",\n                        \"--ignore=tests/data/pytest-of-*\",\n                    ]\n                )\n                # Add test files or directories\n                if test_filter_args:\n                    # test_filter_args contains test file paths (from test-file cache) or test directories\n                    cmd.extend(test_filter_args)\n                else:\n                    cmd.append(\"tests/\")\n            else:\n                # Serial mode - generate JSON directly\n                cmd.extend(\n                    [\n                        *cov_args,\n                        \"--cov-report=term-missing\",\n                        f\"--cov-report=json:{coverage_output.resolve()}\",\n                        f\"--cov-config={self.coverage_config_path.relative_to(self.project_root)}\",\n                        \"--tb=line\",\n                        \"-q\",\n                        f\"--maxfail={self.maxfail}\",\n                        # Ignore temp directories to prevent collecting tests from temp files\n                        \"--ignore=tests/data/pytest-tmp-*\",\n                        \"--ignore=tests/data/pytest-of-*\",\n                    ]\n                )\n                # Add test files or directories\n                if test_filter_args:\n                    # test_filter_args contains test file paths (from test-file cache) or test directories\n                    cmd.extend(test_filter_args)\n                else:\n                    cmd.append(\"tests/\")\n\n            # Note: When using --cov-config, pytest-cov may still use the data_file from the config\n            # even if COVERAGE_FILE is set. We need to ensure the coverage files are created in the\n            # location we specify. The COVERAGE_FILE env var should override, but we'll verify after execution.\n\n            # Check for problematic environment variables\n            pytest_addopts = os.environ.get(\"PYTEST_ADDOPTS\", \"\")\n            if pytest_addopts and \"--cov\" in pytest_addopts:\n                warning_msg = f\"PYTEST_ADDOPTS contains --cov which may conflict: {pytest_addopts}\"\n                if logger:\n                    logger.warning(warning_msg)\n\n            # Use separate coverage data files for parallel and no_parallel runs, then combine them\n            # This allows us to run no_parallel tests separately and merge their coverage\n            # Only needed when parallel execution is enabled\n            # Note: coverage.ini may specify data_file location, so we need to use the same directory\n            parallel_coverage_file = None\n            no_parallel_coverage_file = None\n            if self.parallel:\n                # Use the same directory as the main coverage file (respects coverage.ini data_file setting)\n                parallel_coverage_file = (\n                    self.coverage_data_file.parent / \".coverage_parallel\"\n                )\n                no_parallel_coverage_file = (\n                    self.coverage_data_file.parent / \".coverage_no_parallel\"\n                )\n\n            env = os.environ.copy()\n            # Ensure PATH includes Python executable's directory for Windows DLL resolution\n            env = self._ensure_python_path_in_env(env)\n            if self.parallel:\n                # CRITICAL: Set COVERAGE_FILE to .coverage_parallel (not .coverage) so shard files aren't auto-combined\n                # pytest-xdist workers will create .coverage_parallel.worker0, .coverage_parallel.worker1, etc.\n                # in the same directory. If we used .coverage, pytest-cov would auto-combine them at the end.\n                # We need the shard files to remain separate so we can combine them with no_parallel coverage.\n                env[\"COVERAGE_FILE\"] = str(parallel_coverage_file.resolve())\n                # Also set COVERAGE_RCFILE to ensure workers use the same config\n                if self.coverage_config_path.exists():\n                    env[\"COVERAGE_RCFILE\"] = str(self.coverage_config_path.resolve())\n                if logger:\n                    logger.debug(\n                        f\"Set COVERAGE_FILE={env['COVERAGE_FILE']} for parallel execution (shard files will be created as .coverage_parallel.worker* in {parallel_coverage_file.parent})\"\n                    )\n            else:\n                env[\"COVERAGE_FILE\"] = str(self.coverage_data_file.resolve())\n\n            # Set unique pytest temp directory to avoid conflicts when running in parallel with dev tools coverage\n            # Use a unique identifier based on process/coverage type to ensure isolation\n            import uuid\n\n            unique_id = f\"main_{uuid.uuid4().hex[:8]}\"\n            pytest_temp_base = (\n                self.project_root / \"tests\" / \"data\" / f\"pytest-tmp-{unique_id}\"\n            )\n            pytest_temp_base.mkdir(parents=True, exist_ok=True)\n            # Set PYTEST_CACHE_DIR to ensure pytest uses unique cache directory\n            env[\"PYTEST_CACHE_DIR\"] = str(pytest_temp_base / \".pytest_cache\")\n            # Also set basetemp via command line argument for tmpdir fixture\n            cmd.append(f\"--basetemp={pytest_temp_base}\")\n\n            # Log the full command for debugging (single log entry instead of truncated + full)\n            if logger:\n                logger.debug(f\"Running pytest coverage command: {' '.join(cmd)}\")\n\n            # Get timeout from config, with sensible defaults\n            # Coverage collection adds overhead, so tests take longer than normal runs\n            # Normal test runs take ~5 minutes, with coverage they may take 7-10 minutes\n            # Default: 12 minutes (720 seconds) to allow for coverage overhead and system variations\n            # Configurable via development_tools_config.json: {\"coverage\": {\"pytest_timeout\": 720}}\n            pytest_timeout = coverage_config_data.get(\n                \"pytest_timeout\", 720\n            )  # 12 minutes default\n            if logger:\n                logger.info(f\"Pytest timeout set to {pytest_timeout // 60} minutes\")\n\n            # Create log files BEFORE running subprocess so we can see output even if it hangs\n            # Only create stdout log - user doesn't use stderr logs\n            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n            self.coverage_logs_dir.mkdir(parents=True, exist_ok=True)\n\n            # Rotate old log files before creating new ones (keep 1 current + 7 archived = 8 total)\n            self._rotate_log_files(\"pytest_parallel_stdout\", max_versions=8)\n\n            stdout_log_path = (\n                self.coverage_logs_dir / f\"pytest_parallel_stdout_{timestamp}.log\"\n            )\n            self.pytest_stdout_log = (\n                stdout_log_path  # Keep variable name for backward compatibility\n            )\n            self.pytest_stderr_log = None  # No longer creating stderr logs\n\n            # Redirect output directly to files instead of capturing to avoid buffering/deadlock issues\n            # This allows us to see progress even if the subprocess hangs\n            # Capture stderr to stdout since we're not creating separate stderr logs\n            try:\n                with open(\n                    stdout_log_path, \"w\", encoding=\"utf-8\", buffering=1\n                ) as stdout_file:\n                    result = subprocess.run(\n                        cmd,\n                        stdout=stdout_file,\n                        stderr=subprocess.STDOUT,  # Merge stderr into stdout\n                        text=True,\n                        cwd=self.project_root,\n                        env=env,\n                        timeout=pytest_timeout,\n                    )\n                # Read the captured output for processing\n                result.stdout = stdout_log_path.read_text(\n                    encoding=\"utf-8\", errors=\"ignore\"\n                )\n                result.stderr = \"\"  # Stderr was merged into stdout\n            except subprocess.TimeoutExpired:\n                # Pytest hung or took too long\n                # Read log files to check for progress (they should exist since we created them before running)\n                progress_indicator = False\n                stdout_content = \"\"\n                stderr_content = \"\"\n                if self.pytest_stdout_log and self.pytest_stdout_log.exists():\n                    try:\n                        stdout_content = self.pytest_stdout_log.read_text(\n                            encoding=\"utf-8\", errors=\"ignore\"\n                        )\n                        # Check for progress indicators (test output, percentages, etc.)\n                        progress_indicator = bool(\n                            \"passed\" in stdout_content.lower()\n                            or \"failed\" in stdout_content.lower()\n                            or \"[  \" in stdout_content\n                            or \"%\" in stdout_content\n                            or \"TOTAL\" in stdout_content\n                            or \"bringing up nodes\" in stdout_content.lower()\n                        )\n                    except Exception as e:\n                        if logger:\n                            logger.debug(f\"Failed to read stdout log: {e}\")\n\n                # Stderr is merged into stdout, so we don't need to read separate stderr log\n                stderr_content = \"\"\n\n                if logger:\n                    if progress_indicator:\n                        logger.warning(\n                            f\"Pytest timed out after {pytest_timeout // 60} minutes, but tests were making progress\"\n                        )\n                        logger.warning(\n                            \"Tests may be running slower than expected. Consider increasing timeout in config:\"\n                        )\n                        logger.warning(\n                            '  development_tools_config.json: {\"coverage\": {\"pytest_timeout\": <seconds>}}'\n                        )\n                    else:\n                        logger.error(\n                            f\"Pytest timed out after {pytest_timeout // 60} minutes - tests may have hung or deadlocked\"\n                        )\n                        logger.error(\"This could indicate:\")\n                        logger.error(\n                            \"  - A deadlock in parallel execution (pytest-xdist)\"\n                        )\n                        logger.error(\"  - A test that hangs indefinitely\")\n                        logger.error(\n                            \"  - Resource contention (file locks, network, etc.)\"\n                        )\n                        logger.error(\"  - System resource exhaustion\")\n                        logger.error(\n                            \"Consider running with --no-parallel to isolate the issue\"\n                        )\n                    logger.info(\n                        f'Timeout is configurable via development_tools_config.json: {{\"coverage\": {{\"pytest_timeout\": <seconds>}}}}'\n                    )\n                    if stdout_content:\n                        logger.info(\n                            f\"Last stdout output (last 500 chars): {stdout_content[-500:]}\"\n                        )\n                    if stderr_content:\n                        logger.info(\n                            f\"Last stderr output (last 500 chars): {stderr_content[-500:]}\"\n                        )\n                # Create a mock result to indicate timeout, but include any captured output\n                result = subprocess.CompletedProcess(\n                    cmd,\n                    returncode=1,\n                    stdout=stdout_content,\n                    stderr=stderr_content\n                    or f\"Pytest timed out after {pytest_timeout // 60} minutes\",\n                )\n\n            # Log if the command completed too quickly (suspicious)\n            if result.returncode is not None and logger:\n                if not result.stdout or len(result.stdout) < 100:\n                    logger.warning(\n                        f\"Pytest completed very quickly with minimal output - may not have run tests properly\"\n                    )\n                    if result.stderr:\n                        logger.warning(f\"Pytest stderr: {result.stderr[:500]}\")\n\n            # Log files were already created and written to above\n            if logger and self.pytest_stdout_log and self.pytest_stdout_log.exists():\n                logger.info(f\"Saved pytest output to {self.pytest_stdout_log}\")\n\n            # Check if pytest actually ran by looking at log files (pytest with -q may not output to stdout)\n            # Also check stdout/stderr for output indicators\n            pytest_ran = False\n            log_content = \"\"\n            if self.pytest_stdout_log and self.pytest_stdout_log.exists():\n                log_content = self.pytest_stdout_log.read_text(\n                    encoding=\"utf-8\", errors=\"ignore\"\n                )\n                pytest_ran = bool(\n                    \"TOTAL\" in log_content\n                    or \"passed\" in log_content.lower()\n                    or \"failed\" in log_content.lower()\n                    or \"[  \" in log_content\n                    or \"coverage:\" in log_content.lower()\n                    or \"ERROR\" in log_content\n                    or \"FAILED\" in log_content\n                )\n\n            # Also check stdout/stderr if available\n            if not pytest_ran and result.stdout:\n                stdout_check = bool(\n                    \"TOTAL\" in result.stdout\n                    or \"passed\" in result.stdout.lower()\n                    or \"[  \" in result.stdout\n                    or \"coverage:\" in result.stdout.lower()\n                )\n                if stdout_check:\n                    pytest_ran = True\n                    log_content = result.stdout\n\n            # If we still don't have log content but pytest ran (based on return code or log file existence), use stdout\n            if not log_content and (pytest_ran or result.returncode is not None):\n                log_content = result.stdout or \"\"\n\n            # Parse test results from log file if available, otherwise from stdout\n            test_output = log_content if log_content else (result.stdout or \"\")\n            test_results = self._parse_pytest_test_results(test_output)\n\n            # Parse coverage from log file if available, otherwise from stdout\n            coverage_output_text = log_content if log_content else (result.stdout or \"\")\n            if self.analyzer:\n                coverage_data = self.analyzer.parse_coverage_output(\n                    coverage_output_text\n                )\n            else:\n                coverage_data = {}\n            # Only use existing coverage.json as fallback if pytest actually ran but didn't produce output\n            coverage_collected = bool(coverage_data)\n\n            if not coverage_data and coverage_output.exists() and pytest_ran:\n                if self.analyzer:\n                    coverage_data = self.analyzer.load_coverage_json(coverage_output)\n                else:\n                    coverage_data = {}\n                coverage_collected = bool(coverage_data)\n            elif not pytest_ran and logger:\n                logger.warning(\n                    \"Pytest appears not to have run - no test output detected in stdout or log files\"\n                )\n                if result.stderr:\n                    logger.warning(f\"Pytest stderr: {result.stderr[:500]}\")\n                if self.pytest_stderr_log and self.pytest_stderr_log.exists():\n                    stderr_content = self.pytest_stderr_log.read_text(\n                        encoding=\"utf-8\", errors=\"ignore\"\n                    )\n                    if stderr_content:\n                        logger.warning(\n                            f\"Pytest stderr from log (first 500 chars): {stderr_content[:500]}\"\n                        )\n\n            if self.analyzer:\n                overall_coverage = self.analyzer.extract_overall_coverage(\n                    coverage_output_text\n                )\n            else:\n                overall_coverage = {}\n            # Always try to load from JSON file if it exists and pytest ran, as it's more accurate\n            # Note: In parallel mode, we don't generate JSON here - we'll combine coverage data files and regenerate JSON later\n            # So this code will only execute in serial mode or if coverage.json exists from a previous run\n            fresh_coverage_json = None\n            if coverage_output.exists() and pytest_ran and not self.parallel:\n                # Load fresh coverage JSON\n                try:\n                    with open(coverage_output, \"r\", encoding=\"utf-8\") as f:\n                        fresh_coverage_json = json.load(f)\n\n                    # Validate structure - ensure it has 'files' key\n                    if fresh_coverage_json and not isinstance(\n                        fresh_coverage_json.get(\"files\"), dict\n                    ):\n                        if logger:\n                            logger.warning(\n                                f\"Fresh coverage JSON has invalid structure - missing or invalid 'files' key\"\n                            )\n                        fresh_coverage_json = None\n                    elif fresh_coverage_json:\n                        # Add timestamp metadata\n                        timestamp_str = now_timestamp_full()\n                        timestamp_iso = datetime.now().isoformat()\n                        fresh_coverage_json[\"_metadata\"] = {\n                            \"generated_by\": \"pytest-cov --cov-report=json - Development Tools\",\n                            \"last_generated\": timestamp_str,\n                            \"timestamp\": timestamp_iso,\n                            \"note\": \"This file is auto-generated. Do not edit manually.\",\n                        }\n                        # Save with metadata\n                        try:\n                            with open(coverage_output, \"w\", encoding=\"utf-8\") as f:\n                                json.dump(fresh_coverage_json, f, indent=2)\n                        except Exception as e:\n                            if logger:\n                                logger.debug(\n                                    f\"Failed to add metadata to coverage.json: {e}\"\n                                )\n                        if logger:\n                            file_count = len(fresh_coverage_json.get(\"files\", {}))\n                            logger.debug(\n                                f\"Loaded fresh coverage JSON with {file_count} files\"\n                            )\n                except Exception as e:\n                    if logger:\n                        logger.warning(f\"Failed to load fresh coverage JSON: {e}\")\n                    fresh_coverage_json = None\n\n                if self.analyzer and fresh_coverage_json:\n                    json_data = self.analyzer.load_coverage_json(coverage_output)\n                else:\n                    json_data = {}\n                if json_data:\n                    # Recalculate overall from JSON data\n                    total_statements = sum(\n                        f.get(\"statements\", 0) for f in json_data.values()\n                    )\n                    total_covered = sum(f.get(\"covered\", 0) for f in json_data.values())\n                    if total_statements > 0:\n                        overall_coverage[\"overall_coverage\"] = round(\n                            (total_covered / total_statements) * 100, 1\n                        )\n                        overall_coverage[\"total_statements\"] = total_statements\n                        overall_coverage[\"total_missed\"] = (\n                            total_statements - total_covered\n                        )\n                    if not coverage_data:\n                        coverage_data = json_data\n            elif (\n                not overall_coverage.get(\"overall_coverage\")\n                and coverage_output.exists()\n                and pytest_ran\n                and not self.parallel\n            ):\n                if self.analyzer:\n                    overall_coverage = self.analyzer.extract_overall_from_json(\n                        coverage_output\n                    )\n                    coverage_collected = bool(overall_coverage.get(\"overall_coverage\"))\n\n            # Test-file-based caching: merge cached coverage with fresh coverage\n            # Fresh coverage is from test files that were re-run (covering changed domains)\n            # Cached coverage is from test files that didn't need to run\n            if (\n                self.use_domain_cache\n                and self.test_file_cache\n                and cached_coverage_json\n                and fresh_coverage_json\n            ):\n                # Since we run all tests when domains change, fresh_coverage_json has full coverage\n                # We should merge it with cached coverage, but fresh takes precedence (it's from a full run)\n                cached_files_count = len(cached_coverage_json.get(\"files\", {}))\n                fresh_files_count = len(fresh_coverage_json.get(\"files\", {}))\n                if logger:\n                    logger.info(\n                        f\"Merging coverage: {cached_files_count} cached files + {fresh_files_count} fresh files\"\n                    )\n\n                # Merge cached and fresh coverage JSON\n                # Fresh coverage is from a full test run, so it should be accurate\n                # Provide changed domain context to merge logic\n                self._merge_changed_domains = (\n                    set(changed_domains) if isinstance(changed_domains, set) else set()\n                )\n                merged_coverage_json = self._merge_coverage_json(\n                    cached_coverage_json, fresh_coverage_json\n                )\n                self._merge_changed_domains = None\n\n                # Validate merged result\n                merged_files_count = len(merged_coverage_json.get(\"files\", {}))\n                if logger:\n                    logger.info(\n                        f\"Merged coverage contains {merged_files_count} files (expected ~{cached_files_count + fresh_files_count})\"\n                    )\n\n                # Save merged coverage JSON with timestamp metadata\n                try:\n                    timestamp_str = now_timestamp_full()\n                    timestamp_iso = datetime.now().isoformat()\n                    merged_coverage_json[\"_metadata\"] = {\n                        \"generated_by\": \"pytest-cov + test-file cache merge - Development Tools\",\n                        \"last_generated\": timestamp_str,\n                        \"timestamp\": timestamp_iso,\n                        \"note\": \"This file is auto-generated. Do not edit manually.\",\n                    }\n                    with open(coverage_output, \"w\", encoding=\"utf-8\") as f:\n                        json.dump(merged_coverage_json, f, indent=2)\n                    merged_coverage_saved = True\n                    if logger:\n                        logger.info(\"Merged cached and fresh coverage data\")\n                except Exception as e:\n                    if logger:\n                        logger.warning(f\"Failed to save merged coverage JSON: {e}\")\n\n                # Reload coverage data from merged JSON\n                if self.analyzer:\n                    coverage_data = self.analyzer.load_coverage_json(coverage_output)\n                    # Recalculate overall from merged JSON (which includes both cached and fresh)\n                    merged_totals = merged_coverage_json.get(\"totals\", {})\n                    total_statements = merged_totals.get(\"num_statements\", 0)\n                    total_covered = merged_totals.get(\"covered_lines\", 0)\n                    total_missing = merged_totals.get(\"missing_lines\", 0)\n\n                    if logger:\n                        logger.info(\n                            f\"Using merged totals (serial): {total_statements} statements, {total_covered} covered, {total_missing} missing\"\n                        )\n\n                    if total_statements > 0:\n                        overall_coverage[\"overall_coverage\"] = merged_totals.get(\n                            \"percent_covered\", 0.0\n                        )\n                        overall_coverage[\"total_statements\"] = total_statements\n                        overall_coverage[\"total_missed\"] = total_missing\n\n                    if logger:\n                        logger.info(\n                            f\"Recalculated overall coverage from merged JSON (serial): {overall_coverage.get('overall_coverage', 0):.1f}%\"\n                        )\n\n                # Update cache with coverage (serial mode)\n                # Use merged_coverage_json if we merged, otherwise use fresh_coverage_json\n                coverage_to_cache = (\n                    merged_coverage_json\n                    if (\n                        self.use_domain_cache\n                        and self.test_file_cache\n                        and cached_coverage_json\n                        and fresh_coverage_json\n                    )\n                    else fresh_coverage_json\n                )\n\n                if self.use_domain_cache and self.test_file_cache and coverage_to_cache:\n                    if not test_files_to_run:\n                        # This was a full run (no domains changed or first run)\n                        # Cache the full coverage JSON once (not per test file)\n                        self.test_file_cache.cache_full_coverage(coverage_to_cache)\n\n                        # Also update domain mappings for all test files\n                        test_root = self.project_root / \"tests\"\n                        all_test_files = [\n                            tf\n                            for tf in test_root.rglob(\"test_*.py\")\n                            if self.test_file_cache.is_valid_test_file(tf)\n                        ]\n                        for test_file in all_test_files:\n                            self.test_file_cache.update_test_file_mapping(\n                                test_file, reload_cache=False, save_cache=False\n                            )\n                        # Update and save source file mtimes for all domains (critical for change detection)\n                        if \"source_files_mtime\" not in self.test_file_cache.cache_data:\n                            self.test_file_cache.cache_data[\"source_files_mtime\"] = {}\n                        for (\n                            domain\n                        ) in (\n                            self.test_file_cache.domain_mapper.SOURCE_TO_TEST_MAPPING.keys()\n                        ):\n                            current_mtimes = (\n                                self.test_file_cache.get_source_file_mtimes(domain)\n                            )\n                            self.test_file_cache.cache_data[\"source_files_mtime\"][\n                                domain\n                            ] = current_mtimes\n                        # Save cache\n                        self.test_file_cache._save_cache()\n                        if logger:\n                            logger.info(\n                                f\"Cached full coverage JSON and updated domain mappings for {len(all_test_files)} test files\"\n                            )\n                    elif test_files_to_run:\n                        # Selective run - cache the merged coverage as the new full coverage cache\n                        # (since merged coverage now represents the complete picture)\n                        self.test_file_cache.cache_full_coverage(coverage_to_cache)\n\n                        # Update test file mappings (without coverage_data) for test files that ran\n                        for test_file in test_files_to_run:\n                            self.test_file_cache.update_test_file_mapping(\n                                test_file, reload_cache=False, save_cache=False\n                            )\n\n                        # Update and save source file mtimes for changed domains (critical for change detection)\n                        if \"source_files_mtime\" not in self.test_file_cache.cache_data:\n                            self.test_file_cache.cache_data[\"source_files_mtime\"] = {}\n                        changed_domains = self.test_file_cache.get_changed_domains()\n                        for domain in changed_domains:\n                            current_mtimes = (\n                                self.test_file_cache.get_source_file_mtimes(domain)\n                            )\n                            self.test_file_cache.cache_data[\"source_files_mtime\"][\n                                domain\n                            ] = current_mtimes\n\n                        # Save cache once (much more efficient than saving per test file)\n                        self.test_file_cache._save_cache()\n                        if logger:\n                            logger.info(\n                                f\"Cached merged coverage as full coverage cache and updated mappings for {len(test_files_to_run)} test files\"\n                            )\n\n            # Enhanced error detection and reporting\n            if result.returncode != 0:\n                # Distinguish between coverage collection failures and test failures\n                if coverage_collected:\n                    # Coverage was collected successfully, but tests failed\n                    if logger:\n                        logger.warning(\n                            \"Coverage data collected successfully, but some tests failed\"\n                        )\n\n                    # Report test failures separately\n                    if test_results[\"failed_count\"] > 0:\n                        failure_msg = f\"Test failures: {test_results['test_summary']}\"\n                        if test_results[\"random_seed\"]:\n                            failure_msg += (\n                                f\" (random seed: {test_results['random_seed']})\"\n                            )\n\n                        if logger:\n                            logger.warning(failure_msg)\n\n                            # Check if maxfail was reached\n                            if test_results.get(\"maxfail_reached\", False):\n                                logger.warning(\n                                    \"Test run was ABORTED due to reaching maximum failure limit (--maxfail)\"\n                                )\n\n                            if test_results[\"failed_tests\"]:\n                                logger.warning(\"Failed tests:\")\n                                for test_name in test_results[\"failed_tests\"]:\n                                    logger.warning(f\"  - {test_name}\")\n                            else:\n                                logger.warning(\n                                    f\"  See {self.pytest_stdout_log} for detailed test failure information\"\n                                )\n\n                    # Log skipped tests at info level\n                    if test_results[\"skipped_count\"] > 0:\n                        if logger:\n                            logger.info(\n                                f\"Test skips: {test_results['skipped_count']} test(s) skipped\"\n                            )\n                else:\n                    # Coverage collection failed\n                    error_details = []\n\n                    # Check for common error patterns\n                    stderr_lower = result.stderr.lower() if result.stderr else \"\"\n                    stdout_lower = result.stdout.lower() if result.stdout else \"\"\n\n                    if (\n                        \"unrecognized arguments\" in stderr_lower\n                        or \"unrecognized arguments\" in stdout_lower\n                    ):\n                        error_details.append(\"Unrecognized arguments error detected\")\n                        # Extract the problematic arguments from stderr\n                        if result.stderr:\n                            for line in result.stderr.split(\"\\n\"):\n                                if \"unrecognized arguments\" in line.lower():\n                                    error_details.append(\n                                        f\"  Problematic arguments: {line.strip()}\"\n                                    )\n\n                    # Check for empty --cov pattern: \"--cov --cov\" (two --cov in a row)\n                    if result.stderr and \"--cov\" in result.stderr:\n                        stderr_parts = result.stderr.split()\n                        for i in range(len(stderr_parts) - 1):\n                            if (\n                                stderr_parts[i] == \"--cov\"\n                                and stderr_parts[i + 1] == \"--cov\"\n                            ):\n                                error_details.append(\n                                    \"Detected empty --cov argument in pytest error output\"\n                                )\n                                break\n\n                    if \"error: usage\" in stderr_lower:\n                        error_details.append(\"Pytest usage/argument error detected\")\n\n                    error_msg = (\n                        f\"Coverage analysis failed (exit code {result.returncode})\"\n                    )\n                    if error_details:\n                        error_msg += \":\\n  - \" + \"\\n  - \".join(error_details)\n                    error_msg += (\n                        f\"\\n  See {self.pytest_stderr_log} for full stderr output\"\n                    )\n                    cmd_str = \" \".join(\n                        cmd\n                    )  # Convert command list to string for error message\n                    error_msg += f\"\\n  Command: {cmd_str}\"\n\n                    if logger:\n                        logger.error(error_msg)\n                    else:\n                        print(f\"ERROR: {error_msg}\")\n            else:\n                # Parallel tests passed (not all tests - no_parallel tests run separately)\n                if logger and test_results[\"test_summary\"]:\n                    if self.parallel:\n                        logger.info(\n                            f\"Parallel tests passed: {test_results['test_summary']}\"\n                        )\n                    else:\n                        logger.info(f\"All tests passed: {test_results['test_summary']}\")\n                    if test_results[\"random_seed\"]:\n                        logger.info(f\"Random seed used: {test_results['random_seed']}\")\n\n            # If parallel execution was enabled, also run no_parallel tests separately in serial mode\n            no_parallel_test_results = {\n                \"passed_count\": 0,\n                \"failed_count\": 0,\n                \"skipped_count\": 0,\n                \"test_summary\": \"\",\n            }\n            if self.parallel and pytest_ran:\n                if logger:\n                    logger.debug(\n                        \"Running no_parallel tests separately in serial mode...\"\n                    )\n\n                # Create command for no_parallel tests (serial execution, no parallel flags)\n                no_parallel_cmd = [\n                    sys.executable,\n                    \"-m\",\n                    \"pytest\",\n                    \"-m\",\n                    \"no_parallel and not e2e\",  # Only run tests marked with no_parallel, but exclude e2e\n                    *cov_args,\n                    \"--cov-report=term-missing\",\n                    # Don't write JSON for no_parallel run - we'll combine coverage data files and regenerate JSON\n                    f\"--cov-config={self.coverage_config_path.relative_to(self.project_root)}\",\n                    \"--tb=line\",\n                    \"-q\",\n                    f\"--maxfail={self.maxfail}\",\n                    # Ignore temp directories to prevent collecting tests from temp files\n                    \"--ignore=tests/data/pytest-tmp-*\",\n                    \"--ignore=tests/data/pytest-of-*\",\n                    \"tests/\",\n                ]\n\n                # Use separate coverage data file for no_parallel tests\n                # Use absolute path to ensure coverage.py uses our specified location\n                no_parallel_env = os.environ.copy()\n                # Ensure PATH includes Python executable's directory for Windows DLL resolution\n                no_parallel_env = self._ensure_python_path_in_env(no_parallel_env)\n                no_parallel_env[\"COVERAGE_FILE\"] = str(\n                    no_parallel_coverage_file.resolve()\n                )\n                # Set unique pytest temp directory to avoid conflicts when running in parallel with dev tools coverage\n                import uuid\n\n                unique_id = f\"no_parallel_{uuid.uuid4().hex[:8]}\"\n                pytest_temp_base = (\n                    self.project_root / \"tests\" / \"data\" / f\"pytest-tmp-{unique_id}\"\n                )\n                pytest_temp_base.mkdir(parents=True, exist_ok=True)\n                # Set PYTEST_CACHE_DIR to ensure pytest uses unique cache directory\n                no_parallel_env[\"PYTEST_CACHE_DIR\"] = str(\n                    pytest_temp_base / \".pytest_cache\"\n                )\n                # Also set basetemp via command line argument for tmpdir fixture\n                no_parallel_cmd.append(f\"--basetemp={pytest_temp_base}\")\n\n                # Create log file for no_parallel run (only stdout, stderr merged)\n                # Rotate old log files before creating new ones (keep 1 current + 7 archived = 8 total)\n                timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n                self._rotate_log_files(\"pytest_no_parallel_stdout\", max_versions=8)\n                no_parallel_stdout_log = (\n                    self.coverage_logs_dir\n                    / f\"pytest_no_parallel_stdout_{timestamp}.log\"\n                )\n\n                # Log the full command for debugging (consistent with parallel run)\n                if logger:\n                    logger.debug(\n                        f\"Running no_parallel tests: {' '.join(no_parallel_cmd)}\"\n                    )\n                    logger.debug(\n                        f\"No_parallel tests timeout set to {pytest_timeout // 60} minutes\"\n                    )\n\n                try:\n                    with open(\n                        no_parallel_stdout_log, \"w\", encoding=\"utf-8\", buffering=1\n                    ) as stdout_file:\n                        no_parallel_result = subprocess.run(\n                            no_parallel_cmd,\n                            stdout=stdout_file,\n                            stderr=subprocess.STDOUT,  # Merge stderr into stdout\n                            text=True,\n                            cwd=self.project_root,\n                            env=no_parallel_env,\n                            timeout=pytest_timeout,\n                        )\n                    # Read the captured output for processing\n                    no_parallel_result.stdout = no_parallel_stdout_log.read_text(\n                        encoding=\"utf-8\", errors=\"ignore\"\n                    )\n                    no_parallel_result.stderr = \"\"  # Stderr was merged into stdout\n\n                    # Log where output was saved (consistent with parallel run)\n                    if logger:\n                        logger.info(\n                            f\"Saved no_parallel pytest output to {no_parallel_stdout_log}\"\n                        )\n                except subprocess.TimeoutExpired:\n                    if logger:\n                        logger.warning(\n                            f\"No_parallel tests timed out after {pytest_timeout // 60} minutes\"\n                        )\n                    no_parallel_result = subprocess.CompletedProcess(\n                        no_parallel_cmd,\n                        returncode=1,\n                        stdout=(\n                            no_parallel_stdout_log.read_text(\n                                encoding=\"utf-8\", errors=\"ignore\"\n                            )\n                            if no_parallel_stdout_log.exists()\n                            else \"\"\n                        ),\n                        stderr=f\"No_parallel tests timed out after {pytest_timeout // 60} minutes\",\n                    )\n\n                # Parse no_parallel test results\n                # Read from log file if stdout is empty (can happen with quiet mode)\n                no_parallel_output = no_parallel_result.stdout or \"\"\n                if not no_parallel_output and no_parallel_stdout_log.exists():\n                    no_parallel_output = no_parallel_stdout_log.read_text(\n                        encoding=\"utf-8\", errors=\"ignore\"\n                    )\n\n                # Check if output looks incomplete (just dots, no summary) - indicates early termination\n                output_stripped = no_parallel_output.strip()\n                has_summary = (\n                    \"passed\" in no_parallel_output.lower()\n                    or \"failed\" in no_parallel_output.lower()\n                    or \"coverage\" in no_parallel_output.lower()\n                    or \"TOTAL\" in no_parallel_output\n                )\n                is_only_dots = output_stripped and all(\n                    c in \".sF\\n\\r\" for c in output_stripped.replace(\" \", \"\")\n                )\n\n                if is_only_dots and not has_summary and len(output_stripped) < 200:\n                    # Suspiciously short output with only dots - likely interrupted early\n                    if logger:\n                        logger.warning(\n                            f\"No_parallel test run appears to have stopped early - log contains only {len(output_stripped)} progress dots with no summary\"\n                        )\n                        logger.warning(\n                            f\"Return code: {no_parallel_result.returncode}, Output length: {len(no_parallel_output)} chars\"\n                        )\n                        if no_parallel_result.returncode != 0:\n                            # Check for specific Windows error codes\n                            # 0xC0000135 = 3221226505 (STATUS_DLL_NOT_FOUND)\n                            # 0xC0000005 = 3221225477 (STATUS_ACCESS_VIOLATION)\n                            if (\n                                no_parallel_result.returncode == 3221226505\n                            ):  # 0xC0000135 STATUS_DLL_NOT_FOUND\n                                logger.error(\n                                    f\"Pytest crashed with Windows error 0xC0000135 (STATUS_DLL_NOT_FOUND) - missing DLL required by Python or dependencies\"\n                                )\n                                logger.error(\n                                    f\"This usually indicates: missing system DLL, corrupted Python installation, or PATH issues\"\n                                )\n                                logger.error(\n                                    f\"Check log for details: {no_parallel_stdout_log}\"\n                                )\n                                logger.error(\n                                    f\"Troubleshooting: verify Python installation, check PATH, try reinstalling pytest/dependencies\"\n                                )\n                            elif (\n                                no_parallel_result.returncode == 3221225477\n                            ):  # 0xC0000005 STATUS_ACCESS_VIOLATION\n                                logger.error(\n                                    f\"Pytest crashed with Windows error 0xC0000005 (STATUS_ACCESS_VIOLATION) - memory access violation\"\n                                )\n                                logger.error(\n                                    f\"This usually indicates: memory corruption, threading issue, or library conflict (common with Qt/PyQt UI tests)\"\n                                )\n                                logger.error(\n                                    f\"Check log for details: {no_parallel_stdout_log}\"\n                                )\n                                logger.error(\n                                    f\"Troubleshooting: check for UI test issues, threading problems, or library conflicts\"\n                                )\n                            else:\n                                logger.warning(\n                                    f\"Pytest exited with non-zero code {no_parallel_result.returncode} (0x{no_parallel_result.returncode:08X}) - check log for errors: {no_parallel_stdout_log}\"\n                                )\n                            # Try to extract any error message from the output\n                            if no_parallel_output:\n                                lines = no_parallel_output.split(\"\\n\")\n                                error_lines = [\n                                    line\n                                    for line in lines\n                                    if any(\n                                        keyword in line.lower()\n                                        for keyword in [\n                                            \"error\",\n                                            \"exception\",\n                                            \"traceback\",\n                                            \"failed\",\n                                            \"interrupted\",\n                                        ]\n                                    )\n                                ]\n                                if error_lines:\n                                    logger.warning(\n                                        f\"Potential error indicators in output: {error_lines[-3:]}\"\n                                    )\n                        elif no_parallel_result.returncode == 0:\n                            logger.warning(\n                                f\"Pytest exited with code 0 but no summary found - possible interruption, crash, or incomplete output\"\n                            )\n                            logger.warning(\n                                f\"Check log file for details: {no_parallel_stdout_log}\"\n                            )\n\n                no_parallel_test_results = self._parse_pytest_test_results(\n                    no_parallel_output\n                )\n\n                # Check if output was empty - this indicates tests didn't run or output wasn't captured\n                if not no_parallel_output or (\n                    not no_parallel_test_results.get(\"total_tests\", 0)\n                    and no_parallel_result.returncode != 0\n                ):\n                    if logger:\n                        logger.warning(\n                            f\"No_parallel tests produced no output - subprocess may have failed silently\"\n                        )\n                        logger.warning(\n                            f\"Return code: {no_parallel_result.returncode} (0x{no_parallel_result.returncode:08X}), Log file exists: {no_parallel_stdout_log.exists()}, Log size: {no_parallel_stdout_log.stat().st_size if no_parallel_stdout_log.exists() else 0} bytes\"\n                        )\n                        # Check for specific Windows error codes that indicate crashes\n                        if (\n                            no_parallel_result.returncode == 3221225477\n                        ):  # 0xC0000005 STATUS_ACCESS_VIOLATION\n                            logger.error(\n                                f\"Pytest crashed with Windows error 0xC0000005 (STATUS_ACCESS_VIOLATION) - memory access violation\"\n                            )\n                            logger.error(\n                                f\"This usually indicates: memory corruption, threading issue, or library conflict (common with Qt/PyQt UI tests)\"\n                            )\n                            logger.error(\n                                f\"Check log for details: {no_parallel_stdout_log}\"\n                            )\n                            logger.error(\n                                f\"Troubleshooting: check for UI test issues, threading problems, or library conflicts\"\n                            )\n                        elif (\n                            no_parallel_result.returncode == 3221226505\n                        ):  # 0xC0000135 STATUS_DLL_NOT_FOUND\n                            logger.error(\n                                f\"Pytest crashed with Windows error 0xC0000135 (STATUS_DLL_NOT_FOUND) - missing DLL required by Python or dependencies\"\n                            )\n                            logger.error(\n                                f\"This usually indicates: missing system DLL, corrupted Python installation, or PATH issues\"\n                            )\n                            logger.error(\n                                f\"Check log for details: {no_parallel_stdout_log}\"\n                            )\n                            logger.error(\n                                f\"Troubleshooting: verify Python installation, check PATH, try reinstalling pytest/dependencies\"\n                            )\n                        if no_parallel_stdout_log.exists():\n                            log_content = no_parallel_stdout_log.read_text(\n                                encoding=\"utf-8\", errors=\"ignore\"\n                            )\n                            if log_content:\n                                logger.warning(\n                                    f\"Log file has {len(log_content)} chars but parser found no tests\"\n                                )\n                                # Try to extract crash information from log\n                                if (\n                                    \"Windows fatal exception\" in log_content\n                                    or \"access violation\" in log_content.lower()\n                                ):\n                                    logger.error(\n                                        \"Log contains Windows fatal exception - this indicates a crash, not a test failure\"\n                                    )\n                                    # Extract the test that crashed\n                                    lines = log_content.split(\"\\n\")\n                                    for i, line in enumerate(lines):\n                                        if 'File \"' in line and \"test_\" in line:\n                                            logger.error(\n                                                f\"Crash occurred in: {line.strip()}\"\n                                            )\n                                            # Show a few lines of context\n                                            for j in range(\n                                                max(0, i - 2), min(len(lines), i + 3)\n                                            ):\n                                                if j != i:\n                                                    logger.debug(\n                                                        f\"  {lines[j].strip()}\"\n                                                    )\n                                            break\n                            else:\n                                logger.warning(\n                                    f\"Log file is empty - subprocess may not have started or output was not captured\"\n                                )\n\n                # Combine test results\n                test_results[\"passed_count\"] += no_parallel_test_results.get(\n                    \"passed_count\", 0\n                )\n                test_results[\"failed_count\"] += no_parallel_test_results.get(\n                    \"failed_count\", 0\n                )\n                test_results[\"skipped_count\"] += no_parallel_test_results.get(\n                    \"skipped_count\", 0\n                )\n\n                if logger:\n                    if (\n                        no_parallel_result.returncode == 0\n                        and no_parallel_test_results.get(\"total_tests\", 0) > 0\n                    ):\n                        logger.info(\n                            f\"No_parallel tests completed: {no_parallel_test_results.get('passed_count', 0)} passed, {no_parallel_test_results.get('failed_count', 0)} failed, {no_parallel_test_results.get('skipped_count', 0)} skipped\"\n                        )\n                    elif (\n                        no_parallel_result.returncode == 0\n                        and no_parallel_test_results.get(\"total_tests\", 0) == 0\n                    ):\n                        logger.warning(\n                            f\"No_parallel tests exited with code 0 but no tests were found in output - possible issue with test collection or output capture\"\n                        )\n                    else:\n                        logger.warning(\n                            f\"No_parallel tests had failures: {no_parallel_test_results.get('failed_count', 0)} failed, {no_parallel_test_results.get('passed_count', 0)} passed, {no_parallel_test_results.get('skipped_count', 0)} skipped\"\n                        )\n\n                    # Check if maxfail was reached\n                    if no_parallel_test_results.get(\"maxfail_reached\", False):\n                        logger.warning(\n                            \"No_parallel test run was ABORTED due to reaching maximum failure limit (--maxfail)\"\n                        )\n\n                    # Log skipped tests at info level\n                    if no_parallel_test_results.get(\"skipped_count\", 0) > 0:\n                        logger.info(\n                            f\"No_parallel test skips: {no_parallel_test_results.get('skipped_count', 0)} test(s) skipped\"\n                        )\n\n                    # Log failed tests if any\n                    if no_parallel_test_results.get(\n                        \"failed_count\", 0\n                    ) > 0 and no_parallel_test_results.get(\"failed_tests\"):\n                        logger.warning(\"No_parallel failed tests:\")\n                        for test_name in no_parallel_test_results.get(\n                            \"failed_tests\", []\n                        )[\n                            :10\n                        ]:  # Limit to first 10\n                            logger.warning(f\"  - {test_name}\")\n                        if len(no_parallel_test_results.get(\"failed_tests\", [])) > 10:\n                            logger.warning(\n                                f\"  ... and {len(no_parallel_test_results.get('failed_tests', [])) - 10} more (see log for full list)\"\n                            )\n\n                # Check if coverage files were created and log their locations\n                if logger:\n                    if parallel_coverage_file:\n                        logger.debug(\n                            f\"Parallel coverage file path: {parallel_coverage_file}, exists: {parallel_coverage_file.exists()}\"\n                        )\n                        # Also check what files actually exist in that directory\n                        if parallel_coverage_file.parent.exists():\n                            all_files = list(\n                                parallel_coverage_file.parent.glob(\".coverage*\")\n                            )\n                            logger.debug(\n                                f\"All .coverage* files in {parallel_coverage_file.parent}: {[f.name for f in all_files]}\"\n                            )\n                    if no_parallel_coverage_file:\n                        logger.debug(\n                            f\"No_parallel coverage file path: {no_parallel_coverage_file}, exists: {no_parallel_coverage_file.exists()}\"\n                        )\n\n                # Combine coverage data files using coverage combine\n                # Check if coverage files exist (they may be in tests/ directory due to coverage.ini)\n                # Also check for shard files from parallel execution (e.g., .coverage.worker0, .coverage.worker1, etc.)\n                coverage_dir = self.coverage_data_file.parent\n                parallel_exists = (\n                    parallel_coverage_file and parallel_coverage_file.exists()\n                )\n                no_parallel_exists = (\n                    no_parallel_coverage_file and no_parallel_coverage_file.exists()\n                )\n\n                # Check for shard files from parallel execution (pytest-xdist creates these)\n                # When COVERAGE_FILE is set to .coverage_parallel, workers create .coverage_parallel.worker0, etc.\n                # Shard files should be in the coverage directory (where COVERAGE_FILE points)\n                parallel_shard_files = []\n                if coverage_dir.exists():\n                    # Look for shard files: .coverage_parallel.worker0, .coverage_parallel.worker1, etc.\n                    parallel_shard_files.extend(\n                        list(coverage_dir.glob(\".coverage_parallel.worker*\"))\n                    )\n                    # Also check for .coverage.worker* files (in case COVERAGE_FILE wasn't set correctly)\n                    parallel_shard_files.extend(\n                        [\n                            f\n                            for f in coverage_dir.glob(\".coverage.worker*\")\n                            if f.name not in [\".coverage\"]\n                        ]\n                    )\n                    # Log all .coverage* files found for debugging\n                    if logger:\n                        all_coverage_files = list(coverage_dir.glob(\".coverage*\"))\n                        logger.debug(\n                            f\"All .coverage* files in {coverage_dir}: {[f.name for f in all_coverage_files]}\"\n                        )\n                        # Check if .coverage exists (might have been auto-combined)\n                        coverage_file = coverage_dir / \".coverage\"\n                        if coverage_file.exists():\n                            file_size = coverage_file.stat().st_size\n                            logger.debug(\n                                f\"Found .coverage file ({file_size} bytes) - shard files may have been auto-combined\"\n                            )\n                # Filter out the main parallel coverage file (we want shard files, not the combined one)\n                parallel_shard_files = [\n                    f\n                    for f in parallel_shard_files\n                    if f.name != \".coverage_parallel\"\n                    and f.name != \".coverage_no_parallel\"\n                    and f.name != \".coverage\"\n                ]\n                if logger:\n                    logger.debug(\n                        f\"Found {len(parallel_shard_files)} shard files after filtering: {[f.name for f in parallel_shard_files]}\"\n                    )\n\n                # If no shard files found but .coverage exists, it might contain the parallel coverage\n                # Check if we should use .coverage as the parallel coverage source\n                coverage_file = coverage_dir / \".coverage\"\n                if (\n                    not parallel_shard_files\n                    and not parallel_exists\n                    and coverage_file.exists()\n                    and logger\n                ):\n                    logger.warning(\n                        f\"No shard files found and .coverage_parallel doesn't exist, but .coverage exists. \"\n                        f\"This suggests pytest-cov may have auto-combined shard files into .coverage. \"\n                        f\"Will attempt to use .coverage as parallel coverage source.\"\n                    )\n\n                # Check project root for shard files (shouldn't be there, but log if found)\n                project_root_shards = []\n                project_root_shards.extend(\n                    [f for f in self.project_root.glob(\".coverage_parallel.worker*\")]\n                )\n                project_root_shards.extend(\n                    [\n                        f\n                        for f in self.project_root.glob(\".coverage.worker*\")\n                        if f.name != \".coverage\"\n                    ]\n                )\n                if project_root_shards and logger:\n                    logger.warning(\n                        f\"Found {len(project_root_shards)} shard files in project root (should be in {coverage_dir}). \"\n                        f\"This indicates COVERAGE_FILE environment variable may not have been respected by pytest-xdist workers. \"\n                        f\"Shard files: {[f.name for f in project_root_shards[:5]]}\"\n                    )\n                    # Copy them to the correct location so they can be combined\n                    for shard_file in project_root_shards:\n                        try:\n                            dest_file = coverage_dir / shard_file.name\n                            if not dest_file.exists():\n                                shutil.copy2(shard_file, dest_file)\n                                parallel_shard_files.append(dest_file)\n                                if logger:\n                                    logger.debug(\n                                        f\"Copied misplaced shard file {shard_file.name} from project root to coverage directory\"\n                                    )\n                        except Exception as copy_error:\n                            if logger:\n                                logger.warning(\n                                    f\"Failed to copy misplaced shard file {shard_file.name}: {copy_error}\"\n                                )\n\n                if parallel_exists or no_parallel_exists or parallel_shard_files:\n                    if logger:\n                        logger.info(\n                            \"Combining coverage data from parallel and no_parallel test runs...\"\n                        )\n                        if parallel_shard_files:\n                            logger.debug(\n                                f\"Found {len(parallel_shard_files)} parallel shard files: {[f.name for f in parallel_shard_files[:5]]}\"\n                            )\n\n                    # Copy coverage files to the coverage data file directory with .coverage.* naming for combine\n                    # Coverage combine looks for .coverage.* files in the current directory\n                    # Use the same directory as the coverage data file (respects coverage.ini data_file setting)\n                    project_root_coverage_parallel = coverage_dir / \".coverage.parallel\"\n                    project_root_coverage_no_parallel = (\n                        coverage_dir / \".coverage.no_parallel\"\n                    )\n\n                    try:\n                        # Copy both coverage files to coverage directory with .coverage.* naming\n                        # Also check for shard files from parallel execution\n                        files_to_combine = []\n\n                        # Check for parallel coverage in multiple possible locations\n                        parallel_coverage_source = None\n                        if parallel_coverage_file and parallel_coverage_file.exists():\n                            # Check if file has content (not empty)\n                            file_size = parallel_coverage_file.stat().st_size\n                            if file_size > 0:\n                                parallel_coverage_source = parallel_coverage_file\n                                if logger:\n                                    logger.debug(\n                                        f\"Using {parallel_coverage_file.name} as parallel coverage source ({file_size} bytes)\"\n                                    )\n                            elif logger:\n                                logger.warning(\n                                    f\"Parallel coverage file {parallel_coverage_file.name} exists but is empty ({file_size} bytes) - shard files may not have been combined\"\n                                )\n                        elif not parallel_shard_files:\n                            # If no shard files and no .coverage_parallel, check if .coverage exists\n                            # (pytest-cov might have auto-combined shard files into .coverage if COVERAGE_FILE wasn't set)\n                            coverage_file = coverage_dir / \".coverage\"\n                            if coverage_file.exists():\n                                file_size = coverage_file.stat().st_size\n                                if file_size > 0:\n                                    parallel_coverage_source = coverage_file\n                                    if logger:\n                                        logger.info(\n                                            f\"Using .coverage as parallel coverage source (fallback - COVERAGE_FILE may not have been set correctly, {file_size} bytes)\"\n                                        )\n                                elif logger:\n                                    logger.warning(\n                                        f\".coverage exists but is empty ({file_size} bytes)\"\n                                    )\n\n                        if parallel_coverage_source:\n                            shutil.copy2(\n                                parallel_coverage_source, project_root_coverage_parallel\n                            )\n                            files_to_combine.append(project_root_coverage_parallel)\n                            if logger:\n                                logger.debug(\n                                    f\"Copied parallel coverage file from {parallel_coverage_source} to {project_root_coverage_parallel}\"\n                                )\n                        elif parallel_shard_files:\n                            # If main parallel coverage file doesn't exist (e.g., due to timeout),\n                            # but shard files exist, we can still combine using shard files\n                            if logger:\n                                logger.info(\n                                    f\"Parallel coverage file not found (may have timed out), but {len(parallel_shard_files)} shard files exist - will combine shard files\"\n                                )\n                        else:\n                            if logger:\n                                logger.warning(\n                                    f\"Parallel coverage file not found: {parallel_coverage_file}, and no shard files found\"\n                                )\n\n                        if (\n                            no_parallel_coverage_file\n                            and no_parallel_coverage_file.exists()\n                        ):\n                            shutil.copy2(\n                                no_parallel_coverage_file,\n                                project_root_coverage_no_parallel,\n                            )\n                            files_to_combine.append(project_root_coverage_no_parallel)\n                            if logger:\n                                logger.debug(\n                                    f\"Copied no_parallel coverage file from {no_parallel_coverage_file} to {project_root_coverage_no_parallel}\"\n                                )\n                        else:\n                            if logger:\n                                logger.warning(\n                                    f\"No_parallel coverage file not found: {no_parallel_coverage_file}\"\n                                )\n\n                        # Shard files should already be in the coverage directory (where COVERAGE_FILE points)\n                        # They'll be picked up automatically by coverage combine\n                        if parallel_shard_files:\n                            if logger:\n                                logger.debug(\n                                    f\"Will combine {len(parallel_shard_files)} shard files from parallel execution\"\n                                )\n\n                        # Use coverage combine to merge the coverage data files\n                        # Set COVERAGE_FILE to the final combined file location\n                        combine_env = os.environ.copy()\n                        # Ensure PATH includes Python executable's directory for Windows DLL resolution\n                        combine_env = self._ensure_python_path_in_env(combine_env)\n                        combine_env[\"COVERAGE_FILE\"] = str(\n                            self.coverage_data_file.resolve()\n                        )\n\n                        combine_cmd = [\n                            sys.executable,\n                            \"-m\",\n                            \"coverage\",\n                            \"combine\",\n                            \"--data-file\",\n                            str(self.coverage_data_file.resolve()),\n                        ]\n\n                        if logger:\n                            total_files = len(files_to_combine) + len(\n                                parallel_shard_files\n                            )\n                            logger.debug(\n                                f\"Running coverage combine from {coverage_dir} with {total_files} files to combine\"\n                            )\n                            if parallel_shard_files:\n                                logger.debug(\n                                    f\"Shard files to combine ({len(parallel_shard_files)}): {[f.name for f in parallel_shard_files[:10]]}\"\n                                )\n                            if files_to_combine:\n                                logger.debug(\n                                    f\"Copied files to combine: {[f.name for f in files_to_combine]}\"\n                                )\n\n                        # Run combine from the coverage directory so it finds the .coverage.* files\n                        combine_result = subprocess.run(\n                            combine_cmd,\n                            capture_output=True,\n                            text=True,\n                            cwd=coverage_dir,\n                            env=combine_env,\n                            timeout=60,  # Combine should be fast\n                        )\n\n                        if combine_result.returncode != 0:\n                            if logger:\n                                stderr_msg = (\n                                    combine_result.stderr or combine_result.stdout or \"\"\n                                )\n                                if \"No data to combine\" not in stderr_msg:\n                                    logger.warning(\n                                        f\"Coverage combine exited with code {combine_result.returncode}: {stderr_msg[:500]}\"\n                                    )\n                                else:\n                                    logger.info(\n                                        \"Coverage combine reported no data to combine - this may indicate coverage files weren't created properly\"\n                                    )\n                                    # Log what files exist for debugging\n                                    all_files = (\n                                        list(coverage_dir.glob(\".coverage*\"))\n                                        if coverage_dir.exists()\n                                        else []\n                                    )\n                                    if all_files:\n                                        logger.debug(\n                                            f\"Coverage files found in {coverage_dir}: {[f.name for f in all_files]}\"\n                                        )\n                        elif logger:\n                            # Log what was actually combined for debugging\n                            combine_info = []\n                            if parallel_coverage_source:\n                                combine_info.append(\n                                    f\"parallel source: {parallel_coverage_source.name}\"\n                                )\n                            if parallel_shard_files:\n                                combine_info.append(\n                                    f\"{len(parallel_shard_files)} shard files\"\n                                )\n                            if (\n                                no_parallel_coverage_file\n                                and no_parallel_coverage_file.exists()\n                            ):\n                                combine_info.append(\n                                    f\"no_parallel: {no_parallel_coverage_file.name}\"\n                                )\n                            logger.info(\n                                f\"Successfully combined coverage data from parallel and no_parallel runs ({', '.join(combine_info) if combine_info else 'no sources found'})\"\n                            )\n                            # Validate that we actually have coverage data (detect silent failures)\n                            if self.coverage_data_file.exists():\n                                file_size = self.coverage_data_file.stat().st_size\n                                if file_size == 0:\n                                    logger.warning(\n                                        \"Combined coverage file is empty - combine may have failed silently\"\n                                    )\n                                else:\n                                    logger.debug(\n                                        f\"Combined coverage file: {self.coverage_data_file.name} ({file_size} bytes)\"\n                                    )\n                            else:\n                                logger.warning(\n                                    \"Combined coverage file was not created after combine operation\"\n                                )\n\n                        # Clean up temporary .coverage.* files from project root\n                        try:\n                            if project_root_coverage_parallel.exists():\n                                project_root_coverage_parallel.unlink()\n                            if project_root_coverage_no_parallel.exists():\n                                project_root_coverage_no_parallel.unlink()\n                        except Exception:\n                            pass\n                    except Exception as combine_error:\n                        if logger:\n                            logger.warning(\n                                f\"Failed to combine coverage data: {combine_error}\"\n                            )\n                        # Clean up on error\n                        try:\n                            if project_root_coverage_parallel.exists():\n                                project_root_coverage_parallel.unlink()\n                            if project_root_coverage_no_parallel.exists():\n                                project_root_coverage_no_parallel.unlink()\n                        except Exception:\n                            pass\n\n                    # Capture file existence status BEFORE cleanup for later validation\n                    parallel_file_existed = (\n                        parallel_coverage_file.exists()\n                        if parallel_coverage_file\n                        else False\n                    )\n                    no_parallel_file_existed = (\n                        no_parallel_coverage_file.exists()\n                        if no_parallel_coverage_file\n                        else False\n                    )\n                    shard_files_count = (\n                        len(parallel_shard_files)\n                        if \"parallel_shard_files\" in locals()\n                        else 0\n                    )\n\n                    # Clean up temporary coverage files from original locations\n                    # Also clean up process-specific files (e.g., .coverage_parallel.DESKTOP-*.X.*)\n                    self._cleanup_process_specific_coverage_files(coverage_dir)\n                    try:\n                        if parallel_coverage_file and parallel_coverage_file.exists():\n                            parallel_coverage_file.unlink()\n                        if (\n                            no_parallel_coverage_file\n                            and no_parallel_coverage_file.exists()\n                        ):\n                            no_parallel_coverage_file.unlink()\n                    except Exception as cleanup_error:\n                        if logger:\n                            logger.debug(\n                                f\"Failed to cleanup temporary coverage files: {cleanup_error}\"\n                            )\n\n                    # Regenerate coverage JSON from combined data\n                    if self.coverage_data_file.exists():\n                        # Archive old coverage.json BEFORE regenerating (to keep current file in main directory)\n                        archive_dir = coverage_output.parent / \"archive\"\n                        archive_dir.mkdir(parents=True, exist_ok=True)\n                        if coverage_output.exists():\n                            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n                            archive_name = f\"coverage_{timestamp}.json\"\n                            archive_path = archive_dir / archive_name\n                            shutil.move(str(coverage_output), str(archive_path))\n                            if logger:\n                                logger.debug(\n                                    f\"Archived old coverage.json to {archive_name}\"\n                                )\n\n                            # Clean up old archives to keep only 6 versions (current + 6 archived = 7 total)\n                            from development_tools.shared.file_rotation import (\n                                FileRotator,\n                            )\n\n                            rotator = FileRotator(base_dir=str(coverage_output.parent))\n                            rotator._cleanup_old_versions(\n                                \"coverage\", max_versions=6\n                            )  # Keep 6 archived (current is separate)\n\n                        json_cmd = [\n                            sys.executable,\n                            \"-m\",\n                            \"coverage\",\n                            \"json\",\n                            \"-o\",\n                            str(coverage_output),\n                            \"--data-file\",\n                            str(self.coverage_data_file),\n                        ]\n                        try:\n                            json_result = subprocess.run(\n                                json_cmd,\n                                capture_output=True,\n                                text=True,\n                                cwd=self.project_root,\n                                timeout=120,\n                            )\n                            if json_result.returncode == 0:\n                                if logger:\n                                    logger.info(\n                                        \"Regenerated coverage.json from combined coverage data\"\n                                    )\n\n                                # Add timestamp metadata to coverage.json\n                                if coverage_output.exists():\n                                    try:\n                                        with open(\n                                            coverage_output, \"r\", encoding=\"utf-8\"\n                                        ) as f:\n                                            coverage_data_json = json.load(f)\n                                        timestamp_str = datetime.now().strftime(\n                                            \"%Y-%m-%d %H:%M:%S\"\n                                        )\n                                        timestamp_iso = datetime.now().isoformat()\n                                        coverage_data_json[\"_metadata\"] = {\n                                            \"generated_by\": \"coverage json - Development Tools\",\n                                            \"last_generated\": timestamp_str,\n                                            \"timestamp\": timestamp_iso,\n                                            \"note\": \"This file is auto-generated. Do not edit manually.\",\n                                        }\n                                        with open(\n                                            coverage_output, \"w\", encoding=\"utf-8\"\n                                        ) as f:\n                                            json.dump(coverage_data_json, f, indent=2)\n                                    except Exception as e:\n                                        if logger:\n                                            logger.debug(\n                                                f\"Failed to add metadata to coverage.json: {e}\"\n                                            )\n\n                                # Reload coverage data from the regenerated JSON\n                                if coverage_output.exists():\n                                    # Load fresh coverage JSON for merging\n                                    fresh_coverage_json_parallel = None\n                                    try:\n                                        with open(\n                                            coverage_output, \"r\", encoding=\"utf-8\"\n                                        ) as f:\n                                            fresh_coverage_json_parallel = json.load(f)\n\n                                        # Validate structure - ensure it has 'files' key\n                                        if (\n                                            fresh_coverage_json_parallel\n                                            and not isinstance(\n                                                fresh_coverage_json_parallel.get(\n                                                    \"files\"\n                                                ),\n                                                dict,\n                                            )\n                                        ):\n                                            if logger:\n                                                logger.warning(\n                                                    f\"Fresh coverage JSON (parallel) has invalid structure - missing or invalid 'files' key\"\n                                                )\n                                            fresh_coverage_json_parallel = None\n                                        elif fresh_coverage_json_parallel and logger:\n                                            file_count = len(\n                                                fresh_coverage_json_parallel.get(\n                                                    \"files\", {}\n                                                )\n                                            )\n                                            logger.debug(\n                                                f\"Loaded fresh coverage JSON (parallel) with {file_count} files\"\n                                            )\n                                    except Exception as e:\n                                        if logger:\n                                            logger.warning(\n                                                f\"Failed to load fresh coverage JSON after parallel combine: {e}\"\n                                            )\n                                        fresh_coverage_json_parallel = None\n\n                                    # Test-file-based caching: merge cached coverage with fresh coverage (parallel mode)\n                                    # Fresh coverage is from test files that were re-run (covering changed domains)\n                                    # Cached coverage is from test files that didn't need to run\n                                    # On selective runs, we MUST merge to get complete coverage\n                                    # On full runs, fresh coverage is already complete, but merging is still safe\n                                    if (\n                                        self.use_domain_cache\n                                        and self.test_file_cache\n                                        and cached_coverage_json\n                                        and fresh_coverage_json_parallel\n                                    ):\n                                        cached_files_count = len(\n                                            cached_coverage_json.get(\"files\", {})\n                                        )\n                                        fresh_files_count = len(\n                                            fresh_coverage_json_parallel.get(\n                                                \"files\", {}\n                                            )\n                                        )\n\n                                        # Determine if this was a selective or full run\n                                        # If we have cached_coverage_json and test_files_to_run, it's a selective run\n                                        # If we don't have cached_coverage_json, it's a full run (cache cleared or first run)\n                                        total_test_files = (\n                                            self.test_file_cache.get_cache_stats().get(\n                                                \"total_test_files\", 999\n                                            )\n                                        )\n                                        is_selective_run = (\n                                            bool(cached_coverage_json)\n                                            and bool(test_files_to_run)\n                                            and len(test_files_to_run)\n                                            < total_test_files\n                                        )\n\n                                        # Analyze file domains for debugging\n                                        if logger:\n                                            cached_domains = set()\n                                            fresh_domains = set()\n                                            for file_path in cached_coverage_json.get(\n                                                \"files\", {}\n                                            ).keys():\n                                                domain = self.domain_mapper.get_source_domain(\n                                                    file_path.replace(\"\\\\\", \"/\")\n                                                )\n                                                if domain:\n                                                    cached_domains.add(domain)\n                                            for (\n                                                file_path\n                                            ) in fresh_coverage_json_parallel.get(\n                                                \"files\", {}\n                                            ).keys():\n                                                domain = self.domain_mapper.get_source_domain(\n                                                    file_path.replace(\"\\\\\", \"/\")\n                                                )\n                                                if domain:\n                                                    fresh_domains.add(domain)\n                                            run_type = (\n                                                \"selective\"\n                                                if is_selective_run\n                                                else \"full\"\n                                            )\n                                            logger.info(\n                                                f\"Merging coverage (parallel, {run_type} run): {cached_files_count} cached files from domains {sorted(cached_domains)} + {fresh_files_count} fresh files from test run (domains: {sorted(fresh_domains)})\"\n                                            )\n\n                                        # Merge cached and fresh coverage JSON\n                                        # On selective runs: union executed lines for unchanged domains so coverage doesn't drop.\n                                        # For changed-domain files: prefer fresh to avoid mixing potentially stale line data.\n                                        self._merge_changed_domains = (\n                                            set(changed_domains)\n                                            if isinstance(changed_domains, set)\n                                            else set()\n                                        )\n                                        merged_coverage_json = (\n                                            self._merge_coverage_json(\n                                                cached_coverage_json,\n                                                fresh_coverage_json_parallel,\n                                            )\n                                        )\n                                        self._merge_changed_domains = None\n\n                                        # Validate merged result\n                                        merged_files_count = len(\n                                            merged_coverage_json.get(\"files\", {})\n                                        )\n                                        merged_totals = merged_coverage_json.get(\n                                            \"totals\", {}\n                                        )\n                                        cached_totals = cached_coverage_json.get(\n                                            \"totals\", {}\n                                        )\n                                        fresh_totals = fresh_coverage_json_parallel.get(\n                                            \"totals\", {}\n                                        )\n                                        if logger:\n                                            merged_domains = set()\n                                            for file_path in merged_coverage_json.get(\n                                                \"files\", {}\n                                            ).keys():\n                                                domain = self.domain_mapper.get_source_domain(\n                                                    file_path.replace(\"\\\\\", \"/\")\n                                                )\n                                                if domain:\n                                                    merged_domains.add(domain)\n                                            logger.info(\n                                                f\"Merged coverage (parallel) contains {merged_files_count} files from domains {sorted(merged_domains)} (expected ~{cached_files_count + fresh_files_count})\"\n                                            )\n                                            logger.info(\n                                                f\"Merged totals: {merged_totals.get('num_statements', 0)} statements, {merged_totals.get('covered_lines', 0)} covered, {merged_totals.get('percent_covered', 0.0):.2f}%\"\n                                            )\n                                            if is_selective_run:\n                                                logger.info(\n                                                    f\"  Cached (unchanged tests): {cached_totals.get('num_statements', 0)} statements, {cached_totals.get('covered_lines', 0)} covered, {cached_totals.get('percent_covered', 0.0):.2f}%\"\n                                                )\n                                                logger.info(\n                                                    f\"  Fresh (re-run tests): {fresh_totals.get('num_statements', 0)} statements, {fresh_totals.get('covered_lines', 0)} covered, {fresh_totals.get('percent_covered', 0.0):.2f}%\"\n                                                )\n\n                                        # Save merged coverage JSON with timestamp metadata\n                                        try:\n                                            timestamp_str = datetime.now().strftime(\n                                                \"%Y-%m-%d %H:%M:%S\"\n                                            )\n                                            timestamp_iso = datetime.now().isoformat()\n                                            merged_coverage_json[\"_metadata\"] = {\n                                                \"generated_by\": \"pytest-cov + test-file cache merge (parallel) - Development Tools\",\n                                                \"last_generated\": timestamp_str,\n                                                \"timestamp\": timestamp_iso,\n                                                \"note\": \"This file is auto-generated. Do not edit manually.\",\n                                            }\n                                            with open(\n                                                coverage_output, \"w\", encoding=\"utf-8\"\n                                            ) as f:\n                                                json.dump(\n                                                    merged_coverage_json, f, indent=2\n                                                )\n                                            merged_coverage_saved = True\n                                            if logger:\n                                                logger.info(\n                                                    \"Merged cached and fresh coverage data (parallel mode)\"\n                                                )\n                                        except Exception as e:\n                                            if logger:\n                                                logger.warning(\n                                                    f\"Failed to save merged coverage JSON: {e}\"\n                                                )\n\n                                        # Use merged JSON for analysis and caching\n                                        fresh_coverage_json_parallel = (\n                                            merged_coverage_json\n                                        )\n                                    # Note: If we don't have cached_coverage_json, this is a full run (cache cleared or first run)\n                                    # In that case, fresh_coverage_json_parallel already has complete coverage, no merge needed\n\n                                    if self.analyzer:\n                                        coverage_data = (\n                                            self.analyzer.load_coverage_json(\n                                                coverage_output\n                                            )\n                                        )\n                                        overall_coverage = (\n                                            self.analyzer.extract_overall_from_json(\n                                                coverage_output\n                                            )\n                                        )\n\n                                        # Recalculate overall from merged JSON if we merged\n                                        if (\n                                            self.use_domain_cache\n                                            and self.test_file_cache\n                                            and cached_coverage_json\n                                            and fresh_coverage_json_parallel\n                                            and isinstance(\n                                                fresh_coverage_json_parallel, dict\n                                            )\n                                        ):\n                                            # Use totals from merged JSON (which includes both cached and fresh)\n                                            merged_totals = (\n                                                fresh_coverage_json_parallel.get(\n                                                    \"totals\", {}\n                                                )\n                                            )\n                                            total_statements = merged_totals.get(\n                                                \"num_statements\", 0\n                                            )\n                                            total_covered = merged_totals.get(\n                                                \"covered_lines\", 0\n                                            )\n                                            total_missing = merged_totals.get(\n                                                \"missing_lines\", 0\n                                            )\n\n                                            if logger:\n                                                logger.info(\n                                                    f\"Using merged totals: {total_statements} statements, {total_covered} covered, {total_missing} missing\"\n                                                )\n\n                                            if total_statements > 0:\n                                                overall_coverage[\"overall_coverage\"] = (\n                                                    merged_totals.get(\n                                                        \"percent_covered\", 0.0\n                                                    )\n                                                )\n                                                overall_coverage[\"total_statements\"] = (\n                                                    total_statements\n                                                )\n                                                overall_coverage[\"total_missed\"] = (\n                                                    total_missing\n                                                )\n\n                                            if logger:\n                                                logger.info(\n                                                    f\"Recalculated overall coverage from merged JSON: {overall_coverage.get('overall_coverage', 0):.1f}%\"\n                                                )\n\n                                        # Validate coverage percentage - if it's unexpectedly low, warn\n                                        # Expected coverage should be around 70-75% based on historical data\n                                        coverage_pct = overall_coverage.get(\n                                            \"overall_coverage\", 0\n                                        )\n                                        if coverage_pct > 0 and coverage_pct < 50:\n                                            logger.warning(\n                                                f\"Coverage percentage ({coverage_pct}%) is unexpectedly low. \"\n                                                f\"Expected ~70-75%. This may indicate parallel coverage data was not included. \"\n                                                f\"Parallel file existed before cleanup: {parallel_file_existed}, \"\n                                                f\"No_parallel file existed before cleanup: {no_parallel_file_existed}, \"\n                                                f\"Shard files found: {shard_files_count}\"\n                                            )\n\n                                        # Update cache with coverage (parallel mode)\n                                        # On selective runs, fresh_coverage_json_parallel should already be the merged coverage\n                                        # (set on line 1749 if merge occurred, or original fresh if no merge)\n                                        if (\n                                            self.use_domain_cache\n                                            and self.test_file_cache\n                                            and fresh_coverage_json_parallel\n                                        ):\n                                            # Determine if this is a full run:\n                                            # - No cached coverage (cache cleared or first run) = full run\n                                            # - All test files need to run = full run\n                                            # - Otherwise, it's a selective run\n                                            cache_stats = (\n                                                self.test_file_cache.get_cache_stats()\n                                            )\n                                            total_test_files = cache_stats[\n                                                \"total_test_files\"\n                                            ]\n                                            is_full_run = (\n                                                (not cached_coverage_json)\n                                                or (not test_files_to_run)\n                                                or (\n                                                    len(test_files_to_run)\n                                                    >= total_test_files\n                                                )\n                                            )\n\n                                            if is_full_run:\n                                                # Full run - cache the full coverage JSON once (not per test file)\n                                                self.test_file_cache.cache_full_coverage(\n                                                    fresh_coverage_json_parallel\n                                                )\n\n                                                # Also update domain mappings for all test files\n                                                test_root = self.project_root / \"tests\"\n                                                all_test_files = [\n                                                    tf\n                                                    for tf in test_root.rglob(\n                                                        \"test_*.py\"\n                                                    )\n                                                    if self.test_file_cache.is_valid_test_file(\n                                                        tf\n                                                    )\n                                                ]\n                                                for test_file in all_test_files:\n                                                    self.test_file_cache.update_test_file_mapping(\n                                                        test_file,\n                                                        reload_cache=False,\n                                                        save_cache=False,\n                                                    )\n                                                # Update and save source file mtimes for all domains (critical for change detection)\n                                                if (\n                                                    \"source_files_mtime\"\n                                                    not in self.test_file_cache.cache_data\n                                                ):\n                                                    self.test_file_cache.cache_data[\n                                                        \"source_files_mtime\"\n                                                    ] = {}\n                                                for (\n                                                    domain\n                                                ) in (\n                                                    self.test_file_cache.domain_mapper.SOURCE_TO_TEST_MAPPING.keys()\n                                                ):\n                                                    current_mtimes = self.test_file_cache.get_source_file_mtimes(\n                                                        domain\n                                                    )\n                                                    self.test_file_cache.cache_data[\n                                                        \"source_files_mtime\"\n                                                    ][domain] = current_mtimes\n                                                # Save cache\n                                                self.test_file_cache._save_cache()\n                                                if logger:\n                                                    logger.info(\n                                                        f\"Cached full coverage JSON and updated domain mappings for {len(all_test_files)} test files (parallel mode)\"\n                                                    )\n                                            else:\n                                                # Selective run - cache the merged coverage (fresh_coverage_json_parallel should already be merged if merge occurred)\n                                                # This represents the complete picture: fresh coverage from re-run tests + cached coverage from unchanged tests\n                                                self.test_file_cache.cache_full_coverage(\n                                                    fresh_coverage_json_parallel\n                                                )\n\n                                                # Update test file mappings (without coverage_data) for test files that ran\n                                                for test_file in test_files_to_run:\n                                                    self.test_file_cache.update_test_file_mapping(\n                                                        test_file,\n                                                        reload_cache=False,\n                                                        save_cache=False,\n                                                    )\n\n                                                # Update and save source file mtimes for changed domains (critical for change detection)\n                                                if (\n                                                    \"source_files_mtime\"\n                                                    not in self.test_file_cache.cache_data\n                                                ):\n                                                    self.test_file_cache.cache_data[\n                                                        \"source_files_mtime\"\n                                                    ] = {}\n                                                changed_domains = (\n                                                    self.test_file_cache.get_changed_domains()\n                                                )\n                                                for domain in changed_domains:\n                                                    current_mtimes = self.test_file_cache.get_source_file_mtimes(\n                                                        domain\n                                                    )\n                                                    self.test_file_cache.cache_data[\n                                                        \"source_files_mtime\"\n                                                    ][domain] = current_mtimes\n\n                                                # Save cache once (much more efficient than saving per test file)\n                                                self.test_file_cache._save_cache()\n                                                if logger:\n                                                    logger.info(\n                                                        f\"Cached merged coverage as full coverage cache and updated mappings for {len(test_files_to_run)} test files (parallel mode)\"\n                                                    )\n                                    else:\n                                        coverage_data = {}\n                                        overall_coverage = {}\n                            else:\n                                if logger:\n                                    logger.warning(\n                                        f\"Failed to regenerate coverage JSON: {json_result.stderr}\"\n                                    )\n                        except Exception as json_error:\n                            if logger:\n                                logger.warning(\n                                    f\"Error regenerating coverage JSON: {json_error}\"\n                                )\n\n            try:\n                if self.report_generator:\n                    # Check if we have merged coverage data that should be preserved\n                    # If we merged cached + fresh coverage, don't let finalize_coverage_outputs() overwrite it\n                    merged_coverage_exists = merged_coverage_saved\n                    if (\n                        (not merged_coverage_exists)\n                        and self.use_domain_cache\n                        and self.test_file_cache\n                        and cached_coverage_json\n                    ):\n                        # Check if coverage.json exists and was recently modified (within last 5 seconds)\n                        # This indicates we just saved merged data\n                        if coverage_output.exists():\n                            import time\n\n                            file_age = time.time() - coverage_output.stat().st_mtime\n                            if file_age < 5:  # File was modified in last 5 seconds\n                                merged_coverage_exists = True\n                                if logger:\n                                    logger.info(\n                                        \"Skipping coverage.json regeneration - merged coverage data exists and should be preserved\"\n                                    )\n\n                    if not merged_coverage_exists:\n                        self.report_generator.finalize_coverage_outputs()\n                    else:\n                        # Only generate HTML report, don't regenerate JSON (it would overwrite merged data)\n                        if logger:\n                            logger.info(\n                                \"Generating HTML coverage report (preserving merged coverage.json)...\"\n                            )\n                        # Generate HTML from the coverage data file (not the JSON - JSON is for analysis only)\n                        html_cmd = [\n                            sys.executable,\n                            \"-m\",\n                            \"coverage\",\n                            \"html\",\n                            \"-d\",\n                            str(self.coverage_html_dir),\n                        ]\n                        env = os.environ.copy()\n                        env[\"COVERAGE_FILE\"] = str(self.coverage_data_file)\n                        if self.coverage_config_path.exists():\n                            env[\"COVERAGE_RCFILE\"] = str(self.coverage_config_path)\n                        try:\n                            html_result = subprocess.run(\n                                html_cmd,\n                                capture_output=True,\n                                text=True,\n                                cwd=self.project_root,\n                                env=env,\n                                timeout=600,\n                            )\n                            if html_result.returncode != 0 and logger:\n                                logger.warning(\n                                    f\"coverage html exited with {html_result.returncode}: {html_result.stderr.strip()}\"\n                                )\n                        except Exception as html_error:\n                            if logger:\n                                logger.warning(\n                                    f\"Failed to generate HTML report: {html_error}\"\n                                )\n                else:\n                    # Fallback to old method if report generator not available\n                    self._finalize_coverage_outputs_fallback()\n            except Exception as finalize_error:\n                if logger:\n                    logger.warning(\n                        f\"Failed to finalize coverage artefacts: {finalize_error}\"\n                    )\n\n            return {\n                \"modules\": coverage_data,\n                \"overall\": overall_coverage,\n                \"test_results\": test_results,\n                \"coverage_collected\": coverage_collected\n                and pytest_ran,  # Only true if pytest actually ran\n                \"pytest_ran\": pytest_ran,  # Track whether pytest actually executed\n                \"logs\": {\n                    \"stdout\": (\n                        str(self.pytest_stdout_log) if self.pytest_stdout_log else None\n                    ),\n                    \"stderr\": (\n                        str(self.pytest_stderr_log) if self.pytest_stderr_log else None\n                    ),\n                },\n                \"archived_directories\": self.archived_directories,\n                \"command_logs\": [str(path) for path in self.command_logs],\n            }\n\n        except Exception as e:\n            import traceback\n\n            error_trace = traceback.format_exc()\n            if logger:\n                logger.error(f\"Error running coverage analysis: {e}\")\n                logger.error(f\"Traceback: {error_trace}\")\n            else:\n                print(f\"Error running coverage analysis: {e}\")\n                print(f\"Traceback: {error_trace}\")\n            # Return error info instead of empty dict so caller knows something went wrong\n            return {\n                \"error\": str(e),\n                \"traceback\": error_trace,\n                \"modules\": {},\n                \"overall\": {},\n                \"coverage_collected\": False,\n            }\n\n    def _get_dev_tools_source_mtimes(self) -> Dict[str, float]:\n        \"\"\"\n        Get current modification times for all Python files in development_tools directory.\n\n        Returns:\n            Dictionary mapping file paths to modification times\n        \"\"\"\n        mtimes = {}\n        dev_tools_dir = self.project_root / \"development_tools\"\n        if not dev_tools_dir.exists():\n            return mtimes\n\n        # Get all Python files in development_tools (excluding test files and cache)\n        for py_file in dev_tools_dir.rglob(\"*.py\"):\n            # Skip actual test files (files with test_ prefix) and cache directories\n            # But DO include tool files like run_test_coverage.py even if they're in a tests/ directory\n            if py_file.name.startswith(\"test_\") or \".coverage_cache\" in py_file.parts:\n                continue\n            try:\n                rel_path = str(py_file.relative_to(self.project_root))\n                mtime = py_file.stat().st_mtime\n                mtimes[rel_path] = mtime\n            except OSError:\n                continue\n\n        return mtimes\n\n    def _check_dev_tools_changed(self) -> bool:\n        \"\"\"\n        Check if development_tools source files have changed since last cache.\n\n        Returns:\n            True if files have changed, False if unchanged\n        \"\"\"\n        if not self.use_domain_cache or not self.dev_tools_cache:\n            return True  # If caching disabled, always consider changed\n\n        # Get current mtimes\n        current_mtimes = self._get_dev_tools_source_mtimes()\n        cached_mtimes = self.dev_tools_cache.get_cached_mtimes()\n        if not cached_mtimes:\n            return True  # No cache exists - consider it changed\n\n        # Compare mtimes\n        for file_path, current_mtime in current_mtimes.items():\n            cached_mtime = cached_mtimes.get(file_path)\n            if cached_mtime is None or current_mtime != cached_mtime:\n                return True\n\n        # Check if any files were removed\n        for file_path in cached_mtimes.keys():\n            if file_path not in current_mtimes:\n                return True\n\n        return False\n\n    def run_dev_tools_coverage(self) -> Dict[str, Dict[str, any]]:\n        \"\"\"Run pytest coverage analysis specifically for development_tools directory.\"\"\"\n        if logger:\n            logger.info(\"Running pytest coverage analysis for development_tools...\")\n\n        # Dev tools coverage caching: check if dev tools have changed\n        dev_tools_changed = True\n        cached_coverage_json = None\n\n        if self.use_domain_cache and self.dev_tools_cache:\n            dev_tools_changed = self._check_dev_tools_changed()\n\n            if not dev_tools_changed:\n                # Load cached coverage data\n                cached_coverage_json = self.dev_tools_cache.get_cached_coverage()\n                if cached_coverage_json:\n                    if logger:\n                        logger.info(\n                            \"Dev tools unchanged - using cached coverage data (skipping test execution)\"\n                        )\n                else:\n                    # Cache exists but no coverage data - need to run tests\n                    dev_tools_changed = True\n            else:\n                if logger:\n                    logger.info(\n                        \"Dev tools source files changed - will run tests and update cache\"\n                    )\n\n        try:\n            coverage_output = self.dev_tools_coverage_json\n            coverage_output.parent.mkdir(parents=True, exist_ok=True)\n\n            # Skip test execution if using cached data\n            if not dev_tools_changed and cached_coverage_json:\n                # Save cached JSON to coverage_output so analyzer can load it\n                try:\n                    with open(coverage_output, \"w\", encoding=\"utf-8\") as f:\n                        json.dump(cached_coverage_json, f, indent=2)\n                    if logger:\n                        logger.info(\"Loaded cached dev tools coverage data\")\n                except Exception as e:\n                    if logger:\n                        logger.warning(f\"Failed to save cached coverage JSON: {e}\")\n                    # Fall through to run tests\n                    dev_tools_changed = True\n\n            if not dev_tools_changed and cached_coverage_json:\n                # Use cached data\n                if self.analyzer:\n                    coverage_data = self.analyzer.load_coverage_json(coverage_output)\n                    overall_coverage = self.analyzer.extract_overall_from_json(\n                        coverage_output\n                    )\n                else:\n                    # Fallback: extract from cached JSON structure\n                    coverage_data = {}\n                    for file_path, file_data in cached_coverage_json.get(\n                        \"files\", {}\n                    ).items():\n                        summary = file_data.get(\"summary\", {})\n                        coverage_data[file_path] = {\n                            \"statements\": summary.get(\"num_statements\", 0),\n                            \"covered\": summary.get(\"covered_lines\", 0),\n                            \"missed\": summary.get(\"missing_lines\", 0),\n                            \"coverage\": summary.get(\"percent_covered\", 0.0),\n                        }\n                    overall_coverage = {\n                        \"overall_coverage\": cached_coverage_json[\"totals\"].get(\n                            \"percent_covered\", 0.0\n                        ),\n                        \"total_statements\": cached_coverage_json[\"totals\"].get(\n                            \"num_statements\", 0\n                        ),\n                        \"total_missed\": cached_coverage_json[\"totals\"].get(\n                            \"missing_lines\", 0\n                        ),\n                    }\n\n                return {\n                    \"modules\": coverage_data,\n                    \"overall\": overall_coverage,\n                    \"coverage_collected\": True,\n                    \"from_cache\": True,\n                }\n\n            # Coverage only for development_tools directory\n            # Note: coverage_output path is relative to project_root, so we need to make it absolute\n            coverage_output_abs = coverage_output.resolve()\n            dev_cov_config = (\n                self.dev_tools_coverage_config_path\n                if self.dev_tools_coverage_config_path.exists()\n                else self.coverage_config_path\n            )\n            # Create timestamp for log file\n            from datetime import datetime\n\n            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n            cmd = [\n                sys.executable,\n                \"-m\",\n                \"pytest\",\n                \"-m\",\n                \"not e2e\",  # Exclude e2e tests (slow, excluded from regular runs per pytest.ini)\n                \"--cov=development_tools\",\n                \"--cov-report=term-missing\",\n                f\"--cov-report=json:{coverage_output_abs}\",\n                f\"--cov-config={dev_cov_config}\",\n                \"--tb=line\",\n                \"-q\",\n                \"--maxfail=10\",  # Allow more failures before stopping (some tests may be flaky)\n                \"--continue-on-collection-errors\",  # Continue even if collection fails\n                # Ignore temp directories to prevent collecting tests from temp files\n                \"--ignore=tests/data/pytest-tmp-*\",\n                \"--ignore=tests/data/pytest-of-*\",\n                \"tests/development_tools/\",\n            ]\n\n            if logger:\n                logger.debug(\n                    f\"Running dev tools coverage command: {' '.join(cmd[:5])} ...\"\n                )\n\n            env = os.environ.copy()\n            # Ensure PATH includes Python executable's directory for Windows DLL resolution\n            env = self._ensure_python_path_in_env(env)\n            # Set COVERAGE_FILE to absolute path to ensure files are created in the correct location\n            env[\"COVERAGE_FILE\"] = str(self.dev_tools_coverage_data_file.resolve())\n            if dev_cov_config and dev_cov_config.exists():\n                env[\"COVERAGE_RCFILE\"] = str(dev_cov_config.resolve())\n            # Set unique pytest temp directory to avoid conflicts when running in parallel with main coverage\n            # Use a unique identifier based on process/coverage type to ensure isolation\n            import uuid\n\n            unique_id = f\"dev_tools_{uuid.uuid4().hex[:8]}\"\n            pytest_temp_base = (\n                self.project_root / \"tests\" / \"data\" / f\"pytest-tmp-{unique_id}\"\n            )\n            pytest_temp_base.mkdir(parents=True, exist_ok=True)\n            # Set PYTEST_CACHE_DIR to ensure pytest uses unique cache directory\n            env[\"PYTEST_CACHE_DIR\"] = str(pytest_temp_base / \".pytest_cache\")\n            # Also set basetemp via command line argument for tmpdir fixture\n            cmd.append(f\"--basetemp={pytest_temp_base}\")\n\n            # Set up stdout logging for dev tools coverage (similar to main coverage)\n            # Rotate old log files before creating new ones (keep 1 current + 7 archived = 8 total)\n            self._rotate_log_files(\"pytest_dev_tools_stdout\", max_versions=8)\n\n            # Create timestamp for log file\n            timestamp = now_timestamp_filename()\n            dev_tools_stdout_log = (\n                self.coverage_logs_dir / f\"pytest_dev_tools_stdout_{timestamp}.log\"\n            )\n            dev_tools_stdout_log.parent.mkdir(parents=True, exist_ok=True)\n\n            # Run pytest and capture output to both stdout and log file\n            try:\n                result = subprocess.run(\n                    cmd,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.STDOUT,  # Merge stderr into stdout\n                    text=True,\n                    cwd=self.project_root,\n                    env=env,\n                    timeout=720,  # 12 minutes timeout\n                )\n                # Ensure stdout is not None\n                if result.stdout is None:\n                    result.stdout = \"\"\n                if result.stderr is None:\n                    result.stderr = \"\"\n                # Write to log file\n                with open(\n                    dev_tools_stdout_log, \"w\", encoding=\"utf-8\", errors=\"replace\"\n                ) as log_file:\n                    log_file.write(result.stdout)\n                    if result.stderr and result.stderr != result.stdout:\n                        log_file.write(\"\\n\\n=== STDERR ===\\n\")\n                        log_file.write(result.stderr)\n            except subprocess.TimeoutExpired:\n                if logger:\n                    logger.error(\n                        f\"Dev tools coverage pytest timed out after 12 minutes\"\n                    )\n                with open(\n                    dev_tools_stdout_log, \"w\", encoding=\"utf-8\", errors=\"replace\"\n                ) as log_file:\n                    log_file.write(\"ERROR: pytest timed out after 12 minutes\\n\")\n\n                # Create a fake result object for error handling\n                class FakeResult:\n                    returncode = 1\n                    stdout = \"ERROR: pytest timed out after 12 minutes\\n\"\n                    stderr = \"\"\n\n                result = FakeResult()\n            except Exception as e:\n                if logger:\n                    logger.error(\n                        f\"Error running dev tools coverage pytest: {e}\", exc_info=True\n                    )\n                with open(\n                    dev_tools_stdout_log, \"w\", encoding=\"utf-8\", errors=\"replace\"\n                ) as log_file:\n                    log_file.write(f\"ERROR: Exception running pytest: {e}\\n\")\n\n                # Create a fake result object for error handling\n                class FakeResultException:\n                    returncode = 1\n                    stdout = f\"ERROR: Exception running pytest: {e}\\n\"\n                    stderr = \"\"\n\n                result = FakeResultException()\n\n            if logger:\n                log_size = (\n                    dev_tools_stdout_log.stat().st_size\n                    if dev_tools_stdout_log.exists()\n                    else 0\n                )\n                logger.info(\n                    f\"Saved dev tools pytest output to {dev_tools_stdout_log} ({log_size} bytes)\"\n                )\n\n            # Parse coverage results (even if pytest exited with non-zero code, coverage may still be available)\n            if self.analyzer:\n                coverage_data = self.analyzer.parse_coverage_output(result.stdout)\n            else:\n                coverage_data = {}\n            if not coverage_data and coverage_output.exists():\n                if self.analyzer:\n                    coverage_data = self.analyzer.load_coverage_json(coverage_output)\n\n                    # Add timestamp metadata to coverage_dev_tools.json\n                    try:\n                        with open(coverage_output, \"r\", encoding=\"utf-8\") as f:\n                            dev_tools_coverage_json = json.load(f)\n                        timestamp_str = now_timestamp_full()\n                        timestamp_iso = datetime.now().isoformat()\n                        dev_tools_coverage_json[\"_metadata\"] = {\n                            \"generated_by\": \"pytest-cov --cov-report=json - Development Tools (dev_tools)\",\n                            \"last_generated\": timestamp_str,\n                            \"timestamp\": timestamp_iso,\n                            \"note\": \"This file is auto-generated. Do not edit manually.\",\n                        }\n                        with open(coverage_output, \"w\", encoding=\"utf-8\") as f:\n                            json.dump(dev_tools_coverage_json, f, indent=2)\n                    except Exception as e:\n                        if logger:\n                            logger.debug(\n                                f\"Failed to add metadata to coverage_dev_tools.json: {e}\"\n                            )\n\n                    # Rotate coverage_dev_tools.json after loading\n                    if coverage_output.exists():\n                        from development_tools.shared.file_rotation import FileRotator\n\n                        rotator = FileRotator(base_dir=str(coverage_output.parent))\n                        rotator.rotate_file(str(coverage_output), max_versions=5)\n\n            if self.analyzer:\n                overall_coverage = self.analyzer.extract_overall_coverage(result.stdout)\n            else:\n                overall_coverage = {}\n            if (\n                not overall_coverage.get(\"overall_coverage\")\n                and coverage_output.exists()\n            ):\n                if self.analyzer:\n                    overall_coverage = self.analyzer.extract_overall_from_json(\n                        coverage_output\n                    )\n\n            # Log status after attempting to extract coverage\n            if result.returncode != 0:\n                if logger:\n                    # If we successfully extracted coverage, this is just test failures (not a coverage problem)\n                    if overall_coverage.get(\"overall_coverage\") or coverage_data:\n                        logger.info(\n                            f\"Dev tools coverage pytest completed with exit code {result.returncode} (some tests failed, but coverage data was successfully collected: {overall_coverage.get('overall_coverage', 'N/A')}%)\"\n                        )\n                    else:\n                        logger.warning(\n                            f\"Dev tools coverage pytest exited with code {result.returncode} and no coverage data could be extracted\"\n                        )\n                        if result.stderr:\n                            logger.warning(f\"Pytest stderr: {result.stderr[:500]}\")\n                        if result.stdout:\n                            # Log more of stdout to see what happened\n                            logger.warning(\n                                f\"Pytest stdout (last 1000 chars): {result.stdout[-1000:]}\"\n                            )\n\n            # If still no coverage data, check if the file was created\n            if (\n                not overall_coverage.get(\"overall_coverage\")\n                and not coverage_output.exists()\n            ):\n                if logger:\n                    logger.warning(\n                        f\"Coverage JSON file not created at {coverage_output}\"\n                    )\n                    logger.info(f\"Pytest return code: {result.returncode}\")\n                    if result.stdout:\n                        # Look for coverage summary in stdout\n                        if \"TOTAL\" in result.stdout:\n                            logger.info(\n                                \"Coverage data found in stdout but JSON not created\"\n                            )\n                        # Log more of stdout to diagnose the issue\n                        logger.warning(\n                            f\"Pytest stdout (last 1000 chars): {result.stdout[-1000:]}\"\n                        )\n                        # Also log first part to see if there are early errors\n                        if len(result.stdout) > 1000:\n                            logger.warning(\n                                f\"Pytest stdout (first 500 chars): {result.stdout[:500]}\"\n                            )\n                    if result.stderr:\n                        logger.warning(f\"Pytest stderr: {result.stderr[:500]}\")\n\n            # Cache dev tools coverage if we ran tests\n            if (\n                self.use_domain_cache\n                and self.dev_tools_cache\n                and coverage_output.exists()\n                and dev_tools_changed\n            ):\n                # Load fresh coverage JSON\n                try:\n                    with open(coverage_output, \"r\", encoding=\"utf-8\") as f:\n                        fresh_coverage_json = json.load(f)\n                    source_mtimes = self._get_dev_tools_source_mtimes()\n                    self.dev_tools_cache.update_cache(\n                        fresh_coverage_json, source_mtimes\n                    )\n                    if logger:\n                        logger.debug(\"Cached dev tools coverage data\")\n                except Exception as e:\n                    if logger:\n                        logger.warning(f\"Failed to cache dev tools coverage: {e}\")\n\n            # Clean up temporary coverage data files after generating report\n            self._cleanup_coverage_data_files()\n            # Also clean up any process-specific files that may have been created\n            self._cleanup_process_specific_coverage_files()\n\n            if logger:\n                if overall_coverage.get(\"overall_coverage\"):\n                    logger.info(\n                        f\"Dev tools coverage: {overall_coverage.get('overall_coverage', 0):.1f}%\"\n                    )\n                else:\n                    logger.warning(\"Dev tools coverage data not available\")\n\n            return {\n                \"modules\": coverage_data,\n                \"overall\": overall_coverage,\n                \"coverage_collected\": bool(coverage_data) or coverage_output.exists(),\n                \"output_file\": str(coverage_output),\n                \"html_dir\": None,  # HTML reports disabled\n                \"from_cache\": (\n                    not dev_tools_changed\n                    if self.use_domain_cache and self.dev_tools_cache\n                    else False\n                ),\n            }\n\n        except Exception as e:\n            if logger:\n                logger.error(f\"Error running dev tools coverage analysis: {e}\")\n            else:\n                print(f\"Error running dev tools coverage analysis: {e}\")\n            return {}\n\n    def _cleanup_process_specific_coverage_files(\n        self, coverage_dir: Optional[Path] = None\n    ) -> None:\n        \"\"\"Clean up process-specific coverage files (e.g., .coverage_parallel.DESKTOP-*.X.*).\n\n        These files are created by coverage.py when multiple processes try to write to the same\n        coverage file simultaneously. They need to be cleaned up after coverage collection.\n        \"\"\"\n        if coverage_dir is None:\n            coverage_dir = self.coverage_data_file.parent\n\n        if not coverage_dir.exists():\n            return\n\n        cleanup_count = 0\n        try:\n            # Clean up process-specific parallel files\n            for proc_file in coverage_dir.glob(\".coverage_parallel.*\"):\n                if proc_file.is_file() and proc_file.name != \".coverage_parallel\":\n                    try:\n                        proc_file.unlink()\n                        cleanup_count += 1\n                    except Exception:\n                        pass\n            # Clean up process-specific no_parallel files\n            for proc_file in coverage_dir.glob(\".coverage_no_parallel.*\"):\n                if proc_file.is_file() and proc_file.name != \".coverage_no_parallel\":\n                    try:\n                        proc_file.unlink()\n                        cleanup_count += 1\n                    except Exception:\n                        pass\n            # Also clean up any process-specific .coverage files (from non-parallel runs)\n            for proc_file in coverage_dir.glob(\".coverage.*\"):\n                if proc_file.is_file() and proc_file.name != \".coverage\":\n                    # Check if it's a process-specific file (contains hostname and PID pattern)\n                    # Pattern: .coverage.HOSTNAME.PID.RANDOM\n                    if \".\" in proc_file.name[10:]:  # After \".coverage.\"\n                        try:\n                            proc_file.unlink()\n                            cleanup_count += 1\n                        except Exception:\n                            pass\n        except Exception as exc:\n            if logger:\n                logger.debug(\n                    f\"Failed to cleanup process-specific coverage files: {exc}\"\n                )\n\n        if cleanup_count > 0 and logger:\n            logger.debug(\n                f\"Cleaned up {cleanup_count} process-specific coverage file(s)\"\n            )\n\n    def _cleanup_coverage_data_files(self) -> None:\n        \"\"\"Clean up temporary .coverage_dev_tools.* files after coverage analysis.\"\"\"\n        tests_dir = self.project_root / \"development_tools\" / \"tests\"\n        root_dir = self.project_root / \"development_tools\"\n\n        cleanup_count = 0\n\n        # Clean up files in tests directory (current location)\n        # Include process-specific files (e.g., .coverage_dev_tools.DESKTOP-*.X.*)\n        if tests_dir.exists():\n            # Clean up all .coverage_dev_tools.* files (including process-specific ones)\n            for coverage_file in tests_dir.glob(\".coverage_dev_tools*\"):\n                try:\n                    # Skip directories\n                    if coverage_file.is_file():\n                        coverage_file.unlink()\n                        cleanup_count += 1\n                except Exception as exc:\n                    if logger:\n                        logger.warning(f\"Failed to clean up {coverage_file}: {exc}\")\n\n        # Clean up legacy files in root directory (old location)\n        if root_dir.exists():\n            for coverage_file in root_dir.glob(\".coverage_dev_tools*\"):\n                try:\n                    # Skip if it's a directory\n                    if coverage_file.is_file():\n                        coverage_file.unlink()\n                        cleanup_count += 1\n                except Exception as exc:\n                    if logger:\n                        logger.warning(\n                            f\"Failed to clean up legacy file {coverage_file}: {exc}\"\n                        )\n\n        if cleanup_count > 0 and logger:\n            logger.info(f\"Cleaned up {cleanup_count} temporary coverage data file(s)\")\n\n    def _parse_pytest_test_results(self, stdout: str) -> Dict[str, Any]:\n        \"\"\"Parse pytest output to extract test results, failures, and random seed.\"\"\"\n        results = {\n            \"random_seed\": None,\n            \"test_summary\": None,\n            \"failed_tests\": [],\n            \"passed_count\": 0,\n            \"failed_count\": 0,\n            \"skipped_count\": 0,\n            \"warnings_count\": 0,\n            \"total_tests\": 0,\n            \"maxfail_reached\": False,\n        }\n\n        if not stdout:\n            return results\n\n        # Check if maxfail was reached (pytest stops early when maxfail is hit)\n        # Look for patterns like \"interrupted: stopping after X failures\" or \"maxfail=X reached\"\n        maxfail_patterns = [\n            r\"interrupted:\\s+stopping\\s+after\\s+\\d+\\s+failures?\",\n            r\"maxfail\\s*=\\s*\\d+\\s+reached\",\n            r\"stopping\\s+after\\s+\\d+\\s+failures?\",\n        ]\n        for pattern in maxfail_patterns:\n            if re.search(pattern, stdout, re.IGNORECASE):\n                results[\"maxfail_reached\"] = True\n                break\n\n        # Extract random seed if pytest-randomly is used\n        seed_pattern = r\"--randomly-seed=(\\d+)\"\n        seed_match = re.search(seed_pattern, stdout)\n        if seed_match:\n            results[\"random_seed\"] = seed_match.group(1)\n\n        # Extract test summary (e.g., \"4 failed, 2276 passed, 1 skipped, 4 warnings\" or \"145 passed, 3439 deselected\")\n        # Try full format first\n        summary_pattern = r\"(\\d+)\\s+failed[,\\s]+(\\d+)\\s+passed[,\\s]+(\\d+)\\s+skipped[,\\s]+(\\d+)\\s+warnings\"\n        summary_match = re.search(summary_pattern, stdout)\n        if summary_match:\n            results[\"failed_count\"] = int(summary_match.group(1))\n            results[\"passed_count\"] = int(summary_match.group(2))\n            results[\"skipped_count\"] = int(summary_match.group(3))\n            results[\"warnings_count\"] = int(summary_match.group(4))\n            results[\"total_tests\"] = (\n                results[\"failed_count\"]\n                + results[\"passed_count\"]\n                + results[\"skipped_count\"]\n            )\n            results[\"test_summary\"] = (\n                f\"{results['failed_count']} failed, {results['passed_count']} passed, {results['skipped_count']} skipped, {results['warnings_count']} warnings\"\n            )\n        else:\n            # Try simpler format (e.g., \"145 passed, 3439 deselected\" or \"1 failed, 3437 passed, 1 skipped\")\n            # Look for patterns like \"X passed, Y deselected\" or \"X failed, Y passed, Z skipped\"\n            # Search for all number-label pairs in the summary line\n            simple_pattern = r\"(\\d+)\\s+(failed|passed|skipped|deselected|warnings)\"\n            matches = re.findall(simple_pattern, stdout)\n            for count_str, label in matches:\n                count = int(count_str)\n                if label == \"failed\":\n                    results[\"failed_count\"] = count\n                elif label == \"passed\":\n                    results[\"passed_count\"] = count\n                elif label == \"skipped\":\n                    results[\"skipped_count\"] = count\n                elif label == \"warnings\":\n                    results[\"warnings_count\"] = count\n                # deselected is not counted in total, but we note it\n\n            # If no matches found but output contains only dots (quiet mode with all passed tests),\n            # try to count dots as passed tests (rough estimate)\n            if (\n                not matches\n                and stdout.strip()\n                and all(c in \".sF\" for c in stdout.strip())\n            ):\n                # Count dots as passed, 'F' as failed, 's' as skipped\n                dot_count = stdout.count(\".\")\n                f_count = stdout.count(\"F\")\n                s_count = stdout.count(\"s\")\n                if dot_count > 0 or f_count > 0 or s_count > 0:\n                    results[\"passed_count\"] = dot_count\n                    results[\"failed_count\"] = f_count\n                    results[\"skipped_count\"] = s_count\n                    results[\"total_tests\"] = dot_count + f_count + s_count\n                    if results[\"total_tests\"] > 0:\n                        parts = []\n                        if results[\"failed_count\"] > 0:\n                            parts.append(f\"{results['failed_count']} failed\")\n                        if results[\"passed_count\"] > 0:\n                            parts.append(f\"{results['passed_count']} passed\")\n                        if results[\"skipped_count\"] > 0:\n                            parts.append(f\"{results['skipped_count']} skipped\")\n                        results[\"test_summary\"] = (\n                            \", \".join(parts) if parts else \"0 tests\"\n                        )\n\n            if not results.get(\"test_summary\"):\n                results[\"total_tests\"] = (\n                    results[\"failed_count\"]\n                    + results[\"passed_count\"]\n                    + results[\"skipped_count\"]\n                )\n                if results[\"total_tests\"] > 0 or results[\"passed_count\"] > 0:\n                    parts = []\n                    if results[\"failed_count\"] > 0:\n                        parts.append(f\"{results['failed_count']} failed\")\n                    if results[\"passed_count\"] > 0:\n                        parts.append(f\"{results['passed_count']} passed\")\n                    if results[\"skipped_count\"] > 0:\n                        parts.append(f\"{results['skipped_count']} skipped\")\n                    if results[\"warnings_count\"] > 0:\n                        parts.append(f\"{results['warnings_count']} warnings\")\n                    results[\"test_summary\"] = \", \".join(parts) if parts else \"0 tests\"\n\n        # Extract failed test names from \"FAILED\" section\n        failed_section_pattern = r\"FAILED\\s+(.+?)(?=\\n\\n|\\n===|$)\"\n        failed_matches = re.findall(failed_section_pattern, stdout, re.DOTALL)\n\n        # Also look for \"short test summary info\" section\n        short_summary_pattern = r\"short test summary info[^\\n]*\\n(.*?)(?=\\n===|$)\"\n        short_summary_match = re.search(short_summary_pattern, stdout, re.DOTALL)\n        if short_summary_match:\n            summary_lines = short_summary_match.group(1).strip().split(\"\\n\")\n            for line in summary_lines:\n                if line.strip().startswith(\"FAILED\"):\n                    # Extract test path from \"FAILED tests/path/to/test.py::test_function\"\n                    test_match = re.search(r\"FAILED\\s+(.+)\", line)\n                    if test_match:\n                        results[\"failed_tests\"].append(test_match.group(1).strip())\n\n        return results\n\n    def _rotate_log_files(self, base_name: str, max_versions: int = 7) -> None:\n        \"\"\"Rotate log files, keeping only the last max_versions copies total (consolidated).\"\"\"\n        try:\n            from development_tools.shared.file_rotation import FileRotator\n\n            # Find all log files matching the base name pattern (both in main dir and archive)\n            main_log_files = sorted(\n                self.coverage_logs_dir.glob(f\"{base_name}_*.log\"),\n                key=lambda p: p.stat().st_mtime,\n                reverse=True,\n            )\n\n            archive_dir = self.coverage_logs_dir / \"archive\"\n            archived_files = []\n            if archive_dir.exists():\n                archived_files = sorted(\n                    archive_dir.glob(f\"{base_name}_*.log\"),\n                    key=lambda p: p.stat().st_mtime,\n                    reverse=True,\n                )\n\n            # Combine all files and sort by modification time (newest first)\n            all_files = [(f, f.stat().st_mtime) for f in main_log_files] + [\n                (f, f.stat().st_mtime) for f in archived_files\n            ]\n            all_files.sort(key=lambda x: x[1], reverse=True)\n\n            # Strategy: Keep 1 file in main (newest), max_versions-1 in archive\n            # max_versions=8 means: 1 current + 7 archived = 8 total\n            archive_dir.mkdir(parents=True, exist_ok=True)\n\n            # Separate files by location\n            main_files = [f for f, _ in all_files if f.parent == self.coverage_logs_dir]\n            archive_files = [f for f, _ in all_files if f.parent == archive_dir]\n\n            # Sort by modification time (newest first)\n            main_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n            archive_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n\n            if logger:\n                logger.debug(\n                    f\"Rotating {base_name} logs: {len(main_files)} in main, {len(archive_files)} in archive (target: 1 main + {max_versions-1} archive)\"\n                )\n\n            # Step 1: Move ALL files from main to archive (new file will be created after rotation)\n            # This ensures only the new file (created after rotation) remains in main\n            if len(main_files) > 0:\n                for file_path in main_files:\n                    try:\n                        # Use original filename (it already has timestamp)\n                        archive_path = archive_dir / file_path.name\n                        if not archive_path.exists():\n                            shutil.move(str(file_path), str(archive_path))\n                            if logger:\n                                logger.debug(f\"Archived log file: {file_path.name}\")\n                        else:\n                            # Archive file already exists, delete the duplicate from main\n                            file_path.unlink()\n                            if logger:\n                                logger.debug(\n                                    f\"Removed duplicate log file: {file_path.name}\"\n                                )\n                    except Exception as e:\n                        if logger:\n                            logger.warning(f\"Failed to archive {file_path.name}: {e}\")\n\n            # Step 2: Ensure archive has at most max_versions-1 files (since 1 is in main)\n            max_archived = max_versions - 1  # Keep 7 archived files when max_versions=8\n            final_archive = sorted(\n                archive_dir.glob(f\"{base_name}_*.log\") if archive_dir.exists() else [],\n                key=lambda p: p.stat().st_mtime,\n                reverse=True,\n            )\n\n            if len(final_archive) > max_archived:\n                files_to_delete = final_archive[max_archived:]\n                for file_path in files_to_delete:\n                    try:\n                        file_path.unlink()\n                        if logger:\n                            logger.debug(f\"Removed old archived log: {file_path.name}\")\n                    except Exception as e:\n                        if logger:\n                            logger.warning(\n                                f\"Failed to remove old archive {file_path.name}: {e}\"\n                            )\n\n            # Verify final count\n            final_main = list(self.coverage_logs_dir.glob(f\"{base_name}_*.log\"))\n            final_archive = (\n                list(archive_dir.glob(f\"{base_name}_*.log\"))\n                if archive_dir.exists()\n                else []\n            )\n            final_total = len(final_main) + len(final_archive)\n\n            if logger:\n                logger.debug(\n                    f\"Log rotation complete for {base_name}: {final_total} files total ({len(final_main)} in main, {len(final_archive)} in archive)\"\n                )\n\n            # Final safety check: if we still have too many, delete oldest\n            if len(final_main) > 1:\n                # Keep only newest in main\n                main_sorted = sorted(\n                    final_main, key=lambda p: p.stat().st_mtime, reverse=True\n                )\n                for old_file in main_sorted[1:]:\n                    try:\n                        old_file.unlink()\n                        if logger:\n                            logger.debug(f\"Removed excess main log: {old_file.name}\")\n                    except Exception as e:\n                        if logger:\n                            logger.warning(\n                                f\"Failed to remove excess main file {old_file.name}: {e}\"\n                            )\n            elif logger:\n                logger.debug(\n                    f\"No rotation needed for {base_name}: {len(all_files)} files (max: {max_versions})\"\n                )\n        except ImportError:\n            # FileRotator not available, skip rotation\n            if logger:\n                logger.debug(\"FileRotator not available, skipping log rotation\")\n        except Exception as e:\n            if logger:\n                logger.debug(f\"Failed to rotate log files: {e}\")\n\n    def _record_pytest_output(self, result: subprocess.CompletedProcess) -> None:\n        \"\"\"Persist pytest stdout/stderr for troubleshooting.\"\"\"\n        # This method is deprecated - logs are now created directly in run_coverage_analysis\n        # Keeping for backward compatibility but it shouldn't be called\n        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        self.coverage_logs_dir.mkdir(parents=True, exist_ok=True)\n\n        stdout_content = result.stdout or \"\"\n\n        stdout_path = self.coverage_logs_dir / f\"pytest_parallel_stdout_{timestamp}.log\"\n\n        stdout_path.write_text(stdout_content, encoding=\"utf-8\", errors=\"ignore\")\n\n        self.pytest_stdout_log = stdout_path\n        self.pytest_stderr_log = None  # No longer creating stderr logs\n\n        if logger:\n            logger.info(f\"Saved pytest output to {stdout_path}\")\n\n    def _finalize_coverage_outputs_fallback(self) -> None:\n        \"\"\"Fallback method for finalizing coverage outputs if report generator is not available.\"\"\"\n        # This is a minimal fallback - ideally the report generator should always be available\n        if logger:\n            logger.warning(\n                \"Using fallback coverage finalization - report generator not available\"\n            )\n\n    def _generate_coverage_summary_fallback(\n        self, coverage_data: Dict[str, Dict[str, any]], overall_data: Dict[str, any]\n    ) -> str:\n        \"\"\"Fallback method for generating coverage summary if report generator is not available.\"\"\"\n        # This is a minimal fallback - ideally the report generator should always be available\n        if logger:\n            logger.warning(\n                \"Using fallback coverage summary generation - report generator not available\"\n            )\n        return f\"Overall Coverage: {overall_data.get('overall_coverage', 0):.1f}%\"\n\n    def get_current_timestamp(self) -> str:\n        \"\"\"Get current timestamp in the format used by the plan.\"\"\"\n        from datetime import datetime\n\n        return datetime.now().strftime(\"%Y-%m-%d\")\n\n    def run(\n        self, update_plan: bool = False, dev_tools_only: bool = False\n    ) -> Dict[str, any]:\n        \"\"\"Run the coverage metrics regeneration.\"\"\"\n        if dev_tools_only:\n            # Run dev tools coverage analysis only\n            coverage_results = self.run_dev_tools_coverage()\n\n            if not coverage_results:\n                error_msg = \"Failed to get dev tools coverage data - run_dev_tools_coverage returned empty result\"\n                logger.error(error_msg)\n                print(f\"ERROR: {error_msg}\", file=sys.stderr)\n                sys.exit(1)\n\n            # Check if coverage was actually collected\n            if not coverage_results.get(\"coverage_collected\", False):\n                error_msg = \"Dev tools coverage analysis completed but no coverage data was collected - tests may not have run\"\n                logger.error(error_msg)\n                # Print to stdout so run_script() can capture it\n                print(f\"ERROR: {error_msg}\")\n                # Still return the results dict so caller can check coverage_collected flag\n                # Exit with non-zero code to indicate failure\n                sys.exit(1)\n\n            # Generate summary for dev tools\n            if self.report_generator:\n                coverage_summary = self.report_generator.generate_coverage_summary(\n                    coverage_results.get(\"modules\", {}),\n                    coverage_results.get(\"overall\", {}),\n                )\n            else:\n                # Fallback to old method if report generator not available\n                coverage_summary = self._generate_coverage_summary_fallback(\n                    coverage_results.get(\"modules\", {}),\n                    coverage_results.get(\"overall\", {}),\n                )\n\n            # Print summary (headers removed - added by consolidated report)\n            print(coverage_summary)\n\n            return coverage_results\n        else:\n            logger.info(\"Generating test coverage...\")\n\n            # Run coverage analysis\n            coverage_results = self.run_coverage_analysis()\n\n            if not coverage_results:\n                logger.error(\n                    \"Failed to get coverage data - run_coverage_analysis returned empty result\"\n                )\n                return {}\n\n            # Check if coverage was actually collected (not just using stale data)\n            if not coverage_results.get(\"coverage_collected\", False):\n                logger.error(\n                    \"Coverage analysis completed but no coverage data was collected - tests may not have run\"\n                )\n                if coverage_results.get(\"error\"):\n                    logger.error(\n                        f\"Error from coverage analysis: {coverage_results.get('error')}\"\n                    )\n                return {}\n\n            # Generate summary\n            if self.report_generator:\n                coverage_summary = self.report_generator.generate_coverage_summary(\n                    coverage_results[\"modules\"], coverage_results[\"overall\"]\n                )\n            else:\n                # Fallback to old method if report generator not available\n                coverage_summary = self._generate_coverage_summary_fallback(\n                    coverage_results[\"modules\"], coverage_results[\"overall\"]\n                )\n\n            # Print summary (headers removed - added by consolidated report)\n            print(coverage_summary)\n\n            # Note: --update-plan flag is deprecated. TEST_COVERAGE_REPORT.md is now generated\n            # by the generate_test_coverage_report tool, which runs after this tool in audit orchestration.\n            if update_plan:\n                logger.warning(\n                    \"--update-plan flag is deprecated. TEST_COVERAGE_REPORT.md is now generated by generate_test_coverage_report tool.\"\n                )\n                print(\n                    \"\\n* Note: TEST_COVERAGE_REPORT.md will be generated by generate_test_coverage_report tool (runs after coverage execution)\"\n                )\n\n            return coverage_results\n\n\ndef main():\n    \"\"\"Main entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Run test coverage execution and collect coverage data\"\n    )\n    parser.add_argument(\n        \"--update-plan\",\n        action=\"store_true\",\n        help=\"[DEPRECATED] TEST_COVERAGE_REPORT.md is now generated by generate_test_coverage_report tool. This flag does nothing.\",\n    )\n    parser.add_argument(\n        \"--output-file\", help=\"Output file for coverage report (optional)\"\n    )\n    parser.add_argument(\n        \"--no-parallel\",\n        action=\"store_true\",\n        help=\"Disable parallel test execution (parallel enabled by default)\",\n    )\n    parser.add_argument(\n        \"--workers\",\n        default=\"auto\",\n        help=\"Number of parallel workers (default: 'auto' to let pytest-xdist decide, or specify a number)\",\n    )\n    parser.add_argument(\n        \"--dev-tools-only\",\n        action=\"store_true\",\n        help=\"Run coverage analysis only for development_tools directory (separate evaluation)\",\n    )\n    parser.add_argument(\n        \"--no-domain-cache\",\n        action=\"store_true\",\n        help=\"Disable test-file and dev tools coverage caching (runs all tests regardless of domain changes). Caching is enabled by default.\",\n    )\n\n    args = parser.parse_args()\n\n    # Only use num_workers if parallel is enabled\n    parallel_enabled = not args.no_parallel\n    regenerator = CoverageMetricsRegenerator(\n        parallel=parallel_enabled,\n        num_workers=args.workers if parallel_enabled else None,\n        use_domain_cache=not args.no_domain_cache,\n    )\n    results = regenerator.run(\n        update_plan=args.update_plan, dev_tools_only=args.dev_tools_only\n    )\n\n    if args.output_file and results:\n        # Save detailed results to JSON file\n        output_path = Path(args.output_file)\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            import json\n\n            json.dump(results, f, indent=2)\n        print(f\"\\n\ud83d\udcca Detailed coverage data saved to: {output_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 172,
                    "line_content": "# Fall back to root location for backward compatibility",
                    "start": 6915,
                    "end": 6937
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 237,
                    "line_content": "self.use_domain_cache = use_domain_cache  # Keep name for backward compatibility",
                    "start": 9945,
                    "end": 9967
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1058,
                    "line_content": "stdout_log_path  # Keep variable name for backward compatibility",
                    "start": 49895,
                    "end": 49917
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 3713,
                    "line_content": "# Keeping for backward compatibility but it shouldn't be called",
                    "start": 193988,
                    "end": 194010
                  }
                ]
              ],
              [
                "development_tools\\shared\\service\\audit_orchestration.py",
                "\"\"\"\nAudit orchestration methods for AIToolsService.\n\nContains methods for running audits in three tiers (quick, standard, full)\nand managing audit state.\n\"\"\"\n\nimport json\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict\n\nfrom core.logger import get_component_logger\n\nlogger = get_component_logger(\"development_tools\")\n\n# Import output storage\nfrom ..output_storage import save_tool_result, get_all_tool_results\nfrom ..file_rotation import create_output_file\n\n# Module-level flag to track if ANY audit is in progress\n_AUDIT_IN_PROGRESS_GLOBAL = False\n\n# File-based lock for cross-process protection\n_AUDIT_LOCK_FILE = None\n\n\ndef _get_status_file_mtimes(project_root: Path) -> Dict[str, float]:\n    \"\"\"Get modification times for all status files.\"\"\"\n    # Get status file paths from config\n    try:\n        from .. import config\n        status_config = config.get_status_config()\n        status_files_config = status_config.get('status_files', {})\n        # Use default from STATUS config if status_files_config is empty (matches default config)\n        if not status_files_config:\n            from ...config.config import STATUS\n            status_files_config = STATUS.get('status_files', {})\n        status_files = {\n            'AI_STATUS.md': project_root / status_files_config.get('ai_status', 'development_tools/AI_STATUS.md'),\n            'AI_PRIORITIES.md': project_root / status_files_config.get('ai_priorities', 'development_tools/AI_PRIORITIES.md'),\n            'consolidated_report.txt': project_root / status_files_config.get('consolidated_report', 'development_tools/consolidated_report.txt')\n        }\n    except (ImportError, AttributeError, KeyError):\n        # Fallback to default STATUS config values for backward compatibility\n        try:\n            from ...config.config import STATUS\n            status_files_default = STATUS.get('status_files', {})\n            status_files = {\n                'AI_STATUS.md': project_root / status_files_default.get('ai_status', 'development_tools/AI_STATUS.md'),\n                'AI_PRIORITIES.md': project_root / status_files_default.get('ai_priorities', 'development_tools/AI_PRIORITIES.md'),\n                'consolidated_report.txt': project_root / status_files_default.get('consolidated_report', 'development_tools/consolidated_report.txt')\n            }\n        except (ImportError, AttributeError):\n            # Last resort fallback\n            status_files = {\n                'AI_STATUS.md': project_root / 'development_tools' / 'AI_STATUS.md',\n                'AI_PRIORITIES.md': project_root / 'development_tools' / 'AI_PRIORITIES.md',\n                'consolidated_report.txt': project_root / 'development_tools' / 'consolidated_report.txt'\n            }\n    mtimes = {}\n    for name, path in status_files.items():\n        if path.exists():\n            mtimes[name] = path.stat().st_mtime\n        else:\n            mtimes[name] = 0.0\n    return mtimes\n\n\ndef _is_audit_in_progress(project_root: Path) -> bool:\n    \"\"\"Check if audit is in progress using both in-memory flag and file-based lock.\"\"\"\n    global _AUDIT_IN_PROGRESS_GLOBAL, _AUDIT_LOCK_FILE\n    if _AUDIT_IN_PROGRESS_GLOBAL:\n        return True\n    if _AUDIT_LOCK_FILE is None:\n        # Use generic path relative to project root (no development_tools/ assumption)\n        try:\n            from .. import config\n            lock_file_path = config.get_external_value('paths.audit_lock_file', '.audit_in_progress.lock')\n            _AUDIT_LOCK_FILE = project_root / lock_file_path\n        except (ImportError, AttributeError):\n            _AUDIT_LOCK_FILE = project_root / '.audit_in_progress.lock'\n    audit_lock_exists = _AUDIT_LOCK_FILE.exists()\n    # Use generic path relative to project root (no development_tools/ assumption)\n    try:\n        from .. import config\n        coverage_lock_path = config.get_external_value('paths.coverage_lock_file', '.coverage_in_progress.lock')\n        coverage_lock_file = project_root / coverage_lock_path\n    except (ImportError, AttributeError):\n        coverage_lock_file = project_root / '.coverage_in_progress.lock'\n    coverage_lock_exists = coverage_lock_file.exists()\n    # Also check for dev tools coverage lock file (used when running in parallel)\n    dev_tools_coverage_lock_file = project_root / '.coverage_dev_tools_in_progress.lock'\n    dev_tools_coverage_lock_exists = dev_tools_coverage_lock_file.exists()\n    lock_exists = audit_lock_exists or coverage_lock_exists or dev_tools_coverage_lock_exists\n    if lock_exists:\n        logger.debug(f\"Audit/coverage lock file check: audit={audit_lock_exists}, coverage={coverage_lock_exists}, dev_tools_coverage={dev_tools_coverage_lock_exists}\")\n    return lock_exists\n\n\nclass AuditOrchestrationMixin:\n    \"\"\"Mixin class providing audit orchestration methods to AIToolsService.\"\"\"\n    \n    def _get_audit_lock_file_path(self) -> Path:\n        \"\"\"Get audit lock file path (configurable via config, defaults to .audit_in_progress.lock relative to project root).\"\"\"\n        try:\n            from .. import config\n            # Default to generic path relative to project root (no development_tools/ assumption)\n            lock_file_path = config.get_external_value('paths.audit_lock_file', '.audit_in_progress.lock')\n            return self.project_root / lock_file_path\n        except (ImportError, AttributeError):\n            return self.project_root / 'development_tools' / '.audit_in_progress.lock'\n    \n    def _get_coverage_lock_file_path(self) -> Path:\n        \"\"\"Get coverage lock file path (configurable via config, defaults to .coverage_in_progress.lock relative to project root).\"\"\"\n        try:\n            from .. import config\n            # Default to generic path relative to project root (no development_tools/ assumption)\n            lock_file_path = config.get_external_value('paths.coverage_lock_file', '.coverage_in_progress.lock')\n            return self.project_root / lock_file_path\n        except (ImportError, AttributeError):\n            return self.project_root / 'development_tools' / '.coverage_in_progress.lock'\n    \n    def run_audit(self, quick: bool = False, full: bool = False, include_overlap: bool = False):\n        \"\"\"Run audit workflow with three-tier structure.\"\"\"\n        global _AUDIT_IN_PROGRESS_GLOBAL, _AUDIT_LOCK_FILE\n        \n        # Determine audit tier\n        if quick:\n            tier = 1\n            operation_name = \"audit --quick (Tier 1)\"\n        elif full:\n            tier = 3\n            operation_name = \"audit --full (Tier 3)\"\n        else:\n            tier = 2\n            operation_name = \"audit (Tier 2 - standard)\"\n        \n        # Print to console for user visibility (regardless of log level)\n        print(f\"Starting {operation_name}...\")\n        print(\"=\" * 50)\n        logger.info(f\"Starting {operation_name}...\")\n        logger.info(\"=\" * 50)\n        \n        self.current_audit_tier = tier\n        self._audit_in_progress = True\n        _AUDIT_IN_PROGRESS_GLOBAL = True\n        # Track which tools were actually run in this audit tier\n        self._tools_run_in_current_tier = set()\n        # Track timing for each tool\n        self._tool_timings = {}\n        \n        # Create file-based lock\n        if _AUDIT_LOCK_FILE is None:\n            _AUDIT_LOCK_FILE = self._get_audit_lock_file_path()\n        try:\n            _AUDIT_LOCK_FILE.parent.mkdir(parents=True, exist_ok=True)\n            _AUDIT_LOCK_FILE.touch()\n        except Exception as e:\n            logger.warning(f\"Failed to create audit lock file: {e}\")\n        \n        initial_mtimes = _get_status_file_mtimes(self.project_root)\n        self._audit_start_mtimes = initial_mtimes\n        \n        self._include_overlap = include_overlap\n        if full and not include_overlap:\n            include_overlap = True\n            self._include_overlap = True\n        \n        success = True\n        try:\n            # Tier 1: Quick audit tools\n            print(\"Running Tier 1 tools (quick audit)...\")\n            logger.info(\"Running Tier 1 tools (quick audit)...\")\n            tier1_success = self._run_quick_audit_tools()\n            if not tier1_success:\n                success = False\n            \n            # Tier 2: Standard audit tools\n            if tier >= 2:\n                print(\"Running Tier 2 tools (standard audit)...\")\n                logger.info(\"Running Tier 2 tools (standard audit)...\")\n                tier2_success = self._run_standard_audit_tools()\n                if not tier2_success:\n                    success = False\n            \n            # Tier 3: Full audit tools\n            if tier >= 3:\n                print(\"Running Tier 3 tools (full audit)...\")\n                logger.info(\"Running Tier 3 tools (full audit)...\")\n                tier3_success = self._run_full_audit_tools()\n                if not tier3_success:\n                    success = False\n        except Exception as e:\n            print(f\"ERROR: Error during audit execution: {e}\")\n            logger.error(f\"Error during audit execution: {e}\", exc_info=True)\n            success = False\n        \n        # Save all tool results\n        try:\n            self._save_audit_results_aggregated(tier)\n        except Exception as e:\n            logger.warning(f\"Failed to save aggregated audit results: {e}\", exc_info=True)\n        \n        # Reload cache data\n        self._reload_all_cache_data()\n        \n        # Sync TODO.md with changelog\n        self._sync_todo_with_changelog()\n        \n        # Validate referenced paths\n        try:\n            self._validate_referenced_paths()\n        except Exception as e:\n            logger.warning(f\"Path validation failed (non-blocking): {e}\")\n        \n        # Generate status files\n        if self.current_audit_tier is None:\n            logger.warning(f\"current_audit_tier is None at end of audit! Setting to tier {tier}\")\n        \n        try:\n            pre_final_mtimes = _get_status_file_mtimes(self.project_root)\n            if hasattr(self, '_audit_start_mtimes'):\n                for file_name, mtime in pre_final_mtimes.items():\n                    if mtime > self._audit_start_mtimes.get(file_name, 0):\n                        logger.warning(f\"Status file {file_name} was modified during audit!\")\n            \n            was_audit_in_progress = _AUDIT_IN_PROGRESS_GLOBAL\n            _AUDIT_IN_PROGRESS_GLOBAL = False\n            \n            if _AUDIT_LOCK_FILE and _AUDIT_LOCK_FILE.exists():\n                try:\n                    _AUDIT_LOCK_FILE.unlink()\n                except Exception as e:\n                    logger.warning(f\"Failed to temporarily remove audit lock file: {e}\")\n            \n            try:\n                # Generate status documents\n                # Get status file paths from config\n                try:\n                    from .. import config\n                    status_config = config.get_status_config()\n                    status_files_config = status_config.get('status_files', {})\n                    # Use default from STATUS config if status_files_config is empty (matches default config)\n                    if not status_files_config:\n                        # Fallback to default STATUS config values for backward compatibility\n                        from ...config.config import STATUS\n                        status_files_config = STATUS.get('status_files', {})\n                    ai_status_path = status_files_config.get('ai_status', 'development_tools/AI_STATUS.md')\n                    ai_priorities_path = status_files_config.get('ai_priorities', 'development_tools/AI_PRIORITIES.md')\n                    consolidated_report_path = status_files_config.get('consolidated_report', 'development_tools/consolidated_report.txt')\n                except (ImportError, AttributeError, KeyError):\n                    # Fallback to default STATUS config values for backward compatibility\n                    try:\n                        from ...config.config import STATUS\n                        status_files_default = STATUS.get('status_files', {})\n                        ai_status_path = status_files_default.get('ai_status', 'development_tools/AI_STATUS.md')\n                        ai_priorities_path = status_files_default.get('ai_priorities', 'development_tools/AI_PRIORITIES.md')\n                        consolidated_report_path = status_files_default.get('consolidated_report', 'development_tools/consolidated_report.txt')\n                    except (ImportError, AttributeError):\n                        # Last resort fallback\n                        ai_status_path = 'development_tools/AI_STATUS.md'\n                        ai_priorities_path = 'development_tools/AI_PRIORITIES.md'\n                        consolidated_report_path = 'development_tools/consolidated_report.txt'\n                \n                try:\n                    ai_status = self._generate_ai_status_document()\n                except Exception as e:\n                    logger.warning(f\"Error generating AI_STATUS document: {e}\")\n                    ai_status = \"# AI Status\\n\\nError generating status document.\"\n                ai_status_file = create_output_file(ai_status_path, ai_status, project_root=self.project_root)\n                \n                try:\n                    ai_priorities = self._generate_ai_priorities_document()\n                except Exception as e:\n                    logger.warning(f\"Error generating AI_PRIORITIES document: {e}\")\n                    ai_priorities = \"# AI Priorities\\n\\nError generating priorities document.\"\n                ai_priorities_file = create_output_file(ai_priorities_path, ai_priorities, project_root=self.project_root)\n                \n                try:\n                    consolidated_report = self._generate_consolidated_report()\n                except Exception as e:\n                    logger.warning(f\"Error generating consolidated report: {e}\")\n                    consolidated_report = \"Error generating consolidated report.\"\n                consolidated_file = create_output_file(consolidated_report_path, consolidated_report, project_root=self.project_root)\n                \n                post_final_mtimes = _get_status_file_mtimes(self.project_root)\n                for file_name, mtime in post_final_mtimes.items():\n                    if mtime <= pre_final_mtimes.get(file_name, 0):\n                        logger.warning(f\"Status file {file_name} mtime did not change during final write!\")\n            finally:\n                _AUDIT_IN_PROGRESS_GLOBAL = was_audit_in_progress\n            \n            # Additional checks\n            try:\n                self._check_and_trim_changelog_entries()\n            except Exception as e:\n                logger.warning(f\"Changelog trim check failed (non-blocking): {e}\")\n            \n            try:\n                self._check_documentation_quality()\n            except Exception as e:\n                logger.warning(f\"Documentation quality check failed (non-blocking): {e}\")\n            \n            try:\n                self._check_ascii_compliance()\n            except Exception as e:\n                logger.warning(f\"ASCII compliance check failed (non-blocking): {e}\")\n            \n            print(\"=\" * 50)\n            logger.info(\"=\" * 50)\n            if success:\n                print(f\"Completed {operation_name} successfully!\")\n                logger.info(f\"Completed {operation_name} successfully!\")\n                # Log timing summary\n                if self._tool_timings:\n                    total_time = sum(self._tool_timings.values())\n                    timing_msg = f\"Total tool execution time: {total_time:.2f}s\"\n                    print(f\"  {timing_msg}\")\n                    logger.info(timing_msg)\n                    # Log slowest tools\n                    sorted_timings = sorted(self._tool_timings.items(), key=lambda x: x[1], reverse=True)\n                    if len(sorted_timings) > 0:\n                        slowest_msg = f\"Slowest tools: {', '.join(f'{name} ({time:.2f}s)' for name, time in sorted_timings[:5])}\"\n                        print(f\"  {slowest_msg}\")\n                        logger.info(slowest_msg)\n                print(f\"  * AI Status: {ai_status_file}\")\n                print(f\"  * AI Priorities: {ai_priorities_file}\")\n                print(f\"  * Consolidated Report: {consolidated_file}\")\n                logger.info(f\"* AI Status: {ai_status_file}\")\n                logger.info(f\"* AI Priorities: {ai_priorities_file}\")\n                logger.info(f\"* Consolidated Report: {consolidated_file}\")\n            else:\n                print(f\"Completed {operation_name} with some errors\")\n                logger.warning(f\"Completed {operation_name} with some errors\")\n        except Exception as e:\n            print(f\"ERROR: Error generating status files: {e}\")\n            logger.error(f\"Error generating status files: {e}\", exc_info=True)\n            success = False\n        finally:\n            # Clear flags and remove lock file\n            self._audit_in_progress = False\n            self.current_audit_tier = None\n            self._tools_run_in_current_tier = set()  # Clear tracking when audit completes\n            # Clear results_cache to prevent stale data from being used in next audit\n            if hasattr(self, 'results_cache'):\n                self.results_cache = {}\n            # Save timing data for analysis\n            if hasattr(self, '_tool_timings') and self._tool_timings:\n                self._save_timing_data(tier)\n            _AUDIT_IN_PROGRESS_GLOBAL = False\n            if _AUDIT_LOCK_FILE and _AUDIT_LOCK_FILE.exists():\n                try:\n                    _AUDIT_LOCK_FILE.unlink()\n                except Exception as e:\n                    logger.warning(f\"Failed to remove audit lock file: {e}\")\n        \n        return success\n    \n    def run_quick_audit(self) -> bool:\n        \"\"\"Run quick audit (Tier 1 only).\"\"\"\n        return self.run_audit(quick=True)\n    \n    def _run_quick_audit_tools(self) -> bool:\n        \"\"\"Run Tier 1 tools: Quick audit (core metrics only, \u22642s per tool).\n        \n        Note: Tools moved here based on execution time (\u22642s) while respecting dependencies.\n        \"\"\"\n        successful = []\n        failed = []\n        \n        # Core Tier 1 tools (\u22642s)\n        tier1_core_tools = [\n            ('analyze_system_signals', self.run_analyze_system_signals),  # 1.07s\n        ]\n        \n        # Independent tools (\u22642s)\n        tier1_independent_tools = [\n            ('analyze_documentation', lambda: self.run_analyze_documentation(include_overlap=getattr(self, '_include_overlap', False))),  # 0.21s\n            ('analyze_config', lambda: self.run_script('analyze_config')),  # 0.93s\n            ('analyze_ai_work', self.run_validate),  # 0.95s\n        ]\n        \n        # Dependent groups (all tools \u22642s)\n        tier1_dependent_groups = [\n            # Function patterns group: depends on analyze_functions (runs in Tier 2)\n            [\n                ('analyze_function_patterns', self.run_analyze_function_patterns),  # 1.79s\n            ],\n            # Decision support group: depends on analyze_functions (runs in Tier 2)\n            [\n                ('decision_support', self.run_decision_support),  # 1.96s\n            ],\n        ]\n        \n        tier1_tools = tier1_core_tools\n        \n        # Run core tools first (analyze_functions must run before dependent tools)\n        for tool_name, tool_func in tier1_core_tools:\n            try:\n                # Time tool execution\n                start_time = time.time()\n                # Note: Tools log their own execution, so no need to log here\n                result = tool_func()\n                elapsed_time = time.time() - start_time\n                self._tool_timings[tool_name] = elapsed_time\n                logger.debug(f\"  - {tool_name} completed in {elapsed_time:.2f}s\")\n                if isinstance(result, dict):\n                    success = result.get('success', False)\n                    if 'data' in result:\n                        self._extract_key_info(tool_name, result)\n                else:\n                    success = bool(result)\n                if success:\n                    successful.append(tool_name)\n                    self._tools_run_in_current_tier.add(tool_name)\n                    # Note: Tools save their own results, so no need to save here\n                    # Removed duplicate save_tool_result call to prevent duplicate logging\n                else:\n                    failed.append(tool_name)\n                    logger.warning(f\"[TOOL FAILURE] {tool_name} execution failed - reports may use cached/fallback data\")\n            except Exception as exc:\n                failed.append(tool_name)\n                logger.error(f\"  - {tool_name} failed: {exc}\", exc_info=True)\n                logger.warning(f\"[TOOL FAILURE] {tool_name} execution failed - reports may use cached/fallback data\")\n        \n        # Run independent tools and dependent groups in parallel\n        from concurrent.futures import ThreadPoolExecutor, as_completed\n        \n        def run_tool_group(group_tools):\n            \"\"\"Run a group of tools sequentially and return results.\"\"\"\n            group_results = {}\n            for tool_name, tool_func in group_tools:\n                try:\n                    result, elapsed_time = self._run_tool_with_timing(tool_name, tool_func)\n                    group_results[tool_name] = (result, elapsed_time)\n                except Exception as exc:\n                    logger.error(f\"  - {tool_name} failed: {exc}\", exc_info=True)\n                    group_results[tool_name] = ({'success': False, 'error': str(exc)}, 0.0)\n            return group_results\n        \n        # Combine independent tools (as single-tool groups) and dependent groups\n        all_groups = [[(name, func)] for name, func in tier1_independent_tools] + tier1_dependent_groups\n        max_workers = min(4, len(all_groups))\n        \n        logger.debug(f\"Running Tier 1 additional tools: {len(tier1_independent_tools)} independent + {len(tier1_dependent_groups)} groups with {max_workers} parallel workers...\")\n        \n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            future_to_group = {\n                executor.submit(run_tool_group, group): i\n                for i, group in enumerate(all_groups)\n            }\n            \n            for future in as_completed(future_to_group):\n                group_results = future.result()\n                for tool_name, (result, elapsed_time) in group_results.items():\n                    self._tool_timings[tool_name] = elapsed_time\n                    logger.debug(f\"  - {tool_name} completed in {elapsed_time:.2f}s\")\n                    if isinstance(result, dict):\n                        success = result.get('success', False)\n                        if 'data' in result:\n                            self._extract_key_info(tool_name, result)\n                    else:\n                        success = bool(result)\n                    \n                    if success:\n                        successful.append(tool_name)\n                        self._tools_run_in_current_tier.add(tool_name)\n                    else:\n                        failed.append(tool_name)\n                        logger.warning(f\"[TOOL FAILURE] {tool_name} execution failed - reports may use cached/fallback data\")\n        \n        # Run quick_status at the end of Tier 1 (after other tools have run and potentially created results)\n        # This allows it to use fresh data from the current audit run, but it still works gracefully if data is missing\n        try:\n            # Time quick_status execution\n            start_time = time.time()\n            # Note: quick_status logs its own execution (\"Generating JSON status output\")\n            quick_status_result = self.run_script('quick_status', 'json')\n            elapsed_time = time.time() - start_time\n            self._tool_timings['quick_status'] = elapsed_time\n            logger.debug(f\"  - quick_status completed in {elapsed_time:.2f}s\")\n            if quick_status_result.get('success'):\n                self.status_results = quick_status_result\n                output = quick_status_result.get('output', '')\n                if output:\n                    try:\n                        parsed = json.loads(output)\n                        self.status_summary = parsed\n                        quick_status_result['data'] = parsed\n                        try:\n                            save_tool_result('quick_status', 'reports', parsed, project_root=self.project_root)\n                        except Exception as e:\n                            logger.debug(f\"Failed to save quick_status result: {e}\")\n                        successful.append('quick_status')\n                        self._tools_run_in_current_tier.add('quick_status')\n                    except json.JSONDecodeError:\n                        logger.warning(\"  - quick_status output could not be parsed as JSON\")\n                        failed.append('quick_status')\n                else:\n                    failed.append('quick_status')\n            else:\n                failed.append('quick_status')\n        except Exception as exc:\n            failed.append('quick_status')\n            logger.error(f\"  - quick_status failed: {exc}\")\n        \n        if failed:\n            logger.warning(f\"Tier 1 completed with {len(failed)} failure(s): {', '.join(failed)}\")\n        else:\n            logger.info(f\"Tier 1 completed successfully ({len(successful)} tools)\")\n        \n        return len(failed) == 0\n    \n    def _run_standard_audit_tools(self) -> bool:\n        \"\"\"Run Tier 2 tools: Standard audit (quality checks, >2s but \u226410s per tool).\n        \n        Note: Tools moved here based on execution time (>2s but \u226410s) while respecting dependencies.\n        \"\"\"\n        successful = []\n        failed = []\n        \n        # Independent tools (>2s but \u226410s)\n        tier2_independent_tools = [\n            ('analyze_functions', self.run_analyze_functions),  # 3.41s\n            ('analyze_error_handling', self.run_analyze_error_handling),  # 3.06s\n            ('analyze_package_exports', self.run_analyze_package_exports),  # 9.06s\n        ]\n        \n        # Dependent groups (>2s but \u226410s)\n        tier2_dependent_groups = [\n            # Module imports group: analyze_module_imports \u2192 analyze_dependency_patterns, analyze_module_dependencies\n            [\n                ('analyze_module_imports', self.run_analyze_module_imports),  # 2.18s\n                ('analyze_dependency_patterns', self.run_analyze_dependency_patterns),  # 2.03s\n                ('analyze_module_dependencies', self.run_analyze_module_dependencies),  # 5.94s\n            ],\n            # Function registry group: analyze_function_registry validates generate_function_registry output\n            [\n                ('analyze_function_registry', self.run_analyze_function_registry),  # 2.12s\n            ],\n            # Documentation sync group: includes multiple sub-tools\n            [\n                ('analyze_documentation_sync', self.run_analyze_documentation_sync),  # 7.70s\n            ],\n            # Unused imports group: moved from Tier 3 (both \u226410s)\n            [\n                ('analyze_unused_imports', self.run_unused_imports),  # 7.82s\n                ('generate_unused_imports_report', self.run_generate_unused_imports_report),  # 0.98s\n            ],\n        ]\n        \n        # Run independent tools and dependent groups in parallel\n        from concurrent.futures import ThreadPoolExecutor, as_completed\n        \n        def run_tool_group(group_tools):\n            \"\"\"Run a group of tools sequentially and return results.\"\"\"\n            group_results = {}\n            for tool_name, tool_func in group_tools:\n                try:\n                    result, elapsed_time = self._run_tool_with_timing(tool_name, tool_func)\n                    group_results[tool_name] = (result, elapsed_time)\n                except Exception as exc:\n                    logger.error(f\"  - {tool_name} failed: {exc}\", exc_info=True)\n                    group_results[tool_name] = ({'success': False, 'error': str(exc)}, 0.0)\n            return group_results\n        \n        # Combine independent tools (as single-tool groups) and dependent groups\n        all_groups = [[(name, func)] for name, func in tier2_independent_tools] + tier2_dependent_groups\n        max_workers = min(4, len(all_groups))\n        \n        logger.debug(f\"Running Tier 2 tools: {len(tier2_independent_tools)} independent + {len(tier2_dependent_groups)} groups with {max_workers} parallel workers...\")\n        \n        all_results = {}\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            future_to_group = {\n                executor.submit(run_tool_group, group): i\n                for i, group in enumerate(all_groups)\n            }\n            \n            for future in as_completed(future_to_group):\n                group_results = future.result()\n                for tool_name, (result, elapsed_time) in group_results.items():\n                    all_results[tool_name] = result\n                    self._tool_timings[tool_name] = elapsed_time\n                    logger.debug(f\"  - {tool_name} completed in {elapsed_time:.2f}s\")\n        \n        # Process results\n        for tool_name, result in all_results.items():\n            if isinstance(result, dict):\n                success = result.get('success', False)\n                if 'data' in result:\n                    self._extract_key_info(tool_name, result)\n            else:\n                success = bool(result)\n            \n            if success:\n                successful.append(tool_name)\n                self._tools_run_in_current_tier.add(tool_name)\n            else:\n                failed.append(tool_name)\n                logger.warning(f\"[TOOL FAILURE] {tool_name} execution failed - reports may use cached/fallback data\")\n        \n        if failed:\n            logger.warning(f\"Tier 2 completed with {len(failed)} failure(s): {', '.join(failed)}\")\n        else:\n            logger.info(f\"Tier 2 completed successfully ({len(successful)} tools)\")\n        \n        return len(failed) == 0\n    \n    def _run_tool_with_timing(self, tool_name: str, tool_func) -> tuple:\n        \"\"\"Run a tool and return (result, elapsed_time) tuple.\"\"\"\n        start_time = time.time()\n        try:\n            result = tool_func()\n            elapsed_time = time.time() - start_time\n            return result, elapsed_time\n        except Exception as e:\n            elapsed_time = time.time() - start_time\n            raise\n    \n    def _run_full_audit_tools(self) -> bool:\n        \"\"\"Run Tier 3 tools: Full audit (comprehensive analysis, >10s per tool or groups with >10s tools).\n        \n        Note: Tools moved here based on execution time (>10s) while respecting dependencies:\n        - Coverage tools: run_test_coverage (365.45s) and generate_dev_tools_coverage (94.23s) run in parallel (independent test suites)\n        - Coverage-dependent tools: analyze_test_markers and generate_test_coverage_report run sequentially after coverage completes\n        - Legacy group: analyze_legacy_references (62.11s) is >10s, so entire group stays in Tier 3\n        \"\"\"\n        successful = []\n        failed = []\n        \n        # Coverage tool groups - run in parallel (independent test suites with separate coverage files)\n        # run_test_coverage (365.45s) and generate_dev_tools_coverage (94.23s) are >10s, so entire group stays in Tier 3\n        tier3_coverage_main_group = [\n            ('run_test_coverage', self.run_coverage_regeneration),  # 365.45s\n        ]\n        tier3_coverage_dev_tools_group = [\n            ('generate_dev_tools_coverage', self.run_dev_tools_coverage),  # 94.23s\n        ]\n        \n        # Coverage-dependent tools - must run sequentially after both coverage tools complete\n        tier3_coverage_dependent_group = [\n            ('analyze_test_markers', lambda: self.run_test_markers('check')),  # 1.57s\n            ('generate_test_coverage_report', self.run_generate_test_coverage_report),  # ~5s\n        ]\n        \n        # Legacy group - analyze_legacy_references (62.11s) is >10s, so entire group stays in Tier 3\n        tier3_legacy_group = [\n            ('analyze_legacy_references', self.run_analyze_legacy_references),  # 62.11s\n            ('generate_legacy_reference_report', self.run_generate_legacy_reference_report),  # 0.96s\n        ]\n        \n        # Independent groups that can run in parallel with each other\n        tier3_parallel_groups = [\n            tier3_coverage_main_group,\n            tier3_coverage_dev_tools_group,\n            tier3_legacy_group,\n        ]\n        \n        # Run coverage and legacy groups in parallel (each group runs its tools sequentially)\n        if tier3_parallel_groups:\n            from concurrent.futures import ThreadPoolExecutor, as_completed\n            import time\n            \n            logger.debug(f\"Running {len(tier3_parallel_groups)} independent tool groups in parallel...\")\n            \n            # Track parallel execution start time for accurate wall-clock measurement\n            parallel_start_time = time.time()\n            \n            def run_tool_group(group_tools):\n                \"\"\"Run a group of tools sequentially and return results.\"\"\"\n                group_results = {}\n                for tool_name, tool_func in group_tools:\n                    try:\n                        result, elapsed_time = self._run_tool_with_timing(tool_name, tool_func)\n                        group_results[tool_name] = (result, elapsed_time)\n                    except Exception as exc:\n                        logger.error(f\"  - {tool_name} failed: {exc}\", exc_info=True)\n                        group_results[tool_name] = ({'success': False, 'error': str(exc)}, 0.0)\n                return group_results\n            \n            with ThreadPoolExecutor(max_workers=len(tier3_parallel_groups)) as executor:\n                future_to_group = {\n                    executor.submit(run_tool_group, group): i\n                    for i, group in enumerate(tier3_parallel_groups)\n                }\n                \n                # Track when all parallel groups complete for accurate timing\n                parallel_group_times = {}\n                for future in as_completed(future_to_group):\n                    group_results = future.result()\n                    group_end_time = time.time()\n                    group_wall_clock = group_end_time - parallel_start_time\n                    for tool_name, (result, elapsed_time) in group_results.items():\n                        self._tool_timings[tool_name] = elapsed_time\n                        parallel_group_times[tool_name] = group_wall_clock\n                        logger.debug(f\"  - {tool_name} completed in {elapsed_time:.2f}s (wall-clock: {group_wall_clock:.2f}s)\")\n                        if isinstance(result, dict):\n                            success = result.get('success', False)\n                            if 'data' in result:\n                                self._extract_key_info(tool_name, result)\n                        else:\n                            success = bool(result)\n                        if success:\n                            successful.append(tool_name)\n                            self._tools_run_in_current_tier.add(tool_name)\n                        else:\n                            failed.append(tool_name)\n                            logger.warning(f\"[TOOL FAILURE] {tool_name} execution failed - reports may use cached/fallback data\")\n            \n            # Log parallel execution summary\n            if parallel_group_times:\n                max_parallel_time = max(parallel_group_times.values())\n                sum_individual_times = sum(self._tool_timings.get(name, 0) for name in parallel_group_times.keys())\n                time_saved = sum_individual_times - max_parallel_time\n                if time_saved > 1.0:  # Only log if significant time saved\n                    logger.info(f\"Parallel execution saved ~{time_saved:.1f}s (wall-clock: {max_parallel_time:.1f}s vs sequential: {sum_individual_times:.1f}s)\")\n        \n        # Run coverage-dependent tools sequentially (they depend on coverage data from both test suites)\n        logger.debug(\"Running coverage-dependent tools (sequential, after coverage completion)...\")\n        for tool_name, tool_func in tier3_coverage_dependent_group:\n            try:\n                start_time = time.time()\n                result = tool_func()\n                elapsed_time = time.time() - start_time\n                self._tool_timings[tool_name] = elapsed_time\n                logger.debug(f\"  - {tool_name} completed in {elapsed_time:.2f}s\")\n                if isinstance(result, dict):\n                    success = result.get('success', False)\n                    if 'data' in result:\n                        self._extract_key_info(tool_name, result)\n                else:\n                    success = bool(result)\n                if success:\n                    successful.append(tool_name)\n                    self._tools_run_in_current_tier.add(tool_name)\n                else:\n                    failed.append(tool_name)\n                    logger.warning(f\"[TOOL FAILURE] {tool_name} execution failed - reports may use cached/fallback data\")\n            except Exception as exc:\n                failed.append(tool_name)\n                logger.error(f\"  - {tool_name} failed: {exc}\", exc_info=True)\n                logger.warning(f\"[TOOL FAILURE] {tool_name} execution failed - reports may use cached/fallback data\")\n        \n        if failed:\n            logger.warning(f\"Tier 3 completed with {len(failed)} failure(s): {', '.join(failed)}\")\n        else:\n            logger.info(f\"Tier 3 completed successfully ({len(successful)} tools)\")\n        \n        return len(failed) == 0\n    \n    def _save_additional_tool_results(self):\n        \"\"\"Save results from additional tools to the cached file.\"\"\"\n        try:\n            results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n            if results_file.exists():\n                with open(results_file, 'r', encoding='utf-8') as f:\n                    cached_data = json.load(f)\n            else:\n                cached_data = {'results': {}}\n            \n            if hasattr(self, 'legacy_cleanup_summary') and self.legacy_cleanup_summary:\n                cached_data['results']['fix_legacy_references'] = {\n                    'success': True,\n                    'data': self.legacy_cleanup_summary,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            if hasattr(self, 'validation_results') and self.validation_results:\n                cached_data['results']['analyze_ai_work'] = {\n                    'success': True,\n                    'data': self.validation_results,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            if hasattr(self, 'system_signals') and self.system_signals:\n                cached_data['results']['analyze_system_signals'] = {\n                    'success': True,\n                    'data': self.system_signals,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            decision_metrics = self.results_cache.get('decision_support_metrics', {})\n            if decision_metrics:\n                cached_data['results']['decision_support'] = {\n                    'success': True,\n                    'data': {'decision_support_metrics': decision_metrics},\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            if hasattr(self, 'docs_sync_summary') and self.docs_sync_summary:\n                cached_data['results']['analyze_documentation_sync'] = {\n                    'success': True,\n                    'data': self.docs_sync_summary,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            if 'analyze_documentation' in self.results_cache:\n                analyze_docs_data = self.results_cache['analyze_documentation']\n                cached_data['results']['analyze_documentation'] = {\n                    'success': True,\n                    'data': analyze_docs_data,\n                    'timestamp': datetime.now().isoformat()\n                }\n            \n            create_output_file(str(results_file), json.dumps(cached_data, indent=2), project_root=self.project_root)\n        except Exception as e:\n            logger.warning(f\"Failed to save additional tool results: {e}\")\n    \n    def _reload_all_cache_data(self):\n        \"\"\"Reload all cache data from disk.\"\"\"\n        try:\n            # Clear cache first to prevent accumulation across multiple reloads\n            # This prevents memory leaks when tests run in parallel and share project directories\n            if hasattr(self, 'results_cache'):\n                self.results_cache.clear()\n            else:\n                self.results_cache = {}\n            \n            # Skip loading all results in test directories to prevent memory issues\n            # Tests use temp_project_copy which should have minimal/no results files\n            # Loading all results is expensive and unnecessary for tests\n            project_root_str = str(self.project_root.resolve())\n            is_test_dir = self._is_test_directory(self.project_root)\n            \n            # Enhanced logging for memory leak investigation (DEBUG level to reduce verbosity in production)\n            if is_test_dir:\n                logger.debug(\n                    f\"[MEMORY-LEAK-PREVENTION] Skipping _reload_all_cache_data() in test directory\\n\"\n                    f\"  project_root: {project_root_str}\\n\"\n                    f\"  is_test_directory check: True\"\n                )\n                return\n            else:\n                logger.debug(\n                    f\"[MEMORY-LEAK-PREVENTION] NOT a test directory, proceeding with _reload_all_cache_data()\\n\"\n                    f\"  project_root: {project_root_str}\\n\"\n                    f\"  is_test_directory check: False\"\n                )\n            \n            all_results = get_all_tool_results(project_root=self.project_root)\n            if all_results:\n                logger.debug(\n                    f\"[MEMORY-LEAK-PREVENTION] Loading {len(all_results)} tool results from disk\\n\"\n                    f\"  project_root: {project_root_str}\\n\"\n                    f\"  tools: {list(all_results.keys())[:5]}{'...' if len(all_results) > 5 else ''}\"\n                )\n                for tool_name, result_data in all_results.items():\n                    if isinstance(result_data, dict):\n                        tool_data = result_data.get('data', result_data)\n                        self.results_cache[tool_name] = tool_data\n                        if tool_name == 'analyze_documentation_sync' and isinstance(tool_data, dict):\n                            self.docs_sync_summary = tool_data\n                        if tool_name == 'analyze_legacy_references' and isinstance(tool_data, dict):\n                            self.legacy_cleanup_summary = tool_data\n            else:\n                logger.debug(f\"[MEMORY-LEAK-PREVENTION] No tool results found (project_root: {project_root_str})\")\n            \n            # CRITICAL: Also skip loading analysis_detailed_results.json in test directories\n            # This file can be very large and causes memory leaks in parallel test execution\n            results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n            is_test_dir_check = self._is_test_directory(self.project_root)\n            if results_file.exists() and not is_test_dir_check:\n                file_size_mb = results_file.stat().st_size / (1024 * 1024)\n                logger.debug(\n                    f\"[MEMORY-LEAK-PREVENTION] Loading analysis_detailed_results.json\\n\"\n                    f\"  file: {results_file}\\n\"\n                    f\"  size: {file_size_mb:.2f} MB\\n\"\n                    f\"  project_root: {project_root_str}\"\n                )\n                with open(results_file, 'r', encoding='utf-8') as f:\n                    cached_data = json.load(f)\n                if 'results' in cached_data:\n                    for tool_name, tool_data in cached_data['results'].items():\n                        if tool_name not in self.results_cache and 'data' in tool_data:\n                            self.results_cache[tool_name] = tool_data['data']\n                    if 'decision_support' in cached_data['results']:\n                        ds_data = cached_data['results']['decision_support']\n                        if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                            self.results_cache['decision_support_metrics'] = ds_data['data']['decision_support_metrics']\n                if not self.docs_sync_summary and 'analyze_documentation_sync' in cached_data.get('results', {}):\n                    doc_sync_data = cached_data['results']['analyze_documentation_sync']\n                    if 'data' in doc_sync_data:\n                        self.docs_sync_summary = doc_sync_data['data']\n                if not hasattr(self, 'legacy_cleanup_summary') or not self.legacy_cleanup_summary:\n                    if 'analyze_legacy_references' in cached_data.get('results', {}):\n                        legacy_data = cached_data['results']['analyze_legacy_references']\n                        if 'data' in legacy_data:\n                            self.legacy_cleanup_summary = legacy_data['data']\n                if not hasattr(self, 'dev_tools_coverage_results') or not self.dev_tools_coverage_results:\n                    self._load_dev_tools_coverage()\n                if 'analyze_module_dependencies' in cached_data.get('results', {}):\n                    dep_data = cached_data['results']['analyze_module_dependencies']\n                    if 'data' in dep_data:\n                        self.module_dependency_summary = dep_data['data']\n        except Exception as e:\n            logger.debug(f\"Failed to reload cache data: {e}\")\n    \n    def _is_test_directory(self, path: Path) -> bool:\n        \"\"\"Check if path is within a test directory to avoid loading large result files.\n        \n        This is critical for preventing memory leaks in parallel test execution.\n        \"\"\"\n        try:\n            path_str = str(path.resolve()).replace('\\\\', '/').lower()\n            \n            # Check for Windows temp directories (most common case for pytest-xdist)\n            # Windows temp dirs are typically: C:\\Users\\...\\AppData\\Local\\Temp\\...\n            if 'appdata' in path_str and ('temp' in path_str or 'tmp' in path_str):\n                return True\n            \n            # Check for pytest temp directories (pytest-xdist creates these)\n            if 'pytest' in path_str and ('temp' in path_str or 'tmp' in path_str):\n                return True\n            \n            # Check for common temp directory patterns\n            test_indicators = [\n                '/tmp', '/temp',  # Unix-style temp\n                '/tests/', '/test/',  # Test directories\n                'tests/data/', 'tests/fixtures/', 'tests/temp/',\n                'demo_project',  # Demo project used in tests\n                'pytest-', 'pytest_of_',  # pytest temp directories\n            ]\n            if any(indicator in path_str for indicator in test_indicators):\n                return True\n            \n            # Additional check: if path contains a tempfile pattern (tmpXXXXXX)\n            import re\n            if re.search(r'[\\\\/]tmp[a-z0-9]{6,}[\\\\/]', path_str):\n                return True\n            \n            return False\n        except Exception as e:\n            # If we can't determine, be conservative and assume it's not a test directory\n            # But log it so we can debug\n            logger.debug(f\"Error checking if path is test directory ({path}): {e}\")\n            return False\n    \n    def _save_audit_results_aggregated(self, tier: int):\n        \"\"\"Save aggregated audit results from all tool result files.\"\"\"\n        # In test directories, create minimal file without loading all results from disk\n        # This prevents memory issues while still creating the file tests expect\n        is_test_dir = self._is_test_directory(self.project_root)\n        \n        if is_test_dir:\n            logger.debug(f\"Creating minimal analysis_detailed_results.json in test directory (project_root: {self.project_root})\")\n            # Use only results_cache data (from mocked tools) - don't load from disk\n            enhanced_results = {}\n            successful = []\n            failed = []\n            \n            for tool_name, data in self.results_cache.items():\n                enhanced_results[tool_name] = {\n                    'success': True,\n                    'data': data,\n                    'timestamp': datetime.now().isoformat()\n                }\n                successful.append(tool_name)\n        else:\n            # Real project: Load all results from disk (normal behavior)\n            # Retry logic to handle race conditions where files may not be written yet\n            # This is especially important in test scenarios where file system operations\n            # may not be immediately synchronized\n            import time\n            all_results = {}\n            max_retries = 3\n            initial_delay = 0.05\n            \n            for attempt in range(max_retries):\n                all_results = get_all_tool_results(project_root=self.project_root)\n                # If we found results or this is the last attempt, proceed\n                if all_results or attempt == max_retries - 1:\n                    break\n                # Small delay before retry to allow file system to sync\n                time.sleep(initial_delay * (attempt + 1))\n            \n            # Log warning if no results found (but don't fail - some tools may not produce results)\n            if not all_results:\n                logger.debug(\"No tool results found during aggregation (this may be normal if no tools ran)\")\n            \n            enhanced_results = {}\n            successful = []\n            failed = []\n            \n            for tool_name, result_data in all_results.items():\n                if isinstance(result_data, dict):\n                    tool_data = result_data.get('data', result_data)\n                    enhanced_results[tool_name] = {\n                        'success': True,\n                        'data': tool_data,\n                        'timestamp': result_data.get('timestamp', datetime.now().isoformat())\n                    }\n                    successful.append(tool_name)\n                else:\n                    enhanced_results[tool_name] = {\n                        'success': False,\n                        'data': {},\n                        'error': 'Invalid result format'\n                    }\n                    failed.append(tool_name)\n            \n            # Also include results_cache data (from current audit run)\n            for tool_name, data in self.results_cache.items():\n                if tool_name not in enhanced_results:\n                    enhanced_results[tool_name] = {\n                        'success': True,\n                        'data': data,\n                        'timestamp': datetime.now().isoformat()\n                    }\n                    if tool_name not in successful:\n                        successful.append(tool_name)\n        \n        if tier == 1:\n            source_cmd = 'python development_tools/run_development_tools.py audit --quick'\n        elif tier == 3:\n            source_cmd = 'python development_tools/run_development_tools.py audit --full'\n        else:\n            source_cmd = 'python development_tools/run_development_tools.py audit'\n        \n        timestamp_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        timestamp_iso = datetime.now().isoformat()\n        audit_data = {\n            'generated_by': 'run_development_tools.py - AI Development Tools Runner',\n            'last_generated': timestamp_str,\n            'source': source_cmd,\n            'audit_tier': tier,\n            'note': 'This file is auto-generated. Do not edit manually.',\n            'timestamp': timestamp_iso,\n            'successful': successful,\n            'failed': failed,\n            'results': enhanced_results\n        }\n        \n        results_file_path = self.audit_config.get('results_file', 'development_tools/reports/analysis_detailed_results.json')\n        # Use relative path for create_output_file to ensure proper path resolution\n        # create_output_file handles project_root resolution internally\n        try:\n            # Always create the file, even if results are empty (for test scenarios with mocked tools)\n            json_content = json.dumps(audit_data, indent=2)\n            created_file = create_output_file(results_file_path, json_content, project_root=self.project_root)\n            # Ensure file is flushed to disk and verify it exists\n            if created_file:\n                # Wait a moment for file system to sync (especially important on Windows)\n                import time\n                time.sleep(0.05)\n                if created_file.exists() and created_file.stat().st_size > 0:\n                    logger.debug(f\"Saved aggregated audit results to {created_file}\")\n                else:\n                    logger.warning(f\"File {created_file} was created but doesn't exist or is empty. Path: {created_file.absolute()}\")\n            else:\n                logger.error(f\"create_output_file returned None for {results_file_path}\")\n        except Exception as e:\n            logger.error(f\"Failed to create analysis_detailed_results.json: {e}\", exc_info=True)\n            # Don't raise - allow audit to complete even if results file can't be saved\n            # This prevents test failures when file system operations fail\n    \n    def _generate_audit_report(self):\n        \"\"\"Generate comprehensive audit report.\"\"\"\n        # This method can be implemented if needed\n        # For now, audit reports are generated via the status document methods\n        pass\n    \n    def _check_and_trim_changelog_entries(self) -> None:\n        \"\"\"Check and trim AI_CHANGELOG entries to prevent bloat.\"\"\"\n        try:\n            from ai_development_docs import changelog_manager\n        except Exception:\n            changelog_manager = None\n        if changelog_manager and hasattr(changelog_manager, 'trim_change_log'):\n            try:\n                result = changelog_manager.trim_change_log()\n                if isinstance(result, dict):\n                    trimmed = result.get('trimmed_entries')\n                    archive_created = result.get('archive_created')\n                    if trimmed:\n                        logger.info(f\"   Trimmed {trimmed} old changelog entries\")\n                    if archive_created:\n                        logger.info(\"   Created archive: development_tools/reports/archive/AI_CHANGELOG_ARCHIVE.md\")\n            except Exception as exc:\n                logger.warning(f\"   Changelog check/trim failed: {exc}\")\n        else:\n            logger.warning(\"   Changelog check: Tooling unavailable (skipping trim)\")\n    \n    def _validate_referenced_paths(self) -> None:\n        \"\"\"Validate that all referenced paths in documentation exist.\"\"\"\n        try:\n            from development_tools.docs.fix_version_sync import validate_referenced_paths\n            result = validate_referenced_paths()\n            status = result.get('status') if isinstance(result, dict) else None\n            message = result.get('message') if isinstance(result, dict) else None\n            if isinstance(result, dict):\n                self.path_validation_result = result\n            if status == 'ok':\n                logger.info(f\"   Path validation: {message}\")\n            elif status == 'fail':\n                issues = result.get('issues_found', 'unknown') if isinstance(result, dict) else 'unknown'\n                logger.info(f\"   Path validation: {message}\")\n                logger.info(f\"   Found {issues} path issues\")\n        except Exception as exc:\n            logger.warning(f\"   Path validation failed: {exc}\")\n            self.path_validation_result = None\n    \n    def _check_documentation_quality(self) -> None:\n        \"\"\"Check for documentation duplicates and placeholder content.\"\"\"\n        try:\n            data = self.results_cache.get('analyze_documentation')\n            if not isinstance(data, dict):\n                result = self.run_analyze_documentation()\n                data = result.get('data') if isinstance(result, dict) else None\n                if isinstance(data, dict):\n                    self.results_cache['analyze_documentation'] = data\n            if isinstance(data, dict):\n                duplicates = data.get('duplicates') or []\n                placeholders = data.get('placeholders') or []\n                if duplicates:\n                    logger.warning(f\"   Documentation quality: Found {len(duplicates)} verbatim duplicates\")\n                else:\n                    logger.info(\"   Documentation quality: No verbatim duplicates found\")\n                if placeholders:\n                    logger.warning(f\"   Documentation quality: Found {len(placeholders)} files with placeholders\")\n                else:\n                    logger.info(\"   Documentation quality: No placeholder content found\")\n        except Exception as exc:\n            logger.warning(f\"   Documentation quality check failed: {exc}\")\n    \n    def _check_ascii_compliance(self) -> None:\n        \"\"\"Check for non-ASCII characters in documentation files.\"\"\"\n        try:\n            from development_tools.docs.analyze_ascii_compliance import ASCIIComplianceAnalyzer\n            analyzer = ASCIIComplianceAnalyzer()\n            results = analyzer.check_ascii_compliance()\n            total_issues = sum(len(issues) for issues in results.values())\n            files_with_issues = len(results)\n            if total_issues == 0:\n                logger.info(\"   ASCII compliance: All documentation files use ASCII-only characters\")\n                if not hasattr(self, 'docs_sync_summary') or not self.docs_sync_summary:\n                    self.docs_sync_summary = {}\n                self.docs_sync_summary['ascii_issues'] = 0\n            else:\n                logger.info(f\"   ASCII compliance: Found {total_issues} non-ASCII characters in {files_with_issues} files\")\n                if not hasattr(self, 'docs_sync_summary') or not self.docs_sync_summary:\n                    self.docs_sync_summary = {}\n                self.docs_sync_summary['ascii_issues'] = files_with_issues\n        except Exception as exc:\n            logger.warning(f\"   ASCII compliance check failed: {exc}\")\n    \n    def _save_timing_data(self, tier: int) -> None:\n        \"\"\"Save timing data to a JSON file for analysis.\"\"\"\n        try:\n            timing_file = self.project_root / 'development_tools' / 'reports' / 'tool_timings.json'\n            timing_file.parent.mkdir(parents=True, exist_ok=True)\n            \n            # Load existing timing data\n            existing_data = {}\n            if timing_file.exists():\n                try:\n                    with open(timing_file, 'r', encoding='utf-8') as f:\n                        existing_data = json.load(f)\n                except (json.JSONDecodeError, OSError):\n                    existing_data = {}\n            \n            # Add new timing entry\n            timestamp = datetime.now().isoformat()\n            tier_name = {1: 'quick', 2: 'standard', 3: 'full'}.get(tier, 'unknown')\n            \n            if 'runs' not in existing_data:\n                existing_data['runs'] = []\n            \n            existing_data['runs'].append({\n                'timestamp': timestamp,\n                'tier': tier_name,\n                'tier_number': tier,\n                'tool_timings': self._tool_timings.copy(),\n                'total_time': sum(self._tool_timings.values())\n            })\n            \n            # Keep only last 50 runs\n            if len(existing_data['runs']) > 50:\n                existing_data['runs'] = existing_data['runs'][-50:]\n            \n            # Save updated timing data\n            with open(timing_file, 'w', encoding='utf-8') as f:\n                json.dump(existing_data, f, indent=2)\n        except Exception as e:\n            logger.debug(f\"Failed to save timing data: {e}\")\n    \n    def _sync_todo_with_changelog(self) -> None:\n        \"\"\"Sync TODO.md with AI_CHANGELOG.md to move completed entries.\"\"\"\n        try:\n            from development_tools.docs.fix_version_sync import sync_todo_with_changelog\n            result = sync_todo_with_changelog()\n            if isinstance(result, dict):\n                self.todo_sync_result = result\n                status = result.get('status')\n                if status == 'ok':\n                    moved = result.get('moved_entries', 0)\n                    if moved > 0:\n                        logger.info(f\"   TODO sync: Moved {moved} completed entries to changelog\")\n                    else:\n                        logger.info(\"   TODO sync: No completed entries to move\")\n                else:\n                    message = result.get('message', 'Unknown error')\n                    logger.warning(f\"   TODO sync: {message}\")\n        except Exception as exc:\n            logger.warning(f\"   TODO sync failed: {exc}\")\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 46,
                    "line_content": "# Fallback to default STATUS config values for backward compatibility",
                    "start": 1765,
                    "end": 1787
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 247,
                    "line_content": "# Fallback to default STATUS config values for backward compatibility",
                    "start": 11221,
                    "end": 11243
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 254,
                    "line_content": "# Fallback to default STATUS config values for backward compatibility",
                    "start": 11879,
                    "end": 11901
                  }
                ]
              ],
              [
                "development_tools\\shared\\service\\commands.py",
                "\"\"\"\nCommand execution methods for AIToolsService.\n\nContains methods for executing various CLI commands (docs, validate, config, etc.)\n\"\"\"\n\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\n\nfrom core.logger import get_component_logger\n\nlogger = get_component_logger(\"development_tools\")\n\n# Import output storage\nfrom ..output_storage import save_tool_result\n\n\nclass CommandsMixin:\n    \"\"\"Mixin class providing command execution methods to AIToolsService.\"\"\"\n    \n    def run_docs(self):\n        \"\"\"Update all documentation (OPTIONAL - not essential for audit)\"\"\"\n        logger.info(\"Starting documentation update...\")\n        logger.info(\"Updating documentation...\")\n        logger.info(\"=\" * 50)\n        success = True\n        \n        # Generate function registry\n        try:\n            logger.info(\"  - Generating function registry...\")\n            result = self.run_script(\"generate_function_registry\")\n            if result['success']:\n                logger.info(\"  - Function registry generated successfully\")\n            else:\n                logger.error(f\"  - Function registry generation failed: {result['error']}\")\n                success = False\n        except Exception as exc:\n            logger.error(f\"  - Function registry generation failed: {exc}\")\n            success = False\n        \n        # Generate module dependencies\n        try:\n            logger.info(\"  - Generating module dependencies...\")\n            result = self.run_script(\"generate_module_dependencies\")\n            if result['success']:\n                logger.info(\"  - Module dependencies generated successfully\")\n            else:\n                logger.error(f\"  - Module dependencies generation failed: {result['error']}\")\n                success = False\n        except Exception as exc:\n            logger.error(f\"  - Module dependencies generation failed: {exc}\")\n            success = False\n        \n        # Generate directory trees\n        # NOTE: Static documentation (DIRECTORY_TREE, FUNCTION_REGISTRY, MODULE_DEPENDENCIES)\n        # should ONLY be generated via the 'docs' command, NOT during audits.\n        try:\n            logger.info(\"  - Generating directory trees...\")\n            self.generate_directory_trees()\n        except Exception as exc:\n            logger.error(f\"  - Directory tree generation failed: {exc}\")\n            success = False\n        \n        # Run documentation sync check\n        try:\n            logger.info(\"  - Checking documentation sync...\")\n            if not self._run_doc_sync_check('--check'):\n                success = False\n        except Exception as exc:\n            logger.error(f\"  - Documentation sync check failed: {exc}\")\n            success = False\n        \n        logger.info(\"=\" * 50)\n        if success:\n            logger.info(\"Completed documentation update successfully!\")\n        else:\n            logger.warning(\"Completed documentation update with issues.\")\n        return success\n    \n    def run_validate(self):\n        \"\"\"Validate AI-generated work (simple command)\"\"\"\n        logger.info(\"Analyzing AI work...\")\n        logger.info(\"=\" * 50)\n        # Use --json flag to prevent multiline print output from being captured\n        result = self.run_script('analyze_ai_work', '--json')\n        if result['success']:\n            self.validation_results = result\n            try:\n                data = result.get('data')\n                if not data and result.get('output'):\n                    try:\n                        import json\n                        data = json.loads(result.get('output', ''))\n                    except (json.JSONDecodeError, TypeError):\n                        data = {\n                            'success': result.get('success', False),\n                            'output': result.get('output', ''),\n                            'error': result.get('error', ''),\n                            'returncode': result.get('returncode', 0)\n                        }\n                if data:\n                    save_tool_result('analyze_ai_work', 'ai_work', data, project_root=self.project_root)\n            except Exception as e:\n                logger.debug(f\"Failed to save analyze_ai_work results: {e}\")\n            logger.info(\"=\" * 50)\n            logger.info(\"Validation completed successfully!\")\n            return True\n        else:\n            logger.error(f\"Validation failed: {result['error']}\")\n            return False\n    \n    def run_config(self):\n        \"\"\"Check configuration consistency (simple command)\"\"\"\n        logger.info(\"Running analyze_config...\")\n        logger.info(\"=\" * 50)\n        result = self.run_script('analyze_config')\n        if result['success']:\n            output = result.get('output', '')\n            if output:\n                try:\n                    import json\n                    lines = output.strip().split('\\n')\n                    json_start = None\n                    for i in range(len(lines) - 1, -1, -1):\n                        if lines[i].strip().startswith('{'):\n                            json_start = i\n                            break\n                    if json_start is not None:\n                        json_output = '\\n'.join(lines[json_start:])\n                        json_data = json.loads(json_output)\n                        try:\n                            save_tool_result('analyze_config', 'config', json_data, project_root=self.project_root)\n                            logger.debug(\"Regenerated analyze_config_results.json\")\n                        except Exception as e:\n                            logger.warning(f\"Failed to save analyze_config result: {e}\")\n                    else:\n                        json_data = json.loads(output)\n                        save_tool_result('analyze_config', 'config', json_data, project_root=self.project_root)\n                        logger.debug(\"Regenerated analyze_config_results.json\")\n                except (json.JSONDecodeError, ValueError) as e:\n                    logger.warning(f\"Failed to parse analyze_config JSON output: {e}\")\n                    logger.debug(f\"Output was: {output[:500]}...\")\n                print(output)\n            else:\n                logger.warning(\"No output from analyze_config script\")\n            return True\n        else:\n            logger.error(f\"Configuration check failed: {result['error']}\")\n            return False\n    \n    def run_workflow(self, task_type: str, task_data: Optional[Dict] = None) -> bool:\n        \"\"\"Run workflow with audit-first protocol\"\"\"\n        logger.info(f\"Running workflow: {task_type}\")\n        logger.info(\"=\" * 50)\n        if not self.check_trigger_requirements(task_type):\n            return False\n        audit_results = self.run_audit_first(task_type)\n        if not audit_results['success']:\n            logger.error(f\"Audit failed: {audit_results['error']}\")\n            return False\n        task_success = self.execute_task(task_type, task_data)\n        if task_success:\n            validation_results = self.validate_work(task_type, task_data or {})\n            self.show_validation_report(validation_results)\n        return task_success\n    \n    def run_version_sync(self, scope: str = 'docs'):\n        \"\"\"Sync version numbers\"\"\"\n        logger.info(f\"Syncing versions for scope: {scope}\")\n        logger.info(\"=\" * 50)\n        result = self.run_script('fix_version_sync', 'sync', '--scope', scope)\n        if result['success']:\n            self.fix_version_sync_results = result\n            logger.info(\"Version sync completed!\")\n            return True\n        else:\n            logger.error(f\"Version sync failed: {result['error']}\")\n            return False\n    \n    def run_dev_tools_coverage(self) -> Dict:\n        \"\"\"Run coverage analysis specifically for development_tools directory.\"\"\"\n        logger.info(\"Generating dev tools coverage...\")\n        from .audit_orchestration import _AUDIT_LOCK_FILE\n        # Use separate lock file for dev tools coverage to avoid conflicts when running in parallel with main coverage\n        # Both lock files are checked by _is_audit_in_progress(), so this is safe\n        coverage_lock_file = self._get_coverage_lock_file_path() if hasattr(self, '_get_coverage_lock_file_path') else (self.project_root / 'development_tools' / '.coverage_in_progress.lock')\n        # Use a separate lock file for dev tools to avoid file conflicts when running in parallel\n        coverage_lock_file = coverage_lock_file.parent / '.coverage_dev_tools_in_progress.lock'\n        try:\n            coverage_lock_file.parent.mkdir(parents=True, exist_ok=True)\n            coverage_lock_file.touch()\n        except Exception as e:\n            logger.warning(f\"Failed to create coverage lock file: {e}\")\n        \n        try:\n            result = self.run_script('run_test_coverage', '--dev-tools-only', timeout=720)\n            # Check if coverage was collected, not just if pytest succeeded\n            # pytest can exit with non-zero code if tests fail, but coverage may still be collected\n            coverage_collected = False\n            \n            # First check if coverage file exists (most reliable indicator)\n            dev_tools_coverage_file = self.project_root / 'development_tools' / 'tests' / 'jsons' / 'coverage_dev_tools.json'\n            if dev_tools_coverage_file.exists():\n                coverage_collected = True\n            \n            # Also check output for coverage indicators\n            if result.get('output'):\n                output = result['output']\n                if 'TOTAL' in output or 'coverage' in output.lower() or 'Cover' in output:\n                    coverage_collected = True\n            \n            # If script failed but coverage file exists, we still succeeded\n            if coverage_collected:\n                self._load_dev_tools_coverage()\n                # Save results to standardized storage\n                if hasattr(self, 'dev_tools_coverage_results') and self.dev_tools_coverage_results:\n                    try:\n                        save_tool_result('generate_dev_tools_coverage', 'tests', self.dev_tools_coverage_results, project_root=self.project_root)\n                    except Exception as e:\n                        logger.warning(f\"Failed to save generate_dev_tools_coverage result: {e}\")\n                return {\n                    'success': True,\n                    'data': self.dev_tools_coverage_results\n                }\n            else:\n                # Even if script failed, check one more time if coverage file was created\n                # (it might have been created before the script failed)\n                if dev_tools_coverage_file.exists():\n                    logger.info(\"Coverage file found despite script failure - loading coverage data\")\n                    self._load_dev_tools_coverage()\n                    if hasattr(self, 'dev_tools_coverage_results') and self.dev_tools_coverage_results:\n                        try:\n                            save_tool_result('generate_dev_tools_coverage', 'tests', self.dev_tools_coverage_results, project_root=self.project_root)\n                        except Exception as e:\n                            logger.warning(f\"Failed to save generate_dev_tools_coverage result: {e}\")\n                        return {\n                            'success': True,\n                            'data': self.dev_tools_coverage_results\n                        }\n                \n                # No coverage collected\n                error_msg = result.get('error', 'Unknown error')\n                if result.get('output'):\n                    error_msg = f\"{error_msg}\\nOutput: {result['output'][:500]}\"\n                logger.warning(f\"Dev tools coverage failed: {error_msg}\")\n                return {\n                    'success': False,\n                    'error': error_msg\n                }\n        finally:\n            if coverage_lock_file.exists():\n                try:\n                    coverage_lock_file.unlink()\n                except Exception as e:\n                    logger.warning(f\"Failed to remove coverage lock file: {e}\")\n    \n    def run_status(self, skip_status_files: bool = False):\n        \"\"\"Generate status snapshot (cached data, no audit)\"\"\"\n        logger.info(\"Generating status snapshot...\")\n        logger.info(\"=\" * 50)\n        \n        if not skip_status_files:\n            # Get status file paths from config\n            try:\n                from .. import config\n                status_config = config.get_status_config()\n                status_files_config = status_config.get('status_files', {})\n                # Use default from STATUS config if status_files_config is empty (matches default config)\n                if not status_files_config:\n                    # Fallback to default STATUS config values for backward compatibility\n                    from ...config.config import STATUS\n                    status_files_config = STATUS.get('status_files', {})\n                ai_status_path = status_files_config.get('ai_status', 'development_tools/AI_STATUS.md')\n                ai_priorities_path = status_files_config.get('ai_priorities', 'development_tools/AI_PRIORITIES.md')\n                consolidated_report_path = status_files_config.get('consolidated_report', 'development_tools/consolidated_report.txt')\n            except (ImportError, AttributeError, KeyError):\n                # Fallback to default STATUS config values for backward compatibility\n                try:\n                    from ...config.config import STATUS\n                    status_files_default = STATUS.get('status_files', {})\n                    ai_status_path = status_files_default.get('ai_status', 'development_tools/AI_STATUS.md')\n                    ai_priorities_path = status_files_default.get('ai_priorities', 'development_tools/AI_PRIORITIES.md')\n                    consolidated_report_path = status_files_default.get('consolidated_report', 'development_tools/consolidated_report.txt')\n                except (ImportError, AttributeError):\n                    # Last resort fallback\n                    ai_status_path = 'development_tools/AI_STATUS.md'\n                    ai_priorities_path = 'development_tools/AI_PRIORITIES.md'\n                    consolidated_report_path = 'development_tools/consolidated_report.txt'\n            \n            try:\n                ai_status = self._generate_ai_status_document()\n                from ..file_rotation import create_output_file\n                ai_status_file = create_output_file(ai_status_path, ai_status, project_root=self.project_root)\n                logger.info(f\"Generated: {ai_status_file}\")\n            except Exception as e:\n                logger.warning(f\"Error generating AI_STATUS document: {e}\")\n            \n            try:\n                ai_priorities = self._generate_ai_priorities_document()\n                from ..file_rotation import create_output_file\n                ai_priorities_file = create_output_file(ai_priorities_path, ai_priorities, project_root=self.project_root)\n                logger.info(f\"Generated: {ai_priorities_file}\")\n            except Exception as e:\n                logger.warning(f\"Error generating AI_PRIORITIES document: {e}\")\n            \n            try:\n                consolidated_report = self._generate_consolidated_report()\n                from ..file_rotation import create_output_file\n                consolidated_file = create_output_file(consolidated_report_path, consolidated_report, project_root=self.project_root)\n                logger.info(f\"Generated: {consolidated_file}\")\n            except Exception as e:\n                logger.warning(f\"Error generating consolidated report: {e}\")\n        \n        logger.info(\"=\" * 50)\n        logger.info(\"Status snapshot completed!\")\n    \n    def run_documentation_sync(self):\n        \"\"\"Run documentation sync check\"\"\"\n        logger.info(\"Running documentation sync check...\")\n        logger.info(\"=\" * 50)\n        success = self._run_doc_sync_check('--check')\n        if success:\n            logger.info(\"Documentation sync check completed!\")\n        else:\n            logger.warning(\"Documentation sync check completed with issues.\")\n        return success\n    \n    def run_documentation_fix(self, fix_type: str = 'all', dry_run: bool = False) -> bool:\n        \"\"\"Run documentation fix operations\"\"\"\n        logger.info(f\"Running documentation fix: {fix_type}\")\n        logger.info(\"=\" * 50)\n        args = []\n        if dry_run:\n            args.append('--dry-run')\n        if fix_type == 'all':\n            args.append('--all')\n        elif fix_type == 'ascii' or fix_type == 'fix-ascii':\n            args.append('--fix-ascii')\n        elif fix_type == 'number-headings':\n            args.append('--number-headings')\n        elif fix_type == 'add-addresses':\n            args.append('--add-addresses')\n        elif fix_type == 'convert-links':\n            args.append('--convert-links')\n        result = self.run_script('fix_documentation', *args)\n        if result['success']:\n            logger.info(\"Documentation fix completed!\")\n            return True\n        else:\n            error_msg = result.get('error', '').strip()\n            returncode = result.get('returncode')\n            if not error_msg:\n                if returncode is not None:\n                    error_msg = f\"Script exited with code {returncode}\"\n                else:\n                    error_msg = \"Unknown error (no error message or return code)\"\n            logger.error(f\"Documentation fix failed: {error_msg}\")\n            if result.get('output'):\n                logger.debug(f\"Script output: {result['output'][:200]}\")\n            return False\n    \n    def run_coverage_regeneration(self):\n        \"\"\"Regenerate test coverage data\"\"\"\n        logger.info(\"Generating test coverage...\")\n        logger.info(\"=\" * 50)\n        from .audit_orchestration import _AUDIT_LOCK_FILE\n        # Use helper method if available, otherwise default location\n        coverage_lock_file = self._get_coverage_lock_file_path() if hasattr(self, '_get_coverage_lock_file_path') else (self.project_root / 'development_tools' / '.coverage_in_progress.lock')\n        try:\n            coverage_lock_file.parent.mkdir(parents=True, exist_ok=True)\n            coverage_lock_file.touch()\n        except Exception as e:\n            logger.warning(f\"Failed to create coverage lock file: {e}\")\n        \n        try:\n            # Run test coverage execution (TEST_COVERAGE_REPORT.md is now generated by generate_test_coverage_report tool)\n            # Use 20 minutes timeout (1200s) to allow pytest to complete\n            # The script itself sets a 15-minute timeout for pytest, so we need\n            # a bit more time for script overhead and pytest execution\n            # Note: --update-plan flag is kept for backward compatibility but does nothing\n            result = self.run_script('run_test_coverage', timeout=1200)\n            if result['success']:\n                \n                # Save coverage results to standardized storage\n                # Load coverage data (will check archive if main file was rotated)\n                try:\n                    coverage_data = self._load_coverage_summary()\n                    if coverage_data:\n                        # _load_coverage_summary() returns 'coverage' not 'percent_covered'\n                        overall_coverage = coverage_data.get('overall', {}).get('coverage', 'N/A')\n                        modules_count = len(coverage_data.get('modules', []))\n                        logger.debug(f\"Loaded coverage data: overall={overall_coverage}%, modules={modules_count}\")\n                        from ..result_format import normalize_to_standard_format\n                        from ..output_storage import save_tool_result\n                        # coverage_data from _load_coverage_summary() has 'overall', 'modules', 'worst_files'\n                        # This matches the format expected by normalize_to_standard_format for analyze_test_coverage\n                        standard_format = normalize_to_standard_format('analyze_test_coverage', coverage_data)\n                        save_tool_result('analyze_test_coverage', 'tests', standard_format, project_root=self.project_root)\n                        logger.info(f\"Saved analyze_test_coverage results to standardized storage (coverage: {overall_coverage}%)\")\n                    else:\n                        logger.warning(\"No coverage data available to save - _load_coverage_summary() returned None (coverage.json may not exist or be empty)\")\n                except Exception as save_error:\n                    logger.warning(f\"Failed to save analyze_test_coverage results: {save_error}\")\n                    import traceback\n                    logger.debug(f\"Traceback: {traceback.format_exc()}\")\n                \n                \n                return True\n            else:\n                logger.error(f\"Test coverage regeneration failed: {result['error']}\")\n                return False\n        finally:\n            if coverage_lock_file.exists():\n                try:\n                    coverage_lock_file.unlink()\n                except Exception as e:\n                    logger.warning(f\"Failed to remove coverage lock file: {e}\")\n    \n    def run_legacy_cleanup(self):\n        \"\"\"Run legacy reference cleanup\"\"\"\n        logger.info(\"Starting legacy cleanup...\")\n        logger.info(\"=\" * 50)\n        success = self._run_legacy_cleanup_scan()\n        if success:\n            logger.info(\"Legacy cleanup completed!\")\n        else:\n            logger.warning(\"Legacy cleanup completed with issues.\")\n        return success\n    \n    def run_cleanup(self, cache: bool = False, test_data: bool = False,\n                    reports: bool = False, all: bool = False,\n                    coverage: bool = False, full: bool = False,\n                    dry_run: bool = False):\n        \"\"\"Clean up generated files and caches\"\"\"\n        try:\n            from development_tools.shared.fix_project_cleanup import ProjectCleanup\n            \n            logger.info(\"Starting cleanup...\")\n            logger.info(\"=\" * 50)\n            \n            # If --full is specified, clean everything including tool caches\n            if full:\n                cache = True\n                test_data = True\n                coverage = True\n            # Legacy --all flag support (for backwards compatibility)\n            elif all:\n                cache = True\n                test_data = True\n                reports = True\n                coverage = True\n            \n            cleanup = ProjectCleanup(self.project_root)\n            results = cleanup.cleanup_all(\n                dry_run=dry_run,\n                cache=cache,\n                test_data=test_data,\n                coverage=coverage,\n                include_tool_caches=full  # Only include tool caches when --full is specified\n            )\n            \n            if dry_run:\n                logger.info(\"DRY RUN MODE - No files were actually removed\")\n            \n            logger.info(\"Cleanup completed!\")\n            \n            return {\n                'success': True,\n                'data': results\n            }\n        except Exception as e:\n            logger.error(f\"Cleanup failed: {e}\", exc_info=True)\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    def run_analyze_system_signals(self):\n        \"\"\"Run system signals analysis\"\"\"\n        logger.info(\"Analyzing system signals...\")\n        logger.info(\"=\" * 50)\n        result = self.run_script('analyze_system_signals', '--json')\n        if result['success']:\n            output = result.get('output', '')\n            if output:\n                try:\n                    import json\n                    # Try to find JSON in the output (might have extra text)\n                    # Look for JSON object/array boundaries\n                    json_start = output.find('{')\n                    if json_start == -1:\n                        json_start = output.find('[')\n                    if json_start != -1:\n                        json_end = output.rfind('}') + 1\n                        if json_end == 0:\n                            json_end = output.rfind(']') + 1\n                        if json_end > json_start:\n                            json_str = output[json_start:json_end]\n                            data = json.loads(json_str)\n                        else:\n                            data = json.loads(output)\n                    else:\n                        data = json.loads(output)\n                    self.system_signals = data\n                    try:\n                        save_tool_result('analyze_system_signals', 'reports', data, project_root=self.project_root)\n                    except Exception as e:\n                        logger.debug(f\"Failed to save analyze_system_signals result: {e}\")\n                except json.JSONDecodeError as e:\n                    logger.warning(f\"analyze_system_signals output could not be parsed as JSON: {e}\")\n                    logger.debug(f\"analyze_system_signals raw output (first 500 chars): {output[:500]}\")\n            logger.info(\"System signals analysis completed!\")\n            return True\n        else:\n            logger.error(f\"System signals analysis failed: {result.get('error', 'Unknown error')}\")\n            return False\n    \n    \n    def run_test_markers(self, action: str = 'check', dry_run: bool = False) -> Dict:\n        \"\"\"Run test markers analysis or fix\"\"\"\n        logger.info(f\"Analyzing test markers: {action}\")\n        logger.info(\"=\" * 50)\n        args = []\n        if action == 'check':\n            args.append('--check')\n        elif action == 'analyze':\n            args.append('--analyze')\n        elif action == 'fix':\n            args.append('--fix')\n        if dry_run:\n            args.append('--dry-run')\n        # Always request JSON output for parsing\n        args.append('--json')\n        \n        result = self.run_script('analyze_test_markers', *args)\n        \n        # The script returns exit code 1 if markers are missing (which is valid data, not a failure)\n        # So we check for output rather than just success\n        output = result.get('output', '')\n        if output:\n            try:\n                import json\n                data = json.loads(output)\n                result['data'] = data\n                result['success'] = True  # Mark as success if we got valid JSON\n                try:\n                    save_tool_result('analyze_test_markers', 'tests', data, project_root=self.project_root)\n                except Exception as e:\n                    logger.debug(f\"Failed to save analyze_test_markers result: {e}\")\n                logger.info(\"Test markers analysis completed!\")\n            except json.JSONDecodeError as e:\n                logger.error(f\"Test markers analysis failed: Could not parse JSON output: {e}\")\n                logger.debug(f\"Output was: {output[:200]}\")\n                result['success'] = False\n                result['error'] = f\"Could not parse JSON output: {e}\"\n        elif result.get('error'):\n            error_msg = result['error']\n            logger.error(f\"Test markers analysis failed: {error_msg}\")\n            if result.get('output'):\n                logger.debug(f\"Script stdout: {result['output'][:500]}\")\n            result['success'] = False\n        else:\n            error_msg = f\"No output received (returncode: {result.get('returncode')})\"\n            logger.error(f\"Test markers analysis failed: {error_msg}\")\n            if result.get('error'):\n                logger.debug(f\"Script stderr: {result['error'][:500]}\")\n            result['success'] = False\n            result['error'] = error_msg\n        \n        return result\n    \n    def run_unused_imports(self):\n        \"\"\"Run unused imports analysis (analysis only)\"\"\"\n        logger.info(\"Analyzing unused imports...\")\n        logger.info(\"=\" * 50)\n        result = self.run_analyze_unused_imports()\n        if result.get('success'):\n            logger.info(\"Unused imports analysis completed!\")\n        else:\n            logger.warning(f\"Unused imports analysis completed with issues: {result.get('error', 'Unknown error')}\")\n        return result\n    \n    def run_unused_imports_report(self):\n        \"\"\"Run unused imports report generation (generates markdown report from analysis results)\"\"\"\n        logger.info(\"=\" * 50)\n        result = self.run_generate_unused_imports_report()\n        if result.get('success'):\n            logger.info(\"Unused imports report generated successfully!\")\n        else:\n            logger.warning(f\"Unused imports report generation completed with issues: {result.get('error', 'Unknown error')}\")\n        return result\n    \n    def generate_directory_trees(self):\n        \"\"\"Generate directory tree documentation.\n        \n        NOTE: This should ONLY be called from the 'docs' command, NOT during audits.\n        Static documentation should not be regenerated during audit runs.\n        The safeguard in create_output_file() will automatically prevent writes during audits/tests.\n        \"\"\"\n        logger.info(\"Generating directory trees...\")\n        try:\n            result = self.run_script('generate_directory_tree')\n            if result['success']:\n                logger.info(\"Directory trees generated successfully!\")\n                return True\n            else:\n                logger.error(f\"Directory tree generation failed: {result['error']}\")\n                return False\n        except RuntimeError as e:\n            # Handle safeguard blocking (from create_output_file)\n            if \"Cannot write\" in str(e) and \"DIRECTORY_TREE.md\" in str(e):\n                logger.warning(f\"Skipping DIRECTORY_TREE.md generation: {e}\")\n                return False\n            raise\n    \n    def check_trigger_requirements(self, task_type: str) -> bool:\n        \"\"\"Check if trigger requirements are met for a task\"\"\"\n        # Implementation would check workflow config\n        return True\n    \n    def run_audit_first(self, task_type: str) -> Dict:\n        \"\"\"Run audit first as required by protocol\"\"\"\n        logger.info(\"Running audit-first protocol...\")\n        audit_success = self._run_quick_audit_tools()\n        return {\n            'success': audit_success,\n            'error': '' if audit_success else 'Audit failed'\n        }\n    \n    def execute_task(self, task_type: str, task_data: Optional[Dict] = None) -> bool:\n        \"\"\"Execute the specific task\"\"\"\n        if task_type == 'documentation':\n            return self._execute_documentation_task()\n        elif task_type == 'function_registry':\n            return self._execute_function_registry_task()\n        elif task_type == 'module_dependencies':\n            return self._execute_module_dependencies_task()\n        else:\n            logger.error(f\"Unknown task type: {task_type}\")\n            return False\n    \n    def validate_work(self, work_type: str, work_data: Dict) -> Dict:\n        \"\"\"Validate the work before presenting\"\"\"\n        logger.info(\"Validating work...\")\n        # Use --json flag to prevent multiline print output from being captured\n        result = self.run_script('analyze_ai_work', '--work-type', work_type, '--json')\n        if result['success']:\n            return self.validate_audit_results({'output': result['output']})\n        else:\n            return {\n                'completeness': 0.0,\n                'accuracy': 0.0,\n                'consistency': 0.0,\n                'actionable': 0.0,\n                'overall': 0.0,\n                'issues': [f\"Validation failed: {result['error']}\"]\n            }\n    \n    def validate_audit_results(self, results: Dict) -> Dict:\n        \"\"\"Validate audit results\"\"\"\n        return {\n            'completeness': 95.0,\n            'accuracy': 90.0,\n            'consistency': 85.0,\n            'actionable': 80.0,\n            'overall': 87.5,\n            'issues': []\n        }\n    \n    def show_validation_report(self, validation_results: Dict):\n        \"\"\"Show validation report\"\"\"\n        print(\"\\n\" + \"=\" * 50)\n        print(\"VALIDATION REPORT\")\n        print(\"=\" * 50)\n        scores = [\n            f\"Completeness: {validation_results['completeness']:.1f}%\",\n            f\"Accuracy: {validation_results['accuracy']:.1f}%\",\n            f\"Consistency: {validation_results['consistency']:.1f}%\",\n            f\"Actionable: {validation_results['actionable']:.1f}%\"\n        ]\n        overall = validation_results['overall']\n        status = \"PASSED\" if overall >= 80.0 else \"NEEDS IMPROVEMENT\"\n        print(f\"Overall Score: {overall:.1f}% - {status}\")\n        print(\"\\nComponent Scores:\")\n        for score in scores:\n            print(f\"  {score}\")\n        if validation_results['issues']:\n            print(\"\\nIssues Found:\")\n            for issue in validation_results['issues']:\n                print(f\"  [ISSUE] {issue}\")\n    \n    def print_audit_summary(self, successful: List, failed: List, results: Dict):\n        \"\"\"Print concise audit summary\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"AUDIT SUMMARY\")\n        print(\"=\" * 80)\n        print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        print(f\"Successful: {len(successful)}\")\n        print(f\"Failed: {len(failed)}\")\n        if failed:\n            print(f\"\\n[CRITICAL] Failed audits: {', '.join(failed)}\")\n        key_metrics = self._extract_key_metrics(results)\n        if key_metrics:\n            print(\"\\nKey Metrics:\")\n            for metric, value in key_metrics.items():\n                print(f\"  {metric}: {value}\")\n        # Default to generic path relative to project root (no development_tools/ assumption)\n        results_file = (self.audit_config or {}).get('results_file', 'reports/analysis_detailed_results.json')\n        print(f\"\\nDetailed results saved to: {results_file}\")\n        if self.audit_config.get('prioritize_issues', False):\n            print(f\"Critical issues saved to: {self.audit_config['issues_file']}\")\n    \n    def _execute_documentation_task(self) -> bool:\n        \"\"\"Execute documentation update task\"\"\"\n        logger.info(\"Updating documentation...\")\n        result = self.run_script('generate_documentation')\n        return result['success']\n    \n    def _execute_function_registry_task(self) -> bool:\n        \"\"\"Execute function registry task\"\"\"\n        logger.info(\"Updating function registry...\")\n        result = self.run_script('generate_function_registry')\n        return result['success']\n    \n    def _execute_module_dependencies_task(self) -> bool:\n        \"\"\"Execute module dependencies task\"\"\"\n        logger.info(\"Updating module dependencies...\")\n        result = self.run_script('generate_module_dependencies')\n        return result['success']\n    \n    def _run_doc_sync_check(self, *args) -> bool:\n        \"\"\"Run all documentation sync checks and aggregate results.\"\"\"\n        all_results = {}\n        \n        # Run paired documentation sync\n        # Note: analyze_documentation_sync.py logs this message itself, so we don't duplicate it here\n        result = self.run_script('analyze_documentation_sync', *args)\n        if result.get('output') or result.get('success'):\n            all_results['paired_docs'] = self._parse_documentation_sync_output(result.get('output', ''))\n        else:\n            logger.warning(f\"analyze_documentation_sync failed: {result.get('error', 'Unknown error')}\")\n        \n        # Run path drift analysis\n        logger.info(\"  - Analyzing path drift...\")\n        result = self.run_analyze_path_drift()\n        if result.get('data'):\n            all_results['path_drift'] = result.get('data', {})\n        elif result.get('output') or result.get('success'):\n            all_results['path_drift'] = self._parse_path_drift_output(result.get('output', ''))\n        else:\n            logger.warning(f\"analyze_path_drift failed: {result.get('error', 'Unknown error')}\")\n        \n        # Run ASCII compliance check\n        logger.info(\"  - Analyzing ASCII compliance...\")\n        result = self.run_script('analyze_ascii_compliance', '--json')\n        # Track this sub-tool as run\n        if hasattr(self, '_tools_run_in_current_tier'):\n            self._tools_run_in_current_tier.add('analyze_ascii_compliance')\n        try:\n            ascii_result = self._load_mtime_cached_tool_results(\n                'analyze_ascii_compliance',\n                'docs',\n                result,\n                self._parse_ascii_compliance_output\n            )\n            all_results['ascii_compliance'] = ascii_result\n        except Exception as e:\n            logger.debug(f\"Failed to load ASCII compliance: {e}, falling back to parsing output\")\n            if result.get('output') or result.get('success'):\n                ascii_result = self._parse_ascii_compliance_output(result.get('output', ''))\n                all_results['ascii_compliance'] = ascii_result\n                try:\n                    save_tool_result('analyze_ascii_compliance', 'docs', ascii_result, project_root=self.project_root)\n                except Exception as save_error:\n                    logger.debug(f\"Failed to save ASCII compliance results: {save_error}\")\n            else:\n                logger.warning(f\"analyze_ascii_compliance failed: {result.get('error', 'Unknown error')}\")\n        \n        # Run heading numbering check\n        logger.info(\"  - Analyzing heading numbering...\")\n        result = self.run_script('analyze_heading_numbering', '--json')\n        # Track this sub-tool as run\n        if hasattr(self, '_tools_run_in_current_tier'):\n            self._tools_run_in_current_tier.add('analyze_heading_numbering')\n        try:\n            heading_result = self._load_mtime_cached_tool_results(\n                'analyze_heading_numbering',\n                'docs',\n                result,\n                self._parse_heading_numbering_output\n            )\n            all_results['heading_numbering'] = heading_result\n        except Exception as e:\n            logger.debug(f\"Failed to load heading numbering: {e}, falling back to parsing output\")\n            if result.get('output') or result.get('success'):\n                heading_result = self._parse_heading_numbering_output(result.get('output', ''))\n                all_results['heading_numbering'] = heading_result\n                try:\n                    save_tool_result('analyze_heading_numbering', 'docs', heading_result, project_root=self.project_root)\n                except Exception as save_error:\n                    logger.debug(f\"Failed to save heading numbering results: {save_error}\")\n            else:\n                logger.warning(f\"analyze_heading_numbering failed: {result.get('error', 'Unknown error')}\")\n        \n        # Run missing addresses check\n        logger.info(\"  - Analyzing missing addresses...\")\n        result = self.run_script('analyze_missing_addresses', '--json')\n        # Track this sub-tool as run\n        if hasattr(self, '_tools_run_in_current_tier'):\n            self._tools_run_in_current_tier.add('analyze_missing_addresses')\n        try:\n            missing_result = self._load_mtime_cached_tool_results(\n                'analyze_missing_addresses',\n                'docs',\n                result,\n                self._parse_missing_addresses_output\n            )\n            all_results['missing_addresses'] = missing_result\n        except Exception as e:\n            logger.debug(f\"Failed to load missing addresses: {e}, falling back to parsing output\")\n            if result.get('output') or result.get('success'):\n                missing_result = self._parse_missing_addresses_output(result.get('output', ''))\n                all_results['missing_addresses'] = missing_result\n                try:\n                    save_tool_result('analyze_missing_addresses', 'docs', missing_result, project_root=self.project_root)\n                except Exception as save_error:\n                    logger.debug(f\"Failed to save missing addresses results: {save_error}\")\n            else:\n                logger.warning(f\"analyze_missing_addresses failed: {result.get('error', 'Unknown error')}\")\n        \n        # Run unconverted links check\n        logger.info(\"  - Analyzing unconverted links...\")\n        result = self.run_script('analyze_unconverted_links', '--json')\n        # Track this sub-tool as run\n        if hasattr(self, '_tools_run_in_current_tier'):\n            self._tools_run_in_current_tier.add('analyze_unconverted_links')\n        try:\n            links_result = self._load_mtime_cached_tool_results(\n                'analyze_unconverted_links',\n                'docs',\n                result,\n                self._parse_unconverted_links_output\n            )\n            all_results['unconverted_links'] = links_result\n        except Exception as e:\n            logger.debug(f\"Failed to load unconverted links: {e}, falling back to parsing output\")\n            if result.get('output') or result.get('success'):\n                links_result = self._parse_unconverted_links_output(result.get('output', ''))\n                all_results['unconverted_links'] = links_result\n                try:\n                    save_tool_result('analyze_unconverted_links', 'docs', links_result, project_root=self.project_root)\n                except Exception as save_error:\n                    logger.debug(f\"Failed to save unconverted links results: {save_error}\")\n            else:\n                logger.warning(f\"analyze_unconverted_links failed: {result.get('error', 'Unknown error')}\")\n        \n        # Aggregate all results\n        summary = self._aggregate_doc_sync_results(all_results)\n        self.docs_sync_results = {'success': True, 'summary': summary, 'all_results': all_results}\n        self.docs_sync_summary = summary\n        logger.info(f\"Documentation sync summary: {summary.get('status', 'UNKNOWN')} - {summary.get('total_issues', 0)} total issues\")\n        return True\n    \n    def _run_legacy_cleanup_scan(self, *args) -> bool:\n        \"\"\"Run legacy cleanup and store structured results.\"\"\"\n        result = self.run_script('fix_legacy_references', *args)\n        if result.get('success'):\n            summary = self._parse_legacy_output(result.get('output', ''))\n            result['summary'] = summary\n            self.legacy_cleanup_results = result\n            self.legacy_cleanup_summary = summary\n            return True\n        if result.get('output'):\n            logger.info(result['output'])\n        if result.get('error'):\n            logger.error(result['error'])\n        return False\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 270,
                    "line_content": "# Fallback to default STATUS config values for backward compatibility",
                    "start": 12824,
                    "end": 12846
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 277,
                    "line_content": "# Fallback to default STATUS config values for backward compatibility",
                    "start": 13454,
                    "end": 13476
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 380,
                    "line_content": "# Note: --update-plan flag is kept for backward compatibility but does nothing",
                    "start": 18845,
                    "end": 18867
                  }
                ]
              ],
              [
                "development_tools\\shared\\service\\data_loading.py",
                "\"\"\"\nData loading and parsing methods for AIToolsService.\n\nContains methods for loading tool results, parsing output, and aggregating data.\n\"\"\"\n\nimport json\nimport re\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Optional\n\nfrom core.logger import get_component_logger\n\nlogger = get_component_logger(\"development_tools\")\n\n# Import utilities for helper functions\n# Note: Uses self._create_standard_format_result method instead of imported function\n\n\nclass DataLoadingMixin:\n    \"\"\"Mixin class providing data loading and parsing methods to AIToolsService.\"\"\"\n    \n    def _load_tool_data(self, tool_name: str, domain: Optional[str] = None, log_source: bool = True) -> Dict[str, Any]:\n        \"\"\"\n        Unified data loading helper with consistent fallback chain.\n        \n        Loads tool data from multiple sources in order of preference:\n        1. results_cache (in-memory, current audit run)\n        2. load_tool_result() (standardized storage, recent run)\n        3. Central aggregation file (analysis_detailed_results.json, cached)\n        \n        All data is normalized to standard format before returning.\n        \n        Args:\n            tool_name: Name of the tool (e.g., 'analyze_functions')\n            domain: Domain directory (e.g., 'functions'). If None, inferred from tool_name\n            log_source: If True, log the data source at INFO level for report generation\n            \n        Returns:\n            Dict containing tool data in standard format, or empty dict if not found\n        \"\"\"\n        # Determine if we're in an active audit (should have current data)\n        is_active_audit = hasattr(self, 'current_audit_tier') and self.current_audit_tier is not None\n        audit_tier = getattr(self, 'current_audit_tier', None)\n        \n        # Check if this tool was actually run in the current audit tier\n        tools_run_in_tier = getattr(self, '_tools_run_in_current_tier', set())\n        tool_was_run = tool_name in tools_run_in_tier\n        \n        # Step 1: Check results_cache (in-memory, current audit run)\n        # CRITICAL: Only use cache if tool was actually run in current tier to avoid stale data\n        if tool_was_run and hasattr(self, 'results_cache') and self.results_cache:\n            cached_data = self.results_cache.get(tool_name)\n            if cached_data and isinstance(cached_data, dict):\n                if log_source:\n                    logger.debug(f\"[DATA SOURCE] {tool_name}: loaded from current audit run (Tier {audit_tier})\")\n                # Normalize before returning\n                from ..result_format import normalize_to_standard_format\n                normalized = normalize_to_standard_format(tool_name, cached_data)\n                return normalized\n        \n        # Step 2: Fallback to standardized storage (recent run)\n        # CRITICAL: Do NOT store in results_cache - this is cached data, not current audit run data\n        # Storing it would cause future calls to incorrectly log \"current audit run\"\n        try:\n            from ..output_storage import load_tool_result\n            stored_data = load_tool_result(tool_name, domain, project_root=self.project_root, normalize=True)\n            if stored_data and isinstance(stored_data, dict):\n                if log_source:\n                    logger.debug(f\"[DATA SOURCE] {tool_name}: loaded from standardized storage (cached)\")\n                # Do NOT store in results_cache - this is cached data, not from current audit run\n                return stored_data\n        except Exception as e:\n            logger.debug(f\"Failed to load {tool_name} from standardized storage: {e}\")\n        \n        # Step 3: Fallback to central aggregation file (cached)\n        # CRITICAL: Do NOT store in results_cache - this is cached data, not current audit run data\n        # Skip in test directories to prevent loading large files\n        try:\n            # Check if we're in a test directory (use method from AuditOrchestrationMixin if available)\n            is_test_dir = False\n            if hasattr(self, '_is_test_directory'):\n                is_test_dir = self._is_test_directory(self.project_root)\n            \n            if is_test_dir:\n                return {}\n            \n            results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n            if results_file.exists():\n                with open(results_file, 'r', encoding='utf-8') as f:\n                    cached_data = json.load(f)\n                \n                if 'results' in cached_data and tool_name in cached_data['results']:\n                    tool_data = cached_data['results'][tool_name]\n                    # Handle nested data structure: results.tool_name.data\n                    if 'data' in tool_data:\n                        data = tool_data['data']\n                        if log_source:\n                            logger.debug(f\"[DATA SOURCE] {tool_name}: loaded from central aggregation file (cached)\")\n                        # Normalize before returning\n                        from ..result_format import normalize_to_standard_format\n                        normalized = normalize_to_standard_format(tool_name, data)\n                        # Do NOT store in results_cache - this is cached data, not from current audit run\n                        return normalized\n                    else:\n                        # Some tools store data directly without 'data' wrapper\n                        if log_source:\n                            logger.debug(f\"[DATA SOURCE] {tool_name}: loaded from central aggregation file (cached, direct)\")\n                        # Normalize before returning\n                        from ..result_format import normalize_to_standard_format\n                        normalized = normalize_to_standard_format(tool_name, tool_data)\n                        # Do NOT store in results_cache - this is cached data, not from current audit run\n                        return normalized\n        except Exception as e:\n            logger.debug(f\"Failed to load {tool_name} from central aggregation file: {e}\")\n        \n        # No data found in any source\n        if log_source:\n            logger.warning(f\"[DATA SOURCE] {tool_name}: no data found in any source (using empty fallback)\")\n        return {}\n    \n    def _get_canonical_metrics(self) -> Dict[str, Any]:\n        \"\"\"Provide consistent totals across downstream documents.\"\"\"\n        results_cache = self.results_cache or {}\n        fd_metrics_raw = results_cache.get('analyze_functions', {}) or {}\n        \n        # Handle standard format - extract details if present\n        if 'details' in fd_metrics_raw and isinstance(fd_metrics_raw.get('details'), dict):\n            fd_metrics = fd_metrics_raw['details']\n        else:\n            fd_metrics = fd_metrics_raw\n\n        ds_metrics = results_cache.get('decision_support_metrics', {}) or {}\n\n        audit_data = results_cache.get('analyze_function_registry', {}) or {}\n        \n        # Handle standard format for audit_data\n        if 'details' in audit_data and isinstance(audit_data.get('details'), dict):\n            audit_data_details = audit_data['details']\n            audit_totals = audit_data_details.get('totals') if isinstance(audit_data_details, dict) else {}\n        else:\n            audit_totals = audit_data.get('totals') if isinstance(audit_data, dict) else {}\n        if audit_totals is None or not isinstance(audit_totals, dict):\n            audit_totals = {}\n\n        # PRIORITY: Always use analyze_functions as single source of truth (most accurate and consistent)\n        # decision_support now uses the same categorization logic, so metrics should match\n        # But we prioritize analyze_functions to ensure consistency\n        total_functions = None\n\n        # First priority: analyze_functions (single source of truth)\n        # Check if fd_metrics is not empty (could be empty dict which is falsy)\n        if fd_metrics and isinstance(fd_metrics, dict):\n            total_functions = fd_metrics.get('total_functions')\n\n        # Second priority: decision_support (should match analyze_functions now, but kept as fallback)\n        # Note: decision_support now uses categorize_functions() from analyze_functions, so metrics should be consistent\n        if total_functions is None:\n            total_functions = ds_metrics.get('total_functions')\n\n        # Last resort: analyze_function_registry (but only if it's reasonable)\n        if total_functions is None and isinstance(audit_data, dict):\n            # Check both standard format and legacy format\n            if 'details' in audit_data:\n                audit_total = audit_data['details'].get('totals', {}).get('functions_found')\n            else:\n                audit_total = audit_data.get('total_functions')\n            if audit_total is not None and isinstance(audit_total, int) and audit_total > 100:\n                total_functions = audit_total\n\n        # Final fallback: audit_totals (but validate it's reasonable)\n        if total_functions is None and isinstance(audit_totals, dict):\n            registry_total = audit_totals.get('functions_found')\n            if registry_total is not None and isinstance(registry_total, int) and registry_total > 100:\n                total_functions = registry_total\n\n        # Complexity metrics: prioritize analyze_functions (single source of truth)\n        moderate = fd_metrics.get('moderate_complexity')\n        if moderate is None:\n            moderate = ds_metrics.get('moderate_complexity')\n\n        high = fd_metrics.get('high_complexity')\n        if high is None:\n            high = ds_metrics.get('high_complexity')\n\n        critical = fd_metrics.get('critical_complexity')\n        if critical is None:\n            critical = ds_metrics.get('critical_complexity')\n        \n        # If still missing, try loading from cache (skip in test directories to prevent memory issues)\n        if total_functions is None or moderate is None or high is None or critical is None:\n            try:\n                # Skip in test directories to prevent loading large files\n                # BUT: If we have data in results_cache, we should still return what we have\n                is_test_dir = False\n                if hasattr(self, '_is_test_directory'):\n                    is_test_dir = self._is_test_directory(self.project_root)\n                \n                if is_test_dir:\n                    # In test directories, return what we have from results_cache (don't load from disk)\n                    # Don't return empty dict - return what we found so far, even if incomplete\n                    # This allows tests to work with mocked data in results_cache\n                    pass  # Continue to return statement at end of function\n                \n                results_file = self.project_root / \"development_tools\" / \"reports\" / \"analysis_detailed_results.json\"\n                if results_file.exists():\n                    with open(results_file, 'r', encoding='utf-8') as f:\n                        cached_data = json.load(f)\n                    \n                    # Try analyze_functions first\n                    if 'results' in cached_data and 'analyze_functions' in cached_data['results']:\n                        func_data = cached_data['results']['analyze_functions']\n                        if 'data' in func_data:\n                            cached_metrics_raw = func_data['data']\n                            # Handle standard format\n                            if 'details' in cached_metrics_raw and isinstance(cached_metrics_raw.get('details'), dict):\n                                cached_metrics = cached_metrics_raw['details']\n                            else:\n                                cached_metrics = cached_metrics_raw\n                            if total_functions is None:\n                                total_functions = cached_metrics.get('total_functions')\n                            if moderate is None:\n                                moderate = cached_metrics.get('moderate_complexity')\n                            if high is None:\n                                high = cached_metrics.get('high_complexity')\n                            if critical is None:\n                                critical = cached_metrics.get('critical_complexity')\n                    \n                    # Fallback to decision_support\n                    if (total_functions is None or moderate is None or high is None or critical is None) and 'results' in cached_data:\n                        if 'decision_support' in cached_data['results']:\n                            ds_data = cached_data['results']['decision_support']\n                            if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                                cached_ds_metrics = ds_data['data']['decision_support_metrics']\n                                if total_functions is None:\n                                    total_functions = cached_ds_metrics.get('total_functions')\n                                if moderate is None:\n                                    moderate = cached_ds_metrics.get('moderate_complexity')\n                                if high is None:\n                                    high = cached_ds_metrics.get('high_complexity')\n                                if critical is None:\n                                    critical = cached_ds_metrics.get('critical_complexity')\n                    \n                    # Fallback to analyze_function_registry - parse high_complexity array\n                    if (high is None or critical is None) and 'results' in cached_data:\n                        if 'analyze_function_registry' in cached_data['results']:\n                            afr_data = cached_data['results']['analyze_function_registry']\n                            if 'data' in afr_data and 'analysis' in afr_data['data']:\n                                analysis = afr_data['data']['analysis']\n                                high_complexity_array = analysis.get('high_complexity', [])\n                                if isinstance(high_complexity_array, list):\n                                    # Count by thresholds: MODERATE=50, HIGH=100, CRITICAL=200\n                                    critical_count = sum(1 for f in high_complexity_array \n                                                        if isinstance(f, dict) and f.get('complexity', 0) >= 200)\n                                    high_count = sum(1 for f in high_complexity_array \n                                                   if isinstance(f, dict) and 100 <= f.get('complexity', 0) < 200)\n                                    if critical is None:\n                                        critical = critical_count\n                                    if high is None:\n                                        high = high_count\n                                    # For moderate, we'd need all functions, but we can estimate if we have total\n                                    if moderate is None and total_functions is not None and isinstance(total_functions, int):\n                                        moderate = max(0, total_functions - high_count - critical_count)\n            except Exception as e:\n                logger.debug(f\"Failed to load metrics from cache in _get_canonical_metrics: {e}\")\n                pass\n\n        doc_coverage = audit_data.get('doc_coverage') if isinstance(audit_data, dict) else None\n\n        # Use analyze_functions for docstring coverage (consistent source)\n        if doc_coverage is None:\n            fd_metrics_raw = self.results_cache.get('analyze_functions', {}) or {}\n            # Handle standard format\n            if 'details' in fd_metrics_raw and isinstance(fd_metrics_raw.get('details'), dict):\n                fd_metrics = fd_metrics_raw['details']\n            else:\n                fd_metrics = fd_metrics_raw\n            func_total = fd_metrics.get('total_functions')\n            func_undocumented = fd_metrics.get('undocumented', 0)\n            \n            if func_total is not None and func_total > 0:\n                func_documented = func_total - func_undocumented\n                coverage_pct = (func_documented / func_total) * 100\n                if 0 <= coverage_pct <= 100:\n                    doc_coverage = f\"{coverage_pct:.2f}%\"\n                else:\n                    doc_coverage = 'Unknown'\n            else:\n                doc_coverage = 'Unknown'\n\n        # Validate any existing doc_coverage value and reject invalid ones\n        if isinstance(doc_coverage, str):\n            if '12690' in doc_coverage or 'Unknown' not in doc_coverage:\n                try:\n                    val_str = doc_coverage.strip('%').replace(',', '')\n                    if val_str.replace('.', '').isdigit():\n                        val = float(val_str)\n                        if val > 100:\n                            fd_metrics_raw = self.results_cache.get('analyze_functions', {}) or {}\n                            if 'details' in fd_metrics_raw and isinstance(fd_metrics_raw.get('details'), dict):\n                                fd_metrics = fd_metrics_raw['details']\n                            else:\n                                fd_metrics = fd_metrics_raw\n                            func_total = fd_metrics.get('total_functions')\n                            func_undocumented = fd_metrics.get('undocumented', 0)\n                            if func_total is not None and func_total > 0:\n                                func_documented = func_total - func_undocumented\n                                coverage_pct = (func_documented / func_total) * 100\n                                if 0 <= coverage_pct <= 100:\n                                    doc_coverage = f\"{coverage_pct:.2f}%\"\n                                else:\n                                    doc_coverage = 'Unknown'\n                            else:\n                                doc_coverage = 'Unknown'\n                except (ValueError, TypeError):\n                    if '12690' in doc_coverage:\n                        doc_coverage = 'Unknown'\n        \n        if doc_coverage is None or (isinstance(doc_coverage, str) and '12690' in doc_coverage):\n            doc_coverage = 'Unknown'\n        \n        # Return metrics dict with all values\n        return {\n            'total_functions': total_functions if total_functions is not None else 'Unknown',\n            'moderate': moderate if moderate is not None else 'Unknown',\n            'high': high if high is not None else 'Unknown',\n            'critical': critical if critical is not None else 'Unknown',\n            'doc_coverage': doc_coverage\n        }\n    \n    def _get_missing_doc_files(self, limit: int = 5) -> List[str]:\n        \"\"\"Return the top documentation files missing from the registry.\"\"\"\n        metrics = self.results_cache.get('analyze_function_registry', {})\n        missing_files = []\n        if isinstance(metrics, dict):\n            missing_files = metrics.get('missing_files') or []\n        if not isinstance(missing_files, list):\n            return []\n        return missing_files[:limit]\n    \n    def _get_system_status(self) -> str:\n        \"\"\"Get current system status\"\"\"\n        status_lines = []\n        status_lines.append(\"SYSTEM STATUS\")\n        status_lines.append(\"=\" * 30)\n        \n        # Check key files (configurable for portability)\n        for file_path in self.key_files:\n            if Path(file_path).exists():\n                status_lines.append(f\"[OK] {file_path}\")\n            else:\n                status_lines.append(f\"[MISSING] {file_path}\")\n        \n        # Check recent audit results\n        results_file_name = (self.audit_config or {}).get('results_file', 'development_tools/reports/analysis_detailed_results.json')\n        for prefix in ('ai_tools/', 'development_tools/'):\n            if results_file_name.startswith(prefix):\n                results_file_name = results_file_name[len(prefix):]\n                break\n        results_file = self.project_root / results_file_name\n        if results_file.exists():\n            try:\n                with open(results_file, 'r') as f:\n                    data = json.load(f)\n                    timestamp = data.get('timestamp', 'Unknown')\n                    status_lines.append(f\"[AUDIT] Last audit: {timestamp}\")\n            except:\n                status_lines.append(\"[AUDIT] Last audit: Unknown\")\n        else:\n            status_lines.append(\"[AUDIT] No recent audit found\")\n        return '\\n'.join(status_lines)\n    \n    def _load_coverage_summary(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Load overall and per-module coverage metrics from coverage.json.\"\"\"\n        # Check development_tools/tests/jsons first (new location), then fall back to old location (legacy)\n        coverage_path = self.project_root / \"development_tools\" / \"tests\" / \"jsons\" / \"coverage.json\"\n        if not coverage_path.exists():\n            # Fallback to old location (development_tools/tests/coverage.json) for backward compatibility\n            coverage_path = self.project_root / \"development_tools\" / \"tests\" / \"coverage.json\"\n        \n        # If main file doesn't exist, check archive for most recent coverage.json (may have been rotated)\n        if not coverage_path.exists():\n            archive_dir = self.project_root / \"development_tools\" / \"tests\" / \"jsons\" / \"archive\"\n            if archive_dir.exists():\n                archived_files = sorted(\n                    archive_dir.glob(\"coverage_*.json\"),\n                    key=lambda p: p.stat().st_mtime,\n                    reverse=True\n                )\n                if archived_files:\n                    coverage_path = archived_files[0]\n                    logger.debug(f\"[DATA SOURCE] coverage_summary: using archived file {coverage_path.name} (main file was rotated)\")\n        \n        # Note: Removed project root fallback - coverage.json is always in development_tools/tests/jsons/\n        # If not found there, it doesn't exist (coverage hasn't been run yet)\n        \n        # Log data source\n        audit_tier = getattr(self, 'current_audit_tier', None)\n        if coverage_path.exists():\n            if audit_tier == 3:\n                logger.debug(f\"[DATA SOURCE] coverage_summary: loaded from {coverage_path.name} (current Tier 3 audit)\")\n            else:\n                logger.debug(f\"[DATA SOURCE] coverage_summary: loaded from {coverage_path.name} (cached)\")\n        else:\n            # This is expected if coverage hasn't been run yet - use DEBUG instead of WARNING\n            logger.debug(f\"[DATA SOURCE] coverage_summary: not found at {coverage_path.relative_to(self.project_root)} (coverage not yet generated, using empty fallback)\")\n            return None\n        try:\n            with coverage_path.open('r', encoding='utf-8') as handle:\n                coverage_data = json.load(handle)\n        except (OSError, json.JSONDecodeError):\n            return None\n        files = coverage_data.get('files')\n        if not isinstance(files, dict) or not files:\n            return None\n        total_statements = 0\n        total_covered = 0\n        module_stats = defaultdict(lambda: {'statements': 0, 'covered': 0, 'missed': 0})\n        worst_files: List[Dict[str, Any]] = []\n        for path, info in files.items():\n            summary = info.get('summary') or {}\n            statements = summary.get('num_statements') or 0\n            covered = summary.get('covered_lines') or 0\n            missed = summary.get('missing_lines')\n            if missed is None:\n                missed = max(statements - covered, 0)\n            total_statements += statements\n            total_covered += covered\n            parts = path.replace('/', '\\\\').split('\\\\')\n            module_name = parts[0] if parts and parts[0] else 'root'\n            module_entry = module_stats[module_name]\n            module_entry['statements'] += statements\n            module_entry['covered'] += covered\n            module_entry['missed'] += missed\n            if statements > 0:\n                coverage_pct = round((covered / statements) * 100, 1)\n            else:\n                coverage_pct = 0.0\n            worst_files.append({\n                'path': path.replace('\\\\', '/'),\n                'coverage': coverage_pct,\n                'missing': missed\n            })\n        if total_statements == 0:\n            overall_coverage = 0.0\n        else:\n            overall_coverage = round((total_covered / total_statements) * 100, 1)\n        module_list: List[Dict[str, Any]] = []\n        for module_name, stats in module_stats.items():\n            statements = stats['statements']\n            if statements == 0:\n                coverage_pct = 0.0\n            else:\n                coverage_pct = round((stats['covered'] / statements) * 100, 1)\n            module_list.append({\n                'module': module_name.replace('\\\\', '/'),\n                'coverage': coverage_pct,\n                'missed': stats['missed']\n            })\n        module_list.sort(key=lambda item: item['coverage'])\n        worst_files.sort(key=lambda item: item['coverage'])\n        meta = coverage_data.get('meta', {})\n        timestamp = meta.get('timestamp')\n        return {\n            'overall': {\n                'coverage': overall_coverage,\n                'statements': total_statements,\n                'covered': total_covered,\n                'missed': max(total_statements - total_covered, 0),\n                'generated': timestamp\n            },\n            'modules': module_list,\n            'worst_files': worst_files[:5]\n        }\n    \n    def _load_config_validation_summary(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Load config validation summary from JSON file.\"\"\"\n        try:\n            config_file = self.project_root / \"development_tools\" / \"config\" / \"jsons\" / \"analyze_config_results.json\"\n            if config_file.exists():\n                audit_tier = getattr(self, 'current_audit_tier', None)\n                if audit_tier == 2 or audit_tier == 3:\n                    logger.debug(f\"[DATA SOURCE] config_validation_summary: loaded from {config_file.name} (current Tier {audit_tier} audit)\")\n                else:\n                    logger.debug(f\"[DATA SOURCE] config_validation_summary: loaded from {config_file.name} (cached)\")\n                with open(config_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    # Handle standard format (summary/details) and legacy formats\n                    if 'summary' in data and 'details' in data:\n                        top_summary = data['summary']\n                        details = data['details']\n                        nested_summary = details.get('summary', {})\n                        summary = {**top_summary, **nested_summary}\n                        summary['recommendations'] = details.get('recommendations', [])\n                        summary['tools_analysis'] = details.get('tools_analysis', {})\n                        summary['config_valid'] = nested_summary.get('config_valid', summary.get('config_valid', False))\n                        summary['config_complete'] = nested_summary.get('config_complete', summary.get('config_complete', False))\n                        if 'validation' in details and summary.get('config_valid') is False:\n                            validation = details['validation']\n                            summary['config_valid'] = validation.get('config_structure_valid', False)\n                        if 'completeness' in details and summary.get('config_complete') is False:\n                            completeness = details['completeness']\n                            summary['config_complete'] = completeness.get('sections_complete', False)\n                        return summary\n                    elif 'data' in data:\n                        config_data = data['data']\n                        if 'summary' in config_data and 'details' in config_data:\n                            top_summary = config_data['summary']\n                            details = config_data['details']\n                            nested_summary = details.get('summary', {})\n                            summary = {**top_summary, **nested_summary}\n                            summary['recommendations'] = details.get('recommendations', [])\n                            summary['tools_analysis'] = details.get('tools_analysis', {})\n                            summary['config_valid'] = nested_summary.get('config_valid', summary.get('config_valid', False))\n                            summary['config_complete'] = nested_summary.get('config_complete', summary.get('config_complete', False))\n                            if 'validation' in details and summary.get('config_valid') is False:\n                                validation = details['validation']\n                                summary['config_valid'] = validation.get('config_structure_valid', False)\n                            if 'completeness' in details and summary.get('config_complete') is False:\n                                completeness = details['completeness']\n                                summary['config_complete'] = completeness.get('sections_complete', False)\n                            return summary\n                        else:\n                            summary = config_data.get('summary', {})\n                            summary['recommendations'] = config_data.get('recommendations', [])\n                            summary['tools_analysis'] = config_data.get('tools_analysis', {})\n                            if 'config_validation' in config_data and summary.get('config_valid') is None:\n                                config_validation = config_data['config_validation']\n                                summary['config_valid'] = config_validation.get('config_structure_valid', False)\n                            if 'completeness' in config_data and summary.get('config_complete') is None:\n                                completeness = config_data['completeness']\n                                summary['config_complete'] = completeness.get('sections_complete', False)\n                            return summary\n                    elif 'validation_results' in data:\n                        validation_results = data.get('validation_results', {})\n                        summary = validation_results.get('summary', {})\n                        summary['recommendations'] = validation_results.get('recommendations', [])\n                        summary['tools_analysis'] = validation_results.get('tools_analysis', {})\n                        return summary\n            else:\n                logger.warning(f\"[DATA SOURCE] config_validation_summary: not found at {config_file} (using empty fallback)\")\n        except Exception as e:\n            logger.warning(f\"[DATA SOURCE] config_validation_summary: failed to load - {e} (using empty fallback)\")\n        return None\n    \n    def _load_dev_tools_coverage(self) -> None:\n        \"\"\"Load dev tools coverage from JSON file if it exists.\"\"\"\n        # Check development_tools/tests/jsons first (new location), then fall back to old location (legacy)\n        coverage_path = self.project_root / \"development_tools\" / \"tests\" / \"jsons\" / \"coverage_dev_tools.json\"\n        if not coverage_path.exists():\n            coverage_path = self.project_root / \"development_tools\" / \"tests\" / \"coverage_dev_tools.json\"\n        if not coverage_path.exists():\n            return\n        try:\n            with coverage_path.open('r', encoding='utf-8') as handle:\n                coverage_data = json.load(handle)\n        except (OSError, json.JSONDecodeError):\n            return\n        files = coverage_data.get('files')\n        if not isinstance(files, dict) or not files:\n            return\n        total_statements = 0\n        total_missed = 0\n        for path, info in files.items():\n            summary = info.get('summary') or {}\n            statements = summary.get('num_statements') or 0\n            missed = summary.get('missing_lines') or 0\n            total_statements += statements\n            total_missed += missed\n        if total_statements == 0:\n            overall_coverage = 0.0\n        else:\n            overall_coverage = round((total_statements - total_missed) / total_statements * 100, 1)\n        module_data = self._load_coverage_json(coverage_path)\n        self.dev_tools_coverage_results = {\n            'overall': {\n                'overall_coverage': overall_coverage,\n                'total_statements': total_statements,\n                'total_missed': total_missed\n            },\n            'modules': module_data,\n            'coverage_collected': True,\n            'output_file': str(coverage_path),\n            'html_dir': None\n        }\n    \n    def _load_coverage_json(self, json_path: Path) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Load module metrics from coverage JSON output.\"\"\"\n        try:\n            with json_path.open('r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n        except (OSError, json.JSONDecodeError):\n            return {}\n        files = data.get('files', {})\n        coverage_data: Dict[str, Dict[str, Any]] = {}\n        for module_name, file_data in files.items():\n            summary = file_data.get('summary', {})\n            statements = int(summary.get('num_statements', 0))\n            covered = int(summary.get('covered_lines', statements - summary.get('missing_lines', 0)))\n            missed = int(summary.get('missing_lines', statements - covered))\n            percent = summary.get('percent_covered')\n            if isinstance(percent, float):\n                percent_value = int(round(percent))\n            else:\n                try:\n                    percent_value = int(percent)\n                except (TypeError, ValueError):\n                    percent_value = 0\n            missing_lines = file_data.get('missing_lines', [])\n            missing_line_strings = [str(line) for line in missing_lines]\n            coverage_data[module_name] = {\n                'statements': statements,\n                'missed': missed,\n                'coverage': percent_value,\n                'missing_lines': missing_line_strings,\n                'covered': covered\n            }\n        return coverage_data\n    \n    def _get_dev_tools_coverage_insights(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Return summarized dev tools coverage insights.\"\"\"\n        results = getattr(self, 'dev_tools_coverage_results', None)\n        if not results:\n            return None\n        modules = results.get('modules')\n        if (not modules) and results.get('output_file'):\n            try:\n                modules = self._load_coverage_json(Path(results['output_file']))\n            except Exception:\n                modules = {}\n        if not isinstance(modules, dict):\n            modules = {}\n        overall = results.get('overall') or {}\n        overall_pct = overall.get('overall_coverage') or overall.get('coverage')\n        total_statements = overall.get('total_statements')\n        total_missed = overall.get('total_missed')\n        if (total_statements is None or total_missed is None) and modules:\n            total_statements = sum((data.get('statements') or 0) for data in modules.values())\n            total_missed = sum((data.get('missed') or 0) for data in modules.values())\n        covered = None\n        if total_statements is not None and total_missed is not None:\n            covered = max(total_statements - total_missed, 0)\n        low_modules: List[Dict[str, Any]] = []\n        if modules:\n            sorted_modules = sorted(modules.items(), key=lambda kv: kv[1].get('coverage', 101))\n            for name, data in sorted_modules:\n                coverage_value = data.get('coverage')\n                if coverage_value is None:\n                    continue\n                low_modules.append({\n                    'path': name,\n                    'coverage': coverage_value,\n                    'missed': data.get('missed'),\n                    'statements': data.get('statements')\n                })\n                if len(low_modules) == 3:\n                    break\n        return {\n            'overall_pct': overall_pct,\n            'statements': total_statements,\n            'covered': covered,\n            'html': results.get('html_dir'),\n            'low_modules': low_modules,\n            'module_count': len(modules),\n        }\n    \n    def _parse_module_dependency_report(self, output: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Extract summary statistics from analyze_module_dependencies output.\"\"\"\n        if not output:\n            return None\n        summary: Dict[str, Any] = {}\n        patterns = {\n            'files_scanned': r\"Files scanned:\\s+(\\d+)\",\n            'total_imports': r\"Total imports found:\\s+(\\d+)\",\n            'documented_dependencies': r\"Dependencies documented:\\s+(\\d+)\",\n            'standard_library': r\"Standard library imports:\\s+(\\d+)\",\n            'third_party': r\"Third-party imports:\\s+(\\d+)\",\n            'local_imports': r\"Local imports:\\s+(\\d+)\",\n            'missing_dependencies': r\"Total missing dependencies:\\s+(\\d+)\",\n        }\n        for key, pattern in patterns.items():\n            match = re.search(pattern, output)\n            if match:\n                try:\n                    summary[key] = int(match.group(1))\n                except ValueError:\n                    summary[key] = None\n        missing_files = re.findall(r\"\\[FILE\\]\\s+([^:]+):\", output)\n        missing_sections = re.findall(r\"\\[DIR\\]\\s+(.+?) - ENTIRE FILE MISSING\", output)\n        if not summary and not missing_files and not missing_sections:\n            return None\n        summary['missing_files'] = missing_files\n        summary['missing_sections'] = missing_sections\n        return summary\n    \n    def _parse_test_results_from_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse test results from pytest output.\"\"\"\n        results = {\n            'random_seed': None,\n            'test_summary': None,\n            'failed_tests': [],\n            'passed_count': 0,\n            'failed_count': 0,\n            'skipped_count': 0,\n            'warnings_count': 0\n        }\n        if not output:\n            return results\n        # Extract random seed if pytest-randomly is used\n        seed_pattern = r'--randomly-seed=(\\d+)'\n        seed_match = re.search(seed_pattern, output)\n        if seed_match:\n            results['random_seed'] = seed_match.group(1)\n        # Extract test summary\n        summary_pattern = r'(\\d+)\\s+failed[,\\s]+(\\d+)\\s+passed[,\\s]+(\\d+)\\s+skipped[,\\s]+(\\d+)\\s+warnings'\n        summary_match = re.search(summary_pattern, output)\n        if summary_match:\n            results['failed_count'] = int(summary_match.group(1))\n            results['passed_count'] = int(summary_match.group(2))\n            results['skipped_count'] = int(summary_match.group(3))\n            results['warnings_count'] = int(summary_match.group(4))\n            results['test_summary'] = f\"{results['failed_count']} failed, {results['passed_count']} passed, {results['skipped_count']} skipped, {results['warnings_count']} warnings\"\n        # Extract failed test names\n        short_summary_pattern = r'short test summary info[^\\n]*\\n(.*?)(?=\\n===|$)'\n        short_summary_match = re.search(short_summary_pattern, output, re.DOTALL)\n        if short_summary_match:\n            summary_lines = short_summary_match.group(1).strip().split('\\n')\n            for line in summary_lines:\n                if line.strip().startswith('FAILED'):\n                    test_match = re.search(r'FAILED\\s+(.+)', line)\n                    if test_match:\n                        results['failed_tests'].append(test_match.group(1).strip())\n        return results\n    \n    def _parse_doc_sync_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Derive structured metrics from documentation sync output.\"\"\"\n        summary: Dict[str, Any] = {\n            'status': None,\n            'total_issues': None,\n            'paired_doc_issues': None,\n            'path_drift_issues': None,\n            'ascii_issues': None,\n            'path_drift_files': []\n        }\n        if not isinstance(output, str) or not output.strip():\n            return summary\n        lines_iter = output.splitlines()\n        path_section = False\n        for raw_line in lines_iter:\n            line = raw_line.strip()\n            if not line:\n                if path_section:\n                    path_section = False\n                continue\n            if line.startswith('Status:'):\n                summary['status'] = line.split(':', 1)[1].strip() or None\n                continue\n            if line.startswith('Total Issues:'):\n                value = self._extract_first_int(line)\n                if value is not None:\n                    summary['total_issues'] = value\n                continue\n            if line.startswith('Paired Doc Issues:'):\n                value = self._extract_first_int(line)\n                if value is not None:\n                    summary['paired_doc_issues'] = value\n                continue\n            if line.startswith('Path Drift Issues:'):\n                value = self._extract_first_int(line)\n                if value is not None:\n                    summary['path_drift_issues'] = value\n                continue\n            if line.startswith('ASCII Compliance Issues:'):\n                value = self._extract_first_int(line)\n                if value is not None:\n                    summary['ascii_issues'] = value\n                path_section = False\n                continue\n            if line.startswith('Heading Numbering Issues:'):\n                path_section = False\n                continue\n            if line.startswith('Top files with most issues:'):\n                path_section = True\n                continue\n            if (line.isupper() and ('ISSUES' in line or 'COMPLIANCE' in line or 'DOCUMENTATION' in line)) or \\\n               line.startswith('HEADING NUMBERING') or \\\n               line.startswith('ASCII COMPLIANCE') or \\\n               line.startswith('PAIRED DOCUMENTATION'):\n                path_section = False\n                continue\n            if path_section:\n                cleaned = line.lstrip('-*').strip()\n                if not cleaned:\n                    continue\n                if ':' in cleaned:\n                    file_part = cleaned.split(':', 1)[0].strip()\n                else:\n                    file_part = cleaned\n                if file_part and file_part.isupper() and ('ISSUES' in file_part or 'COMPLIANCE' in file_part):\n                    path_section = False\n                    continue\n                if file_part and file_part not in summary['path_drift_files']:\n                    summary['path_drift_files'].append(file_part)\n        return summary\n    \n    def _parse_documentation_sync_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse paired documentation sync output.\"\"\"\n        issues = {}\n        if not isinstance(output, str) or not output.strip():\n            return issues\n        lines = output.splitlines()\n        current_section = None\n        for line in lines:\n            line = line.strip()\n            if 'PAIRED DOCUMENTATION ISSUES:' in line:\n                current_section = 'paired_docs'\n                continue\n            if current_section == 'paired_docs' and line.startswith('   '):\n                if ':' in line:\n                    issue_type = line.split(':')[0].strip()\n                    if issue_type not in issues:\n                        issues[issue_type] = []\n                elif line.startswith('     - '):\n                    if current_section:\n                        last_type = list(issues.keys())[-1] if issues else None\n                        if last_type:\n                            issues[last_type].append(line[7:])\n        return issues\n    \n    def _parse_path_drift_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse path drift analysis output.\"\"\"\n        # Try to parse as JSON first\n        if output.strip().startswith('{'):\n            try:\n                parsed = json.loads(output)\n                from ..result_format import normalize_to_standard_format\n                normalized = normalize_to_standard_format('analyze_path_drift', parsed)\n                # Format conversion: Convert standard format to simplified format expected by aggregation code\n                # The aggregation code in _aggregate_documentation_sync_results() expects:\n                # {'files': {file: count}, 'total_issues': int}\n                # TODO: Consider refactoring aggregation code to use standard format directly\n                if 'summary' in normalized:\n                    logger.debug(\"_parse_path_drift_output: Converting standard format to simplified format for aggregation\")\n                    return {\n                        'files': normalized.get('files', {}),\n                        'total_issues': normalized.get('summary', {}).get('total_issues', 0),\n                        'detailed_issues': normalized.get('details', {}).get('detailed_issues', {})\n                    }\n                return normalized\n            except (json.JSONDecodeError, ValueError):\n                pass\n        # Fall back to text parsing if JSON parsing failed\n        result = {'files': {}, 'total_issues': 0}\n        if not isinstance(output, str) or not output.strip():\n            return result\n        lines = output.splitlines()\n        in_files_section = False\n        for line in lines:\n            line = line.strip()\n            if 'Total issues found:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['total_issues'] = value\n            elif 'Top files with most issues:' in line:\n                in_files_section = True\n                continue\n            elif in_files_section and ':' in line and line.endswith('issues'):\n                parts = line.split(':')\n                if len(parts) == 2:\n                    file_path = parts[0].strip()\n                    issue_count = self._extract_first_int(parts[1])\n                    if issue_count is not None:\n                        result['files'][file_path] = issue_count\n        return result\n    \n    def _parse_ascii_compliance_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse ASCII compliance check output. Returns standard format.\"\"\"\n        # Try to parse as JSON first\n        if output.strip().startswith('{'):\n            try:\n                parsed = json.loads(output)\n                from ..result_format import normalize_to_standard_format\n                return normalize_to_standard_format(\"analyze_ascii_compliance\", parsed)\n            except (json.JSONDecodeError, ValueError):\n                pass\n        # Fall back to text parsing if JSON parsing failed\n        files = {}\n        total_issues = 0\n        file_count = 0\n        if not isinstance(output, str) or not output.strip():\n            return self._create_standard_format_result(total_issues, file_count, files)\n        lines = output.splitlines()\n        for line in lines:\n            line = line.strip()\n            if 'Total files with non-ASCII characters:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    file_count = value\n            elif 'Total issues found:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    total_issues = value\n            elif ':' in line and ('issues' in line.lower() or 'characters' in line.lower()):\n                parts = line.split(':')\n                if len(parts) == 2:\n                    file_path = parts[0].strip()\n                    issue_text = parts[1].strip()\n                    issue_count = self._extract_first_int(issue_text)\n                    if issue_count is not None:\n                        files[file_path] = issue_count\n        return self._create_standard_format_result(total_issues, file_count, files)\n    \n    def _parse_heading_numbering_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse heading numbering check output. Returns standard format.\"\"\"\n        # Try to parse as JSON first\n        if output.strip().startswith('{'):\n            try:\n                parsed = json.loads(output)\n                from ..result_format import normalize_to_standard_format\n                return normalize_to_standard_format(\"analyze_heading_numbering\", parsed)\n            except (json.JSONDecodeError, ValueError):\n                pass\n        # Fall back to text parsing if JSON parsing failed\n        files = {}\n        total_issues = 0\n        file_count = 0\n        if not isinstance(output, str) or not output.strip():\n            return self._create_standard_format_result(total_issues, file_count, files)\n        lines = output.splitlines()\n        for line in lines:\n            line = line.strip()\n            if 'Total files with numbering issues:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    file_count = value\n            elif 'Total issues found:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    total_issues = value\n            elif ':' in line and 'issues' in line.lower():\n                parts = line.split(':')\n                if len(parts) == 2:\n                    file_path = parts[0].strip()\n                    issue_count = self._extract_first_int(parts[1])\n                    if issue_count is not None:\n                        files[file_path] = issue_count\n        return self._create_standard_format_result(total_issues, file_count, files)\n    \n    def _parse_missing_addresses_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse missing addresses check output.\"\"\"\n        # Try to parse as JSON first\n        if output.strip().startswith('{'):\n            try:\n                parsed = json.loads(output)\n                from ..result_format import normalize_to_standard_format\n                return normalize_to_standard_format(\"analyze_missing_addresses\", parsed)\n            except (json.JSONDecodeError, ValueError):\n                pass\n        # Fall back to text parsing if JSON parsing failed\n        result = {'files': [], 'total_issues': 0}\n        if not isinstance(output, str) or not output.strip():\n            return result\n        if 'All documentation files have file addresses!' in output:\n            return result\n        lines = output.splitlines()\n        for line in lines:\n            line = line.strip()\n            if 'Total files missing addresses:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['total_issues'] = value\n            elif line.startswith('- ') or line.startswith('  - '):\n                file_path = line.lstrip('- ').strip()\n                if file_path:\n                    result['files'].append(file_path)\n        return result\n    \n    def _parse_unconverted_links_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Parse unconverted links check output.\"\"\"\n        # Try to parse as JSON first\n        if output.strip().startswith('{'):\n            try:\n                parsed = json.loads(output)\n                from ..result_format import normalize_to_standard_format\n                return normalize_to_standard_format(\"analyze_unconverted_links\", parsed)\n            except (json.JSONDecodeError, ValueError):\n                pass\n        # Fall back to text parsing if JSON parsing failed\n        result = {'files': {}, 'total_issues': 0}\n        if not isinstance(output, str) or not output.strip():\n            return result\n        lines = output.splitlines()\n        for line in lines:\n            line = line.strip()\n            if 'Total files with unconverted links:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['file_count'] = value\n            elif 'Total issues found:' in line:\n                value = self._extract_first_int(line)\n                if value is not None:\n                    result['total_issues'] = value\n            elif ':' in line and 'issues' in line.lower():\n                parts = line.split(':')\n                if len(parts) == 2:\n                    file_path = parts[0].strip()\n                    issue_count = self._extract_first_int(parts[1])\n                    if issue_count is not None:\n                        result['files'][file_path] = issue_count\n        return result\n    \n    def _aggregate_doc_sync_results(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Aggregate results from all documentation sync tools into unified summary.\"\"\"\n        summary: Dict[str, Any] = {\n            'status': 'PASS',\n            'total_issues': 0,\n            'paired_doc_issues': 0,\n            'path_drift_issues': 0,\n            'ascii_issues': 0,\n            'heading_numbering_issues': 0,\n            'missing_address_issues': 0,\n            'unconverted_link_issues': 0,\n            'path_drift_files': []\n        }\n        # Aggregate paired docs\n        paired_docs = all_results.get('paired_docs', {})\n        if isinstance(paired_docs, dict):\n            for issue_type, issues in paired_docs.items():\n                if isinstance(issues, list):\n                    summary['paired_doc_issues'] += len(issues)\n                    summary['total_issues'] += len(issues)\n        # Aggregate path drift\n        path_drift = all_results.get('path_drift', {})\n        if isinstance(path_drift, dict):\n            summary['path_drift_issues'] = path_drift.get('total_issues', 0)\n            summary['total_issues'] += summary['path_drift_issues']\n            files = path_drift.get('files', {})\n            if isinstance(files, dict):\n                summary['path_drift_files'] = list(files.keys())[:10]\n        # Aggregate ASCII compliance\n        ascii_compliance = all_results.get('ascii_compliance', {})\n        if isinstance(ascii_compliance, dict):\n            summary['ascii_issues'] = ascii_compliance.get('total_issues', 0)\n            summary['total_issues'] += summary['ascii_issues']\n        # Aggregate heading numbering\n        heading_numbering = all_results.get('heading_numbering', {})\n        if isinstance(heading_numbering, dict):\n            summary['heading_numbering_issues'] = heading_numbering.get('total_issues', 0)\n            summary['total_issues'] += summary['heading_numbering_issues']\n        # Aggregate missing addresses\n        missing_addresses = all_results.get('missing_addresses', {})\n        if isinstance(missing_addresses, dict):\n            summary['missing_address_issues'] = missing_addresses.get('total_issues', 0)\n            summary['total_issues'] += summary['missing_address_issues']\n        # Aggregate unconverted links\n        unconverted_links = all_results.get('unconverted_links', {})\n        if isinstance(unconverted_links, dict):\n            summary['unconverted_link_issues'] = unconverted_links.get('total_issues', 0)\n            summary['total_issues'] += summary['unconverted_link_issues']\n        # Determine overall status\n        if summary['total_issues'] > 0:\n            summary['status'] = 'FAIL'\n        else:\n            summary['status'] = 'PASS'\n        return summary\n    \n    def _parse_legacy_output(self, output: str) -> Dict[str, Any]:\n        \"\"\"Extract headline metrics from the legacy cleanup output.\"\"\"\n        summary: Dict[str, Any] = {\n            'files_with_issues': None,\n            'legacy_markers': None,\n            'report_path': None\n        }\n        if not isinstance(output, str) or not output.strip():\n            return summary\n        for raw_line in output.splitlines():\n            line = raw_line.strip()\n            if not line:\n                continue\n            if line.startswith('Files with issues:'):\n                value = self._extract_first_int(line)\n                if value is not None:\n                    summary['files_with_issues'] = value\n                continue\n            if line.startswith('legacy_compatibility_markers:'):\n                value = self._extract_first_int(line)\n                if value is not None:\n                    summary['legacy_markers'] = value\n                continue\n            if 'report saved to:' in line.lower():\n                parts = line.split(':', 1)\n                if len(parts) == 2:\n                    summary['report_path'] = parts[1].strip()\n        return summary\n    \n    def _load_mtime_cached_tool_results(\n        self, \n        tool_name: str, \n        domain: str, \n        result: Dict,\n        parse_output_func: Optional[Callable[[str], Dict[str, Any]]] = None,\n        cache_converter_func: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Unified helper for loading results from mtime-cached tools.\n        \n        After tool execution, this method:\n        1. Loads from cache FIRST (fresh data after tool execution)\n        2. Converts cache format to results format (using converter or default)\n        3. Saves to results JSON\n        4. Returns results dict\n        5. Falls back to parsing output if cache fails\n        \"\"\"\n        from ..output_storage import save_tool_result, load_tool_cache\n        \n        # Step 1: Load from cache FIRST using standardized storage\n        cache_data = None\n        try:\n            cache_data = load_tool_cache(tool_name, domain, project_root=self.project_root)\n            if cache_data:\n                logger.debug(f\"Loaded {tool_name} from cache (fresh data after execution)\")\n        except Exception as cache_error:\n            logger.debug(f\"Failed to load {tool_name} from cache: {cache_error}\")\n        \n        # Step 2: Convert cache format to results format\n        if cache_data and isinstance(cache_data, dict):\n            # Handle both formats: wrapped {'data': {...}} and direct {file_path: {...}}\n            if 'data' in cache_data:\n                file_data = cache_data.get('data', {})\n            else:\n                # Direct format - cache_data IS the file data\n                file_data = cache_data\n            if file_data and isinstance(file_data, dict):\n                # CRITICAL: Filter out entries for files that no longer exist\n                # The cache may contain entries for deleted files, which would cause stale data\n                filtered_file_data = {}\n                for file_path_str, file_info in file_data.items():\n                    if isinstance(file_info, dict):\n                        # Check if file still exists\n                        try:\n                            file_path = self.project_root / file_path_str\n                            if file_path.exists():\n                                # Verify mtime matches (file hasn't been modified)\n                                cached_mtime = file_info.get('mtime')\n                                if cached_mtime is not None:\n                                    current_mtime = file_path.stat().st_mtime\n                                    if current_mtime == cached_mtime:\n                                        filtered_file_data[file_path_str] = file_info\n                                    else:\n                                        logger.debug(f\"Skipping {file_path_str} - mtime mismatch (file modified)\")\n                                else:\n                                    # No mtime in cache, include it but it will be re-scanned\n                                    filtered_file_data[file_path_str] = file_info\n                            else:\n                                logger.debug(f\"Skipping {file_path_str} - file no longer exists\")\n                        except (OSError, ValueError) as e:\n                            logger.debug(f\"Skipping {file_path_str} - error checking file: {e}\")\n                    else:\n                        # Invalid format, skip\n                        logger.debug(f\"Skipping {file_path_str} - invalid cache entry format\")\n                \n                # Use filtered data\n                file_data = filtered_file_data\n                # Use custom converter if provided, otherwise use default\n                if cache_converter_func:\n                    tool_result = cache_converter_func(file_data)\n                else:\n                    # Default converter\n                    files_with_issues = {}\n                    total_issues = 0\n                    for file_path, file_info in file_data.items():\n                        if isinstance(file_info, dict):\n                            results = file_info.get('results', [])\n                            if results:\n                                files_with_issues[file_path] = len(results)\n                                total_issues += len(results)\n                    tool_result = {\n                        'files': files_with_issues,\n                        'file_count': len(files_with_issues),\n                        'total_issues': total_issues\n                    }\n                \n                # Step 3: Save to results JSON and populate in-memory cache\n                try:\n                    save_tool_result(tool_name, domain, tool_result, project_root=self.project_root)\n                    logger.debug(f\"Regenerated {tool_name}_results.json from fresh cache\")\n                    # Populate in-memory cache for current audit run\n                    if not hasattr(self, 'results_cache'):\n                        self.results_cache = {}\n                    self.results_cache[tool_name] = tool_result\n                except Exception as save_error:\n                    logger.debug(f\"Failed to save {tool_name} results: {save_error}\")\n                \n                return tool_result\n        \n        # Step 4: Fallback to parsing output if cache failed\n        if result.get('output') or result.get('success'):\n            if parse_output_func:\n                tool_result = parse_output_func(result.get('output', ''))\n                try:\n                    save_tool_result(tool_name, domain, tool_result, project_root=self.project_root)\n                    logger.debug(f\"Saved {tool_name} results from parsed output\")\n                    # Populate in-memory cache for current audit run\n                    if not hasattr(self, 'results_cache'):\n                        self.results_cache = {}\n                    self.results_cache[tool_name] = tool_result\n                except Exception as save_error:\n                    logger.debug(f\"Failed to save {tool_name} results: {save_error}\")\n                return tool_result\n            else:\n                logger.warning(f\"No parser function provided for {tool_name}, cannot parse output\")\n        else:\n            logger.warning(f\"{tool_name} failed: {result.get('error', 'Unknown error')}\")\n        \n        # Return empty result if all else fails\n        return {\n            'files': {},\n            'file_count': 0,\n            'total_issues': 0\n        }\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 379,
                    "line_content": "# Fallback to old location (development_tools/tests/coverage.json) for backward compatibility",
                    "start": 21133,
                    "end": 21155
                  }
                ]
              ],
              [
                "development_tools\\shared\\service\\report_generation.py",
                "\"\"\"\nReport generation methods for AIToolsService.\n\nContains methods for generating AI_STATUS.md, AI_PRIORITIES.md, and consolidated_report.txt.\nThese methods are large (~4,300 lines total) and generate comprehensive status reports.\n\"\"\"\n\nimport re\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nfrom core.logger import get_component_logger\n\nlogger = get_component_logger(\"development_tools\")\n\n# Import audit orchestration helper\nfrom .audit_orchestration import _is_audit_in_progress\n\n\nclass ReportGenerationMixin:\n    \"\"\"Mixin class providing report generation methods to AIToolsService.\"\"\"\n    \n    def _resolve_report_path(self, report_path: str) -> Path:\n        \"\"\"Helper to resolve relative report paths to absolute paths.\"\"\"\n        if isinstance(report_path, str):\n            if not Path(report_path).is_absolute():\n                return self.project_root / report_path\n            else:\n                return Path(report_path)\n        else:\n            return Path(report_path) if not isinstance(report_path, Path) else report_path\n    \n    def _get_results_file_path(self) -> Path:\n        \"\"\"Get the results file path from config.\"\"\"\n        # Default matches development_tools_config.json for backward compatibility\n        results_file_path = (self.audit_config or {}).get('results_file', 'development_tools/reports/analysis_detailed_results.json')\n        return self._resolve_report_path(results_file_path)\n    \n    def _is_test_directory(self, path: Path) -> bool:\n        \"\"\"Check if path is within a test directory to avoid loading large result files.\n        \n        This is critical for preventing memory leaks in parallel test execution.\n        \"\"\"\n        try:\n            path_str = str(path.resolve()).replace('\\\\', '/').lower()\n            \n            # Check for Windows temp directories (most common case for pytest-xdist)\n            # Windows temp dirs are typically: C:\\Users\\...\\AppData\\Local\\Temp\\...\n            if 'appdata' in path_str and ('temp' in path_str or 'tmp' in path_str):\n                return True\n            \n            # Check for pytest temp directories (pytest-xdist creates these)\n            if 'pytest' in path_str and ('temp' in path_str or 'tmp' in path_str):\n                return True\n            \n            # Check for common temp directory patterns\n            test_indicators = [\n                '/tmp', '/temp',  # Unix-style temp\n                '/tests/', '/test/',  # Test directories\n                'tests/data/', 'tests/fixtures/', 'tests/temp/',\n                'demo_project',  # Demo project used in tests\n                'pytest-', 'pytest_of_',  # pytest temp directories\n            ]\n            if any(indicator in path_str for indicator in test_indicators):\n                return True\n            \n            # Additional check: if path contains a tempfile pattern (tmpXXXXXX)\n            import re\n            if re.search(r'[\\\\/]tmp[a-z0-9]{6,}[\\\\/]', path_str):\n                return True\n            \n            return False\n        except Exception as e:\n            # If we can't determine, be conservative and assume it's not a test directory\n            # But log it so we can debug\n            logger.debug(f\"Error checking if path is test directory ({path}): {e}\")\n            return False\n    \n    def _load_results_file_safe(self) -> Optional[Dict]:\n        \"\"\"Load analysis_detailed_results.json if it exists and we're not in a test directory.\n        \n        Returns None if in test directory or file doesn't exist to prevent loading large files in tests.\n        \"\"\"\n        if self._is_test_directory(self.project_root):\n            return None\n        results_file = self._get_results_file_path()\n        if not results_file.exists():\n            return None\n        try:\n            import json\n            with open(results_file, 'r', encoding='utf-8') as f:\n                return json.load(f)\n        except Exception:\n            return None\n    \n    def _generate_ai_status_document(self) -> str:\n        \"\"\"Generate AI-optimized status document.\"\"\"\n        # Log data source context\n        audit_tier = getattr(self, 'current_audit_tier', None)\n        if audit_tier:\n            logger.info(f\"[REPORT GENERATION] Generating AI_STATUS.md using data from Tier {audit_tier} audit\")\n        else:\n            logger.info(f\"[REPORT GENERATION] Generating AI_STATUS.md using cached data (no active audit)\")\n        \n        # Check if this is a mid-audit write\n        instance_flag = hasattr(self, '_audit_in_progress') and self._audit_in_progress\n        audit_in_progress = instance_flag or _is_audit_in_progress(self.project_root)\n        is_legitimate_end_write = hasattr(self, 'current_audit_tier') and self.current_audit_tier is not None\n        \n        if audit_in_progress and not is_legitimate_end_write:\n            if not instance_flag:\n                logger.warning(\"_generate_ai_status_document() called from NEW instance during audit! This should only happen at the end.\")\n            else:\n                logger.warning(\"_generate_ai_status_document() called during audit! This should only happen at the end.\")\n            import traceback\n            logger.debug(f\"Call stack:\\n{''.join(traceback.format_stack())}\")\n        \n        lines: List[str] = []\n        lines.append(\"# AI Status - Current Codebase State\")\n        lines.append(\"\")\n        lines.append(\"> **File**: `development_tools/AI_STATUS.md`\")\n        lines.append(\"> **Generated**: This file is auto-generated. Do not edit manually.\")\n        lines.append(f\"> **Last Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        \n        if hasattr(self, '_audit_in_progress') and self._audit_in_progress and self.current_audit_tier is None:\n            logger.warning(\"_generate_ai_status_document() called during audit but current_audit_tier is None!\")\n        \n        if self.current_audit_tier == 1:\n            source_cmd = \"python development_tools/run_development_tools.py audit --quick\"\n            tier_name = \"Tier 1 (Quick Audit)\"\n        elif self.current_audit_tier == 3:\n            source_cmd = \"python development_tools/run_development_tools.py audit --full\"\n            tier_name = \"Tier 3 (Full Audit)\"\n        elif self.current_audit_tier == 2:\n            source_cmd = \"python development_tools/run_development_tools.py audit\"\n            tier_name = \"Tier 2 (Standard Audit)\"\n        else:\n            source_cmd = \"python development_tools/run_development_tools.py status\"\n            tier_name = \"Status Check (cached data)\"\n        lines.append(f\"> **Source**: `{source_cmd}`\")\n        if self.current_audit_tier:\n            lines.append(f\"> **Last Audit Tier**: {tier_name}\")\n        lines.append(\"> **Generated by**: run_development_tools.py - AI Development Tools Runner\")\n        lines.append(\"\")\n        \n        def percent_text(value: Any, decimals: int = 1) -> str:\n            if value is None:\n                return \"Unknown\"\n            if isinstance(value, str):\n                return value if value.strip().endswith('%') else f\"{value}%\"\n            return self._format_percentage(value, decimals)\n        \n        def to_int(value: Any) -> Optional[int]:\n            if isinstance(value, int):\n                return value\n            if isinstance(value, float):\n                return int(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return int(float(stripped))\n                except ValueError:\n                    return None\n            if isinstance(value, dict):\n                count = value.get('count')\n                return to_int(count)\n            return None\n        \n        def to_float(value: Any) -> Optional[float]:\n            if isinstance(value, (int, float)):\n                return float(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return float(stripped)\n                except ValueError:\n                    return None\n            return None\n        \n        metrics = self._get_canonical_metrics()\n        if not isinstance(metrics, dict):\n            metrics = {}\n        \n        # Load all tool data using unified loader\n        doc_metrics = self._load_tool_data('analyze_function_registry', 'functions')\n        error_metrics = self._load_tool_data('analyze_error_handling', 'error_handling')\n        function_metrics = self._load_tool_data('analyze_functions', 'functions')\n        analyze_docs_data = self._load_tool_data('analyze_documentation', 'docs')\n        ascii_data = self._load_tool_data('analyze_ascii_compliance', 'docs')\n        heading_data = self._load_tool_data('analyze_heading_numbering', 'docs')\n        missing_addresses_data = self._load_tool_data('analyze_missing_addresses', 'docs')\n        unconverted_links_data = self._load_tool_data('analyze_unconverted_links', 'docs')\n        \n        # Extract overlap analysis data\n        details = analyze_docs_data.get('details', {})\n        overlap_analysis_ran = (\n            'section_overlaps' in analyze_docs_data or \n            'consolidation_recommendations' in analyze_docs_data or\n            'section_overlaps' in details or \n            'consolidation_recommendations' in details\n        )\n        section_overlaps = (\n            analyze_docs_data.get('section_overlaps') or \n            details.get('section_overlaps', {})\n        ) if overlap_analysis_ran else {}\n        consolidation_recs = (\n            analyze_docs_data.get('consolidation_recommendations') or \n            details.get('consolidation_recommendations', [])\n        ) if overlap_analysis_ran else []\n        if section_overlaps is None:\n            section_overlaps = {}\n        if consolidation_recs is None:\n            consolidation_recs = []\n        \n        doc_coverage = doc_metrics.get('doc_coverage', metrics.get('doc_coverage', 'Unknown'))\n        missing_docs = doc_metrics.get('missing_docs') or doc_metrics.get('missing_items')\n        missing_files = self._get_missing_doc_files(limit=4)\n        \n        def get_error_field(field_name, default=None):\n            error_details = error_metrics.get('details', {})\n            return error_details.get(field_name, error_metrics.get(field_name, default))\n        \n        error_summary = error_metrics.get('summary', {})\n        error_details = error_metrics.get('details', {})\n        missing_error_handlers = to_int(error_summary.get('total_issues'))\n        if missing_error_handlers is None:\n            missing_error_handlers = to_int(error_details.get('functions_missing_error_handling')) or to_int(error_metrics.get('functions_missing_error_handling')) or 0\n        details_missing = to_int(error_details.get('functions_missing_error_handling'))\n        if details_missing is not None and details_missing > 0 and (missing_error_handlers is None or missing_error_handlers == 0):\n            missing_error_handlers = details_missing\n        \n        error_coverage = error_details.get('analyze_error_handling') or error_details.get('error_handling_coverage') or error_metrics.get('analyze_error_handling') or error_metrics.get('error_handling_coverage')\n        error_total = error_details.get('total_functions') or error_metrics.get('total_functions')\n        error_with_handling = error_details.get('functions_with_error_handling') or error_metrics.get('functions_with_error_handling')\n        canonical_total = metrics.get('total_functions')\n        \n        if error_total and error_with_handling:\n            calc_coverage = (error_with_handling / error_total) * 100\n            if 0 <= calc_coverage <= 100:\n                error_coverage = calc_coverage\n        elif error_coverage is None and error_total and error_with_handling:\n            error_coverage = (error_with_handling / error_total) * 100\n        \n        worst_error_modules = error_details.get('worst_modules') or error_metrics.get('worst_modules') or []\n        if worst_error_modules is None or not isinstance(worst_error_modules, (list, tuple)):\n            worst_error_modules = []\n        \n        coverage_summary = self._load_coverage_summary() or {}\n        \n        if not hasattr(self, 'dev_tools_coverage_results') or not self.dev_tools_coverage_results:\n            self._load_dev_tools_coverage()\n        \n        doc_sync_data = self._load_tool_data('analyze_documentation_sync', 'docs', log_source=True)\n        doc_sync_summary = self.docs_sync_summary or doc_sync_data or {}\n        if not isinstance(doc_sync_summary, dict):\n            doc_sync_summary = {}\n        \n        legacy_data = self._load_tool_data('analyze_legacy_references', 'legacy')\n        legacy_summary = self.legacy_cleanup_summary or legacy_data or {}\n        \n        lines.append(\"## Snapshot\")\n        \n        total_functions = metrics.get('total_functions', 'Unknown') if metrics else 'Unknown'\n        moderate = metrics.get('moderate', 'Unknown') if metrics else 'Unknown'\n        high = metrics.get('high', 'Unknown') if metrics else 'Unknown'\n        critical = metrics.get('critical', 'Unknown') if metrics else 'Unknown'\n        \n        # Try loading from cache if Unknown (skip in test directories to prevent memory issues)\n        if total_functions == 'Unknown' or moderate == 'Unknown':\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data:\n                    if 'results' in cached_data and 'analyze_functions' in cached_data['results']:\n                        func_data = cached_data['results']['analyze_functions']\n                        if 'data' in func_data:\n                            cached_metrics_raw = func_data['data']\n                            if 'details' in cached_metrics_raw and isinstance(cached_metrics_raw.get('details'), dict):\n                                cached_metrics = cached_metrics_raw['details']\n                            else:\n                                cached_metrics = cached_metrics_raw\n                            if total_functions == 'Unknown':\n                                total_functions = cached_metrics.get('total_functions', 'Unknown')\n                            if moderate == 'Unknown':\n                                moderate = cached_metrics.get('moderate_complexity', 'Unknown')\n                            if high == 'Unknown':\n                                high = cached_metrics.get('high_complexity', 'Unknown')\n                            if critical == 'Unknown':\n                                critical = cached_metrics.get('critical_complexity', 'Unknown')\n                    if (total_functions == 'Unknown' or moderate == 'Unknown') and 'results' in cached_data:\n                        if 'decision_support' in cached_data['results']:\n                            ds_data = cached_data['results']['decision_support']\n                            if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                                ds_metrics = ds_data['data']['decision_support_metrics']\n                                if total_functions == 'Unknown':\n                                    total_functions = ds_metrics.get('total_functions', 'Unknown')\n                                if moderate == 'Unknown':\n                                    moderate = ds_metrics.get('moderate_complexity', 'Unknown')\n                                if high == 'Unknown':\n                                    high = ds_metrics.get('high_complexity', 'Unknown')\n                                if critical == 'Unknown':\n                                    critical = ds_metrics.get('critical_complexity', 'Unknown')\n            except Exception as e:\n                logger.debug(f\"Failed to load metrics from cache: {e}\")\n                pass\n        \n        if total_functions == 'Unknown':\n            lines.append(\"- **Total Functions**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n        else:\n            lines.append(f\"- **Total Functions**: {total_functions} (Moderate: {moderate}, High: {high}, Critical: {critical})\")\n        \n        # Use registry data for accurate docstring coverage (includes handlers)\n        # Registry data is more accurate because it includes all functions, not just non-handlers\n        doc_coverage = metrics.get('doc_coverage', 'Unknown')\n        functions_without_docstrings = None\n        missing_docs = None\n        missing_files = []\n        \n        # First, try to get coverage from registry (most accurate)\n        registry_data = self._load_tool_data('analyze_function_registry', 'functions')\n        if isinstance(registry_data, dict):\n            registry_details = registry_data.get('details', {})\n            # Check for coverage percentage directly\n            registry_coverage = registry_details.get('coverage')\n            if registry_coverage is not None:\n                doc_coverage = f\"{registry_coverage:.2f}%\"\n                # Calculate undocumented count from registry data\n                undocumented_handlers = registry_details.get('undocumented_handlers_total', 0)\n                undocumented_other = registry_details.get('undocumented_other_total', 0)\n                functions_without_docstrings = undocumented_handlers + undocumented_other\n        \n        # Fallback to analyze_functions if registry data not available\n        if doc_coverage == 'Unknown' or functions_without_docstrings is None:\n            function_data = self._load_tool_data('analyze_functions', 'functions')\n            if isinstance(function_data, dict):\n                func_details = function_data.get('details', {})\n                func_total = func_details.get('total_functions') or function_data.get('total_functions')\n                func_undocumented = func_details.get('undocumented', 0) or function_data.get('undocumented', 0)\n                if func_total is not None and func_total > 0:\n                    func_documented = func_total - func_undocumented\n                    coverage_pct = (func_documented / func_total) * 100\n                    doc_coverage = f\"{coverage_pct:.2f}%\"\n                    functions_without_docstrings = int(func_undocumented) if func_undocumented else 0\n        \n        # Fallback to cached results\n        if (doc_coverage == 'Unknown' or doc_coverage is None) and functions_without_docstrings is None:\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data:\n                    if 'results' in cached_data and 'analyze_functions' in cached_data['results']:\n                        func_data = cached_data['results']['analyze_functions']\n                        if 'data' in func_data:\n                            func_metrics_raw = func_data['data']\n                            if 'details' in func_metrics_raw and isinstance(func_metrics_raw.get('details'), dict):\n                                func_metrics = func_metrics_raw['details']\n                            else:\n                                func_metrics = func_metrics_raw\n                            func_total = func_metrics.get('total_functions')\n                            func_undocumented = func_metrics.get('undocumented', 0)\n                            if func_total is not None and func_total > 0:\n                                func_documented = func_total - func_undocumented\n                                coverage_pct = (func_documented / func_total) * 100\n                                doc_coverage = f\"{coverage_pct:.2f}%\"\n                                functions_without_docstrings = func_undocumented\n            except Exception as e:\n                logger.debug(f\"Failed to load doc_coverage from cache in status: {e}\")\n                pass\n        \n        # Final fallback to results cache\n        if (doc_coverage == 'Unknown' or doc_coverage is None) and functions_without_docstrings is None and total_functions is not None:\n            func_cache_raw = self.results_cache.get('analyze_functions', {})\n            if isinstance(func_cache_raw, dict):\n                if 'details' in func_cache_raw and isinstance(func_cache_raw.get('details'), dict):\n                    func_cache = func_cache_raw['details']\n                else:\n                    func_cache = func_cache_raw\n                func_undocumented = func_cache.get('undocumented', 0)\n                if isinstance(func_undocumented, (int, float)) and isinstance(total_functions, (int, float)) and total_functions > 0:\n                    func_documented = total_functions - func_undocumented\n                    coverage_pct = (func_documented / total_functions) * 100\n                    doc_coverage = f\"{coverage_pct:.2f}%\"\n                    functions_without_docstrings = int(func_undocumented)\n        \n        # Check registry for missing items (if not already loaded above)\n        if not isinstance(registry_data, dict) or missing_docs is None:\n            if not isinstance(registry_data, dict):\n                registry_data = self._load_tool_data('analyze_function_registry', 'functions')\n            missing_docs = None\n            missing_files = []\n            \n            if isinstance(registry_data, dict):\n                registry_details = registry_data.get('details', {})\n                missing_docs_raw = registry_details.get('missing_docs') or registry_details.get('missing_items') or registry_data.get('missing_docs') or registry_data.get('missing_items')\n                if not missing_docs_raw:\n                    data_section = registry_data.get('data', {})\n                    missing_docs_raw = data_section.get('missing', {}) if isinstance(data_section, dict) else {}\n                if isinstance(missing_docs_raw, dict):\n                    missing_docs = missing_docs_raw\n                    missing_files = missing_docs_raw.get('missing_files', [])\n                elif missing_docs_raw:\n                    missing_docs = {'count': to_int(missing_docs_raw) or 0}\n        \n        # Fallback to cached results\n        if not missing_docs:\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data:\n                    if 'results' in cached_data and 'analyze_function_registry' in cached_data['results']:\n                        func_reg_data = cached_data['results']['analyze_function_registry']\n                        if 'data' in func_reg_data:\n                            cached_metrics = func_reg_data['data']\n                            missing_docs_raw = cached_metrics.get('missing') or cached_metrics.get('missing_docs') or cached_metrics.get('missing_items')\n                            if missing_docs_raw:\n                                if isinstance(missing_docs_raw, dict):\n                                    missing_docs = missing_docs_raw\n                                else:\n                                    missing_docs = {'count': to_int(missing_docs_raw) or 0}\n                            missing_files = cached_metrics.get('missing_files', [])\n            except Exception as e:\n                logger.debug(f\"Failed to load registry missing data: {e}\")\n                pass\n        \n        missing_docstrings_count = functions_without_docstrings if functions_without_docstrings else 0\n        doc_line = f\"- **Function Docstring Coverage**: {percent_text(doc_coverage, 2)}\"\n        if missing_docstrings_count > 0:\n            doc_line += f\" ({missing_docstrings_count} functions missing docstrings)\"\n        else:\n            doc_line += \" (0 functions missing docstrings)\"\n        lines.append(doc_line)\n        \n        missing_count = 0\n        if missing_docs:\n            if isinstance(missing_docs, dict):\n                missing_count = missing_docs.get('count', 0)\n            else:\n                missing_count = to_int(missing_docs) or 0\n        \n        if missing_count > 0:\n            lines.append(f\"- **Registry Gaps**: {missing_count} items missing from registry\")\n        else:\n            lines.append(f\"- **Registry Gaps**: 0 items missing from registry\")\n        \n        if missing_files:\n            lines.append(f\"- **Missing Documentation Files**: {self._format_list_for_display(missing_files, limit=4)}\")\n        \n        # Error handling coverage\n        error_coverage_from_cache = 'Unknown'\n        error_total = None\n        error_with_handling = None\n        if not error_metrics or error_coverage == 'Unknown':\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data and 'results' in cached_data and 'analyze_error_handling' in cached_data['results']:\n                        error_data = cached_data['results']['analyze_error_handling']\n                        if 'data' in error_data:\n                            cached_metrics = error_data['data']\n                            error_coverage_from_cache = cached_metrics.get('analyze_error_handling') or cached_metrics.get('error_handling_coverage', 'Unknown')\n                            if missing_error_handlers is None or missing_error_handlers == 0:\n                                missing_error_handlers = to_int(cached_metrics.get('functions_missing_error_handling'))\n                            error_total = cached_metrics.get('total_functions')\n                            error_with_handling = cached_metrics.get('functions_with_error_handling')\n                            if error_coverage_from_cache != 'Unknown':\n                                error_coverage = error_coverage_from_cache\n            except Exception:\n                pass\n        \n        if missing_error_handlers is None or missing_error_handlers == 0:\n            if error_metrics:\n                error_details = error_metrics.get('details', {})\n                missing_error_handlers = to_int(error_details.get('functions_missing_error_handling')) or to_int(error_metrics.get('functions_missing_error_handling')) or 0\n        \n        canonical_total = metrics.get('total_functions')\n        if error_coverage is not None and canonical_total and error_total and error_with_handling:\n            if error_total != canonical_total:\n                recalc_coverage = (error_with_handling / canonical_total) * 100\n                if 0 <= recalc_coverage <= 100:\n                    error_coverage = recalc_coverage\n        \n        lines.append(\n            f\"- **Error Handling Coverage**: {percent_text(error_coverage, 1)}\"\n            + (f\" ({missing_error_handlers} functions without handlers)\" if missing_error_handlers is not None else \"\")\n        )\n        \n        # Doc sync status\n        if not doc_sync_summary:\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data and 'results' in cached_data and 'analyze_documentation' in cached_data['results']:\n                        doc_sync_data = cached_data['results']['analyze_documentation']\n                        if 'data' in doc_sync_data:\n                            cached_metrics = doc_sync_data['data']\n                            doc_sync_summary = {\n                                'status': 'GOOD' if not cached_metrics.get('artifacts') else 'NEEDS REVIEW',\n                                'total_issues': len(cached_metrics.get('artifacts', []))\n                            }\n            except Exception:\n                pass\n        \n        if doc_sync_summary:\n            def get_doc_sync_field(data, field_name, default=None):\n                if not data or not isinstance(data, dict):\n                    return default\n                if 'summary' in data and isinstance(data.get('summary'), dict):\n                    if field_name == 'status':\n                        return data['summary'].get('status', default)\n                    elif field_name == 'total_issues':\n                        return data['summary'].get('total_issues', default)\n                    else:\n                        return data.get('details', {}).get(field_name, default)\n                else:\n                    return data.get(field_name, default)\n            \n            sync_status = get_doc_sync_field(doc_sync_summary, 'status', 'Unknown')\n            total_issues = get_doc_sync_field(doc_sync_summary, 'total_issues')\n            if total_issues is None or total_issues == 0:\n                path_drift_issues = get_doc_sync_field(doc_sync_summary, 'path_drift_issues', 0)\n                paired_doc_issues = get_doc_sync_field(doc_sync_summary, 'paired_doc_issues', 0)\n                ascii_issues = get_doc_sync_field(doc_sync_summary, 'ascii_issues', 0)\n                total_issues = (path_drift_issues or 0) + (paired_doc_issues or 0) + (ascii_issues or 0)\n            \n            sync_line = f\"- **Doc Sync**: {sync_status}\"\n            if total_issues is not None and total_issues > 0:\n                sync_line += f\" ({total_issues} tracked issues)\"\n            lines.append(sync_line)\n        else:\n            lines.append(\"- **Doc Sync**: Not collected in this run (pending doc-sync refresh)\")\n        \n        # Test coverage\n        if coverage_summary and isinstance(coverage_summary, dict):\n            overall = coverage_summary.get('overall') or {}\n            if overall.get('coverage') is not None:\n                lines.append(\n                    f\"- **Test Coverage**: {percent_text(overall.get('coverage'), 1)} \"\n                    f\"({overall.get('covered')} of {overall.get('statements')} statements)\"\n                )\n        \n        lines.append(\"\")\n        lines.append(\"## Documentation Signals\")\n        \n        # Helper to extract doc sync field (handles both standard and old format)\n        def get_doc_sync_field(data, field_name, default=None):\n            if not data or not isinstance(data, dict):\n                return default\n            # Check standard format first\n            if 'summary' in data and isinstance(data.get('summary'), dict):\n                # Standard format - check summary for status and total_issues, details for other fields\n                if field_name == 'status':\n                    return data['summary'].get('status', default)\n                elif field_name == 'total_issues':\n                    return data['summary'].get('total_issues', default)\n                else:\n                    return data.get('details', {}).get(field_name, default)\n            else:\n                # Old format - direct access\n                return data.get(field_name, default)\n        \n        # Use aggregated doc sync summary from current run first, then fall back to cache\n        doc_sync_summary_for_signals = None\n        if hasattr(self, 'docs_sync_summary') and self.docs_sync_summary and isinstance(self.docs_sync_summary, dict):\n            # Use the aggregated summary from _run_doc_sync_check() - use helper to handle both formats\n            doc_sync_summary_for_signals = {\n                'status': get_doc_sync_field(self.docs_sync_summary, 'status', 'UNKNOWN'),\n                'path_drift_issues': get_doc_sync_field(self.docs_sync_summary, 'path_drift_issues', 0),\n                'paired_doc_issues': get_doc_sync_field(self.docs_sync_summary, 'paired_doc_issues', 0),\n                'ascii_issues': get_doc_sync_field(self.docs_sync_summary, 'ascii_issues', 0),\n                'heading_numbering_issues': get_doc_sync_field(self.docs_sync_summary, 'heading_numbering_issues', 0),\n                'missing_address_issues': get_doc_sync_field(self.docs_sync_summary, 'missing_address_issues', 0),\n                'unconverted_link_issues': get_doc_sync_field(self.docs_sync_summary, 'unconverted_link_issues', 0),\n                'path_drift_files': get_doc_sync_field(self.docs_sync_summary, 'path_drift_files', [])\n            }\n        \n        # Fall back to cache if not available in memory\n        if not doc_sync_summary_for_signals:\n            # Load doc sync data using unified loader (returns standard format)\n            doc_sync_result = self._load_tool_data('analyze_documentation_sync', 'docs')\n            if doc_sync_result:\n                doc_sync_summary_for_signals = {\n                    'status': get_doc_sync_field(doc_sync_result, 'status', 'UNKNOWN'),\n                    'path_drift_issues': get_doc_sync_field(doc_sync_result, 'path_drift_issues', 0),\n                    'paired_doc_issues': get_doc_sync_field(doc_sync_result, 'paired_doc_issues', 0),\n                    'ascii_issues': get_doc_sync_field(doc_sync_result, 'ascii_issues', 0),\n                    'heading_numbering_issues': get_doc_sync_field(doc_sync_result, 'heading_numbering_issues', 0),\n                    'missing_address_issues': get_doc_sync_field(doc_sync_result, 'missing_address_issues', 0),\n                    'unconverted_link_issues': get_doc_sync_field(doc_sync_result, 'unconverted_link_issues', 0),\n                    'path_drift_files': get_doc_sync_field(doc_sync_result, 'path_drift_files', [])\n                }\n        \n        # Extract values from doc_sync_summary_for_signals (if available) for use throughout the section\n        path_drift = None\n        paired = None\n        ascii_issues = None\n        if doc_sync_summary_for_signals:\n            path_drift = get_doc_sync_field(doc_sync_summary_for_signals, 'path_drift_issues', 0)\n            if path_drift is None:\n                path_drift = 0\n            \n            paired = get_doc_sync_field(doc_sync_summary_for_signals, 'paired_doc_issues', 0)\n            if paired is None:\n                paired = 0\n            \n            ascii_issues = get_doc_sync_field(doc_sync_summary_for_signals, 'ascii_issues', 0)\n            if ascii_issues is None:\n                ascii_issues = 0\n        \n        if doc_sync_summary_for_signals:\n            \n            # Check for path validation issues first (separate from path_drift - checks if referenced paths exist)\n            # Path validation is more critical than path drift\n            path_val_issues = None\n            path_val_status = None\n            if hasattr(self, 'path_validation_result') and self.path_validation_result:\n                if isinstance(self.path_validation_result, dict):\n                    path_val_status = self.path_validation_result.get('status')\n                    path_val_issues = self.path_validation_result.get('issues_found', 0)\n                    # Also check if issues_found is in the result but status might be different\n                    if path_val_issues is None or path_val_issues == 0:\n                        # Try to get issues from details if available\n                        details = self.path_validation_result.get('details', {})\n                        if details and isinstance(details, dict):\n                            # Count total issues from details\n                            total_issues = sum(len(issues) if isinstance(issues, (list, dict)) else 1 for issues in details.values())\n                            if total_issues > 0:\n                                path_val_issues = total_issues\n                                if path_val_status != 'ok':\n                                    path_val_status = 'fail'\n            \n            # Then check path drift (documentation path changes)\n            # Path drift checks for documentation path inconsistencies\n            # Path validation checks if referenced paths actually exist\n            # Get path_drift using helper to handle standard format\n            path_drift = get_doc_sync_field(doc_sync_summary_for_signals, 'path_drift_issues', 0)\n            if path_drift is None:\n                path_drift = 0\n            # They can have different issues, but if path validation found issues, we show those under Path Drift\n            # to avoid duplication\n            if path_drift is not None:\n                if path_drift == 0:\n                    # Path drift tool found 0 issues\n                    if path_val_issues is None:\n                        # Path validation didn't run, so trust path drift\n                        severity = \"CLEAN\"\n                        lines.append(f\"- **Path Drift**: {severity} ({path_drift} issues)\")\n                    elif path_val_issues == 0:\n                        # Both path drift and path validation found 0 issues\n                        severity = \"CLEAN\"\n                        lines.append(f\"- **Path Drift**: {severity} ({path_drift} issues)\")\n                    else:\n                        # Path drift found 0, but path validation found issues - show path validation issues\n                        # Don't duplicate by showing both Path Validation and Path Drift\n                        severity = \"NEEDS ATTENTION\"\n                        lines.append(f\"- **Path Drift**: {severity} ({path_val_issues} referenced paths don't exist)\")\n                else:\n                    # Path drift found issues - show those\n                    severity = \"NEEDS ATTENTION\"\n                    lines.append(f\"- **Path Drift**: {severity} ({path_drift} issues)\")\n            elif path_val_issues is not None and path_val_issues > 0:\n                # Path drift didn't run, but path validation found issues\n                lines.append(f\"- **Path Drift**: NEEDS ATTENTION ({path_val_issues} referenced paths don't exist)\")\n            else:\n                # If neither path_drift nor path_validation ran, show unknown\n                lines.append(\"- **Path Drift**: Unknown (run `audit` to check)\")\n            \n            if paired is not None:\n                status_label = \"SYNCHRONIZED\" if paired == 0 else \"NEEDS ATTENTION\"\n                lines.append(f\"- **Paired Docs**: {status_label} ({paired} issues)\")\n                # Add details about paired doc issues if available\n                if paired > 0 and doc_sync_summary_for_signals:\n                    paired_docs_data = doc_sync_summary_for_signals.get('paired_docs', {})\n                    if isinstance(paired_docs_data, dict):\n                        content_sync_issues = paired_docs_data.get('content_sync', [])\n                        if content_sync_issues:\n                            # Show first 2-3 issues\n                            for issue in content_sync_issues[:3]:\n                                lines.append(f\"  - {issue}\")\n                            if len(content_sync_issues) > 3:\n                                lines.append(f\"  - ...and {len(content_sync_issues) - 3} more issue(s)\")\n        \n        # Add ASCII Compliance to Documentation Signals\n        # First check doc_sync_summary (aggregated), then check direct tool result\n        if ascii_issues is not None and ascii_issues > 0:\n            lines.append(f\"- **ASCII Compliance**: {ascii_issues} files contain non-ASCII characters\")\n        elif ascii_data and isinstance(ascii_data, dict):\n            # Use standard format\n            summary = ascii_data.get('summary', {})\n            ascii_total = summary.get('total_issues', 0)\n            ascii_file_count = summary.get('files_affected', 0)\n            if ascii_total > 0 or ascii_file_count > 0:\n                lines.append(f\"- **ASCII Compliance**: {ascii_total} issues in {ascii_file_count} files\")\n            else:\n                lines.append(\"- **ASCII Compliance**: CLEAN (all files are ASCII-compliant)\")\n        \n        # Add Heading Numbering to Documentation Signals\n        if heading_data and isinstance(heading_data, dict):\n            # Use standard format\n            summary = heading_data.get('summary', {})\n            heading_total = summary.get('total_issues', 0)\n            heading_file_count = summary.get('files_affected', 0)\n            if heading_total > 0 or heading_file_count > 0:\n                lines.append(f\"- **Heading Numbering**: {heading_total} issues in {heading_file_count} files\")\n            else:\n                lines.append(\"- **Heading Numbering**: CLEAN (all headings properly numbered)\")\n        \n        # Add Missing Addresses to Documentation Signals\n        if missing_addresses_data and isinstance(missing_addresses_data, dict):\n            # Use standard format\n            summary = missing_addresses_data.get('summary', {})\n            missing_total = summary.get('total_issues', 0)\n            missing_file_count = summary.get('files_affected', 0)\n            if missing_total > 0 or missing_file_count > 0:\n                lines.append(f\"- **Missing Addresses**: {missing_total} issues in {missing_file_count} files\")\n            else:\n                lines.append(\"- **Missing Addresses**: CLEAN (all documentation addresses present)\")\n        \n        # Add Unconverted Links to Documentation Signals\n        if unconverted_links_data and isinstance(unconverted_links_data, dict):\n            # Use standard format\n            summary = unconverted_links_data.get('summary', {})\n            links_total = summary.get('total_issues', 0)\n            links_file_count = summary.get('files_affected', 0)\n            if links_total > 0 or links_file_count > 0:\n                lines.append(f\"- **Unconverted Links**: {links_total} issues in {links_file_count} files\")\n            else:\n                lines.append(\"- **Unconverted Links**: CLEAN (all links properly converted)\")\n        \n        # Add Dependency Docs to Documentation Signals\n        dependency_summary = getattr(self, 'module_dependency_summary', None) or (hasattr(self, 'results_cache') and self.results_cache.get('analyze_module_dependencies'))\n        if not dependency_summary:\n            # Try loading from tool data\n            dependency_data = self._load_tool_data('analyze_module_dependencies', 'imports')\n            if dependency_data and isinstance(dependency_data, dict):\n                dependency_summary = dependency_data\n        \n        if dependency_summary:\n            missing_deps = dependency_summary.get('missing_dependencies')\n            if missing_deps:\n                lines.append(f\"- **Dependency Docs**: {missing_deps} undocumented references detected\")\n            else:\n                lines.append(\"- **Dependency Docs**: CLEAN (no undocumented dependencies)\")\n        else:\n            # Always show Dependency Docs status, even if data not available\n            lines.append(\"- **Dependency Docs**: CLEAN (no undocumented dependencies)\")\n        \n        if not doc_sync_summary_for_signals:\n            lines.append(\"- Run `python development_tools/run_development_tools.py doc-sync` for drift details\")\n        \n        # Add config validation status\n        config_validation_summary = self._load_config_validation_summary()\n        if config_validation_summary:\n            config_valid = config_validation_summary.get('config_valid', False)\n            config_complete = config_validation_summary.get('config_complete', False)\n            total_recommendations = config_validation_summary.get('total_recommendations', 0)\n            if config_valid and config_complete and total_recommendations == 0:\n                lines.append(\"- **Config Validation**: CLEAN (no issues)\")\n            elif total_recommendations > 0:\n                lines.append(f\"- **Config Validation**: {total_recommendations} recommendation(s)\")\n            else:\n                lines.append(\"- **Config Validation**: Needs attention\")\n        \n        # Add TODO sync status\n        todo_sync_result = getattr(self, 'todo_sync_result', None)\n        if not todo_sync_result:\n            # Try loading from tool data if not in memory\n            todo_data = self._load_tool_data('analyze_todo_sync', 'docs')\n            if todo_data and isinstance(todo_data, dict):\n                todo_sync_result = todo_data\n        \n        if todo_sync_result and isinstance(todo_sync_result, dict):\n            completed_entries = todo_sync_result.get('completed_entries', [])\n            if isinstance(completed_entries, list):\n                completed_count = len(completed_entries)\n            else:\n                completed_count = todo_sync_result.get('completed_entries', 0)\n            if completed_count > 0:\n                lines.append(f\"- **TODO Sync**: {completed_count} completed entries need review\")\n            else:\n                lines.append(\"- **TODO Sync**: CLEAN (no completed entries)\")\n        else:\n            # Always show TODO Sync status, even if data not available\n            lines.append(\"- **TODO Sync**: CLEAN (no completed entries)\")\n        \n        # Add overlap analysis summary (always show, even if no overlaps found)\n        lines.append(\"\")\n        lines.append(\"## Documentation Overlap\")\n        overlap_count = len(section_overlaps) if section_overlaps else 0\n        consolidation_count = len(consolidation_recs) if consolidation_recs else 0\n        \n        if overlap_count > 0 or consolidation_count > 0:\n            if section_overlaps and overlap_count > 0:\n                lines.append(f\"- **Section Overlaps**: {overlap_count} sections duplicated across files\")\n                # Show first few overlaps\n                top_overlaps = sorted(section_overlaps.items(), key=lambda x: len(x[1]), reverse=True)[:3]\n                for section, files in top_overlaps:\n                    lines.append(f\"  - `{section}` appears in: {', '.join(files[:3])}{'...' if len(files) > 3 else ''}\")\n        else:\n            if overlap_analysis_ran:\n                lines.append(\"- **Status**: No overlaps detected (analysis performed)\")\n                lines.append(\"  - Overlap analysis ran but found no section overlaps or consolidation opportunities\")\n            else:\n                lines.append(\"- **Status**: Overlap analysis not run (use `audit --full` or `--overlap` flag)\")\n                lines.append(\"  - Standard audits skip overlap analysis by default; run `audit --full` or use `--overlap` flag to include it\")\n        \n        doc_artifacts = analyze_docs_data.get('artifacts') if isinstance(analyze_docs_data, dict) else None\n        \n        if doc_artifacts:\n            primary_artifact = doc_artifacts[0]\n            file_name = primary_artifact.get('file')\n            line_no = primary_artifact.get('line')\n            pattern = primary_artifact.get('pattern')\n            lines.append(\n                f\"- **Content Cleanup**: {file_name} line {line_no} flagged for {pattern.replace('_', ' ')}\"\n            )\n            if len(doc_artifacts) > 1:\n                lines.append(f\"- Additional documentation artifacts: {len(doc_artifacts) - 1} more findings\")\n        \n        lines.append(\"\")\n        lines.append(\"## Error Handling\")\n        \n        if error_metrics:\n            # Always show missing error handling count (even if 0)\n            if missing_error_handlers is not None:\n                lines.append(f\"- **Missing Error Handling**: {missing_error_handlers} functions lack protections\")\n            # Get decorator count from details (standard format) or top-level (legacy format)\n            decorated = get_error_field('functions_with_decorators')\n            if decorated is not None:\n                lines.append(f\"- **@handle_errors Usage**: {decorated} functions already use the decorator\")\n            # Show error handling coverage if available\n            if error_coverage is not None:\n                lines.append(f\"- **Error Handling Coverage**: {error_coverage:.1f}%\")\n            # Show functions with error handling if available\n            if error_with_handling is not None:\n                lines.append(f\"- **Functions with Error Handling**: {error_with_handling}\")\n            \n            # Phase 1: Candidates for decorator replacement\n            # Use helper to access from details or top level\n            error_details = error_metrics.get('details', {})\n            phase1_total = error_details.get('phase1_total', error_metrics.get('phase1_total', 0))\n            \n            if phase1_total > 0:\n                phase1_by_priority = error_details.get('phase1_by_priority', error_metrics.get('phase1_by_priority', {})) or {}\n                if not isinstance(phase1_by_priority, dict):\n                    phase1_by_priority = {}\n                \n                priority_counts = []\n                if phase1_by_priority.get('high', 0) > 0:\n                    priority_counts.append(f\"{phase1_by_priority['high']} high\")\n                if phase1_by_priority.get('medium', 0) > 0:\n                    priority_counts.append(f\"{phase1_by_priority['medium']} medium\")\n                if phase1_by_priority.get('low', 0) > 0:\n                    priority_counts.append(f\"{phase1_by_priority['low']} low\")\n                \n                priority_text = ', '.join(priority_counts) if priority_counts else '0'\n                lines.append(f\"- **Phase 1 Candidates**: {phase1_total} functions need decorator replacement ({priority_text} priority)\")\n            \n            # Phase 2: Generic exception raises\n            # Use helper to access from details or top level\n            error_details = error_metrics.get('details', {})\n            phase2_total = error_details.get('phase2_total', error_metrics.get('phase2_total', 0))\n            \n            if phase2_total > 0:\n                phase2_by_type = error_details.get('phase2_by_type', error_metrics.get('phase2_by_type', {})) or {}\n                if not isinstance(phase2_by_type, dict):\n                    phase2_by_type = {}\n                \n                type_counts = [f\"{count} {exc_type}\" for exc_type, count in sorted(phase2_by_type.items(), key=lambda x: x[1], reverse=True)[:3]]\n                type_text = ', '.join(type_counts) if type_counts else '0'\n                if len(phase2_by_type) > 3:\n                    type_text += f\", ... +{len(phase2_by_type) - 3} more\"\n                lines.append(f\"- **Phase 2 Exceptions**: {phase2_total} generic exception raises need categorization ({type_text})\")\n        else:\n            # Try to load cached error handling data\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data and 'results' in cached_data and 'analyze_error_handling' in cached_data['results']:\n                    error_data = cached_data['results']['analyze_error_handling']\n                    if 'data' in error_data:\n                        error_metrics = error_data['data']\n                    else:\n                        # Try loading from standardized output storage\n                        from ..output_storage import load_tool_result\n                        loaded_data = load_tool_result('analyze_error_handling', 'error_handling', project_root=self.project_root)\n                        if loaded_data:\n                            error_metrics = loaded_data\n                        else:\n                            error_metrics = None\n                    \n                    if error_metrics:\n                        error_details = error_metrics.get('details', {})\n                        def get_error_field(field_name, default=None):\n                            return error_details.get(field_name, error_metrics.get(field_name, default))\n                        \n                        coverage = get_error_field('analyze_error_handling') or get_error_field('error_handling_coverage', 'Unknown')\n                        if coverage != 'Unknown':\n                            lines.append(f\"- **Error Handling Coverage**: {coverage:.1f}%\")\n                            lines.append(f\"- **Functions with Error Handling**: {get_error_field('functions_with_error_handling', 'Unknown')}\")\n                            lines.append(f\"- **Functions Missing Error Handling**: {get_error_field('functions_missing_error_handling', 'Unknown')}\")\n                            \n                            # Add Phase 1 and Phase 2 if available\n                            phase1_total = get_error_field('phase1_total', 0)\n                            if phase1_total > 0:\n                                phase1_by_priority = get_error_field('phase1_by_priority', {}) or {}\n                                if not isinstance(phase1_by_priority, dict):\n                                    phase1_by_priority = {}\n                                priority_counts = []\n                                if phase1_by_priority.get('high', 0) > 0:\n                                    priority_counts.append(f\"{phase1_by_priority['high']} high\")\n                                if phase1_by_priority.get('medium', 0) > 0:\n                                    priority_counts.append(f\"{phase1_by_priority['medium']} medium\")\n                                if phase1_by_priority.get('low', 0) > 0:\n                                    priority_counts.append(f\"{phase1_by_priority['low']} low\")\n                                priority_text = ', '.join(priority_counts) if priority_counts else '0'\n                                lines.append(f\"- **Phase 1 Candidates**: {phase1_total} functions need decorator replacement ({priority_text} priority)\")\n                            \n                            phase2_total = get_error_field('phase2_total', 0)\n                            if phase2_total > 0:\n                                phase2_by_type = get_error_field('phase2_by_type', {}) or {}\n                                if not isinstance(phase2_by_type, dict):\n                                    phase2_by_type = {}\n                                type_counts = [f\"{count} {exc_type}\" for exc_type, count in sorted(phase2_by_type.items(), key=lambda x: x[1], reverse=True)[:3]]\n                                type_text = ', '.join(type_counts) if type_counts else '0'\n                                if len(phase2_by_type) > 3:\n                                    type_text += f\", ... +{len(phase2_by_type) - 3} more\"\n                                lines.append(f\"- **Phase 2 Exceptions**: {phase2_total} generic exception raises need categorization ({type_text})\")\n                        else:\n                            lines.append(\"- **Error Handling**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n            except Exception:\n                pass\n        \n        lines.append(\"\")\n        lines.append(\"## Test Coverage\")\n        \n        dev_tools_insights = self._get_dev_tools_coverage_insights()\n        \n        if coverage_summary and isinstance(coverage_summary, dict):\n            overall = coverage_summary.get('overall') or {}\n            lines.append(\n                f\"- **Overall Coverage**: {percent_text(overall.get('coverage'), 1)} \"\n                f\"({overall.get('covered')} of {overall.get('statements')} statements)\"\n            )\n            coverage_report_path = self.project_root / \"development_docs\" / \"TEST_COVERAGE_REPORT.md\"\n            if coverage_report_path.exists():\n                lines.append(f\"    - **Detailed Report**: [TEST_COVERAGE_REPORT.md](development_docs/TEST_COVERAGE_REPORT.md)\")\n            \n            if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n                dev_pct = dev_tools_insights['overall_pct']\n                dev_statements = dev_tools_insights.get('statements')\n                dev_covered = dev_tools_insights.get('covered')\n                summary_line = f\"- **Development Tools Coverage**: {percent_text(dev_pct, 1)}\"\n                if dev_statements is not None and dev_covered is not None:\n                    summary_line += f\" ({dev_covered} of {dev_statements} statements)\"\n                lines.append(summary_line)\n        else:\n            lines.append(\"- Coverage data unavailable; run `audit --full` to regenerate metrics\")\n            if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n                dev_pct = dev_tools_insights['overall_pct']\n                dev_statements = dev_tools_insights.get('statements')\n                dev_covered = dev_tools_insights.get('covered')\n                summary_line = f\"- **Development Tools Coverage**: {percent_text(dev_pct, 1)}\"\n                if dev_statements is not None and dev_covered is not None:\n                    summary_line += f\" ({dev_covered} of {dev_statements} statements)\"\n                lines.append(summary_line)\n        \n        # Test markers\n        test_markers_data = self._load_tool_data('analyze_test_markers', 'tests')\n        if test_markers_data and isinstance(test_markers_data, dict):\n            if 'summary' in test_markers_data:\n                summary = test_markers_data.get('summary', {})\n                missing_count = summary.get('total_issues', 0)\n                details = test_markers_data.get('details', {})\n                missing_list = details.get('missing', [])\n            else:\n                missing_count = test_markers_data.get('missing_count', 0)\n                missing_list = test_markers_data.get('missing', [])\n            \n            if missing_count > 0 or (missing_list and len(missing_list) > 0):\n                lines.append(\"## Test Markers\")\n                actual_count = missing_count if missing_count > 0 else len(missing_list) if missing_list else 0\n                lines.append(f\"- **Missing Category Markers**: {actual_count} tests missing pytest category markers\")\n            else:\n                lines.append(\"## Test Markers\")\n                lines.append(\"- **Status**: CLEAN (all tests have category markers)\")\n        \n        lines.append(\"\")\n        \n        # Unused imports\n        unused_imports_data = self._load_tool_data('analyze_unused_imports', 'imports')\n        if unused_imports_data and isinstance(unused_imports_data, dict):\n            summary = unused_imports_data.get('summary', {})\n            total_unused = summary.get('total_issues', 0)\n            files_with_issues = summary.get('files_affected', 0)\n            status = summary.get('status', 'GOOD')\n            \n            if total_unused > 0 or files_with_issues > 0:\n                lines.append(\"## Unused Imports\")\n                lines.append(f\"- **Total Unused**: {total_unused} imports across {files_with_issues} files\")\n                if status:\n                    lines.append(f\"- **Status**: {status}\")\n                details = unused_imports_data.get('details', {})\n                by_category = details.get('by_category') or {}\n                if by_category:\n                    obvious = by_category.get('obvious_unused', 0)\n                    type_only = by_category.get('type_hints_only', 0)\n                    if obvious > 0:\n                        lines.append(f\"    - **Obvious Removals**: {obvious} imports\")\n                    if type_only > 0:\n                        lines.append(f\"    - **Type-Only Imports**: {type_only} imports\")\n            else:\n                lines.append(\"## Unused Imports\")\n                lines.append(\"- **Status**: CLEAN (no unused imports detected)\")\n            \n            unused_imports_report_path = self.project_root / \"development_docs\" / \"UNUSED_IMPORTS_REPORT.md\"\n            if unused_imports_report_path.exists():\n                lines.append(f\"- **Detailed Report**: [UNUSED_IMPORTS_REPORT.md](development_docs/UNUSED_IMPORTS_REPORT.md)\")\n        else:\n            lines.append(\"## Unused Imports\")\n            lines.append(\"- **Status**: Data unavailable (run `audit --full` for latest scan)\")\n        \n        lines.append(\"\")\n        lines.append(\"## Legacy References\")\n        \n        if legacy_summary:\n            if 'summary' in legacy_summary and isinstance(legacy_summary.get('summary'), dict):\n                summary = legacy_summary['summary']\n                legacy_issues = summary.get('files_affected', 0)\n                details = legacy_summary.get('details', {})\n                report_path = details.get('report_path') or legacy_summary.get('report_path')\n            else:\n                legacy_issues = legacy_summary.get('files_with_issues')\n                report_path = legacy_summary.get('report_path')\n            \n            if legacy_issues == 0:\n                lines.append(\"- **Legacy References**: CLEAN (0 files flagged)\")\n            elif legacy_issues is not None:\n                lines.append(f\"- **Legacy References**: {legacy_issues} files still reference legacy patterns\")\n            \n            if report_path:\n                if isinstance(report_path, str):\n                    if not Path(report_path).is_absolute():\n                        report_path_obj = self.project_root / report_path\n                    else:\n                        report_path_obj = Path(report_path)\n                else:\n                    report_path_obj = report_path\n                \n                if report_path_obj.exists():\n                    rel_path = report_path_obj.relative_to(self.project_root)\n                    lines.append(f\"- **Detailed Report**: [LEGACY_REFERENCE_REPORT.md]({rel_path.as_posix()})\")\n                else:\n                    lines.append(f\"- **Detailed Report**: {report_path}\")\n        else:\n            if not legacy_summary:\n                legacy_summary = legacy_data\n            if legacy_summary:\n                if 'summary' in legacy_summary and isinstance(legacy_summary.get('summary'), dict):\n                    summary = legacy_summary['summary']\n                    legacy_issues = summary.get('files_affected', 0)\n                    details = legacy_summary.get('details', {})\n                    report_path = details.get('report_path') or 'development_docs/LEGACY_REFERENCE_REPORT.md'\n                else:\n                    legacy_issues = legacy_summary.get('files_with_issues')\n                    report_path = legacy_summary.get('report_path') or 'development_docs/LEGACY_REFERENCE_REPORT.md'\n                \n                if legacy_issues is not None:\n                    if legacy_issues == 0:\n                        lines.append(\"- **Legacy References**: CLEAN (0 files flagged)\")\n                    else:\n                        lines.append(f\"- **Legacy References**: {legacy_issues} files still reference legacy patterns\")\n                    if report_path:\n                        report_path_obj = self._resolve_report_path(report_path)\n                        if report_path_obj.exists():\n                            rel_path = report_path_obj.relative_to(self.project_root)\n                            lines.append(f\"- **Detailed Report**: [LEGACY_REFERENCE_REPORT.md]({rel_path.as_posix()})\")\n                else:\n                    lines.append(\"- Legacy reference data unavailable (run `audit --full` for latest scan)\")\n            else:\n                lines.append(\"- Legacy reference data unavailable (run `audit --full` for latest scan)\")\n        \n        lines.append(\"\")\n        lines.append(\"## Validation Status\")\n        \n        validation_output = ''\n        if hasattr(self, 'validation_results') and self.validation_results:\n            validation_output = self.validation_results.get('output', '') or ''\n        \n        if not validation_output or not validation_output.strip():\n            validation_data = self._load_tool_data('analyze_ai_work', 'ai_work', log_source=True)\n            if validation_data:\n                validation_output = validation_data.get('output', '') or ''\n        \n        if validation_output and validation_output.strip():\n            if 'POOR' in validation_output:\n                lines.append(\"- **AI Work Validation**: POOR - documentation or tests missing\")\n            elif 'GOOD' in validation_output:\n                lines.append(\"- **AI Work Validation**: GOOD - keep current standards\")\n            elif 'NEEDS ATTENTION' in validation_output or 'FAIR' in validation_output:\n                lines.append(\"- **AI Work Validation**: NEEDS ATTENTION - see consolidated report for details\")\n            else:\n                lines.append(\"- **AI Work Validation**: Status available (see consolidated report)\")\n        else:\n            # Check if we're in a tier that doesn't run analyze_ai_work (Tier 1)\n            tools_run = getattr(self, '_tools_run_in_current_tier', set())\n            if 'analyze_ai_work' not in tools_run:\n                # Try to load cached validation data\n                validation_data = self._load_tool_data('analyze_ai_work', 'ai_work', log_source=False)\n                if validation_data:\n                    cached_output = validation_data.get('output', '') or ''\n                    if cached_output and cached_output.strip():\n                        if 'POOR' in cached_output:\n                            lines.append(\"- **AI Work Validation**: POOR - documentation or tests missing (cached)\")\n                        elif 'GOOD' in cached_output:\n                            lines.append(\"- **AI Work Validation**: GOOD - keep current standards (cached)\")\n                        elif 'NEEDS ATTENTION' in cached_output or 'FAIR' in cached_output:\n                            lines.append(\"- **AI Work Validation**: NEEDS ATTENTION - see consolidated report for details (cached)\")\n                        else:\n                            lines.append(\"- **AI Work Validation**: Status available (cached, see consolidated report)\")\n                    else:\n                        lines.append(\"- **AI Work Validation**: Using cached data (run `audit` or `audit --full` for latest validation)\")\n                else:\n                    lines.append(\"- **AI Work Validation**: Using cached data (run `audit` or `audit --full` for latest validation)\")\n            else:\n                lines.append(\"- Validation results unavailable (run `audit` for latest validation)\")\n        \n        lines.append(\"\")\n        lines.append(\"## System Signals\")\n        \n        if hasattr(self, 'system_signals') and self.system_signals:\n            system_health = self.system_signals.get('system_health', {})\n            overall_status = system_health.get('overall_status', 'OK')\n            lines.append(f\"- **System Health**: {overall_status}\")\n            \n            # Add audit freshness (consolidated - don't show redundant last_audit)\n            audit_freshness = system_health.get('audit_freshness')\n            if audit_freshness:\n                lines.append(f\"  - Audit data: {audit_freshness}\")\n            \n            # Add test coverage status (doc sync is redundant - already shown in Snapshot and Documentation Signals)\n            test_coverage_status = system_health.get('test_coverage_status')\n            if test_coverage_status and test_coverage_status != 'Unknown':\n                lines.append(f\"  - Test coverage: {test_coverage_status}\")\n            \n            # Show actual warnings/critical issues (not just counts)\n            severity_levels = system_health.get('severity_levels', {})\n            if severity_levels:\n                critical_issues = severity_levels.get('CRITICAL', [])\n                warnings = severity_levels.get('WARNING', [])\n                if critical_issues:\n                    for issue in critical_issues[:2]:  # Show first 2 critical issues\n                        lines.append(f\"  - Critical: {issue}\")\n                if warnings:\n                    for warning in warnings[:2]:  # Show first 2 warnings\n                        lines.append(f\"  - Warning: {warning}\")\n            \n            missing_core = [\n                name for name, state in (system_health.get('core_files') or {}).items()\n                if state != 'OK'\n            ]\n            if missing_core:\n                lines.append(f\"- **Core File Issues**: {self._format_list_for_display(missing_core, limit=3)}\")\n            \n            recent_activity = self.system_signals.get('recent_activity', {})\n            recent_changes = recent_activity.get('recent_changes') or []\n            if recent_changes:\n                lines.append(f\"- **Recent Changes**: {self._format_list_for_display(recent_changes, limit=3)}\")\n            \n            critical_alerts = self.system_signals.get('critical_alerts', [])\n            if critical_alerts:\n                lines.append(f\"- **Critical Alerts**: {len(critical_alerts)} active alert(s)\")\n                for alert in critical_alerts[:3]:\n                    alert_text = alert if isinstance(alert, str) else alert.get('message', str(alert))\n                    lines.append(f\"  - {alert_text}\")\n        else:\n            # Try to load cached system signals\n            signals_loaded = False\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data:\n                    signals_data = None\n                    if 'results' in cached_data:\n                        if 'analyze_system_signals' in cached_data['results']:\n                            signals_data = cached_data['results']['analyze_system_signals']\n                    \n                    if signals_data:\n                        if 'data' in signals_data:\n                            system_signals = signals_data['data']\n                        else:\n                            system_signals = signals_data\n                        \n                        if system_signals:\n                            signals_loaded = True\n                            system_health = system_signals.get('system_health', {})\n                            overall_status = system_health.get('overall_status', 'OK')\n                            lines.append(f\"- **System Health**: {overall_status}\")\n                            \n                            # Add audit freshness (consolidated - don't show redundant last_audit)\n                            audit_freshness = system_health.get('audit_freshness')\n                            if audit_freshness:\n                                lines.append(f\"  - Audit data: {audit_freshness}\")\n                            \n                            # Add test coverage status (doc sync is redundant - already shown in Snapshot and Documentation Signals)\n                            test_coverage_status = system_health.get('test_coverage_status')\n                            if test_coverage_status and test_coverage_status != 'Unknown':\n                                lines.append(f\"  - Test coverage: {test_coverage_status}\")\n                            \n                            # Show actual warnings/critical issues (not just counts)\n                            severity_levels = system_health.get('severity_levels', {})\n                            if severity_levels:\n                                critical_issues = severity_levels.get('CRITICAL', [])\n                                warnings = severity_levels.get('WARNING', [])\n                                if critical_issues:\n                                    for issue in critical_issues[:2]:  # Show first 2 critical issues\n                                        lines.append(f\"  - Critical: {issue}\")\n                                if warnings:\n                                    for warning in warnings[:2]:  # Show first 2 warnings\n                                        lines.append(f\"  - Warning: {warning}\")\n                            \n                            missing_core = [\n                                name for name, state in (system_health.get('core_files') or {}).items()\n                                if state != 'OK'\n                            ]\n                            if missing_core:\n                                lines.append(f\"- **Core File Issues**: {self._format_list_for_display(missing_core, limit=3)}\")\n                            \n                            recent_activity = system_signals.get('recent_activity', {})\n                            recent_changes = recent_activity.get('recent_changes') or []\n                            if recent_changes:\n                                lines.append(f\"- **Recent Changes**: {self._format_list_for_display(recent_changes, limit=3)}\")\n                            \n                            critical_alerts = system_signals.get('critical_alerts', [])\n                            if critical_alerts:\n                                lines.append(f\"- **Critical Alerts**: {len(critical_alerts)} active alerts\")\n            except Exception as e:\n                logger.debug(f\"Failed to load system signals from cache: {e}\")\n            \n            if not signals_loaded:\n                lines.append(\"- System signals data unavailable (run `system-signals` command)\")\n        \n        lines.append(\"\")\n        lines.append(\"## Quick Commands\")\n        lines.append(\"- `python development_tools/run_development_tools.py status` - Refresh this snapshot\")\n        lines.append(\"- `python development_tools/run_development_tools.py audit --full` - Regenerate all metrics\")\n        lines.append(\"- `python development_tools/run_development_tools.py doc-sync` - Update documentation pairing data\")\n        lines.append(\"\")\n        \n        return \"\\n\".join(lines)\n    \n    def _generate_ai_priorities_document(self) -> str:\n        \"\"\"Generate AI-optimized priorities document with immediate next steps.\"\"\"\n        # Log data source context\n        audit_tier = getattr(self, 'current_audit_tier', None)\n        if audit_tier:\n            logger.info(f\"[REPORT GENERATION] Generating AI_PRIORITIES.md using data from Tier {audit_tier} audit\")\n        else:\n            logger.info(f\"[REPORT GENERATION] Generating AI_PRIORITIES.md using cached data (no active audit)\")\n        \n        # Check if this is a mid-audit write\n        instance_flag = hasattr(self, '_audit_in_progress') and self._audit_in_progress\n        audit_in_progress = instance_flag or _is_audit_in_progress(self.project_root)\n        is_legitimate_end_write = hasattr(self, 'current_audit_tier') and self.current_audit_tier is not None\n        \n        if audit_in_progress and not is_legitimate_end_write:\n            if not instance_flag:\n                logger.warning(\"_generate_ai_priorities_document() called from NEW instance during audit! This should only happen at the end.\")\n            else:\n                logger.warning(\"_generate_ai_priorities_document() called during audit! This should only happen at the end.\")\n            import traceback\n            logger.debug(f\"Call stack:\\n{''.join(traceback.format_stack())}\")\n        \n        lines: List[str] = []\n        lines.append(\"# AI Priorities - Immediate Next Steps\")\n        lines.append(\"\")\n        lines.append(\"> **File**: `development_tools/AI_PRIORITIES.md`\")\n        lines.append(\"> **Generated**: This file is auto-generated. Do not edit manually.\")\n        lines.append(f\"> **Last Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        \n        if self.current_audit_tier == 1:\n            source_cmd = \"python development_tools/run_development_tools.py audit --quick\"\n            tier_name = \"Tier 1 (Quick Audit)\"\n        elif self.current_audit_tier == 3:\n            source_cmd = \"python development_tools/run_development_tools.py audit --full\"\n            tier_name = \"Tier 3 (Full Audit)\"\n        elif self.current_audit_tier == 2:\n            source_cmd = \"python development_tools/run_development_tools.py audit\"\n            tier_name = \"Tier 2 (Standard Audit)\"\n        else:\n            source_cmd = \"python development_tools/run_development_tools.py status\"\n            tier_name = \"Status Check (cached data)\"\n        lines.append(f\"> **Source**: `{source_cmd}`\")\n        if self.current_audit_tier:\n            lines.append(f\"> **Last Audit Tier**: {tier_name}\")\n        lines.append(\"> **Generated by**: run_development_tools.py - AI Development Tools Runner\")\n        lines.append(\"\")\n        \n        def percent_text(value: Any, decimals: int = 1) -> str:\n            if value is None:\n                return \"Unknown\"\n            if isinstance(value, str):\n                trimmed = value.strip()\n                return trimmed if trimmed.endswith('%') else f\"{trimmed}%\"\n            return self._format_percentage(value, decimals)\n        \n        def to_int(value: Any) -> Optional[int]:\n            if isinstance(value, int):\n                return value\n            if isinstance(value, float):\n                return int(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return int(float(stripped))\n                except ValueError:\n                    return None\n            if isinstance(value, dict):\n                count = value.get('count')\n                return to_int(count)\n            return None\n        \n        def to_float(value: Any) -> Optional[float]:\n            if isinstance(value, (int, float)):\n                return float(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return float(stripped)\n                except ValueError:\n                    return None\n            return None\n        \n        metrics = self._get_canonical_metrics()\n        doc_metrics = self._load_tool_data('analyze_function_registry', 'functions')\n        error_metrics = self._load_tool_data('analyze_error_handling', 'error_handling')\n        function_metrics = self._load_tool_data('analyze_functions', 'functions')\n        doc_sync_data = self._load_tool_data('analyze_documentation_sync', 'docs', log_source=True)\n        doc_sync_summary = self.docs_sync_summary or doc_sync_data or {}\n        legacy_data = self._load_tool_data('analyze_legacy_references', 'legacy')\n        legacy_summary = self.legacy_cleanup_summary or legacy_data or {}\n        coverage_summary = self._load_coverage_summary()\n        \n        if not hasattr(self, 'dev_tools_coverage_results') or not self.dev_tools_coverage_results:\n            self._load_dev_tools_coverage()\n        \n        analyze_data = self._load_tool_data('analyze_documentation', 'docs')\n        ascii_data = self._load_tool_data('analyze_ascii_compliance', 'docs')\n        heading_data = self._load_tool_data('analyze_heading_numbering', 'docs')\n        missing_addresses_data = self._load_tool_data('analyze_missing_addresses', 'docs')\n        unconverted_links_data = self._load_tool_data('analyze_unconverted_links', 'docs')\n        test_markers_data = self._load_tool_data('analyze_test_markers', 'tests')\n        unused_imports_data = self._load_tool_data('analyze_unused_imports', 'imports')\n        \n        section_overlaps = analyze_data.get('section_overlaps', {})\n        consolidation_recs = analyze_data.get('consolidation_recommendations', [])\n        \n        doc_metrics_details = doc_metrics.get('details', {})\n        doc_coverage_value = metrics.get('doc_coverage')\n        if doc_coverage_value is None or doc_coverage_value == 'Unknown':\n            doc_coverage_value = doc_metrics_details.get('doc_coverage') or doc_metrics.get('doc_coverage')\n        \n        # Use analyze_functions for missing docstrings (checks code docstrings)\n        # NOT analyze_function_registry (checks registry file coverage - different metric)\n        function_metrics_for_missing = function_metrics\n        func_metrics_details_for_missing = function_metrics_for_missing.get('details', {}) if isinstance(function_metrics_for_missing, dict) else {}\n        missing_docstrings_from_functions = func_metrics_details_for_missing.get('undocumented', 0) or function_metrics_for_missing.get('undocumented', 0) if isinstance(function_metrics_for_missing, dict) else 0\n        \n        data_section = doc_metrics.get('data', {}) if isinstance(doc_metrics, dict) else {}\n        missing_from_data = data_section.get('missing', {}) if isinstance(data_section, dict) else {}\n        missing_docs_raw = doc_metrics_details.get('missing_docs') or doc_metrics_details.get('missing_items') or doc_metrics.get('missing_docs') or doc_metrics.get('missing_items')\n        \n        if not missing_docs_raw and isinstance(missing_from_data, dict):\n            missing_docs_raw = missing_from_data\n        \n        if isinstance(missing_docs_raw, dict):\n            missing_docs_count = missing_docs_raw.get('count', 0)\n        else:\n            missing_docs_count = to_int(missing_docs_raw) or 0\n        \n        missing_doc_files = doc_metrics_details.get('missing_files') or doc_metrics.get('missing_files') or self._get_missing_doc_files(limit=5)\n        \n        # Use analyze_functions data for missing docstrings (code docstrings, not registry file)\n        # Calculate from function_metrics, not doc_metrics (registry)\n        if missing_docstrings_from_functions is not None and missing_docstrings_from_functions >= 0:\n            missing_docs_count_for_priority = missing_docstrings_from_functions\n        else:\n            # Fallback: calculate from function_metrics if undocumented count not available\n            func_total = func_metrics_details_for_missing.get('total_functions') or function_metrics_for_missing.get('total_functions') if isinstance(function_metrics_for_missing, dict) else None\n            if func_total and func_total > 0:\n                # Try to get documented count from function_metrics\n                func_documented = func_metrics_details_for_missing.get('documented', 0) or function_metrics_for_missing.get('documented', 0) if isinstance(function_metrics_for_missing, dict) else None\n                if func_documented is not None:\n                    missing_docs_calculated = func_total - func_documented\n                    missing_docs_count_for_priority = missing_docs_calculated\n                else:\n                    # Last resort: use registry data (but this measures registry file, not code docstrings)\n                    total_funcs = metrics.get('total_functions')\n                    doc_totals = doc_metrics_details.get('totals') or doc_metrics.get('totals') or {}\n                    documented_funcs = doc_totals.get('functions_documented') if isinstance(doc_totals, dict) else None\n                    if total_funcs and documented_funcs is not None:\n                        missing_docs_calculated = total_funcs - documented_funcs\n                        missing_docs_count_for_priority = missing_docs_calculated if missing_docs_count is None or missing_docs_count == 0 else missing_docs_count\n                    else:\n                        missing_docs_count_for_priority = missing_docs_count or 0\n            else:\n                missing_docs_count_for_priority = missing_docs_count or 0\n        \n        error_details = error_metrics.get('details', {})\n        def get_error_field(field_name, default=None):\n            return error_details.get(field_name, error_metrics.get(field_name, default))\n        \n        error_coverage = get_error_field('analyze_error_handling') or get_error_field('error_handling_coverage')\n        error_total = get_error_field('total_functions')\n        error_with_handling = get_error_field('functions_with_error_handling')\n        canonical_total = metrics.get('total_functions')\n        missing_error_handlers = to_int(get_error_field('functions_missing_error_handling'))\n        \n        if error_total and error_with_handling:\n            calc_coverage = (error_with_handling / error_total) * 100\n            if 0 <= calc_coverage <= 100:\n                error_coverage = calc_coverage\n        elif error_coverage is None and error_total and error_with_handling:\n            error_coverage = (error_with_handling / error_total) * 100\n        \n        worst_error_modules = get_error_field('worst_modules', []) or []\n        if worst_error_modules is None or not isinstance(worst_error_modules, (list, tuple)):\n            worst_error_modules = []\n        \n        def get_doc_sync_field(data, field_name, default=None):\n            if not data or not isinstance(data, dict):\n                return default\n            if 'summary' in data and isinstance(data.get('summary'), dict):\n                if field_name == 'status':\n                    return data['summary'].get('status', default)\n                elif field_name == 'total_issues':\n                    return data['summary'].get('total_issues', default)\n                else:\n                    return data.get('details', {}).get(field_name, default)\n            else:\n                return data.get(field_name, default)\n        \n        path_drift_count = to_int(get_doc_sync_field(doc_sync_summary, 'path_drift_issues')) if doc_sync_summary else None\n        path_drift_files = get_doc_sync_field(doc_sync_summary, 'path_drift_files', []) if doc_sync_summary else []\n        if path_drift_files is None or not isinstance(path_drift_files, list):\n            path_drift_files = []\n        path_drift_files = [f for f in path_drift_files if f and isinstance(f, str) and not (f.isupper() and ('ISSUES' in f or 'COMPLIANCE' in f or 'DOCUMENTATION' in f or 'NUMBERING' in f))]\n        paired_doc_issues = to_int(get_doc_sync_field(doc_sync_summary, 'paired_doc_issues')) if doc_sync_summary else None\n        ascii_issues = to_int(get_doc_sync_field(doc_sync_summary, 'ascii_issues')) if doc_sync_summary else None\n        \n        if legacy_summary and isinstance(legacy_summary, dict):\n            if 'summary' in legacy_summary and isinstance(legacy_summary.get('summary'), dict):\n                legacy_files = to_int(legacy_summary['summary'].get('files_affected', 0))\n                legacy_details = legacy_summary.get('details', {})\n                legacy_markers = to_int(legacy_details.get('legacy_markers', 0))\n                legacy_report = legacy_details.get('report_path')\n            else:\n                legacy_files = to_int(legacy_summary.get('files_with_issues'))\n                legacy_markers = to_int(legacy_summary.get('legacy_markers'))\n                legacy_report = legacy_summary.get('report_path')\n        else:\n            legacy_files = None\n            legacy_markers = None\n            legacy_report = None\n        \n        low_coverage_modules: List[Dict[str, Any]] = []\n        coverage_overall = None\n        worst_coverage_files: List[Dict[str, Any]] = []\n        if coverage_summary:\n            coverage_overall = (coverage_summary or {}).get('overall')\n            module_entries = (coverage_summary or {}).get('modules') or []\n            for module in module_entries:\n                coverage_value = to_float(module.get('coverage'))\n                coverage_float = to_float(coverage_value) if coverage_value is not None else None\n                if coverage_float is not None and coverage_float < 80:\n                    low_coverage_modules.append(module)\n            low_coverage_modules = low_coverage_modules[:3]\n            worst_coverage_files = (coverage_summary or {}).get('worst_files') or []\n        \n        dev_tools_coverage_overall = None\n        if hasattr(self, 'dev_tools_coverage_results') and self.dev_tools_coverage_results:\n            dev_tools_coverage_overall = self.dev_tools_coverage_results.get('overall', {})\n        dev_tools_insights = self._get_dev_tools_coverage_insights()\n        \n        analyze_artifacts = analyze_data.get('artifacts') or []\n        analyze_duplicates = analyze_data.get('duplicates') or []\n        analyze_placeholders = analyze_data.get('placeholders') or []\n        \n        critical_examples = function_metrics.get('critical_complexity_examples') or []\n        high_examples = function_metrics.get('high_complexity_examples') or []\n        \n        # Try loading from multiple sources, prioritizing in-memory cache, then tool data (may be cached)\n        decision_metrics = self.results_cache.get('decision_support_metrics', {})\n        if decision_metrics:\n            if (not critical_examples or len(critical_examples) == 0) and 'critical_complexity_examples' in decision_metrics:\n                critical_examples = decision_metrics.get('critical_complexity_examples', [])\n                if critical_examples:\n                    function_metrics['critical_complexity_examples'] = critical_examples\n            if (not high_examples or len(high_examples) == 0) and 'high_complexity_examples' in decision_metrics:\n                high_examples = decision_metrics.get('high_complexity_examples', [])\n                if high_examples:\n                    function_metrics['high_complexity_examples'] = high_examples\n        \n        # If still not available, try loading from decision_support tool data (may be cached)\n        if (not critical_examples or len(critical_examples) == 0) or (not high_examples or len(high_examples) == 0):\n            decision_data = self._load_tool_data('decision_support', 'functions', log_source=False)\n            if decision_data and isinstance(decision_data, dict):\n                decision_details = decision_data.get('details', {})\n                decision_metrics_from_tool = decision_details.get('decision_support_metrics', {}) or decision_data.get('decision_support_metrics', {})\n                if decision_metrics_from_tool:\n                    if not decision_metrics:\n                        decision_metrics = decision_metrics_from_tool\n                    if (not critical_examples or len(critical_examples) == 0) and 'critical_complexity_examples' in decision_metrics_from_tool:\n                        critical_examples = decision_metrics_from_tool.get('critical_complexity_examples', [])\n                        if critical_examples:\n                            function_metrics['critical_complexity_examples'] = critical_examples\n                    if (not high_examples or len(high_examples) == 0) and 'high_complexity_examples' in decision_metrics_from_tool:\n                        high_examples = decision_metrics_from_tool.get('high_complexity_examples', [])\n                        if high_examples:\n                            function_metrics['high_complexity_examples'] = high_examples\n        \n        # If still not available, try loading from analyze_functions tool data (may be cached)\n        if (not critical_examples or len(critical_examples) == 0) or (not high_examples or len(high_examples) == 0):\n            func_result = self._load_tool_data('analyze_functions', 'functions', log_source=False)\n            if func_result and isinstance(func_result, dict):\n                func_details = func_result.get('details', {})\n                if (not critical_examples or len(critical_examples) == 0):\n                    if 'critical_complexity_examples' in func_details:\n                        critical_examples = func_details.get('critical_complexity_examples', [])\n                        if critical_examples:\n                            function_metrics['critical_complexity_examples'] = critical_examples\n                    elif 'critical_complexity_examples' in func_result:\n                        critical_examples = func_result.get('critical_complexity_examples', [])\n                        if critical_examples:\n                            function_metrics['critical_complexity_examples'] = critical_examples\n                if (not high_examples or len(high_examples) == 0):\n                    if 'high_complexity_examples' in func_details:\n                        high_examples = func_details.get('high_complexity_examples', [])\n                        if high_examples:\n                            function_metrics['high_complexity_examples'] = high_examples\n                    elif 'high_complexity_examples' in func_result:\n                        high_examples = func_result.get('high_complexity_examples', [])\n                        if high_examples:\n                            function_metrics['high_complexity_examples'] = high_examples\n        \n        moderate_complex = to_int(metrics.get('moderate'))\n        high_complex = to_int(metrics.get('high'))\n        critical_complex = to_int(metrics.get('critical'))\n        \n        if moderate_complex is None or high_complex is None or critical_complex is None:\n            decision_metrics = self.results_cache.get('decision_support_metrics', {})\n            if decision_metrics:\n                if moderate_complex is None:\n                    moderate_complex = to_int(decision_metrics.get('moderate_complexity'))\n                if high_complex is None:\n                    high_complex = to_int(decision_metrics.get('high_complexity'))\n                if critical_complex is None:\n                    critical_complex = to_int(decision_metrics.get('critical_complexity'))\n        \n        priority_items: List[Dict[str, Any]] = []\n        \n        # Fixed tier ordering: tier number * 100 + insertion order within tier\n        # This ensures predictable ordering: Tier 1 items (100-199), Tier 2 (200-299), etc.\n        tier_insertion_counters = {1: 0, 2: 0, 3: 0, 4: 0}\n        \n        def validate_recommendation(title: str, reason: str, data_source: Optional[str] = None, \n                                   count: Optional[int] = None, expected_min: Optional[int] = None) -> bool:\n            \"\"\"\n            Validate a recommendation before adding it.\n            \n            Args:\n                title: Recommendation title\n                reason: Recommendation reason text\n                data_source: Source of data (e.g., 'analyze_unused_imports')\n                count: Actual count/value being recommended\n                expected_min: Minimum expected value (warns if count is suspiciously high)\n            \n            Returns:\n                True if recommendation is valid, False if it should be skipped\n            \"\"\"\n            # Check for empty reason\n            if not reason or not reason.strip():\n                logger.debug(f\"Skipping recommendation '{title}': empty reason\")\n                return False\n            \n            # Check for suspicious counts (e.g., recommending 358 imports when only 1 is obvious)\n            if count is not None and expected_min is not None:\n                if count > expected_min * 10:  # More than 10x expected\n                    logger.warning(\n                        f\"Suspicious recommendation '{title}': count {count} is much higher than expected minimum {expected_min}. \"\n                        f\"Data source: {data_source}\"\n                    )\n                    # Still allow it, but log warning\n            \n            # Check if data source is available (basic staleness check)\n            if data_source:\n                # Try to load data to verify it exists\n                try:\n                    data = self._load_tool_data(data_source, log_source=False)\n                    if not data:\n                        logger.debug(f\"Skipping recommendation '{title}': data source '{data_source}' not available\")\n                        return False\n                except Exception:\n                    # If we can't check, allow the recommendation\n                    pass\n            \n            return True\n        \n        def add_priority(tier: int, title: str, reason: str, bullets: List[str], \n                       validate: bool = True, data_source: Optional[str] = None,\n                       count: Optional[int] = None, expected_min: Optional[int] = None) -> None:\n            nonlocal tier_insertion_counters\n            if not reason:\n                return\n            \n            # Validate recommendation if requested\n            if validate:\n                if not validate_recommendation(title, reason, data_source, count, expected_min):\n                    return\n            \n            # Normalize tier to valid range (1-4)\n            normalized_tier = max(1, min(4, tier))\n            \n            # Get insertion order within this tier\n            insertion_order = tier_insertion_counters[normalized_tier]\n            tier_insertion_counters[normalized_tier] += 1\n            \n            # Calculate order: tier * 100 + insertion order\n            # This ensures Tier 1 items come first (100-199), then Tier 2 (200-299), etc.\n            order = normalized_tier * 100 + insertion_order\n            \n            priority_items.append({\n                'order': order,\n                'title': title,\n                'reason': reason,\n                'bullets': [bullet for bullet in bullets if bullet]\n            })\n        \n        # Add priorities based on issues found\n        # Path drift priority\n        if path_drift_count and path_drift_count > 0:\n            drift_details: List[str] = []\n            if path_drift_files:\n                drift_details.append(\n                    f\"Top offenders: {self._format_list_for_display(path_drift_files, limit=3)}\"\n                )\n            if paired_doc_issues:\n                drift_details.append(\n                    f\"{paired_doc_issues} paired documentation sets affected alongside drift.\"\n                )\n            drift_details.append(\n                \"Action: Fix broken paths in top offender files, then run `python development_tools/run_development_tools.py doc-sync`\"\n            )\n            drift_details.append(\n                \"Effort: Small (update file paths in documentation, run automated fix tool)\"\n            )\n            drift_details.append(\n                \"Why this matters: Broken paths in documentation reduce trust and make navigation difficult\"\n            )\n            add_priority(\n                tier=1,  # Tier 1: Critical\n                title=\"Stabilize documentation drift\",\n                reason=f\"{path_drift_count} documentation paths are out of sync.\",\n                bullets=drift_details\n            )\n        \n        # Missing docstrings priority\n        if missing_docs_count_for_priority and missing_docs_count_for_priority > 0:\n            doc_bullets: List[str] = []\n            # Try to get examples of undocumented functions (prioritized list)\n            function_metrics_details = function_metrics.get('details', {})\n            undocumented_examples = function_metrics_details.get('undocumented_examples') or function_metrics.get('undocumented_examples') or []\n            \n            # Also try loading directly from tool data if not in metrics\n            if not undocumented_examples or len(undocumented_examples) == 0:\n                function_data_for_examples = self._load_tool_data('analyze_functions', 'functions')\n                if isinstance(function_data_for_examples, dict):\n                    func_details_for_examples = function_data_for_examples.get('details', {})\n                    undocumented_examples = func_details_for_examples.get('undocumented_examples') or function_data_for_examples.get('undocumented_examples') or []\n            \n            if undocumented_examples and isinstance(undocumented_examples, list) and len(undocumented_examples) > 0:\n                # Sort by complexity if available (prioritize complex functions), otherwise use order\n                sorted_examples = sorted(\n                    undocumented_examples,\n                    key=lambda x: x.get('complexity', 0) if isinstance(x, dict) else 0,\n                    reverse=True\n                )[:5]  # Top 5\n                \n                # Format examples with file paths\n                example_items = []\n                for ex in sorted_examples:\n                    if isinstance(ex, dict):\n                        func_name = ex.get('name', ex.get('function', 'unknown'))\n                        file_path = ex.get('file', '')\n                        if file_path:\n                            file_name = Path(file_path).name\n                            example_items.append(f\"{func_name} ({file_name})\")\n                        else:\n                            example_items.append(func_name)\n                    else:\n                        example_items.append(str(ex))\n                if example_items:\n                    # Add \", ... +N\" format if there are more than 5 total\n                    total_undocumented = missing_docs_count_for_priority if missing_docs_count_for_priority else len(undocumented_examples)\n                    if total_undocumented > 5:\n                        example_items.append(f\"... +{total_undocumented - 5} more\")\n                    doc_bullets.append(\n                        f\"Top functions: {', '.join(example_items)}\"\n                    )\n            doc_bullets.append(\n                \"Action: Add docstrings to functions missing them\"\n            )\n            doc_bullets.append(\n                \"Effort: Medium (requires understanding each function's purpose and parameters)\"\n            )\n            doc_bullets.append(\n                \"Why this matters: Docstrings help AI collaborators and future developers understand code intent\"\n            )\n            # Calculate total and documented for better context\n            # Use analyze_functions data (code docstrings), not registry data\n            total_funcs = func_metrics_details_for_missing.get('total_functions') or function_metrics_for_missing.get('total_functions') if isinstance(function_metrics_for_missing, dict) else metrics.get('total_functions')\n            if total_funcs and missing_docs_count_for_priority is not None:\n                documented_funcs = total_funcs - missing_docs_count_for_priority\n                reason_text = f\"{missing_docs_count_for_priority} functions are missing docstrings ({total_funcs} total, {documented_funcs} documented).\"\n            else:\n                reason_text = f\"{missing_docs_count_for_priority} functions are missing docstrings.\"\n            add_priority(\n                tier=1,  # Tier 1: Critical\n                title=\"Add docstrings to missing functions\",\n                reason=reason_text,\n                bullets=doc_bullets\n            )\n        \n        # Registry gaps priority (separate from missing docstrings)\n        # Get missing_docs_list from doc_metrics (structure: missing_docs.missing_files)\n        missing_docs_list = None\n        if isinstance(doc_metrics, dict):\n            # Try multiple paths to find missing_files structure\n            missing_docs_raw_for_list = doc_metrics_details.get('missing_docs') or doc_metrics.get('missing_docs') or {}\n            if isinstance(missing_docs_raw_for_list, dict):\n                missing_docs_list = missing_docs_raw_for_list.get('missing_files', {})\n            # Also try data.missing.missing_files\n            data_section_for_list = doc_metrics.get('data', {})\n            if not missing_docs_list and isinstance(data_section_for_list, dict):\n                missing_from_data_for_list = data_section_for_list.get('missing', {})\n                if isinstance(missing_from_data_for_list, dict):\n                    missing_docs_list = missing_from_data_for_list.get('missing_files', {})\n        \n        if missing_docs_count and missing_docs_count > 0:\n            registry_bullets: List[str] = []\n            \n            # Show top missing items if available\n            if missing_docs_list and isinstance(missing_docs_list, dict):\n                sorted_files = sorted(\n                    missing_docs_list.items(),\n                    key=lambda x: len(x[1]) if isinstance(x[1], list) else 1,\n                    reverse=True\n                )[:5]\n                \n                if sorted_files:\n                    item_list = []\n                    for file_path, funcs in sorted_files:\n                        func_count = len(funcs) if isinstance(funcs, list) else 1\n                        file_name = Path(file_path).name if file_path else 'Unknown'\n                        if func_count == 1 and isinstance(funcs, list) and funcs:\n                            func_name = funcs[0] if funcs else 'Unknown'\n                            item_list.append(f\"{func_name} ({file_name})\")\n                        else:\n                            item_list.append(f\"{file_name} ({func_count} functions)\")\n                    \n                    if len(sorted_files) < len(missing_docs_list):\n                        total_files = len(missing_docs_list)\n                        item_list.append(f\"... +{total_files - len(sorted_files)} more\")\n                    \n                    if item_list:\n                        registry_bullets.append(f\"Top items: {', '.join(item_list)}\")\n            \n            registry_bullets.append(\n                \"Action: Regenerate registry entries via `python development_tools/run_development_tools.py docs`\"\n            )\n            registry_bullets.append(\n                \"Effort: Small (automated command regenerates FUNCTION_REGISTRY_DETAIL.md)\"\n            )\n            registry_bullets.append(\n                \"Why this matters: Registry documentation helps track all functions in the codebase and their relationships\"\n            )\n            \n            add_priority(\n                tier=2,  # Tier 2: Important but less critical than missing docstrings\n                title=\"Update function registry\",\n                reason=f\"{missing_docs_count} items missing from FUNCTION_REGISTRY_DETAIL.md registry.\",\n                bullets=registry_bullets\n            )\n        \n        # Error handling to missing functions (before Phase 1/2, as it's more critical)\n        if missing_error_handlers and missing_error_handlers > 0:\n            error_handling_bullets: List[str] = []\n            \n            # List modules with function counts\n            if worst_error_modules:\n                module_list = []\n                for module in worst_error_modules[:10]:  # Show up to 10 modules\n                    module_name = module.get('module', 'Unknown')\n                    missing_count = module.get('missing', 0)\n                    if missing_count > 0:\n                        module_list.append(f\"{module_name}: {missing_count} functions\")\n                \n                if module_list:\n                    module_list_str = self._format_list_for_display(module_list, limit=10)\n                    error_handling_bullets.append(module_list_str)\n            \n            error_handling_bullets.append(\n                \"Add error handling decorators or try-except blocks to protect these functions.\"\n            )\n            # Add specific file paths and line numbers if available\n            missing_error_list = get_error_field('missing_error_handling', []) or []\n            if missing_error_list and isinstance(missing_error_list, list):\n                specific_functions = []\n                for func_info in missing_error_list[:3]:\n                    if isinstance(func_info, dict):\n                        file_path = func_info.get('file', '')\n                        func_name = func_info.get('function', '')\n                        line_num = func_info.get('line', '')\n                        if file_path and func_name:\n                            if line_num:\n                                specific_functions.append(f\"{file_path}:{line_num} ({func_name})\")\n                            else:\n                                specific_functions.append(f\"{file_path} ({func_name})\")\n                if specific_functions:\n                    error_handling_bullets.append(\n                        f\"Specific functions: {self._format_list_for_display(specific_functions, limit=3)}\"\n                    )\n            error_handling_bullets.append(\n                \"Effort: Small (add @handle_errors decorator or wrap in try-except)\"\n            )\n            error_handling_bullets.append(\n                \"Why this matters: Functions without error handling can crash the application on unexpected errors\"\n            )\n            \n            add_priority(\n                tier=1,  # Tier 1: Critical\n                title=\"Add error handling to missing functions\",\n                reason=f\"{missing_error_handlers} functions have no error handling.\",\n                bullets=error_handling_bullets\n            )\n        \n        # Phase 1: Decorator replacement candidates (after missing error handlers)\n        phase1_total = to_int(get_error_field('phase1_total', 0))\n        phase1_by_priority = get_error_field('phase1_by_priority', {}) or {}\n        phase1_high = to_int(phase1_by_priority.get('high', 0))\n        \n        if phase1_total and phase1_total > 0:\n            phase1_bullets: List[str] = []\n            # Get phase1 candidates to group by module\n            phase1_candidates = get_error_field('phase1_candidates', []) or []\n            if not isinstance(phase1_candidates, list):\n                phase1_candidates = []\n            \n            # Group by module (file_path) and priority\n            if phase1_candidates:\n                from collections import defaultdict\n                high_by_module = defaultdict(int)\n                medium_by_module = defaultdict(int)\n                \n                for candidate in phase1_candidates:\n                    if isinstance(candidate, dict):\n                        file_path = candidate.get('file_path', '')\n                        priority = candidate.get('priority', '').lower()\n                        if file_path:\n                            # Extract module name (file path without extension)\n                            module = file_path.replace('\\\\', '/')\n                            if priority == 'high':\n                                high_by_module[module] += 1\n                            elif priority == 'medium':\n                                medium_by_module[module] += 1\n                \n                # Show top modules with high-priority candidates, or medium if no high\n                if phase1_high and phase1_high > 0:\n                    phase1_bullets.append(\n                        f\"Start with {phase1_high} high-priority candidates (entry points and critical operations).\"\n                    )\n                    if high_by_module:\n                        # Sort by count and show top 3\n                        top_modules = sorted(high_by_module.items(), key=lambda x: x[1], reverse=True)[:3]\n                        module_list = [f\"{Path(m).name} ({count})\" for m, count in top_modules]\n                        if module_list:\n                            phase1_bullets.append(f\"Top modules: {self._format_list_for_display(module_list, limit=3)}\")\n                else:\n                    # No high priority, show medium priority modules\n                    phase1_medium = to_int(phase1_by_priority.get('medium', 0))\n                    if phase1_medium and phase1_medium > 0:\n                        phase1_bullets.append(\n                            f\"Process {phase1_medium} medium-priority functions.\"\n                        )\n                        if medium_by_module:\n                            top_modules = sorted(medium_by_module.items(), key=lambda x: x[1], reverse=True)[:3]\n                            module_list = [f\"{Path(m).name} ({count})\" for m, count in top_modules]\n                            if module_list:\n                                phase1_bullets.append(f\"Top modules: {self._format_list_for_display(module_list, limit=3)}\")\n            \n            phase1_medium = to_int(phase1_by_priority.get('medium', 0))\n            if phase1_medium and phase1_medium > 0 and phase1_high and phase1_high > 0:\n                phase1_bullets.append(\n                    f\"Then process {phase1_medium} medium-priority functions.\"\n                )\n            phase1_bullets.append(\n                \"Apply `@handle_errors` decorator to replace basic try-except blocks.\"\n            )\n            # Add effort estimate and context\n            phase1_bullets.append(\n                \"Effort: Medium (requires reviewing each function's error handling needs)\"\n            )\n            phase1_bullets.append(\n                \"Why this matters: Centralized error handling provides consistent logging and recovery patterns\"\n            )\n            add_priority(\n                tier=2,  # Tier 2: High\n                title=\"Phase 1: Replace basic try-except with decorators\",\n                reason=f\"{phase1_total} functions have try-except blocks that should use `@handle_errors` decorator.\",\n                bullets=phase1_bullets\n            )\n        \n        # Phase 2: Generic exception categorization (after Phase 1)\n        phase2_total = to_int(get_error_field('phase2_total', 0))\n        phase2_by_type = get_error_field('phase2_by_type', {}) or {}\n        \n        if phase2_total and phase2_total > 0:\n            phase2_bullets: List[str] = []\n            top_exceptions = sorted(phase2_by_type.items(), key=lambda x: x[1], reverse=True)[:3]\n            if top_exceptions:\n                exc_details = [f\"{count} {exc_type}\" for exc_type, count in top_exceptions]\n                phase2_bullets.append(\n                    f\"Most common: {self._format_list_for_display(exc_details, limit=3)}\"\n                )\n            \n            # Get phase2 exceptions to group by module\n            phase2_exceptions = get_error_field('phase2_exceptions', []) or []\n            if not isinstance(phase2_exceptions, list):\n                phase2_exceptions = []\n            if phase2_exceptions:\n                from collections import defaultdict\n                exceptions_by_module = defaultdict(int)\n                \n                for exc in phase2_exceptions:\n                    if isinstance(exc, dict):\n                        file_path = exc.get('file_path', '')\n                        if file_path:\n                            module = file_path.replace('\\\\', '/')\n                            exceptions_by_module[module] += 1\n                \n                if exceptions_by_module:\n                    # Sort by count and show top 3\n                    top_modules = sorted(exceptions_by_module.items(), key=lambda x: x[1], reverse=True)[:3]\n                    module_list = [f\"{Path(m).name} ({count})\" for m, count in top_modules]\n                    if module_list:\n                        phase2_bullets.append(f\"Top modules: {self._format_list_for_display(module_list, limit=3)}\")\n            \n            phase2_bullets.append(\n                \"Replace generic exceptions (ValueError, Exception, KeyError, TypeError) with specific project error classes.\"\n            )\n            phase2_bullets.append(\n                \"See `ai_development_docs/AI_ERROR_HANDLING_GUIDE.md` for categorization rules.\"\n            )\n            phase2_bullets.append(\n                \"Effort: Small (replace exception class names, update imports)\"\n            )\n            phase2_bullets.append(\n                \"Why this matters: Specific error classes enable targeted error handling and better debugging\"\n            )\n            add_priority(\n                tier=2,  # Tier 2: High\n                title=\"Phase 2: Categorize generic exceptions\",\n                reason=f\"{phase2_total} generic exception raises need categorization into project-specific error classes.\",\n                bullets=phase2_bullets\n            )\n        \n        # Coverage priorities\n        if low_coverage_modules:\n            coverage_highlights = [\n                f\"{module.get('module', 'module')} ({percent_text(module.get('coverage'), 1)}, {module.get('missed')} lines missing)\"\n                for module in low_coverage_modules\n            ]\n            coverage_bullets = [\n                f\"Target domains: {self._format_list_for_display(coverage_highlights, limit=3)}\",\n                \"Action: Add scenario tests for uncovered code paths in target domains\",\n                f\"Effort: Medium (writing tests for {len(low_coverage_modules)} domains requires understanding business logic)\",\n                \"Why this matters: Higher test coverage reduces bug risk and improves code maintainability\",\n                \"Command: Run `python development_tools/run_development_tools.py audit --full` after adding tests to verify coverage improvement\"\n            ]\n            add_priority(\n                tier=2,  # Tier 2: High\n                title=\"Raise coverage for domains below target\",\n                reason=f\"{len(low_coverage_modules)} key domains remain below the 80% target.\",\n                bullets=coverage_bullets\n            )\n        \n        if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n            dev_pct = dev_tools_insights['overall_pct']\n            low_dev_modules = dev_tools_insights.get('low_modules') or []\n            if dev_pct < 60 or low_dev_modules:\n                dev_bullets: List[str] = []\n                if low_dev_modules:\n                    highlights = [\n                        f\"{Path(item['path']).name} ({percent_text(item.get('coverage'), 1)})\"\n                        for item in low_dev_modules\n                    ]\n                    dev_bullets.append(f\"Focus on: {self._format_list_for_display(highlights, limit=3)}\")\n                if dev_tools_insights.get('html'):\n                    dev_bullets.append(f\"Review HTML report at {dev_tools_insights['html']}\")\n                dev_bullets.append(\"Action: Strengthen tests in `tests/development_tools/` for fragile helpers and low-coverage modules\")\n                dev_bullets.append(f\"Effort: Medium (adding tests for {len(low_dev_modules) if low_dev_modules else 'multiple'} modules requires understanding tool behavior)\")\n                dev_bullets.append(\"Why this matters: Development tools need high test coverage to ensure reliability and prevent regressions\")\n                dev_bullets.append(\"Command: Run `python run_tests.py --coverage` to verify coverage improvements\")\n                add_priority(\n                    tier=2,  # Tier 2: High\n                    title=\"Raise development tools coverage\",\n                    reason=f\"Development tools coverage is {percent_text(dev_pct, 1)} (target 60%+).\",\n                    bullets=dev_bullets\n                )\n        \n        # Legacy references priority\n        if legacy_files and legacy_files > 0:\n            legacy_bullets: List[str] = []\n            if legacy_markers:\n                legacy_bullets.append(f\"{legacy_markers} legacy markers still surface during scans.\")\n            if legacy_report:\n                legacy_bullets.append(f\"Review {legacy_report} for exact locations.\")\n            legacy_bullets.append(\n                \"Action: Remove legacy compatibility code and update references to use new implementations\"\n            )\n            legacy_bullets.append(\n                f\"Effort: Medium (reviewing {legacy_files} files and updating references requires testing)\"\n            )\n            legacy_bullets.append(\n                \"Why this matters: Legacy code increases maintenance burden and technical debt\"\n            )\n            legacy_bullets.append(\n                \"Command: `python development_tools/run_development_tools.py legacy` to scan for legacy references\"\n            )\n            add_priority(\n                tier=3,  # Tier 3: Medium\n                title=\"Retire remaining legacy references\",\n                reason=f\"{legacy_files} files still depend on legacy compatibility markers.\",\n                bullets=legacy_bullets\n            )\n        \n        # Config validation priority\n        config_validation_summary = self._load_config_validation_summary()\n        if config_validation_summary:\n            total_recommendations = config_validation_summary.get('total_recommendations', 0)\n            recommendations = config_validation_summary.get('recommendations', [])\n            if total_recommendations > 0 and recommendations:\n                config_bullets: List[str] = []\n                # Deduplicate recommendations using a more robust approach\n                # Extract (tool_name, issue_type) tuples for deduplication\n                seen_tool_issues = set()\n                unique_recs = []\n                \n                for rec in recommendations:\n                    rec_text = rec if isinstance(rec, str) else rec.get('message', str(rec))\n                    \n                    # Extract tool name and issue type for deduplication\n                    # Format is typically: \"Update {tool_name} to import config module\"\n                    # or \"Replace hardcoded values in {tool_name} with config functions\"\n                    # or \"Fix issues in {tool_name}: {issues}\"\n                    tool_name = None\n                    issue_type = None\n                    \n                    # Try to extract tool name (usually after \"Update\", \"Replace\", \"Fix\")\n                    patterns = [\n                        (r'Update\\s+([^:]+?)\\s+to\\s+import\\s+config', 'import_config'),\n                        (r'Replace\\s+hardcoded\\s+values\\s+in\\s+([^:]+?)\\s+with\\s+config', 'hardcoded_values'),\n                        (r'Fix\\s+issues\\s+in\\s+([^:]+?):', 'issues'),\n                    ]\n                    \n                    for pattern, issue in patterns:\n                        match = re.search(pattern, rec_text, re.IGNORECASE)\n                        if match:\n                            tool_name = match.group(1).strip()\n                            issue_type = issue\n                            break\n                    \n                    # If we couldn't extract tool/issue, use the full text for deduplication\n                    if tool_name and issue_type:\n                        dedup_key = (tool_name.lower(), issue_type)\n                    else:\n                        # Fallback: use normalized text\n                        normalized = rec_text.lower().strip()\n                        dedup_key = ('_general', normalized)\n                    \n                    # Only add if we haven't seen this (tool, issue) combination\n                    if dedup_key not in seen_tool_issues:\n                        seen_tool_issues.add(dedup_key)\n                        unique_recs.append(rec_text)\n                    \n                    # Limit to top 3 unique recommendations\n                    if len(unique_recs) >= 3:\n                        break\n                \n                # Add unique recommendations to bullets\n                for rec_text in unique_recs:\n                    config_bullets.append(rec_text)\n                \n                # Show count of remaining recommendations if any\n                if len(recommendations) > len(unique_recs):\n                    config_bullets.append(f\"...and {len(recommendations) - len(unique_recs)} more recommendation(s)\")\n                \n                # Only add priority if we have unique recommendations\n                if unique_recs:\n                    config_bullets.append(\n                        \"Action: Update tools to import and use centralized config module instead of hardcoded values\"\n                    )\n                    config_bullets.append(\n                        f\"Effort: Small to Medium (updating {len(unique_recs)} tool(s) requires understanding config structure)\"\n                    )\n                    config_bullets.append(\n                        \"Why this matters: Centralized configuration improves maintainability and consistency across tools\"\n                    )\n                    add_priority(\n                        tier=4,  # Tier 4: Low\n                        title=\"Update tools to use centralized config\",\n                        reason=f\"{len(unique_recs)} unique config validation recommendation(s) pending review.\",\n                        bullets=config_bullets\n                    )\n        \n        # Handler classes without docs priority\n        function_patterns_data = self._load_tool_data('analyze_function_patterns', 'functions')\n        if function_patterns_data and isinstance(function_patterns_data, dict):\n            if 'details' in function_patterns_data:\n                handlers = function_patterns_data['details'].get('handlers', [])\n            else:\n                handlers = function_patterns_data.get('handlers', [])\n            \n            if handlers:\n                handlers_no_doc = [h for h in handlers if not h.get('has_doc', True)]\n                if handlers_no_doc:\n                    if len(handlers_no_doc) <= 5:\n                        if not hasattr(self, '_quick_wins_handlers'):\n                            handler_examples = []\n                            for h in handlers_no_doc[:3]:\n                                class_name = h.get('class', 'Unknown')\n                                file_path = h.get('file', '')\n                                if file_path:\n                                    handler_examples.append(f\"{class_name} ({file_path})\")\n                                else:\n                                    handler_examples.append(class_name)\n                            self._quick_wins_handlers = {\n                                'count': len(handlers_no_doc),\n                                'examples': handler_examples\n                            }\n                    else:\n                        # Add as priority item\n                        handlers_bullets: List[str] = []\n                        top_handlers = sorted(handlers_no_doc, key=lambda x: x.get('methods', 0), reverse=True)[:5]\n                        handler_list = [f\"{h.get('class', 'Unknown')} ({Path(h.get('file', '')).name}, {h.get('methods', 0)} methods)\" for h in top_handlers]\n                        if len(handlers_no_doc) > 5:\n                            handler_list.append(f\"... +{len(handlers_no_doc) - 5} more\")\n                        handlers_bullets.append(\n                            f\"Top handlers: {', '.join(handler_list)}\"\n                        )\n                        handlers_bullets.append(\n                            \"Action: Add class-level docstrings to handler classes (e.g., class AccountManagementHandler: \\\"\\\"\\\"Handler for...\\\"\\\"\\\")\"\n                        )\n                        handlers_bullets.append(\n                            f\"Effort: Small (adding docstrings to {len(handlers_no_doc)} handler classes is straightforward)\"\n                        )\n                        handlers_bullets.append(\n                            \"Why this matters: Class docstrings improve code documentation and help developers understand handler purpose\"\n                        )\n                        add_priority(\n                            tier=1,  # Tier 1: Critical (documentation)\n                            title=\"Add docstrings to handler classes\",\n                            reason=f\"{len(handlers_no_doc)} handler classes missing class docstrings.\",\n                            bullets=handlers_bullets\n                        )\n        \n        # Test markers priority\n        if test_markers_data and isinstance(test_markers_data, dict):\n            # Handle both standard format (with summary/details) and legacy format (direct keys)\n            if 'summary' in test_markers_data:\n                summary = test_markers_data.get('summary', {})\n                missing_count = summary.get('total_issues', 0)\n                details = test_markers_data.get('details', {})\n                missing_list = details.get('missing', [])\n            else:\n                # Legacy format or direct data structure\n                missing_count = test_markers_data.get('missing_count', 0)\n                missing_list = test_markers_data.get('missing', [])\n                # Also check if data is wrapped in 'data' key\n                if not missing_count and 'data' in test_markers_data:\n                    data_wrapper = test_markers_data.get('data', {})\n                    if isinstance(data_wrapper, dict):\n                        missing_count = data_wrapper.get('missing_count', 0)\n                        missing_list = data_wrapper.get('missing', [])\n            \n            # Use missing_count if available, otherwise count from missing_list\n            actual_count = missing_count if missing_count > 0 else (len(missing_list) if missing_list else 0)\n            \n            if actual_count > 0:\n                test_markers_bullets: List[str] = []\n                \n                # Get top files with missing markers if available\n                if missing_list and isinstance(missing_list, list):\n                    from collections import defaultdict\n                    files_with_missing = defaultdict(list)\n                    for item in missing_list:\n                        if isinstance(item, dict):\n                            file_path = item.get('file', '')\n                            test_name = item.get('name', '')\n                            if file_path:\n                                files_with_missing[file_path].append(test_name)\n                    \n                    if files_with_missing:\n                        sorted_files = sorted(\n                            files_with_missing.items(),\n                            key=lambda x: len(x[1]),\n                            reverse=True\n                        )[:5]\n                        file_list = []\n                        for file_path, tests in sorted_files:\n                            test_count = len(tests)\n                            file_name = Path(file_path).name if file_path else 'Unknown'\n                            file_list.append(f\"{file_name} ({test_count} tests)\")\n                        if len(sorted_files) < len(files_with_missing):\n                            file_list.append(f\"... +{len(files_with_missing) - len(sorted_files)} more files\")\n                        test_markers_bullets.append(f\"Top files: {self._format_list_for_display(file_list, limit=5)}\")\n                \n                test_markers_bullets.append(\n                    \"Action: Add pytest category markers to tests missing them\"\n                )\n                test_markers_bullets.append(\n                    \"Effort: Small (run automated fix tool or add markers manually)\"\n                )\n                test_markers_bullets.append(\n                    \"Why this matters: Category markers help organize tests and enable selective test execution\"\n                )\n                test_markers_bullets.append(\n                    \"Command: `python development_tools/tests/fix_test_markers.py`\"\n                )\n                \n                add_priority(\n                    tier=3,  # Tier 3: Medium priority (test organization)\n                    title=\"Add pytest category markers to tests\",\n                    reason=f\"{actual_count} tests are missing pytest category markers.\",\n                    bullets=test_markers_bullets\n                )\n        \n        # Unused imports priority\n        # Only recommend removing \"Obvious Unused\" imports (not test mocking, Qt testing, etc.)\n        if unused_imports_data and isinstance(unused_imports_data, dict):\n            summary = unused_imports_data.get('summary', {})\n            details = unused_imports_data.get('details', {})\n            by_category = details.get('by_category') or unused_imports_data.get('by_category', {})\n            obvious_unused = by_category.get('obvious_unused', 0) if isinstance(by_category, dict) else 0\n            \n            # Only create recommendation if there are obvious unused imports\n            if obvious_unused and obvious_unused > 0:\n                unused_bullets: List[str] = []\n                \n                # Get files with obvious unused imports\n                files_dict = unused_imports_data.get('files', {})\n                obvious_files = []\n                if files_dict and isinstance(files_dict, dict):\n                    # Filter files to only those with obvious unused imports\n                    # We need to check the category for each file's imports\n                    for file_path, imports in files_dict.items():\n                        if isinstance(imports, list):\n                            # Check if any import in this file is marked as obvious_unused\n                            # The structure may vary, so we'll count files that have obvious unused\n                            # For now, we'll show top files from the report\n                            pass\n                \n                # Get top files if available (showing all files, but recommendation is only for obvious)\n                if files_dict and isinstance(files_dict, dict):\n                    sorted_files = sorted(\n                        files_dict.items(),\n                        key=lambda x: len(x[1]) if isinstance(x[1], list) else 1,\n                        reverse=True\n                    )[:5]\n                    if sorted_files:\n                        file_list = []\n                        for file_path, imports in sorted_files:\n                            import_count = len(imports) if isinstance(imports, list) else 1\n                            file_name = Path(file_path).name if file_path else 'Unknown'\n                            file_list.append(f\"{file_name} ({import_count} imports)\")\n                        if len(sorted_files) < len(files_dict):\n                            file_list.append(f\"... +{len(files_dict) - len(sorted_files)} more files\")\n                        unused_bullets.append(f\"Top files: {self._format_list_for_display(file_list, limit=5)}\")\n                \n                unused_bullets.append(f\"{obvious_unused} obvious removals (safe to delete)\")\n                \n                type_only = by_category.get('type_hints_only', 0) if isinstance(by_category, dict) else 0\n                if type_only and type_only > 0:\n                    unused_bullets.append(f\"Note: {type_only} type-only imports exist but are not recommended for removal (consider TYPE_CHECKING guard if needed)\")\n                \n                unused_bullets.append(\n                    \"Action: Review and remove obvious unused imports (safe to delete)\"\n                )\n                unused_bullets.append(\n                    \"Effort: Small (remove safe imports identified as obvious unused)\"\n                )\n                unused_bullets.append(\n                    \"Why this matters: Unused imports add noise, slow imports, and can hide real dependencies\"\n                )\n                \n                # Check if there's a detailed report\n                unused_imports_report_path = self.project_root / \"development_docs\" / \"UNUSED_IMPORTS_REPORT.md\"\n                if unused_imports_report_path.exists():\n                    unused_bullets.append(\n                        f\"Detailed Report: [UNUSED_IMPORTS_REPORT.md](development_docs/UNUSED_IMPORTS_REPORT.md)\"\n                    )\n                \n                # Determine tier based on obvious unused count (not total)\n                tier = 1 if obvious_unused > 50 else 2  # Critical if >50 obvious, High otherwise\n                \n                # Count files with obvious unused (approximate - we show all files but note it's only obvious)\n                files_with_obvious = summary.get('files_affected', 0) or unused_imports_data.get('files_with_issues', 0)\n                \n                add_priority(\n                    tier=tier,\n                    title=\"Remove obvious unused imports\",\n                    reason=f\"{obvious_unused} obvious unused import(s) can be safely removed.\",\n                    bullets=unused_bullets,\n                    validate=True,\n                    data_source='analyze_unused_imports',\n                    count=obvious_unused,\n                    expected_min=None  # No minimum expectation - any number of obvious unused imports is valid\n                )\n        \n        # Complexity refactoring priority\n        if critical_complex and critical_complex > 0:\n            complexity_bullets: List[str] = []\n            # Ensure we have examples - try loading if not available\n            if not critical_examples:\n                # Try loading from decision_support_metrics (may be cached)\n                decision_metrics = self.results_cache.get('decision_support_metrics', {})\n                if decision_metrics and 'critical_complexity_examples' in decision_metrics:\n                    critical_examples = decision_metrics.get('critical_complexity_examples', [])\n                # If still not available, try loading from analyze_functions (may be cached)\n                if not critical_examples:\n                    func_result = self._load_tool_data('analyze_functions', 'functions', log_source=False)\n                    if func_result and isinstance(func_result, dict):\n                        func_details = func_result.get('details', {})\n                        if 'critical_complexity_examples' in func_details:\n                            critical_examples = func_details.get('critical_complexity_examples', [])\n                        elif 'critical_complexity_examples' in func_result:\n                            critical_examples = func_result.get('critical_complexity_examples', [])\n                        # Also check decision_support tool data if available\n                        if not critical_examples or len(critical_examples) == 0:\n                            decision_data = self._load_tool_data('decision_support', 'functions', log_source=False)\n                            if decision_data and isinstance(decision_data, dict):\n                                decision_details = decision_data.get('details', {})\n                                decision_metrics_from_tool = decision_details.get('decision_support_metrics', {}) or decision_data.get('decision_support_metrics', {})\n                                if decision_metrics_from_tool and 'critical_complexity_examples' in decision_metrics_from_tool:\n                                    critical_examples = decision_metrics_from_tool.get('critical_complexity_examples', [])\n                                # Also try loading high_examples if not already loaded\n                                if (not high_examples or len(high_examples) == 0) and 'high_complexity_examples' in decision_metrics_from_tool:\n                                    high_examples = decision_metrics_from_tool.get('high_complexity_examples', [])\n            \n            if critical_examples:\n                sorted_examples = sorted(\n                    critical_examples[:10],\n                    key=lambda x: x.get('complexity', x.get('nodes', 0)) if isinstance(x, dict) else 0,\n                    reverse=True\n                )[:3]\n                example_names = []\n                for ex in sorted_examples:\n                    if isinstance(ex, dict):\n                        func_name = ex.get('name', ex.get('function', 'unknown'))\n                        file_name = ex.get('file', '')\n                        if file_name:\n                            # Extract just the filename, not full path\n                            file_name = Path(file_name).name\n                            example_names.append(f\"{func_name} ({file_name})\")\n                        else:\n                            example_names.append(func_name)\n                    else:\n                        example_names.append(str(ex))\n                if example_names:\n                    complexity_bullets.append(\n                        f\"Highest complexity: {self._format_list_for_display(example_names, limit=3)}\"\n                    )\n            if high_complex and high_complex > 0 and critical_complex <= 10:\n                complexity_bullets.append(\n                    f\"Then address {high_complex} high-complexity functions (100-199 nodes).\"\n                )\n            complexity_bullets.append(\n                \"Action: Break down complex functions into smaller, focused functions with single responsibilities\"\n            )\n            complexity_bullets.append(\n                f\"Effort: Large (refactoring {critical_complex} critical functions requires careful analysis and testing)\"\n            )\n            complexity_bullets.append(\n                \"Why this matters: High complexity functions are harder to understand, test, and maintain, increasing bug risk\"\n            )\n            add_priority(\n                tier=3,  # Tier 3: Medium\n                title=\"Refactor high-complexity functions\",\n                reason=f\"{critical_complex} critical-complexity functions (>199 nodes) need immediate attention.\",\n                bullets=complexity_bullets\n            )\n        elif high_complex and high_complex > 0:\n            high_complexity_bullets: List[str] = []\n            if high_examples:\n                sorted_examples = sorted(\n                    high_examples[:10],\n                    key=lambda x: x.get('complexity', 0) if isinstance(x, dict) else 0,\n                    reverse=True\n                )[:3]\n                example_names = []\n                for ex in sorted_examples:\n                    if isinstance(ex, dict):\n                        func_name = ex.get('name', ex.get('function', 'unknown'))\n                        file_name = ex.get('file', '')\n                        if file_name:\n                            example_names.append(f\"{func_name} ({file_name})\")\n                        else:\n                            example_names.append(func_name)\n                    else:\n                        example_names.append(str(ex))\n                if example_names:\n                    high_complexity_bullets.append(\n                        f\"Highest complexity: {self._format_list_for_display(example_names, limit=3)}\"\n                    )\n            high_complexity_bullets.append(\n                \"Action: Simplify high-complexity functions by extracting helper functions and reducing nesting\"\n            )\n            high_complexity_bullets.append(\n                f\"Effort: Medium (refactoring {high_complex} high-complexity functions requires careful planning)\"\n            )\n            high_complexity_bullets.append(\n                \"Why this matters: High complexity functions are harder to understand, test, and maintain, increasing bug risk\"\n            )\n            add_priority(\n                tier=3,  # Tier 3: Medium\n                title=\"Refactor high-complexity functions\",\n                reason=f\"{high_complex} high-complexity functions (100-199 nodes) should be simplified.\",\n                bullets=high_complexity_bullets\n            )\n        \n        lines.append(\"## Immediate Focus (Ranked)\")\n        if priority_items:\n            for idx, item in enumerate(sorted(priority_items, key=lambda entry: entry['order']), start=1):\n                lines.append(f\"{idx}. **{item['title']}**  -  {item['reason']}\")\n                for bullet in item['bullets']:\n                    lines.append(f\"   - {bullet}\")\n        else:\n            lines.append(\"All signals are green. Re-run `python development_tools/run_development_tools.py status` to monitor.\")\n        lines.append(\"\")\n        \n        # Store TODO sync for Quick Wins instead of Immediate Focus\n        todo_sync_result = getattr(self, 'todo_sync_result', None)\n        if todo_sync_result and isinstance(todo_sync_result, dict):\n            completed_entries = todo_sync_result.get('completed_entries', 0)\n            if completed_entries > 0:\n                if not hasattr(self, '_quick_wins_todo'):\n                    entries = todo_sync_result.get('entries', [])\n                    line_numbers = [str(entry.get('line_number', '')) for entry in entries if entry.get('line_number')]\n                    self._quick_wins_todo = {\n                        'count': completed_entries,\n                        'line_numbers': line_numbers\n                    }\n        \n        quick_wins: List[str] = []\n        if ascii_issues is not None and ascii_issues > 0:\n            quick_wins.append(f\"Normalize {ascii_issues} file(s) with non-ASCII characters via doc-fix.\")\n        \n        # Add dependency documentation refresh to Quick Wins\n        dependency_summary = getattr(self, 'module_dependency_summary', None) or (hasattr(self, 'results_cache') and self.results_cache.get('analyze_module_dependencies'))\n        if dependency_summary and dependency_summary.get('missing_dependencies'):\n            missing = dependency_summary['missing_dependencies']\n            files = dependency_summary.get('missing_files') or dependency_summary.get('missing_sections') or []\n            if missing > 0:\n                files_str = self._format_list_for_display(files, limit=2) if files else \"affected files\"\n                quick_wins.append(f\"Refresh dependency documentation: {missing} module dependencies are undocumented ({files_str}). Regenerate via `python development_tools/run_development_tools.py docs`.\")\n        \n        # Add handler classes without docs to Quick Wins (if small count)\n        if hasattr(self, '_quick_wins_handlers') and self._quick_wins_handlers:\n            handler_info = self._quick_wins_handlers\n            count = handler_info.get('count', 0)\n            examples = handler_info.get('examples', [])\n            if count > 0:\n                examples_str = self._format_list_for_display(examples, limit=2) if examples else \"\"\n                quick_wins.append(f\"Add documentation to {count} handler class(es) missing docs{(' (' + examples_str + ')') if examples_str else ''}.\")\n        \n        # Add TODO sync to Quick Wins\n        if hasattr(self, '_quick_wins_todo') and self._quick_wins_todo:\n            if isinstance(self._quick_wins_todo, dict):\n                count = self._quick_wins_todo.get('count', 0)\n                line_numbers = self._quick_wins_todo.get('line_numbers', [])\n                if line_numbers:\n                    lines_str = ', '.join(line_numbers[:5])\n                    if len(line_numbers) > 5:\n                        lines_str += f\", ... +{len(line_numbers) - 5}\"\n                    quick_wins.append(f\"Review {count} completed TODO entry/entries (lines {lines_str}) - if documented in changelogs, remove from TODO.md; otherwise move to CHANGELOG_DETAIL.md and AI_CHANGELOG.md first.\")\n                else:\n                    quick_wins.append(f\"Review {count} completed TODO entry/entries - if documented in changelogs, remove from TODO.md; otherwise move to CHANGELOG_DETAIL.md and AI_CHANGELOG.md first.\")\n            else:\n                quick_wins.append(f\"Review {self._quick_wins_todo} completed TODO entry/entries - if documented in changelogs, remove from TODO.md; otherwise move to CHANGELOG_DETAIL.md and AI_CHANGELOG.md first.\")\n        \n        if paired_doc_issues and not (path_drift_count and path_drift_count > 0):\n            if doc_sync_summary:\n                paired_docs_data = doc_sync_summary.get('paired_docs', {})\n                if isinstance(paired_docs_data, dict):\n                    content_sync_issues = paired_docs_data.get('content_sync', [])\n                    if content_sync_issues:\n                        quick_wins.append(f\"Resolve {paired_doc_issues} paired doc sync issue(s). Start with: {content_sync_issues[0]}\")\n                        if len(content_sync_issues) > 1:\n                            quick_wins.append(f\"  - Plus {len(content_sync_issues) - 1} more issue(s) to address\")\n                    else:\n                        quick_wins.append(f\"Resolve {paired_doc_issues} unpaired documentation sets flagged by doc-sync.\")\n                else:\n                    quick_wins.append(f\"Resolve {paired_doc_issues} unpaired documentation sets flagged by doc-sync.\")\n            else:\n                quick_wins.append(f\"Resolve {paired_doc_issues} unpaired documentation sets flagged by doc-sync.\")\n        \n        if analyze_artifacts:\n            artifact = analyze_artifacts[0]\n            location = artifact.get('file', 'unknown')\n            line_number = artifact.get('line')\n            if line_number:\n                location = f\"{location}:{line_number}\"\n            pattern = artifact.get('pattern', 'lint issue')\n            quick_wins.append(f\"Clean `{pattern}` marker in {location}.\")\n        if analyze_duplicates:\n            quick_wins.append(f\"Merge {len(analyze_duplicates)} duplicate documentation block(s).\")\n        if analyze_placeholders:\n            quick_wins.append(f\"Replace {len(analyze_placeholders)} placeholder section(s) flagged by docs scan.\")\n        \n        # Add documentation analysis quick wins\n        if ascii_data and isinstance(ascii_data, dict):\n            summary = ascii_data.get('summary', {})\n            ascii_total = summary.get('total_issues', 0)\n            ascii_file_count = summary.get('files_affected', 0)\n            if ascii_total > 0:\n                quick_wins.append(f\"Fix {ascii_total} ASCII compliance issue(s) in {ascii_file_count} file(s) - run `python development_tools/run_development_tools.py doc-fix --fix-ascii`\")\n        \n        if heading_data and isinstance(heading_data, dict):\n            summary = heading_data.get('summary', {})\n            heading_total = summary.get('total_issues', 0)\n            heading_file_count = summary.get('files_affected', 0)\n            if heading_total > 0:\n                quick_wins.append(f\"Fix {heading_total} heading numbering issue(s) in {heading_file_count} file(s) - run `python development_tools/run_development_tools.py doc-fix --number-headings`\")\n        \n        if missing_addresses_data and isinstance(missing_addresses_data, dict):\n            summary = missing_addresses_data.get('summary', {})\n            missing_total = summary.get('total_issues', 0)\n            missing_file_count = summary.get('files_affected', 0)\n            if missing_total > 0:\n                quick_wins.append(f\"Add file address metadata to {missing_file_count} file(s) missing addresses - run `python development_tools/run_development_tools.py doc-fix --add-addresses`\")\n        \n        if unconverted_links_data and isinstance(unconverted_links_data, dict):\n            summary = unconverted_links_data.get('summary', {})\n            links_total = summary.get('total_issues', 0)\n            links_file_count = summary.get('files_affected', 0)\n            if links_total > 0:\n                quick_wins.append(f\"Convert {links_total} unconverted link(s) in {links_file_count} file(s) - run `python development_tools/run_development_tools.py doc-fix --convert-links`\")\n        \n        lines.append(\"## Quick Wins\")\n        if quick_wins:\n            for win in quick_wins:\n                lines.append(f\"- {win}\")\n        else:\n            lines.append(\"- No immediate quick wins identified. Re-run doc-sync after tackling focus items.\")\n        \n        # Add overlap analysis information only if there are issues to prioritize\n        consolidation_count = len(consolidation_recs) if consolidation_recs else 0\n        overlap_count = len(section_overlaps) if section_overlaps else 0\n        \n        # Add consolidation opportunities as priority items\n        if consolidation_recs and consolidation_count > 0:\n            # Use tier 4 for consolidation (low priority)\n            consolidation_bullets: List[str] = []\n            for rec in consolidation_recs[:3]:  # Show top 3\n                category = rec.get('category', 'Unknown')\n                files = rec.get('files', [])\n                suggestion = rec.get('suggestion', '')\n                if files and len(files) > 1:\n                    consolidation_bullets.append(f\"{category}: {len(files)} files - {suggestion}\")\n                    consolidation_bullets.append(f\"  Files: {', '.join(files[:3])}{'...' if len(files) > 3 else ''}\")\n            add_priority(\n                tier=4,  # Tier 4: Low\n                title=\"Consolidate documentation files\",\n                reason=f\"{consolidation_count} file groups could be consolidated to reduce redundancy.\",\n                bullets=consolidation_bullets\n            )\n        elif overlap_count > 0:\n            # If only overlaps (no consolidation recs), add as lower priority\n            # Use tier 4 for overlap (low priority)\n            overlap_bullets = [f\"{overlap_count} section overlaps detected - review for consolidation opportunities\"]\n            add_priority(\n                tier=4,  # Tier 4: Low\n                title=\"Review documentation overlaps\",\n                reason=f\"{overlap_count} section overlaps detected across documentation files.\",\n                bullets=overlap_bullets\n            )\n        \n        lines.append(\"\")\n        \n        watch_list: List[str] = []\n        doc_coverage_float = to_float(doc_coverage_value) if doc_coverage_value is not None else None\n        # Get documentation coverage threshold from config (defaults to 80.0 if not found)\n        try:\n            from ... import config\n            validation_config = config.get_validation_config()\n            doc_threshold = validation_config.get('documentation_coverage_threshold', 80.0)\n        except (ImportError, AttributeError):\n            doc_threshold = 80.0  # Default fallback\n        if doc_coverage_float is not None and doc_coverage_float < doc_threshold:\n            watch_list.append(f\"Documentation coverage sits at {percent_text(doc_coverage_value, 2)} (target {doc_threshold}%).\")\n        if coverage_overall:\n            coverage_pct = coverage_overall.get('coverage', 0)\n            target = 80\n            watch_list.append(\n                f\"Overall test coverage is {percent_text(coverage_pct, 1)} (target {target}%) \"\n                f\"({coverage_overall.get('covered')} / {coverage_overall.get('statements')} statements).\"\n            )\n        if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n            dev_pct = dev_tools_insights['overall_pct']\n            detail = f\"Development tools coverage is {percent_text(dev_pct, 1)} (target 60%+).\"\n            watch_list.append(detail)\n        \n        if high_examples:\n            high_focus = [\n                f\"{entry['function']} ({entry['file']})\"\n                for entry in high_examples[:3]\n            ]\n            if high_focus:\n                watch_list.append(f\"Monitor high complexity functions: {self._format_list_for_display(high_focus, limit=3)}.\")\n        \n        if legacy_markers and (not legacy_files or legacy_files == 0):\n            watch_list.append(f\"{legacy_markers} legacy markers remain; schedule periodic cleanup post-sprint.\")\n        \n        validation_output = ''\n        if hasattr(self, 'validation_results') and self.validation_results:\n            validation_output = self.validation_results.get('output', '')\n        \n        if not validation_output:\n            # Try loading from tool data (may be cached)\n            validation_data = self._load_tool_data('analyze_ai_work', 'ai_work', log_source=True)\n            if validation_data:\n                validation_output = validation_data.get('output', '') or ''\n            # Fallback to direct cache file access if tool data loader didn't find it\n            if not validation_output:\n                try:\n                    cached_data = self._load_results_file_safe()\n                    if cached_data and 'results' in cached_data and 'analyze_ai_work' in cached_data['results']:\n                            validation_result = cached_data['results']['analyze_ai_work']\n                            if 'data' in validation_result:\n                                data = validation_result['data']\n                                validation_output = data.get('output', '') or ''\n                except Exception:\n                    pass\n        \n        if validation_output and ('POOR' in validation_output or 'NEEDS ATTENTION' in validation_output or 'FAIR' in validation_output):\n            watch_list.append(\"AI Work Validation: Structural validation issues detected (see consolidated report)\")\n        \n        lines.append(\"## Watch List\")\n        if watch_list:\n            for item in watch_list:\n                lines.append(f\"- {item}\")\n        else:\n            lines.append(\"- No outstanding watch items. Continue regular audits to maintain signal quality.\")\n        lines.append(\"\")\n        \n        lines.append(\"## Follow-up Commands\")\n        lines.append(\"- `python development_tools/run_development_tools.py doc-sync`  -  refresh drift, pairing, and ASCII metrics.\")\n        lines.append(\"- `python development_tools/run_development_tools.py audit --full`  -  rebuild coverage and hygiene data after fixes.\")\n        lines.append(\"- `python development_tools/run_development_tools.py status`  -  confirm the latest health snapshot.\")\n        \n        return '\\n'.join(lines)\n    \n    def _generate_consolidated_report(self) -> str:\n        \"\"\"Generate comprehensive consolidated report combining all tool outputs.\"\"\"\n        # Log data source context\n        audit_tier = getattr(self, 'current_audit_tier', None)\n        if audit_tier:\n            logger.info(f\"[REPORT GENERATION] Generating consolidated_report.txt using data from Tier {audit_tier} audit\")\n        else:\n            logger.info(f\"[REPORT GENERATION] Generating consolidated_report.txt using cached data (no active audit)\")\n        \n        # Check if this is a mid-audit write\n        instance_flag = hasattr(self, '_audit_in_progress') and self._audit_in_progress\n        audit_in_progress = instance_flag or _is_audit_in_progress(self.project_root)\n        is_legitimate_end_write = hasattr(self, 'current_audit_tier') and self.current_audit_tier is not None\n        \n        if audit_in_progress and not is_legitimate_end_write:\n            if not instance_flag:\n                logger.warning(\"_generate_consolidated_report() called from NEW instance during audit! This should only happen at the end.\")\n            else:\n                logger.warning(\"_generate_consolidated_report() called during audit! This should only happen at the end.\")\n            import traceback\n            logger.debug(f\"Call stack:\\n{''.join(traceback.format_stack())}\")\n        \n        lines: List[str] = []\n        lines.append(\"# Comprehensive AI Development Tools Report\")\n        lines.append(\"\")\n        lines.append(\"> **Generated**: This file is auto-generated. Do not edit manually.\")\n        lines.append(f\"> **Last Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        \n        if self.current_audit_tier == 1:\n            source_cmd = \"python development_tools/run_development_tools.py audit --quick\"\n            tier_name = \"Tier 1 (Quick Audit)\"\n        elif self.current_audit_tier == 3:\n            source_cmd = \"python development_tools/run_development_tools.py audit --full\"\n            tier_name = \"Tier 3 (Full Audit)\"\n        elif self.current_audit_tier == 2:\n            source_cmd = \"python development_tools/run_development_tools.py audit\"\n            tier_name = \"Tier 2 (Standard Audit)\"\n        else:\n            source_cmd = \"python development_tools/run_development_tools.py status\"\n            tier_name = \"Status Check (cached data)\"\n        lines.append(f\"> **Source**: `{source_cmd}`\")\n        if self.current_audit_tier:\n            lines.append(f\"> **Last Audit Tier**: {tier_name}\")\n        lines.append(\"> **Generated by**: run_development_tools.py - AI Development Tools Runner\")\n        lines.append(\"\")\n        \n        def percent_text(value: Any, decimals: int = 1) -> str:\n            if value is None:\n                return \"Unknown\"\n            if isinstance(value, str):\n                return value if value.strip().endswith('%') else f\"{value}%\"\n            return self._format_percentage(value, decimals)\n        \n        def to_float(value: Any) -> Optional[float]:\n            if isinstance(value, (int, float)):\n                return float(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return float(stripped)\n                except ValueError:\n                    return None\n            return None\n        \n        def to_int(value: Any) -> Optional[int]:\n            if isinstance(value, int):\n                return value\n            if isinstance(value, float):\n                return int(value)\n            if isinstance(value, str):\n                stripped = value.strip().rstrip('%')\n                try:\n                    return int(float(stripped))\n                except ValueError:\n                    return None\n            if isinstance(value, dict):\n                count = value.get('count')\n                return to_int(count)\n            return None\n        \n        # Load all tool data using unified loader (returns standard format)\n        metrics = self._get_canonical_metrics()\n        doc_metrics = self._load_tool_data('analyze_function_registry', 'functions', log_source=True)\n        # Access doc_metrics - check details first (normalized), then top level (backward compat)\n        doc_metrics_details = doc_metrics.get('details', {})\n        doc_coverage = doc_metrics_details.get('doc_coverage') or doc_metrics.get('doc_coverage', metrics.get('doc_coverage'))\n        \n        # Try multiple paths to find missing_docs count\n        data_section = doc_metrics.get('data', {}) if isinstance(doc_metrics, dict) else {}\n        missing_from_data = data_section.get('missing', {}) if isinstance(data_section, dict) else {}\n        \n        missing_docs_raw = doc_metrics_details.get('missing_docs') or doc_metrics_details.get('missing_items') or doc_metrics.get('missing_docs') or doc_metrics.get('missing_items')\n        \n        if not missing_docs_raw and isinstance(missing_from_data, dict):\n            missing_docs_raw = missing_from_data\n        \n        if isinstance(missing_docs_raw, dict):\n            missing_docs_count = missing_docs_raw.get('count', 0)\n            missing_docs_list = missing_docs_raw.get('files', {})\n        else:\n            missing_docs_count = to_int(missing_docs_raw) or 0\n            missing_docs_list = {}\n        doc_totals = doc_metrics_details.get('totals') or doc_metrics.get('totals') or {}\n        documented_functions = doc_totals.get('functions_documented') if isinstance(doc_totals, dict) else None\n        \n        doc_sync_data = self._load_tool_data('analyze_documentation_sync', 'docs', log_source=True)\n        doc_sync_summary = getattr(self, 'docs_sync_summary', None) or doc_sync_data or {}\n        \n        unused_imports_data = self._load_tool_data('analyze_unused_imports', 'imports', log_source=True)\n        analyze_docs_data = self._load_tool_data('analyze_documentation', 'docs', log_source=True)\n        ascii_data = self._load_tool_data('analyze_ascii_compliance', 'docs', log_source=True)\n        heading_data = self._load_tool_data('analyze_heading_numbering', 'docs', log_source=True)\n        missing_addresses_data = self._load_tool_data('analyze_missing_addresses', 'docs', log_source=True)\n        unconverted_links_data = self._load_tool_data('analyze_unconverted_links', 'docs', log_source=True)\n        \n        function_metrics = self._load_tool_data('analyze_functions', 'functions', log_source=True)\n        function_metrics_details = function_metrics.get('details', {})\n        \n        doc_artifacts = analyze_docs_data.get('artifacts') if isinstance(analyze_docs_data, dict) else None\n        \n        # Extract overlap analysis data\n        details = analyze_docs_data.get('details', {})\n        overlap_analysis_ran = (\n            'section_overlaps' in analyze_docs_data or \n            'consolidation_recommendations' in analyze_docs_data or\n            'section_overlaps' in details or \n            'consolidation_recommendations' in details\n        )\n        \n        section_overlaps = (\n            analyze_docs_data.get('section_overlaps') or \n            details.get('section_overlaps', {})\n        ) if overlap_analysis_ran else {}\n        consolidation_recs = (\n            analyze_docs_data.get('consolidation_recommendations') or \n            details.get('consolidation_recommendations', [])\n        ) if overlap_analysis_ran else []\n        file_purposes = (\n            analyze_docs_data.get('file_purposes') or \n            details.get('file_purposes', {})\n        )\n        \n        if section_overlaps is None:\n            section_overlaps = {}\n        if consolidation_recs is None:\n            consolidation_recs = []\n        \n        error_metrics = self._load_tool_data('analyze_error_handling', 'error_handling')\n        error_summary = error_metrics.get('summary', {})\n        error_details = error_metrics.get('details', {})\n        missing_error_handlers = to_int(error_summary.get('total_issues'))\n        if missing_error_handlers is None:\n            missing_error_handlers = to_int(error_details.get('functions_missing_error_handling')) or 0\n        details_missing = to_int(error_details.get('functions_missing_error_handling'))\n        if details_missing is not None and details_missing > 0 and (missing_error_handlers is None or missing_error_handlers == 0):\n            missing_error_handlers = details_missing\n        error_recommendations = error_details.get('recommendations') or []\n        worst_error_modules = error_details.get('worst_modules') or []\n        if worst_error_modules is None or not isinstance(worst_error_modules, (list, tuple)):\n            worst_error_modules = []\n        \n        coverage_summary = self._load_coverage_summary()\n        \n        if not hasattr(self, 'dev_tools_coverage_results') or not self.dev_tools_coverage_results:\n            self._load_dev_tools_coverage()\n        \n        legacy_data = self._load_tool_data('analyze_legacy_references', 'legacy')\n        legacy_summary = getattr(self, 'legacy_cleanup_summary', None) or legacy_data or {}\n        if not legacy_summary and isinstance(legacy_data, dict):\n            legacy_summary = {\n                'files_with_issues': legacy_data.get('files_with_issues', 0),\n                'legacy_markers': legacy_data.get('legacy_markers', 0),\n                'report_path': legacy_data.get('report_path', 'development_docs/LEGACY_REFERENCE_REPORT.md')\n            }\n        \n        # Get missing docstrings count for consolidated report\n        func_metrics_details_for_undoc = function_metrics.get('details', {}) if isinstance(function_metrics, dict) else {}\n        func_undocumented = func_metrics_details_for_undoc.get('undocumented', 0) or (function_metrics.get('undocumented', 0) if isinstance(function_metrics, dict) else 0)\n        \n        # Also get handler classes count for consolidated report\n        function_patterns_data_for_report = self._load_tool_data('analyze_function_patterns', 'functions')\n        handler_classes_no_doc = 0\n        handler_classes_total = 0\n        if function_patterns_data_for_report and isinstance(function_patterns_data_for_report, dict):\n            if 'details' in function_patterns_data_for_report:\n                handlers = function_patterns_data_for_report['details'].get('handlers', [])\n            else:\n                handlers = function_patterns_data_for_report.get('handlers', [])\n            if handlers:\n                handler_classes_total = len(handlers)\n                handler_classes_no_doc = len([h for h in handlers if not h.get('has_doc', True)])\n        \n        # Recalculate doc_coverage if Unknown\n        if doc_coverage == 'Unknown' or doc_coverage is None:\n            function_data_for_coverage = self._load_tool_data('analyze_functions', 'functions')\n            if isinstance(function_data_for_coverage, dict):\n                func_details_for_coverage = function_data_for_coverage.get('details', {})\n                func_total_for_coverage = func_details_for_coverage.get('total_functions') or function_data_for_coverage.get('total_functions')\n                func_undocumented_for_coverage = func_details_for_coverage.get('undocumented', 0) or function_data_for_coverage.get('undocumented', 0)\n                if func_total_for_coverage and isinstance(func_total_for_coverage, (int, float)) and func_total_for_coverage > 0:\n                    func_documented_for_coverage = func_total_for_coverage - func_undocumented_for_coverage\n                    coverage_pct = (func_documented_for_coverage / func_total_for_coverage) * 100\n                    if 0 <= coverage_pct <= 100:\n                        doc_coverage = f\"{coverage_pct:.2f}%\"\n                    else:\n                        doc_coverage = 'Unknown'\n                else:\n                    doc_coverage = 'Unknown'\n            else:\n                doc_coverage = 'Unknown'\n        \n        lines.append(\"## Executive Summary\")\n        \n        # Get complexity metrics, trying multiple sources\n        total_funcs = metrics.get('total_functions', 'Unknown')\n        moderate = metrics.get('moderate', 'Unknown')\n        high = metrics.get('high', 'Unknown')\n        critical = metrics.get('critical', 'Unknown')\n        \n        # If still Unknown, try loading from analyze_functions or decision_support cache\n        if moderate == 'Unknown' or high == 'Unknown':\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data and 'results' in cached_data and 'analyze_functions' in cached_data['results']:\n                    func_data = cached_data['results']['analyze_functions']\n                    if 'data' in func_data:\n                        cached_metrics_raw = func_data['data']\n                        if 'details' in cached_metrics_raw and isinstance(cached_metrics_raw.get('details'), dict):\n                            cached_metrics = cached_metrics_raw['details']\n                        else:\n                            cached_metrics = cached_metrics_raw\n                        if moderate == 'Unknown':\n                            moderate = cached_metrics.get('moderate_complexity', 'Unknown')\n                        if high == 'Unknown':\n                            high = cached_metrics.get('high_complexity', 'Unknown')\n                        if critical == 'Unknown':\n                            critical = cached_metrics.get('critical_complexity', 'Unknown')\n                        if total_funcs == 'Unknown':\n                            total_funcs = cached_metrics.get('total_functions', 'Unknown')\n                if (moderate == 'Unknown' or high == 'Unknown') and cached_data and 'results' in cached_data and 'decision_support' in cached_data['results']:\n                        ds_data = cached_data['results']['decision_support']\n                        if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                            ds_metrics = ds_data['data']['decision_support_metrics']\n                            if moderate == 'Unknown':\n                                moderate = ds_metrics.get('moderate_complexity', 'Unknown')\n                            if high == 'Unknown':\n                                high = ds_metrics.get('high_complexity', 'Unknown')\n                            if critical == 'Unknown':\n                                critical = ds_metrics.get('critical_complexity', 'Unknown')\n                            if total_funcs == 'Unknown':\n                                total_funcs = ds_metrics.get('total_functions', 'Unknown')\n            except Exception as e:\n                logger.debug(f\"Failed to load complexity from cache in consolidated report: {e}\")\n                pass\n        \n        if total_funcs == 'Unknown':\n            lines.append(\"- **Total Functions**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n        else:\n            lines.append(f\"- **Total Functions**: {total_funcs} (Critical: {critical}, High: {high}, Moderate: {moderate})\")\n        \n        # Add test markers to executive summary\n        test_markers_data = self._load_tool_data('analyze_test_markers', 'tests')\n        if test_markers_data and isinstance(test_markers_data, dict):\n            if 'summary' in test_markers_data:\n                missing_markers = test_markers_data['summary'].get('total_issues', 0)\n            else:\n                missing_markers = test_markers_data.get('missing_count', 0)\n            if missing_markers > 0:\n                lines.append(f\"- Test markers: {missing_markers} tests missing category markers\")\n        \n        error_cov = error_metrics.get('analyze_error_handling') or error_metrics.get('error_handling_coverage')\n        missing_error_handlers_for_summary = to_int(error_metrics.get('functions_missing_error_handling', 0))\n        error_total = error_metrics.get('total_functions')\n        error_with_handling = error_metrics.get('functions_with_error_handling')\n        \n        if error_total and error_with_handling:\n            calc_coverage = (error_with_handling / error_total) * 100\n            if 0 <= calc_coverage <= 100:\n                error_cov = calc_coverage\n        elif error_cov is None and error_total and error_with_handling:\n            error_cov = (error_with_handling / error_total) * 100\n        \n        if error_cov is not None:\n            lines.append(f\"- Error handling coverage {percent_text(error_cov, 1)}; {missing_error_handlers_for_summary or 0} functions need protection\")\n        \n        dev_tools_insights = self._get_dev_tools_coverage_insights()\n        \n        if coverage_summary:\n            overall = coverage_summary.get('overall') or {}\n            overall_cov = overall.get('coverage')\n            lines.append(f\"- **Overall test coverage**: {percent_text(overall_cov, 1)} across {overall.get('statements', 0)} statements\")\n            \n            if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n                dev_pct = dev_tools_insights['overall_pct']\n                summary_line = f\"- **Development tools coverage**: {percent_text(dev_pct, 1)}\"\n                if dev_tools_insights.get('covered') is not None and dev_tools_insights.get('statements') is not None:\n                    summary_line += f\" ({dev_tools_insights['covered']} of {dev_tools_insights['statements']} statements)\"\n                lines.append(summary_line)\n        elif hasattr(self, 'coverage_results') and self.coverage_results:\n            lines.append(\"- Coverage regeneration flagged issues; inspect coverage.json for details\")\n            if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n                dev_pct = dev_tools_insights['overall_pct']\n                summary_line = f\"- **Development tools coverage**: {percent_text(dev_pct, 1)}\"\n                if dev_tools_insights.get('covered') is not None and dev_tools_insights.get('statements') is not None:\n                    summary_line += f\" ({dev_tools_insights['covered']} of {dev_tools_insights['statements']} statements)\"\n                lines.append(summary_line)\n        else:\n            if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n                dev_pct = dev_tools_insights['overall_pct']\n                summary_line = f\"- **Development tools coverage**: {percent_text(dev_pct, 1)}\"\n                if dev_tools_insights.get('covered') is not None and dev_tools_insights.get('statements') is not None:\n                    summary_line += f\" ({dev_tools_insights['covered']} of {dev_tools_insights['statements']} statements)\"\n                lines.append(summary_line)\n        \n        # Get path_drift - check standard format first\n        path_drift = None\n        if doc_sync_summary and isinstance(doc_sync_summary, dict):\n            if 'summary' in doc_sync_summary and isinstance(doc_sync_summary.get('summary'), dict):\n                path_drift = doc_sync_summary.get('details', {}).get('path_drift_issues', 0)\n            else:\n                path_drift = doc_sync_summary.get('path_drift_issues')\n        \n        if path_drift is not None:\n            lines.append(f\"- **Documentation path drift**: {path_drift} issues need sync\")\n        \n        # Get legacy_issues - check standard format first\n        legacy_issues = None\n        if legacy_summary and isinstance(legacy_summary, dict):\n            if 'summary' in legacy_summary and isinstance(legacy_summary.get('summary'), dict):\n                legacy_issues = legacy_summary['summary'].get('files_affected', 0)\n            else:\n                legacy_issues = legacy_summary.get('files_with_issues')\n        \n        if legacy_issues is not None:\n            lines.append(f\"- **Legacy references**: {legacy_issues} files still reference legacy patterns\")\n        \n        lines.append(\"\")\n        \n        # Documentation Status section\n        lines.append(\"## Documentation Status\")\n        \n        # Use aggregated doc sync summary from current run first, then fall back to cache\n        doc_sync_summary_for_signals = None\n        if hasattr(self, 'docs_sync_summary') and self.docs_sync_summary and isinstance(self.docs_sync_summary, dict):\n            doc_sync_summary_for_signals = {\n                'status': self.docs_sync_summary.get('status', 'UNKNOWN'),\n                'path_drift_issues': self.docs_sync_summary.get('path_drift_issues', 0),\n                'paired_doc_issues': self.docs_sync_summary.get('paired_doc_issues', 0),\n                'ascii_issues': self.docs_sync_summary.get('ascii_issues', 0),\n                'heading_numbering_issues': self.docs_sync_summary.get('heading_numbering_issues', 0),\n                'missing_address_issues': self.docs_sync_summary.get('missing_address_issues', 0),\n                'unconverted_link_issues': self.docs_sync_summary.get('unconverted_link_issues', 0),\n                'path_drift_files': self.docs_sync_summary.get('path_drift_files', [])\n            }\n        \n        if not doc_sync_summary_for_signals:\n            doc_sync_result = self._load_tool_data('analyze_documentation_sync', 'docs')\n            if doc_sync_result:\n                cached_metrics = doc_sync_result if isinstance(doc_sync_result, dict) else {}\n                doc_sync_summary_for_signals = {\n                    'status': cached_metrics.get('status', 'UNKNOWN'),\n                    'path_drift_issues': cached_metrics.get('path_drift_issues', 0),\n                    'paired_doc_issues': cached_metrics.get('paired_doc_issues', 0),\n                    'ascii_issues': cached_metrics.get('ascii_issues', 0),\n                    'path_drift_files': cached_metrics.get('path_drift_files', [])\n                }\n        \n        effective_summary = doc_sync_summary if doc_sync_summary else doc_sync_summary_for_signals\n        \n        def get_doc_sync_field(data, field_name, default=None):\n            if not data or not isinstance(data, dict):\n                return default\n            if 'summary' in data and isinstance(data.get('summary'), dict):\n                if field_name == 'status':\n                    return data['summary'].get('status', default)\n                elif field_name == 'total_issues':\n                    return data['summary'].get('total_issues', default)\n                else:\n                    return data.get('details', {}).get(field_name, default)\n            else:\n                return data.get(field_name, default)\n        \n        def extract_files_with_issue_counts(tool_data):\n            \"\"\"Extract file paths with their issue counts from tool data.\"\"\"\n            if not tool_data or not isinstance(tool_data, dict):\n                return {}\n            \n            file_counts = {}\n            \n            if 'files' in tool_data:\n                files_dict = tool_data.get('files', {})\n                if isinstance(files_dict, dict):\n                    for file_path, value in files_dict.items():\n                        if isinstance(file_path, str):\n                            # CRITICAL: Filter out files that no longer exist\n                            try:\n                                file_path_obj = self.project_root / file_path\n                                if not file_path_obj.exists():\n                                    # File doesn't exist, skip it\n                                    continue\n                            except (OSError, ValueError):\n                                # Error checking file, skip it\n                                continue\n                            \n                            if isinstance(value, (int, float)):\n                                file_counts[file_path] = int(value)\n                            elif isinstance(value, dict):\n                                results = value.get('results', [])\n                                if isinstance(results, list):\n                                    file_counts[file_path] = len(results)\n                                else:\n                                    file_counts[file_path] = 1\n            \n            if not file_counts:\n                data_dict = tool_data.get('data', tool_data) if 'data' in tool_data else tool_data\n                if isinstance(data_dict, dict):\n                    for f, v in data_dict.items():\n                        if isinstance(f, str) and isinstance(v, dict):\n                            # CRITICAL: Filter out files that no longer exist\n                            try:\n                                file_path_obj = self.project_root / f\n                                if not file_path_obj.exists():\n                                    # File doesn't exist, skip it\n                                    continue\n                            except (OSError, ValueError):\n                                # Error checking file, skip it\n                                continue\n                            \n                            results = v.get('results', [])\n                            if isinstance(results, list) and len(results) > 0:\n                                file_counts[f] = len(results)\n            \n            return file_counts\n        \n        def get_issue_count(summary_key, tool_data, default=0):\n            \"\"\"Get issue count from summary, or calculate from tool data.\"\"\"\n            count = 0\n            if effective_summary:\n                if 'summary' in effective_summary and isinstance(effective_summary.get('summary'), dict):\n                    count = effective_summary.get('details', {}).get(summary_key, 0)\n                else:\n                    count = effective_summary.get(summary_key, 0)\n            \n            if count == 0 and tool_data and isinstance(tool_data, dict):\n                if 'summary' in tool_data and isinstance(tool_data.get('summary'), dict):\n                    count = tool_data['summary'].get('total_issues', 0)\n                elif 'total_issues' in tool_data:\n                    count = tool_data.get('total_issues', 0)\n                elif 'files' in tool_data:\n                    files_dict = tool_data.get('files', {})\n                    if isinstance(files_dict, dict):\n                        count = sum(v if isinstance(v, (int, float)) else 1 for v in files_dict.values())\n                else:\n                    data_dict = tool_data.get('data', tool_data) if 'data' in tool_data else tool_data\n                    if isinstance(data_dict, dict):\n                        total = 0\n                        for f, v in data_dict.items():\n                            if isinstance(v, dict):\n                                results = v.get('results', [])\n                                if isinstance(results, list) and len(results) > 0:\n                                    total += len(results)\n                        if total > 0:\n                            count = total\n            return count if count > 0 else default\n        \n        if effective_summary:\n            # Path Drift\n            path_drift = get_doc_sync_field(effective_summary, 'path_drift_issues', 0)\n            if path_drift is None:\n                path_drift = 0\n            if path_drift > 0:\n                drift_files = get_doc_sync_field(effective_summary, 'path_drift_files', [])\n                if not isinstance(drift_files, list):\n                    drift_files = []\n                lines.append(\"**Path Drift** FAIL\")\n                lines.append(f\"  - {path_drift} problem files (total issues)\")\n                if len(drift_files) <= 4:\n                    drift_hotspots = ', '.join(drift_files)\n                else:\n                    visible = ', '.join(drift_files[:4])\n                    remaining = path_drift - 4\n                    drift_hotspots = f\"{visible}, ... +{remaining}\"\n                lines.append(f\"  - Top files: {drift_hotspots}\")\n            else:\n                lines.append(\"**Path Drift** CLEAN\")\n                lines.append(\"  - 0 problem files\")\n            \n            # Paired Docs\n            paired = get_doc_sync_field(effective_summary, 'paired_doc_issues', 0) or 0\n            if paired == 0:\n                lines.append(\"**Paired Docs** CLEAN\")\n                lines.append(\"  - 0 H2 mismatches\")\n            else:\n                lines.append(\"**Paired Docs** FAIL\")\n                lines.append(f\"  - {paired} H2 mismatches\")\n            \n            # ASCII Compliance\n            ascii_issues = get_issue_count('ascii_issues', ascii_data)\n            if ascii_issues > 0:\n                ascii_file_counts = extract_files_with_issue_counts(ascii_data)\n                if ascii_data and isinstance(ascii_data, dict):\n                    if 'summary' in ascii_data:\n                        file_count = ascii_data['summary'].get('files_affected', 0)\n                    else:\n                        file_count = ascii_data.get('file_count', 0)\n                else:\n                    file_count = len(ascii_file_counts)\n                if file_count == 0:\n                    file_count = len(ascii_file_counts)\n                lines.append(\"**ASCII Compliance** FAIL\")\n                lines.append(f\"  - {file_count} files with {ascii_issues} total issues\")\n                if ascii_file_counts:\n                    sorted_files = sorted(ascii_file_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n                    file_list = [f\"{file_path} ({count})\" for file_path, count in sorted_files]\n                    if len(ascii_file_counts) > 5:\n                        file_list.append(f\"... +{len(ascii_file_counts) - 5}\")\n                    lines.append(f\"  - Top files: {', '.join(file_list)}\")\n            else:\n                lines.append(\"**ASCII Compliance** CLEAN\")\n                lines.append(\"  - 0 files with issues\")\n            \n            # Heading Numbering\n            heading_issues = get_issue_count('heading_numbering_issues', heading_data)\n            if heading_issues > 0:\n                heading_file_counts = extract_files_with_issue_counts(heading_data)\n                if heading_data and isinstance(heading_data, dict):\n                    if 'summary' in heading_data:\n                        file_count = heading_data['summary'].get('files_affected', 0)\n                    else:\n                        file_count = heading_data.get('file_count', 0)\n                else:\n                    file_count = len(heading_file_counts)\n                if file_count == 0:\n                    file_count = len(heading_file_counts)\n                lines.append(\"**Heading Numbering** FAIL\")\n                lines.append(f\"  - {file_count} files with {heading_issues} total issues\")\n                if heading_file_counts:\n                    sorted_files = sorted(heading_file_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n                    file_list = [f\"{file_path} ({count})\" for file_path, count in sorted_files]\n                    if len(heading_file_counts) > 5:\n                        file_list.append(f\"... +{len(heading_file_counts) - 5}\")\n                    lines.append(f\"  - Top files: {', '.join(file_list)}\")\n            else:\n                lines.append(\"**Heading Numbering** CLEAN\")\n                lines.append(\"  - 0 files with issues\")\n            \n            # Missing Addresses\n            missing_address_issues = get_issue_count('missing_address_issues', missing_addresses_data)\n            if missing_address_issues > 0:\n                missing_file_counts = extract_files_with_issue_counts(missing_addresses_data)\n                if missing_addresses_data and isinstance(missing_addresses_data, dict):\n                    if 'summary' in missing_addresses_data:\n                        file_count = missing_addresses_data['summary'].get('files_affected', 0)\n                    else:\n                        file_count = missing_addresses_data.get('file_count', 0)\n                else:\n                    file_count = len(missing_file_counts)\n                # If file_count is still 0 but we have issues, try to get from file_counts or set to at least 1\n                if file_count == 0:\n                    file_count = len(missing_file_counts) if missing_file_counts else (1 if missing_address_issues > 0 else 0)\n                lines.append(\"**Missing Addresses** FAIL\")\n                lines.append(f\"  - {file_count} files with {missing_address_issues} total documentation missing addresses\")\n                if missing_file_counts:\n                    sorted_files = sorted(missing_file_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n                    file_list = [f\"{file_path} ({count})\" for file_path, count in sorted_files]\n                    if len(missing_file_counts) > 5:\n                        file_list.append(f\"... +{len(missing_file_counts) - 5}\")\n                    lines.append(f\"  - Top files: {', '.join(file_list)}\")\n            else:\n                lines.append(\"**Missing Addresses** CLEAN\")\n                lines.append(\"  - 0 files with missing addresses\")\n            \n            # Unconverted Links\n            unconverted_link_issues = get_issue_count('unconverted_link_issues', unconverted_links_data)\n            if unconverted_link_issues > 0:\n                unconverted_file_counts = extract_files_with_issue_counts(unconverted_links_data)\n                if unconverted_links_data and isinstance(unconverted_links_data, dict):\n                    if 'summary' in unconverted_links_data:\n                        file_count = unconverted_links_data['summary'].get('files_affected', 0)\n                    else:\n                        file_count = unconverted_links_data.get('file_count', 0)\n                else:\n                    file_count = len(unconverted_file_counts)\n                if file_count == 0:\n                    file_count = len(unconverted_file_counts)\n                lines.append(\"**Unconverted Links** FAIL\")\n                lines.append(f\"  - {file_count} files with {unconverted_link_issues} total issues\")\n                if unconverted_file_counts:\n                    sorted_files = sorted(unconverted_file_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n                    file_list = [f\"{file_path} ({count})\" for file_path, count in sorted_files]\n                    if len(unconverted_file_counts) > 5:\n                        file_list.append(f\"... +{len(unconverted_file_counts) - 5}\")\n                    lines.append(f\"  - Top files: {', '.join(file_list)}\")\n            else:\n                lines.append(\"**Unconverted Links** CLEAN\")\n                lines.append(\"  - 0 files with unconverted links\")\n            \n            # Dependency Documentation\n            dependency_summary = getattr(self, 'module_dependency_summary', None) or (hasattr(self, 'results_cache') and self.results_cache.get('analyze_module_dependencies'))\n            if not dependency_summary:\n                dependency_data = self._load_tool_data('analyze_module_dependencies', 'imports')\n                if dependency_data and isinstance(dependency_data, dict):\n                    dependency_summary = dependency_data\n            \n            missing_deps = dependency_summary.get('missing_dependencies') if dependency_summary else None\n            if missing_deps:\n                lines.append(\"**Dependency Documentation** FAIL\")\n                missing_files = dependency_summary.get('missing_files') or dependency_summary.get('missing_sections') or []\n                file_count = len(missing_files) if missing_files else missing_deps\n                lines.append(f\"  - {file_count} files with {missing_deps} total undocumented references detected\")\n                if missing_files:\n                    file_list = []\n                    for i, file_path in enumerate(missing_files[:5]):\n                        file_list.append(f\"{file_path} (1)\")\n                    if len(missing_files) > 5:\n                        file_list.append(f\"... +{len(missing_files) - 5}\")\n                    lines.append(f\"  - Top files: {', '.join(file_list)}\")\n            else:\n                lines.append(\"**Dependency Documentation** CLEAN\")\n                lines.append(\"  - 0 undocumented references\")\n            \n            # TODO.md Status\n            todo_sync_result = getattr(self, 'todo_sync_result', None)\n            if todo_sync_result and isinstance(todo_sync_result, dict):\n                entries = todo_sync_result.get('entries', [])\n                completed_entries = todo_sync_result.get('completed_entries', [])\n                \n                if (entries and isinstance(entries, list) and len(entries) > 0) or (completed_entries and len(completed_entries) > 0):\n                    entry_count = len(entries) if entries else len(completed_entries)\n                    lines.append(\"**TODO.md Status** FAIL\")\n                    lines.append(f\"  - {entry_count} completed entries detected\")\n                else:\n                    lines.append(\"**TODO.md Status** CLEAN\")\n                    lines.append(\"  - 0 completed entries detected\")\n            else:\n                lines.append(\"**TODO.md Status** CLEAN\")\n                lines.append(\"  - 0 completed entries detected\")\n            \n            # Function Docstring Coverage\n            # Use function_metrics already loaded at start of function (line 1232)\n            func_metrics_details_for_docstatus = function_metrics.get('details', {}) if isinstance(function_metrics, dict) else {}\n            total_funcs_docstatus = func_metrics_details_for_docstatus.get('total_functions') or function_metrics.get('total_functions') if isinstance(function_metrics, dict) else None\n            undocumented_funcs_docstatus = func_metrics_details_for_docstatus.get('undocumented', 0) or function_metrics.get('undocumented', 0) if isinstance(function_metrics, dict) else 0\n            \n            if total_funcs_docstatus and total_funcs_docstatus > 0:\n                documented_funcs_docstatus = total_funcs_docstatus - undocumented_funcs_docstatus\n                coverage_pct_docstatus = (documented_funcs_docstatus / total_funcs_docstatus) * 100\n                if undocumented_funcs_docstatus > 0:\n                    lines.append(\"**Function Docstring Coverage** FAIL\")\n                    lines.append(f\"  - {undocumented_funcs_docstatus} functions missing docstrings ({coverage_pct_docstatus:.2f}% coverage)\")\n                else:\n                    lines.append(\"**Function Docstring Coverage** CLEAN\")\n                    lines.append(f\"  - 0 functions missing docstrings ({coverage_pct_docstatus:.2f}% coverage)\")\n            else:\n                lines.append(\"**Function Docstring Coverage** UNKNOWN\")\n                lines.append(\"  - Function data not available\")\n        \n        lines.append(\"\")\n        \n        # Documentation Overlap\n        lines.append(\"## Documentation Overlap\")\n        overlap_count = len(section_overlaps) if section_overlaps else 0\n        consolidation_count = len(consolidation_recs) if consolidation_recs else 0\n        \n        if overlap_count > 0 or consolidation_count > 0:\n            if section_overlaps and overlap_count > 0:\n                lines.append(f\"- **Section Overlaps**: {overlap_count} sections duplicated across files\")\n                top_overlaps = sorted(section_overlaps.items(), key=lambda x: len(x[1]), reverse=True)[:3]\n                for section, files in top_overlaps:\n                    lines.append(f\"  - `{section}` appears in: {', '.join(files[:3])}{'...' if len(files) > 3 else ''}\")\n            if consolidation_recs:\n                lines.append(f\"- **Consolidation Opportunities**: {consolidation_count} file groups identified for potential consolidation\")\n                for rec in consolidation_recs[:3]:\n                    category = rec.get('category', 'Unknown')\n                    files = rec.get('files', [])\n                    suggestion = rec.get('suggestion', '')\n                    if files:\n                        lines.append(f\"  - {category}: {len(files)} files ({', '.join(files[:2])}{'...' if len(files) > 2 else ''}) - {suggestion}\")\n        else:\n            if overlap_analysis_ran:\n                lines.append(\"- **Status**: No overlaps detected (analysis performed)\")\n                lines.append(\"  - Overlap analysis ran but found no section overlaps or consolidation opportunities\")\n            else:\n                lines.append(\"- **Status**: Overlap analysis not run (use `audit --full` or `--overlap` flag)\")\n                lines.append(\"  - Standard audits skip overlap analysis by default; run `audit --full` or use `--overlap` flag to include it\")\n        \n        lines.append(\"\")\n        \n        # Error Handling\n        lines.append(\"## Error Handling\")\n        \n        if error_metrics and isinstance(error_metrics, dict):\n            error_details = error_metrics.get('details', {})\n            def get_error_field(field_name, default=None):\n                return error_details.get(field_name, error_metrics.get(field_name, default))\n            \n            if missing_error_handlers is None:\n                missing_error_handlers = 0\n            lines.append(f\"- **Missing Error Handling**: {missing_error_handlers} functions lack protections\")\n            \n            if worst_error_modules:\n                module_summaries = []\n                modules_needing_attention = [\n                    m for m in worst_error_modules[:3] \n                    if m.get('missing', 0) > 0 and m.get('coverage', 100) < 100\n                ]\n                \n                for module in modules_needing_attention:\n                    module_name = module.get('module', 'Unknown')\n                    coverage_pct = percent_text(module.get('coverage'), 1)\n                    missing = module.get('missing')\n                    total = module.get('total')\n                    detail = f\"{module_name} ({coverage_pct}\"\n                    if missing is not None and total is not None:\n                        detail += f\", missing {missing}/{total}\"\n                    detail += \")\"\n                    module_summaries.append(detail)\n                \n                if module_summaries:\n                    lines.append(f\"  - Top candidate modules: {', '.join(module_summaries)}\")\n            \n            decorated = get_error_field('functions_with_decorators')\n            if decorated is not None:\n                lines.append(f\"- **@handle_errors Usage**: {decorated} functions already use the decorator\")\n            \n            # Phase 1: Decorator replacement candidates\n            phase1_total = get_error_field('phase1_total', 0)\n            \n            if phase1_total > 0:\n                phase1_by_priority = get_error_field('phase1_by_priority', {}) or {}\n                if not isinstance(phase1_by_priority, dict):\n                    phase1_by_priority = {}\n                \n                priority_breakdown = []\n                if phase1_by_priority.get('high', 0) > 0:\n                    priority_breakdown.append(f\"{phase1_by_priority['high']} high\")\n                if phase1_by_priority.get('medium', 0) > 0:\n                    priority_breakdown.append(f\"{phase1_by_priority['medium']} medium\")\n                if phase1_by_priority.get('low', 0) > 0:\n                    priority_breakdown.append(f\"{phase1_by_priority['low']} low\")\n                \n                priority_text = ', '.join(priority_breakdown) if priority_breakdown else '0'\n                lines.append(f\"- **Phase 1 Candidates**: {phase1_total} functions with basic try-except blocks need decorator replacement ({priority_text} priority)\")\n                \n                phase1_candidates = get_error_field('phase1_candidates', []) or []\n                if not isinstance(phase1_candidates, list):\n                    phase1_candidates = []\n                if phase1_candidates:\n                    from collections import defaultdict\n                    by_module = defaultdict(int)\n                    \n                    for candidate in phase1_candidates:\n                        if isinstance(candidate, dict):\n                            file_path = candidate.get('file_path', '')\n                            if file_path:\n                                module = Path(file_path).name\n                                by_module[module] += 1\n                    \n                    if by_module:\n                        top_modules = sorted(by_module.items(), key=lambda x: x[1], reverse=True)[:3]\n                        module_list = [f\"{module} ({count})\" for module, count in top_modules]\n                        if len(by_module) > 3:\n                            module_list.append(f\"... +{len(by_module) - 3}\")\n                        lines.append(f\"  - Top candidate modules with function counts: {', '.join(module_list)}\")\n            \n            # Phase 2: Generic exception categorization\n            phase2_total = get_error_field('phase2_total', 0)\n            \n            if phase2_total > 0:\n                phase2_by_type = get_error_field('phase2_by_type', {}) or {}\n                if not isinstance(phase2_by_type, dict):\n                    phase2_by_type = {}\n                \n                type_breakdown = [f\"{count} {exc_type}\" for exc_type, count in sorted(phase2_by_type.items(), key=lambda x: x[1], reverse=True)[:3]]\n                type_text = ', '.join(type_breakdown) if type_breakdown else '0'\n                \n                if len(phase2_by_type) > 3:\n                    type_text += f\", ... +{len(phase2_by_type) - 3} more\"\n                \n                lines.append(f\"- **Phase 2 Exceptions**: {phase2_total} generic exception raises need categorization ({type_text})\")\n                \n                phase2_exceptions = get_error_field('phase2_exceptions', []) or []\n                if not isinstance(phase2_exceptions, list):\n                    phase2_exceptions = []\n                if phase2_exceptions:\n                    from collections import defaultdict\n                    by_module = defaultdict(lambda: defaultdict(int))\n                    \n                    for exc in phase2_exceptions:\n                        if isinstance(exc, dict):\n                            file_path = exc.get('file_path', '')\n                            exc_type = exc.get('exception_type', 'Unknown')\n                            if file_path:\n                                module = Path(file_path).name\n                                by_module[module][exc_type] += 1\n                    \n                    if by_module:\n                        module_list = []\n                        for module, exc_types in sorted(by_module.items(), key=lambda x: sum(x[1].values()), reverse=True)[:3]:\n                            exc_details = [f\"{count} {exc_type}\" for exc_type, count in sorted(exc_types.items(), key=lambda x: x[1], reverse=True)]\n                            module_list.append(f\"{module} ({', '.join(exc_details)})\")\n                        \n                        if len(by_module) > 3:\n                            module_list.append(f\"... +{len(by_module) - 3}\")\n                        lines.append(f\"  - Top candidate modules with function counts and exception types: {', '.join(module_list)}\")\n        else:\n            lines.append(\"- **Error Handling**: Run `python development_tools/run_development_tools.py audit` for detailed metrics\")\n        \n        lines.append(\"\")\n        \n        # Test Coverage\n        lines.append(\"## Test Coverage\")\n        \n        if coverage_summary and isinstance(coverage_summary, dict):\n            overall = coverage_summary.get('overall') or {}\n            lines.append(f\"- **Overall Coverage**: {percent_text(overall.get('coverage'), 1)} ({overall.get('covered')} of {overall.get('statements')} statements)\")\n            \n            # Domains with Lowest Coverage\n            domain_gaps = []\n            for m in (coverage_summary.get('modules') or []):\n                coverage_val = m.get('coverage', 100)\n                if isinstance(coverage_val, str):\n                    coverage_val = to_float(coverage_val) or 100\n                elif not isinstance(coverage_val, (int, float)):\n                    coverage_val = 100\n                if coverage_val < 80:\n                    domain_gaps.append(m)\n            \n            if domain_gaps:\n                domain_descriptions = [\n                    f\"{m['module']} ({percent_text(m.get('coverage'), 1)}, missing {m.get('missed')} lines)\"\n                    for m in domain_gaps[:5]\n                ]\n                lines.append(f\"    - **Domains with Lowest Coverage**: {', '.join(domain_descriptions)}\")\n            \n            worst_files = (coverage_summary or {}).get('worst_files') or []\n            if worst_files:\n                file_descriptions = [\n                    f\"{item['path']} ({percent_text(item.get('coverage'), 1)}, missing {item.get('missing', item.get('missed', 0))} lines)\"\n                    for item in worst_files[:5]\n                ]\n                lines.append(f\"    - **Modules with Lowest Coverage**: {', '.join(file_descriptions)}\")\n                lines.append(f\"    - **Detailed Report**: [TEST_COVERAGE_REPORT.md](development_docs/TEST_COVERAGE_REPORT.md)\")\n            \n            # Development tools coverage\n            dev_tools_insights = self._get_dev_tools_coverage_insights()\n            if dev_tools_insights and dev_tools_insights.get('overall_pct') is not None:\n                dev_pct = dev_tools_insights['overall_pct']\n                dev_statements = dev_tools_insights.get('statements')\n                dev_covered = dev_tools_insights.get('covered')\n                summary_line = f\"- **Development Tools Coverage**: {percent_text(dev_pct, 1)}\"\n                if dev_statements is not None and dev_covered is not None:\n                    summary_line += f\" ({dev_covered} of {dev_statements} statements)\"\n                lines.append(summary_line)\n                low_modules = dev_tools_insights.get('low_modules') or []\n                if low_modules:\n                    dev_descriptions = [\n                        f\"{Path(item['path']).name} ({percent_text(item.get('coverage'), 1)}, missing {item.get('missed')} lines)\"\n                        for item in low_modules[:5]\n                    ]\n                    lines.append(f\"    - **Modules with Lowest Coverage**: {', '.join(dev_descriptions)}\")\n            \n            # Test markers\n            test_markers_data = self._load_tool_data('analyze_test_markers', 'tests')\n            if test_markers_data and isinstance(test_markers_data, dict):\n                if 'summary' in test_markers_data:\n                    summary = test_markers_data.get('summary', {})\n                    missing_count = summary.get('total_issues', 0)\n                    details = test_markers_data.get('details', {})\n                    missing_list = details.get('missing', [])\n                else:\n                    missing_count = test_markers_data.get('missing_count', 0)\n                    missing_list = test_markers_data.get('missing', [])\n                \n                if missing_count > 0 or (missing_list and len(missing_list) > 0):\n                    lines.append(\"## Test Markers\")\n                    actual_count = missing_count if missing_count > 0 else len(missing_list) if missing_list else 0\n                    lines.append(f\"- **Missing Category Markers**: {actual_count} tests missing pytest category markers\")\n                    from collections import defaultdict\n                    files_with_missing = defaultdict(list)\n                    for item in missing_list:\n                        if isinstance(item, dict):\n                            file_path = item.get('file', '')\n                            test_name = item.get('name', '')\n                            if file_path:\n                                files_with_missing[file_path].append(test_name)\n                    \n                    if files_with_missing:\n                        sorted_files = sorted(files_with_missing.items(), key=lambda x: len(x[1]), reverse=True)[:5]\n                        file_list = [f\"{Path(f).name} ({len(tests)} tests)\" for f, tests in sorted_files]\n                        if len(files_with_missing) > 5:\n                            file_list.append(f\"... +{len(files_with_missing) - 5} files\")\n                        lines.append(f\"    - **Top files**: {', '.join(file_list)}\")\n                else:\n                    lines.append(\"## Test Markers\")\n                    lines.append(\"- **Status**: CLEAN (all tests have category markers)\")\n        \n        elif hasattr(self, 'coverage_results') and self.coverage_results:\n            lines.append(\"- Coverage regeneration completed with issues; inspect coverage.json for gap details\")\n        else:\n            lines.append(\"- Run `audit --full` to regenerate coverage metrics\")\n        \n        lines.append(\"\")\n        \n        # Unused Imports\n        lines.append(\"## Unused Imports\")\n        \n        if unused_imports_data:\n            summary = unused_imports_data.get('summary', {})\n            details = unused_imports_data.get('details', {})\n            total_unused = summary.get('total_issues', 0) or unused_imports_data.get('total_unused', 0)\n            files_with_issues = summary.get('files_affected', 0) or unused_imports_data.get('files_with_issues', 0)\n            if total_unused > 0 or files_with_issues > 0:\n                lines.append(f\"- **Total Unused**: {total_unused} imports across {files_with_issues} files\")\n                by_category = details.get('by_category') or unused_imports_data.get('by_category') or {}\n                obvious = by_category.get('obvious_unused')\n                if obvious:\n                    lines.append(f\"    - **Obvious Removals**: {obvious} imports\")\n                type_only = by_category.get('type_hints_only')\n                if type_only:\n                    lines.append(f\"    - **Type-Only Imports**: {type_only} imports\")\n                \n                # Add top files with unused imports\n                from collections import defaultdict\n                import json\n                file_counts = defaultdict(int)\n                \n                # First, try to get file counts from the tool's JSON output (most current)\n                if unused_imports_data and isinstance(unused_imports_data, dict):\n                    files_dict = unused_imports_data.get('files', {})\n                    if isinstance(files_dict, dict):\n                        for file_path_str, count in files_dict.items():\n                            if isinstance(file_path_str, str) and isinstance(count, (int, float)):\n                                # CRITICAL: Filter out files that no longer exist\n                                try:\n                                    file_path = self.project_root / file_path_str\n                                    if file_path.exists():\n                                        file_counts[file_path_str] = int(count)\n                                    # If file doesn't exist, skip it\n                                except (OSError, ValueError):\n                                    # Error checking file, skip it\n                                    pass\n                \n                # Fallback to cache file if no data from JSON output\n                if not file_counts:\n                    try:\n                        from ..output_storage import load_tool_cache\n                        cache_data = load_tool_cache('analyze_unused_imports', 'imports', project_root=self.project_root)\n                        if cache_data:\n                            # CRITICAL: Filter out entries for files that no longer exist\n                            for file_path_str, file_data in cache_data.items():\n                                if isinstance(file_data, dict):\n                                    # Check if file still exists\n                                    try:\n                                        file_path = self.project_root / file_path_str\n                                        if file_path.exists():\n                                            # Verify mtime matches (file hasn't been modified)\n                                            cached_mtime = file_data.get('mtime')\n                                            if cached_mtime is not None:\n                                                current_mtime = file_path.stat().st_mtime\n                                                if current_mtime == cached_mtime:\n                                                    results = file_data.get('results', [])\n                                                    if isinstance(results, list) and len(results) > 0:\n                                                        file_counts[file_path_str] = len(results)\n                                            else:\n                                                # No mtime in cache, include it\n                                                results = file_data.get('results', [])\n                                                if isinstance(results, list) and len(results) > 0:\n                                                    file_counts[file_path_str] = len(results)\n                                        # If file doesn't exist, skip it (don't count deleted files)\n                                    except (OSError, ValueError):\n                                        # File doesn't exist or error checking, skip it\n                                        pass\n                    except Exception:\n                        pass\n                \n                if file_counts:\n                    sorted_files = sorted(file_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n                    file_list = [f\"{Path(f).name} ({count})\" for f, count in sorted_files]\n                    if unused_imports_data and isinstance(unused_imports_data, dict):\n                        summary = unused_imports_data.get('summary', {})\n                        total_files_with_issues = summary.get('files_affected', 0) or unused_imports_data.get('files_with_issues', len(file_counts))\n                    else:\n                        total_files_with_issues = len(file_counts)\n                    if total_files_with_issues > 5:\n                        file_list.append(f\"... +{total_files_with_issues - 5}\")\n                    lines.append(f\"    - **Top files**: {', '.join(file_list)}\")\n                \n                report_path = self.project_root / 'development_docs' / 'UNUSED_IMPORTS_REPORT.md'\n                if isinstance(report_path, Path) and report_path.exists():\n                    rel_path = report_path.relative_to(self.project_root)\n                    lines.append(f\"- **Detailed Report**: [UNUSED_IMPORTS_REPORT.md]({rel_path.as_posix()})\")\n            else:\n                lines.append(\"- **Unused Imports**: CLEAN (no unused imports detected)\")\n        else:\n            lines.append(\"- **Unused Imports**: Data unavailable (run `audit --full` for latest scan)\")\n        \n        lines.append(\"\")\n        \n        # Legacy References\n        lines.append(\"## Legacy References\")\n        \n        if not legacy_summary and legacy_data:\n            if isinstance(legacy_data, dict):\n                findings = legacy_data.get('findings', {})\n                if findings:\n                    total_files = sum(len(file_list) for file_list in findings.values())\n                    total_markers = 0\n                    for pattern_type, file_list in findings.items():\n                        for file_entry in file_list:\n                            if len(file_entry) >= 3:\n                                matches = file_entry[2]\n                                if isinstance(matches, list):\n                                    total_markers += len(matches)\n                    \n                    legacy_issues = legacy_data.get('files_with_issues') or total_files\n                    legacy_markers = legacy_data.get('legacy_markers') or total_markers\n                else:\n                    legacy_issues = legacy_data.get('files_with_issues') or 0\n                    legacy_markers = legacy_data.get('legacy_markers') or 0\n                \n                report_path = legacy_data.get('report_path') or 'development_docs/LEGACY_REFERENCE_REPORT.md'\n                if legacy_issues is not None:\n                    legacy_summary = {\n                        'files_with_issues': legacy_issues,\n                        'legacy_markers': legacy_markers,\n                        'report_path': report_path,\n                        'findings': findings\n                    }\n        \n        if legacy_summary:\n            if 'summary' in legacy_summary and isinstance(legacy_summary.get('summary'), dict):\n                legacy_issues = legacy_summary['summary'].get('files_affected', 0)\n                details = legacy_summary.get('details', {})\n                legacy_markers = details.get('legacy_markers', 0)\n                findings = details.get('findings', {})\n            else:\n                legacy_issues = legacy_summary.get('files_with_issues')\n                legacy_markers = legacy_summary.get('legacy_markers')\n                findings = legacy_summary.get('findings', {})\n            \n            report_path = details.get('report_path') if 'details' in legacy_summary else legacy_summary.get('report_path')\n            \n            if legacy_issues is not None:\n                lines.append(f\"- **Files with Legacy Markers**: {legacy_issues}\")\n            else:\n                lines.append(\"- **Files with Legacy Markers**: Unknown\")\n            \n            if legacy_markers is not None:\n                lines.append(f\"- **Markers Found**: {legacy_markers}\")\n            else:\n                lines.append(\"- **Markers Found**: Unknown\")\n            \n            if findings and isinstance(findings, dict):\n                from collections import defaultdict\n                file_counts = defaultdict(int)\n                for pattern_type, file_list in findings.items():\n                    for file_entry in file_list:\n                        if isinstance(file_entry, list) and len(file_entry) >= 3:\n                            file_path = file_entry[0]\n                            matches = file_entry[2]\n                            if isinstance(matches, list):\n                                file_counts[file_path] += len(matches)\n                            else:\n                                file_counts[file_path] += 1\n                \n                if file_counts:\n                    sorted_files = sorted(file_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n                    file_list = [f\"{Path(f).name} ({count})\" for f, count in sorted_files]\n                    if len(file_counts) > 5:\n                        file_list.append(f\"... +{len(file_counts) - 5}\")\n                    lines.append(f\"    - **Top files**: {', '.join(file_list)}\")\n            \n            if report_path:\n                report_path_obj = self._resolve_report_path(report_path)\n                if report_path_obj.exists():\n                    rel_path = report_path_obj.relative_to(self.project_root)\n                    lines.append(f\"- **Detailed Report**: [LEGACY_REFERENCE_REPORT.md]({rel_path.as_posix()})\")\n        else:\n            lines.append(\"- **Files with Legacy Markers**: Data unavailable (run `audit --full` for latest scan)\")\n            lines.append(\"- **Markers Found**: Data unavailable\")\n            legacy_report = self.project_root / 'development_docs' / 'LEGACY_REFERENCE_REPORT.md'\n            if legacy_report.exists():\n                rel_path = legacy_report.relative_to(self.project_root)\n                lines.append(f\"- **Detailed Report**: [LEGACY_REFERENCE_REPORT.md]({rel_path.as_posix()})\")\n        \n        lines.append(\"\")\n        \n        # Complexity & Refactoring\n        lines.append(\"## Complexity & Refactoring\")\n        \n        function_metrics_details = function_metrics.get('details', {}) if isinstance(function_metrics, dict) else {}\n        def get_function_field(field_name, default=None):\n            return function_metrics_details.get(field_name, function_metrics.get(field_name, default) if isinstance(function_metrics, dict) else default)\n        \n        # Try to load complexity from decision_support if analyze_functions doesn't have it\n        decision_metrics = getattr(self, 'results_cache', {}).get('decision_support_metrics', {}) or {}\n        # If not in cache, try loading from decision_support tool data\n        if not decision_metrics:\n            decision_data = self._load_tool_data('decision_support', 'functions', log_source=False)\n            if decision_data and isinstance(decision_data, dict):\n                decision_details = decision_data.get('details', {})\n                decision_metrics = decision_details.get('decision_support_metrics', {}) or decision_data.get('decision_support_metrics', {})\n        \n        high_complexity = get_function_field('high_complexity')\n        if high_complexity == 'Unknown' or high_complexity is None:\n            if decision_metrics:\n                high_complexity = decision_metrics.get('high_complexity', 'Unknown')\n                critical_complexity = decision_metrics.get('critical_complexity', 'Unknown')\n                moderate_complexity = decision_metrics.get('moderate_complexity', 'Unknown')\n                function_metrics_details['high_complexity'] = high_complexity\n                function_metrics_details['critical_complexity'] = critical_complexity\n                function_metrics_details['moderate_complexity'] = moderate_complexity\n        \n        # If still unknown, try loading from cache\n        if high_complexity == 'Unknown' or high_complexity is None:\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data and 'results' in cached_data and 'analyze_functions' in cached_data['results']:\n                    func_data = cached_data['results']['analyze_functions']\n                    if 'data' in func_data:\n                        cached_metrics = func_data['data']\n                        function_metrics_details['high_complexity'] = cached_metrics.get('high_complexity', 'Unknown')\n                        function_metrics_details['critical_complexity'] = cached_metrics.get('critical_complexity', 'Unknown')\n                        function_metrics_details['moderate_complexity'] = cached_metrics.get('moderate_complexity', 'Unknown')\n                        if 'critical_complexity_examples' in cached_metrics:\n                            function_metrics_details['critical_complexity_examples'] = cached_metrics.get('critical_complexity_examples', [])\n                        if 'high_complexity_examples' in cached_metrics:\n                            function_metrics_details['high_complexity_examples'] = cached_metrics.get('high_complexity_examples', [])\n                if cached_data and function_metrics.get('high_complexity') == 'Unknown' and 'results' in cached_data and 'decision_support' in cached_data['results']:\n                        ds_data = cached_data['results']['decision_support']\n                        if 'data' in ds_data and 'decision_support_metrics' in ds_data['data']:\n                            ds_metrics = ds_data['data']['decision_support_metrics']\n                            function_metrics_details['high_complexity'] = ds_metrics.get('high_complexity', 'Unknown')\n                            function_metrics_details['critical_complexity'] = ds_metrics.get('critical_complexity', 'Unknown')\n                            function_metrics_details['moderate_complexity'] = ds_metrics.get('moderate_complexity', 'Unknown')\n                            if 'critical_complexity_examples' in ds_metrics:\n                                function_metrics_details['critical_complexity_examples'] = ds_metrics.get('critical_complexity_examples', [])\n                            if 'high_complexity_examples' in ds_metrics:\n                                function_metrics_details['high_complexity_examples'] = ds_metrics.get('high_complexity_examples', [])\n            except Exception:\n                pass\n        \n        lines.append(f\"- **Critical Complexity Functions**: {get_function_field('critical_complexity', 'Unknown')}\")\n        lines.append(f\"- **High Complexity Functions**: {get_function_field('high_complexity', 'Unknown')}\")\n        lines.append(f\"- **Moderate Complexity Functions**: {get_function_field('moderate_complexity', 'Unknown')}\")\n        \n        # Add highest complexity functions list\n        critical_examples = get_function_field('critical_complexity_examples', [])\n        high_examples = get_function_field('high_complexity_examples', [])\n        \n        # Try loading from decision_support_metrics (may be cached)\n        # decision_metrics should already be loaded above, but ensure we have examples\n        if decision_metrics:\n            if not critical_examples and 'critical_complexity_examples' in decision_metrics:\n                critical_examples = decision_metrics.get('critical_complexity_examples', [])\n            if not high_examples and 'high_complexity_examples' in decision_metrics:\n                high_examples = decision_metrics.get('high_complexity_examples', [])\n        \n        # If still missing examples, try loading from decision_support tool data (may be cached)\n        if not critical_examples or not high_examples:\n            decision_data = self._load_tool_data('decision_support', 'functions', log_source=False)\n            if decision_data and isinstance(decision_data, dict):\n                decision_details = decision_data.get('details', {})\n                decision_metrics_from_tool = decision_details.get('decision_support_metrics', {}) or decision_data.get('decision_support_metrics', {})\n                if decision_metrics_from_tool:\n                    decision_metrics = decision_metrics_from_tool\n                    if not critical_examples and 'critical_complexity_examples' in decision_metrics_from_tool:\n                        critical_examples = decision_metrics_from_tool.get('critical_complexity_examples', [])\n                    if not high_examples and 'high_complexity_examples' in decision_metrics_from_tool:\n                        high_examples = decision_metrics_from_tool.get('high_complexity_examples', [])\n        \n        if not critical_examples and not high_examples:\n            func_result = self._load_tool_data('analyze_functions', 'functions', log_source=False)\n            if func_result and isinstance(func_result, dict):\n                func_details = func_result.get('details', {})\n                if 'critical_complexity_examples' in func_details:\n                    critical_examples = func_details.get('critical_complexity_examples', [])\n                elif 'critical_complexity_examples' in func_result:\n                    critical_examples = func_result.get('critical_complexity_examples', [])\n                if 'high_complexity_examples' in func_details:\n                    high_examples = func_details.get('high_complexity_examples', [])\n                elif 'high_complexity_examples' in func_result:\n                    high_examples = func_result.get('high_complexity_examples', [])\n            # Also try loading from decision_support if available\n            if (not critical_examples or not high_examples):\n                decision_data = self._load_tool_data('decision_support', 'functions', log_source=False)\n                if decision_data and isinstance(decision_data, dict):\n                    decision_details = decision_data.get('details', {})\n                    decision_metrics_from_tool = decision_details.get('decision_support_metrics', {}) or decision_data.get('decision_support_metrics', {})\n                    if decision_metrics_from_tool:\n                        if not critical_examples and 'critical_complexity_examples' in decision_metrics_from_tool:\n                            critical_examples = decision_metrics_from_tool.get('critical_complexity_examples', [])\n                        if not high_examples and 'high_complexity_examples' in decision_metrics_from_tool:\n                            high_examples = decision_metrics_from_tool.get('high_complexity_examples', [])\n        \n        all_examples = []\n        for item in critical_examples[:5]:\n            if isinstance(item, dict):\n                complexity = item.get('complexity', item.get('nodes', 0))\n                all_examples.append({\n                    'function': item.get('function', 'Unknown'),\n                    'file': item.get('file', 'Unknown'),\n                    'complexity': complexity,\n                    'priority': 'critical'\n                })\n        \n        for item in high_examples[:3]:\n            if isinstance(item, dict):\n                complexity = item.get('complexity', item.get('nodes', 0))\n                all_examples.append({\n                    'function': item.get('function', 'Unknown'),\n                    'file': item.get('file', 'Unknown'),\n                    'complexity': complexity,\n                    'priority': 'high'\n                })\n        \n        if all_examples:\n            all_examples.sort(key=lambda x: x.get('complexity', 0), reverse=True)\n            top_functions = all_examples[:5]\n            function_list = []\n            for func_info in top_functions:\n                func_name = func_info.get('function', 'Unknown')\n                file_name = Path(func_info.get('file', 'Unknown')).name\n                complexity = func_info.get('complexity', 0)\n                if complexity > 0:\n                    function_list.append(f\"{func_name} ({file_name}, {complexity} nodes)\")\n                else:\n                    function_list.append(f\"{func_name} ({file_name})\")\n            \n            if function_list:\n                if len(all_examples) > 5:\n                    function_list.append(f\"... +{len(all_examples) - 5} more\")\n                lines.append(f\"- **Top functions**: {', '.join(function_list)}\")\n        else:\n            # If no examples found, try one more time to load from analyze_functions directly\n            func_result = self._load_tool_data('analyze_functions', 'functions', log_source=False)\n            if func_result and isinstance(func_result, dict):\n                func_details = func_result.get('details', {})\n                # Try to extract from function analysis results\n                if 'functions' in func_details:\n                    functions_list = func_details.get('functions', [])\n                    if isinstance(functions_list, list):\n                        # Find highest complexity functions\n                        complex_funcs = []\n                        for func in functions_list:\n                            if isinstance(func, dict):\n                                complexity = func.get('complexity', func.get('nodes', 0))\n                                if complexity and complexity > 199:  # Critical complexity\n                                    complex_funcs.append({\n                                        'function': func.get('name', 'Unknown'),\n                                        'file': func.get('file', 'Unknown'),\n                                        'complexity': complexity\n                                    })\n                        if complex_funcs:\n                            complex_funcs.sort(key=lambda x: x.get('complexity', 0), reverse=True)\n                            top_funcs = complex_funcs[:5]\n                            function_list = []\n                            for func_info in top_funcs:\n                                func_name = func_info.get('function', 'Unknown')\n                                file_name = Path(func_info.get('file', 'Unknown')).name\n                                complexity = func_info.get('complexity', 0)\n                                function_list.append(f\"{func_name} ({file_name}, {complexity} nodes)\")\n                            if function_list:\n                                lines.append(f\"- **Top functions**: {', '.join(function_list)}\")\n        \n        lines.append(\"\")\n        \n        # Function Patterns\n        lines.append(\"## Function Patterns\")\n        \n        # Registry Gaps\n        if missing_docs_count > 0:\n            lines.append(f\"- **Registry Gaps**: {missing_docs_count} items missing from registry\")\n            if missing_docs_list and isinstance(missing_docs_list, dict):\n                sorted_files = sorted(\n                    missing_docs_list.items(), \n                    key=lambda x: len(x[1]) if isinstance(x[1], list) else 1, \n                    reverse=True\n                )[:5]\n                if sorted_files:\n                    item_list = []\n                    for file_path, funcs in sorted_files:\n                        func_count = len(funcs) if isinstance(funcs, list) else 1\n                        file_name = Path(file_path).name if file_path else 'Unknown'\n                        if func_count == 1 and isinstance(funcs, list) and funcs:\n                            func_name = funcs[0] if funcs else 'Unknown'\n                            item_list.append(f\"{func_name} ({file_name})\")\n                        else:\n                            item_list.append(f\"{file_name} ({func_count} functions)\")\n                    if len(sorted_files) < len(missing_docs_list):\n                        total_files = len(missing_docs_list)\n                        item_list.append(f\"... +{total_files - len(sorted_files)} more\")\n                    if item_list:\n                        lines.append(f\"    - **Top items**: {', '.join(item_list)}\")\n        else:\n            lines.append(f\"- **Registry Gaps**: 0 items missing from registry\")\n        \n        # Function Docstring Coverage\n        if not func_undocumented and total_funcs and doc_coverage:\n            doc_coverage_float = float(doc_coverage.replace('%', '')) if isinstance(doc_coverage, str) else doc_coverage\n            if doc_coverage_float < 100 and total_funcs:\n                func_undocumented = int(total_funcs * (100 - doc_coverage_float) / 100)\n        \n        lines.append(f\"- **Function Docstring Coverage**: {percent_text(doc_coverage, 2)}\")\n        if func_undocumented and func_undocumented > 0:\n            lines.append(f\"   - **Functions Missing Docstrings**: {func_undocumented} functions need docstrings\")\n            undocumented_examples = get_function_field('undocumented_examples', [])\n            if not undocumented_examples or len(undocumented_examples) == 0:\n                function_data_for_consolidated = self._load_tool_data('analyze_functions', 'functions')\n                if isinstance(function_data_for_consolidated, dict):\n                    func_details_for_consolidated = function_data_for_consolidated.get('details', {})\n                    undocumented_examples = func_details_for_consolidated.get('undocumented_examples') or function_data_for_consolidated.get('undocumented_examples') or []\n            \n            if undocumented_examples and isinstance(undocumented_examples, list) and len(undocumented_examples) > 0:\n                sorted_undoc = sorted(\n                    undocumented_examples,\n                    key=lambda x: x.get('complexity', 0) if isinstance(x, dict) else 0,\n                    reverse=True\n                )[:5]\n                func_list = []\n                for item in sorted_undoc:\n                    if isinstance(item, dict):\n                        func_name = item.get('function', item.get('name', 'unknown'))\n                        file_path = item.get('file', '')\n                        if file_path:\n                            file_name = Path(file_path).name\n                            func_list.append(f\"{func_name} ({file_name})\")\n                        else:\n                            func_list.append(func_name)\n                    else:\n                        func_list.append(str(item))\n                total_undoc_count = func_undocumented if func_undocumented else len(undocumented_examples)\n                if total_undoc_count > 5:\n                    func_list.append(f\"... +{total_undoc_count - 5} more\")\n                if func_list:\n                    lines.append(f\"   - **Top functions**: {', '.join(func_list)}\")\n        else:\n            lines.append(f\"   - **Functions Missing Docstrings**: 0 functions need docstrings\")\n        \n        # Handler Classes Docstring Coverage\n        if function_patterns_data_for_report and isinstance(function_patterns_data_for_report, dict):\n            if 'details' in function_patterns_data_for_report:\n                handlers = function_patterns_data_for_report['details'].get('handlers', [])\n            else:\n                handlers = function_patterns_data_for_report.get('handlers', [])\n            \n            if handlers:\n                handlers_no_doc = [h for h in handlers if not h.get('has_doc', True)]\n                handler_classes_total = len(handlers)\n                handler_classes_no_doc = len(handlers_no_doc)\n                \n                if handler_classes_total > 0:\n                    handler_coverage_pct = ((handler_classes_total - handler_classes_no_doc) / handler_classes_total) * 100\n                    lines.append(f\"- **Handler Classes Docstring Coverage**: {percent_text(handler_coverage_pct, 0)}\")\n                    \n                    if handlers_no_doc:\n                        lines.append(f\"   - **Handler Classes Without Class Docstrings**: {handler_classes_no_doc} of {handler_classes_total} handler classes\")\n                        top_handlers = sorted(handlers_no_doc, key=lambda x: x.get('methods', 0), reverse=True)[:5]\n                        handler_list = [f\"{h.get('class', 'Unknown')} ({Path(h.get('file', '')).name}, {h.get('methods', 0)} methods)\" for h in top_handlers]\n                        if len(handlers_no_doc) > 5:\n                            handler_list.append(f\"... +{len(handlers_no_doc) - 5} more\")\n                        lines.append(f\"    - **Top handlers**: {', '.join(handler_list)}\")\n                    else:\n                        lines.append(f\"   - **Handler Classes Without Class Docstrings**: 0 of {handler_classes_total} handler classes (all have class docstrings)\")\n        \n        lines.append(\"\")\n        \n        # Module Imports\n        module_imports_data = self._load_tool_data('analyze_module_imports', 'imports')\n        if module_imports_data and isinstance(module_imports_data, dict):\n            details = module_imports_data.get('details', {})\n            import_data = details.get('data', details) if 'data' in details else details\n            if not import_data or (isinstance(import_data, dict) and len(import_data) == 0):\n                import_data = module_imports_data.get('data', module_imports_data)\n            \n            if isinstance(import_data, dict):\n                total_files = len(import_data)\n                total_imports = sum(\n                    v.get('total_imports', 0) if isinstance(v, dict) else 0\n                    for v in import_data.values()\n                )\n            else:\n                total_files = 0\n                total_imports = 0\n            \n            if total_files > 0:\n                lines.append(\"## Module Imports\")\n                lines.append(f\"- **Files Analyzed**: {total_files} Python files\")\n                if total_imports > 0:\n                    lines.append(f\"- **Total Imports**: {total_imports} import statements\")\n        \n        lines.append(\"\")\n        \n        # Dependency Patterns\n        dependency_patterns_data = self._load_tool_data('analyze_dependency_patterns', 'imports')\n        if dependency_patterns_data and isinstance(dependency_patterns_data, dict):\n            details = dependency_patterns_data.get('details', {})\n            patterns_data = details.get('data', details) if 'data' in details else details\n            if not patterns_data or (isinstance(patterns_data, dict) and len(patterns_data) == 0):\n                patterns_data = dependency_patterns_data.get('data', dependency_patterns_data)\n            \n            if isinstance(patterns_data, dict):\n                circular_deps = len(patterns_data.get('circular_dependencies', []))\n                high_coupling = len(patterns_data.get('high_coupling', []))\n            else:\n                circular_deps = 0\n                high_coupling = 0\n            \n            if circular_deps > 0 or high_coupling > 0:\n                lines.append(\"## Dependency Patterns\")\n                if circular_deps > 0:\n                    lines.append(f\"- **Circular Dependencies**: {circular_deps} circular dependency chains detected\")\n                if high_coupling > 0:\n                    lines.append(f\"- **High Coupling**: {high_coupling} modules with high coupling\")\n            elif circular_deps == 0 and high_coupling == 0 and patterns_data:\n                lines.append(\"## Dependency Patterns\")\n                lines.append(\"- **Status**: CLEAN (no circular dependencies or high coupling detected)\")\n        \n        lines.append(\"\")\n        \n        # Validation Status\n        lines.append(\"## Validation Status\")\n        \n        validation_output_for_report = \"\"\n        if hasattr(self, 'validation_results') and self.validation_results:\n            validation_output_for_report = self.validation_results.get('output', '') or \"\"\n        \n        if not validation_output_for_report or not validation_output_for_report.strip():\n            validation_data = self._load_tool_data('analyze_ai_work', 'ai_work', log_source=True)\n            if validation_data:\n                validation_output_for_report = validation_data.get('output', '') or ''\n        \n        if validation_output_for_report and validation_output_for_report.strip():\n            if 'POOR' in validation_output_for_report:\n                lines.append(\"- **AI Work Validation**: POOR - documentation or tests missing\")\n            elif 'GOOD' in validation_output_for_report:\n                lines.append(\"- **AI Work Validation**: GOOD - keep current standards\")\n            elif 'NEEDS ATTENTION' in validation_output_for_report or 'FAIR' in validation_output_for_report:\n                lines.append(\"- **AI Work Validation**: NEEDS ATTENTION - structural validation issues detected\")\n            else:\n                lines.append(\"- **AI Work Validation**: Status available (see validation output)\")\n        else:\n            # Check if we're in a tier that doesn't run analyze_ai_work (Tier 1)\n            tools_run = getattr(self, '_tools_run_in_current_tier', set())\n            if 'analyze_ai_work' not in tools_run:\n                # Try to load cached validation data\n                validation_data = self._load_tool_data('analyze_ai_work', 'ai_work', log_source=False)\n                if validation_data:\n                    cached_output = validation_data.get('output', '') or ''\n                    if cached_output and cached_output.strip():\n                        if 'POOR' in cached_output:\n                            lines.append(\"- **AI Work Validation**: POOR - documentation or tests missing (cached)\")\n                        elif 'GOOD' in cached_output:\n                            lines.append(\"- **AI Work Validation**: GOOD - keep current standards (cached)\")\n                        elif 'NEEDS ATTENTION' in cached_output or 'FAIR' in cached_output:\n                            lines.append(\"- **AI Work Validation**: NEEDS ATTENTION - structural validation issues detected (cached)\")\n                        else:\n                            lines.append(\"- **AI Work Validation**: Status available (cached, see validation output)\")\n                    else:\n                        lines.append(\"- **AI Work Validation**: Using cached data (run `audit` or `audit --full` for latest validation)\")\n                else:\n                    lines.append(\"- **AI Work Validation**: Using cached data (run `audit` or `audit --full` for latest validation)\")\n            else:\n                lines.append(\"- **AI Work Validation**: Data unavailable (run `audit --full` for validation)\")\n        \n        # Config Validation\n        config_validation_summary = self._load_config_validation_summary()\n        if config_validation_summary:\n            config_valid = config_validation_summary.get('config_valid', False)\n            config_complete = config_validation_summary.get('config_complete', False)\n            recommendations = config_validation_summary.get('recommendations', [])\n            tools_using_config = config_validation_summary.get('tools_using_config', 0)\n            total_tools = config_validation_summary.get('total_tools', 0)\n            tools_analysis = config_validation_summary.get('tools_analysis', {})\n            \n            lines.append(\"- **Configuration Validation**\")\n            lines.append(f\"  - Config valid: {'Yes' if config_valid else 'No'}\")\n            lines.append(f\"  - Config complete: {'Yes' if config_complete else 'No'}\")\n            if total_tools > 0:\n                lines.append(f\"  - Tools using config: {tools_using_config}/{total_tools}\")\n            \n            missing_import_tools = []\n            if tools_analysis and isinstance(tools_analysis, dict):\n                for tool_name, tool_data in tools_analysis.items():\n                    if isinstance(tool_data, dict):\n                        if not tool_data.get('imports_config', True):\n                            missing_import_tools.append(tool_name)\n                        else:\n                            issues = tool_data.get('issues', [])\n                            if isinstance(issues, list):\n                                for issue in issues:\n                                    if isinstance(issue, str) and 'import config' in issue.lower():\n                                        if tool_name not in missing_import_tools:\n                                            missing_import_tools.append(tool_name)\n                                        break\n            \n            if missing_import_tools:\n                lines.append(f\"  - Tools missing config module import: {', '.join(missing_import_tools)}\")\n        \n        lines.append(\"\")\n        \n        # System Signals\n        lines.append(\"## System Signals\")\n        \n        system_signals_data = None\n        if hasattr(self, 'system_signals') and self.system_signals:\n            system_signals_data = self.system_signals\n        else:\n            try:\n                cached_data = self._load_results_file_safe()\n                if cached_data:\n                    signals_result = None\n                    if 'results' in cached_data:\n                        if 'analyze_system_signals' in cached_data['results']:\n                            signals_result = cached_data['results']['analyze_system_signals']\n                    \n                    if signals_result:\n                        if 'data' in signals_result:\n                            system_signals_data = signals_result['data']\n            except Exception as e:\n                logger.debug(f\"Failed to load system signals from cache: {e}\")\n        \n        # Extract system health from system_signals_data\n        system_health = None\n        if system_signals_data and isinstance(system_signals_data, dict):\n            system_health = system_signals_data.get('system_health', {})\n        elif hasattr(self, 'system_signals') and self.system_signals:\n            if isinstance(self.system_signals, dict):\n                system_health = self.system_signals.get('system_health', {})\n        \n        if system_health and isinstance(system_health, dict):\n            overall_status = system_health.get('overall_status', 'OK')\n            lines.append(f\"- **System Health**: {overall_status}\")\n            \n            # Add audit freshness (consolidated - single line, not redundant)\n            audit_freshness = system_health.get('audit_freshness')\n            if audit_freshness:\n                lines.append(f\"  - Audit data: {audit_freshness}\")\n            \n            # Add test coverage status if available\n            test_coverage_status = system_health.get('test_coverage_status')\n            if test_coverage_status and test_coverage_status != 'Unknown':\n                lines.append(f\"  - Test coverage: {test_coverage_status}\")\n            \n            # Documentation sync status is shown in Documentation Signals section, not here\n            \n            # Show actual warnings/critical issues with details (not just counts)\n            severity_levels = system_health.get('severity_levels', {})\n            if severity_levels:\n                critical_issues = severity_levels.get('CRITICAL', [])\n                warnings = severity_levels.get('WARNING', [])\n                if critical_issues:\n                    lines.append(f\"  - Critical issues ({len(critical_issues)}):\")\n                    for issue in critical_issues:\n                        lines.append(f\"    * {issue}\")\n                if warnings:\n                    lines.append(f\"  - Warnings ({len(warnings)}):\")\n                    for warning in warnings:\n                        lines.append(f\"    * {warning}\")\n            \n            # Add recent activity\n            recent_activity = system_signals_data.get('recent_activity', {}) if system_signals_data else (self.system_signals.get('recent_activity', {}) if hasattr(self, 'system_signals') and self.system_signals else {})\n            recent_changes = recent_activity.get('recent_changes') or [] if isinstance(recent_activity, dict) else []\n            if recent_changes and isinstance(recent_changes, list):\n                changes_str = self._format_list_for_display(recent_changes, limit=3)\n                lines.append(f\"- **Recent Changes**: {changes_str}\")\n        else:\n            lines.append(\"- **System Health**: OK (data unavailable)\")\n            lines.append(\"- **Recent Changes**: Data unavailable (run `system-signals` command)\")\n        \n        lines.append(\"\")\n        \n        # Reference Files\n        lines.append(\"## Reference Files\")\n        \n        # Default to generic path relative to project root (no development_tools/ assumption)\n        issues_file_str = getattr(self, 'audit_config', {}).get('issues_file', 'critical_issues.txt')\n        issues_file = self.project_root / issues_file_str if isinstance(issues_file_str, str) else Path(issues_file_str)\n        if issues_file.exists():\n            rel_path = issues_file.relative_to(self.project_root)\n            lines.append(f\"- Critical issues summary: [{rel_path.as_posix()}]({rel_path.as_posix()})\")\n        \n        # Get status file paths from config for links\n        try:\n            from ... import config\n            status_config = config.get_status_config()\n            status_files_config = status_config.get('status_files', {})\n            # Use default from STATUS config if status_files_config is empty (matches default config)\n            if not status_files_config:\n                # Fallback to default STATUS config values for backward compatibility\n                from ...config.config import STATUS\n                status_files_config = STATUS.get('status_files', {})\n            ai_status_path = status_files_config.get('ai_status', 'development_tools/AI_STATUS.md')\n            ai_priorities_path = status_files_config.get('ai_priorities', 'development_tools/AI_PRIORITIES.md')\n            results_file_path = (self.audit_config or {}).get('results_file', 'development_tools/reports/analysis_detailed_results.json')\n        except (ImportError, AttributeError, KeyError):\n            # Fallback to default STATUS config values for backward compatibility\n            try:\n                from ...config.config import STATUS\n                status_files_default = STATUS.get('status_files', {})\n                ai_status_path = status_files_default.get('ai_status', 'development_tools/AI_STATUS.md')\n                ai_priorities_path = status_files_default.get('ai_priorities', 'development_tools/AI_PRIORITIES.md')\n                results_file_path = 'development_tools/reports/analysis_detailed_results.json'\n            except (ImportError, AttributeError):\n                # Last resort fallback\n                ai_status_path = 'development_tools/AI_STATUS.md'\n                ai_priorities_path = 'development_tools/AI_PRIORITIES.md'\n                results_file_path = 'development_tools/reports/analysis_detailed_results.json'\n        \n        lines.append(f\"- Latest AI status: [AI_STATUS.md]({ai_status_path})\")\n        lines.append(f\"- Current AI priorities: [AI_PRIORITIES.md]({ai_priorities_path})\")\n        lines.append(f\"- Detailed JSON results: [analysis_detailed_results.json]({results_file_path})\")\n        \n        legacy_report = self.project_root / 'development_docs' / 'LEGACY_REFERENCE_REPORT.md'\n        if legacy_report.exists():\n            rel_path = legacy_report.relative_to(self.project_root)\n            lines.append(f\"- Legacy reference report: [LEGACY_REFERENCE_REPORT.md]({rel_path.as_posix()})\")\n        \n        coverage_report = self.project_root / 'development_docs' / 'TEST_COVERAGE_REPORT.md'\n        if coverage_report.exists():\n            rel_path = coverage_report.relative_to(self.project_root)\n            lines.append(f\"- Test coverage report: [TEST_COVERAGE_REPORT.md]({rel_path.as_posix()})\")\n        \n        unused_imports_report = self.project_root / 'development_docs' / 'UNUSED_IMPORTS_REPORT.md'\n        if unused_imports_report.exists():\n            rel_path = unused_imports_report.relative_to(self.project_root)\n            lines.append(f\"- Unused imports detail: [UNUSED_IMPORTS_REPORT.md]({rel_path.as_posix()})\")\n        \n        archive_dir = self.project_root / 'development_tools' / 'reports' / 'archive'\n        if archive_dir.exists():\n            lines.append(f\"- Historical audit data: development_tools/reports/archive\")\n        \n        lines.append(\"\")\n        \n        return '\\n'.join(lines)\n    \n    def _identify_critical_issues(self) -> List[str]:\n        \"\"\"Identify critical issues from audit results\"\"\"\n        issues = []\n        if 'analyze_functions' in self.results_cache:\n            metrics = self.results_cache['analyze_functions']\n            if 'coverage' in metrics:\n                try:\n                    coverage = float(metrics['coverage'].replace('%', ''))\n                    if coverage < 90:\n                        issues.append(f\"Low documentation coverage: {coverage}%\")\n                except:\n                    pass\n        if hasattr(self, '_last_failed_audits'):\n            for audit in self._last_failed_audits:\n                issues.append(f\"Failed audit: {audit}\")\n        return issues\n    \n    def _generate_action_items(self) -> List[str]:\n        \"\"\"Generate actionable items from audit results\"\"\"\n        actions = []\n        if 'analyze_functions' in self.results_cache:\n            metrics = self.results_cache['analyze_functions']\n            if 'coverage' in metrics:\n                try:\n                    coverage = float(metrics['coverage'].replace('%', ''))\n                    if coverage < 95:\n                        actions.append(f\"Improve documentation coverage (currently {coverage}%)\")\n                except:\n                    pass\n        if 'decision_support' in self.results_cache:\n            insights = self.results_cache['decision_support']\n            if isinstance(insights, list) and insights:\n                complexity_warnings = [insight for insight in insights if 'complexity' in insight.lower()]\n                if complexity_warnings:\n                    actions.append(\"Refactor high complexity functions for maintainability\")\n        actions.append(\"Review TODO.md for next development priorities\")\n        actions.append(\"Run comprehensive testing before major changes\")\n        actions.append(\"Update AI_CHANGELOG.md and CHANGELOG_DETAIL.md with recent changes\")\n        return actions\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 36,
                    "line_content": "# Default matches development_tools_config.json for backward compatibility",
                    "start": 1261,
                    "end": 1283
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 4256,
                    "line_content": "# Fallback to default STATUS config values for backward compatibility",
                    "start": 251533,
                    "end": 251555
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 4263,
                    "line_content": "# Fallback to default STATUS config values for backward compatibility",
                    "start": 252142,
                    "end": 252164
                  }
                ]
              ],
              [
                "development_tools\\shared\\service\\tool_wrappers.py",
                "\"\"\"\nTool wrapper methods for AIToolsService.\n\nContains methods for running analysis, generation, and fix tools.\n\"\"\"\n\nimport json\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\n\nfrom core.logger import get_component_logger\n\nlogger = get_component_logger(\"development_tools\")\n\n# Import output storage\nfrom ..output_storage import save_tool_result\n\n# Script registry - maps tool names to their file paths\nSCRIPT_REGISTRY = {\n    'analyze_documentation': 'docs/analyze_documentation.py',\n    'analyze_function_registry': 'functions/analyze_function_registry.py',\n    'analyze_module_dependencies': 'imports/analyze_module_dependencies.py',\n    'analyze_config': 'config/analyze_config.py',\n    'decision_support': 'reports/decision_support.py',\n    'analyze_documentation_sync': 'docs/analyze_documentation_sync.py',\n    'analyze_path_drift': 'docs/analyze_path_drift.py',\n    'analyze_missing_addresses': 'docs/analyze_missing_addresses.py',\n    'analyze_ascii_compliance': 'docs/analyze_ascii_compliance.py',\n    'analyze_heading_numbering': 'docs/analyze_heading_numbering.py',\n    'analyze_unconverted_links': 'docs/analyze_unconverted_links.py',\n    'generate_directory_tree': 'docs/generate_directory_tree.py',\n    'analyze_error_handling': 'error_handling/analyze_error_handling.py',\n    'generate_error_handling_report': 'error_handling/generate_error_handling_report.py',\n    'analyze_functions': 'functions/analyze_functions.py',\n    'analyze_function_patterns': 'functions/analyze_function_patterns.py',\n    'analyze_package_exports': 'functions/analyze_package_exports.py',\n    'generate_function_registry': 'functions/generate_function_registry.py',\n    'generate_module_dependencies': 'imports/generate_module_dependencies.py',\n    'analyze_module_imports': 'imports/analyze_module_imports.py',\n    'analyze_dependency_patterns': 'imports/analyze_dependency_patterns.py',\n    'fix_legacy_references': 'legacy/fix_legacy_references.py',\n    'analyze_legacy_references': 'legacy/analyze_legacy_references.py',\n    'generate_legacy_reference_report': 'legacy/generate_legacy_reference_report.py',\n    'quick_status': 'reports/quick_status.py',\n    'run_test_coverage': 'tests/run_test_coverage.py',\n    'analyze_test_coverage': 'tests/analyze_test_coverage.py',\n    'generate_test_coverage_report': 'tests/generate_test_coverage_report.py',\n    'analyze_test_markers': 'tests/analyze_test_markers.py',\n    'analyze_unused_imports': 'imports/analyze_unused_imports.py',\n    'generate_unused_imports_report': 'imports/generate_unused_imports_report.py',\n    'analyze_ai_work': 'ai_work/analyze_ai_work.py',\n    'fix_version_sync': 'docs/fix_version_sync.py',\n    'fix_documentation': 'docs/fix_documentation.py',\n    'analyze_system_signals': 'reports/analyze_system_signals.py',\n    'cleanup_project': 'shared/fix_project_cleanup.py'\n}\n\n\nclass ToolWrappersMixin:\n    \"\"\"Mixin class providing tool execution methods to AIToolsService.\"\"\"\n    \n    def run_script(self, script_name: str, *args, timeout: Optional[int] = 300) -> Dict:\n        \"\"\"Run a registered helper script from development_tools.\"\"\"\n        script_rel_path = SCRIPT_REGISTRY.get(script_name)\n        if not script_rel_path:\n            return {\n                'success': False,\n                'output': '',\n                'error': f\"Script '{script_name}' is not registered\"\n            }\n        script_path = Path(__file__).resolve().parent.parent.parent / script_rel_path\n        if not script_path.exists():\n            return {\n                'success': False,\n                'output': '',\n                'error': f\"Registered script '{script_name}' not found at {script_rel_path}\"\n            }\n        cmd = [sys.executable, str(script_path)] + list(args)\n        try:\n            result = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                cwd=str(self.project_root),\n                timeout=timeout\n            )\n            return {\n                'success': result.returncode == 0,\n                'output': result.stdout,\n                'error': result.stderr,\n                'returncode': result.returncode\n            }\n        except subprocess.TimeoutExpired:\n            return {\n                'success': False,\n                'output': '',\n                'error': f\"Script '{script_name}' timed out after {timeout // 60 if timeout else 'N/A'} minutes\",\n                'returncode': None\n            }\n    \n    def run_analyze_documentation(self, include_overlap: bool = False) -> Dict:\n        \"\"\"Run analyze_documentation with structured JSON handling.\"\"\"\n        logger.info(\"Analyzing documentation...\")\n        args = [\"--json\"]\n        if include_overlap:\n            args.append(\"--overlap\")\n        \n        # Before running, check if we have cached overlap data to preserve\n        cached_overlap_data = None\n        cached_overlap_in_details = False\n        if not include_overlap:\n            try:\n                from ..output_storage import load_tool_result\n                cached_data = load_tool_result('analyze_documentation', 'docs', project_root=self.project_root, normalize=False)\n                if cached_data and isinstance(cached_data, dict):\n                    cached_data_dict = cached_data.get('data', cached_data)\n                    details = cached_data_dict.get('details', {})\n                    has_section_overlaps = 'section_overlaps' in cached_data_dict or 'section_overlaps' in details\n                    has_consolidation = 'consolidation_recommendations' in cached_data_dict or 'consolidation_recommendations' in details\n                    if has_section_overlaps or has_consolidation:\n                        cached_overlap_in_details = ('section_overlaps' in details or 'consolidation_recommendations' in details)\n                        cached_overlap_data = {\n                            'section_overlaps': details.get('section_overlaps') if cached_overlap_in_details else cached_data_dict.get('section_overlaps'),\n                            'consolidation_recommendations': details.get('consolidation_recommendations') if cached_overlap_in_details else cached_data_dict.get('consolidation_recommendations')\n                        }\n            except Exception as e:\n                logger.debug(f\"Failed to load cached overlap data: {e}\")\n        \n        result = self.run_script(\"analyze_documentation\", *args)\n        output = result.get('output', '')\n        data = None\n        if output:\n            try:\n                data = json.loads(output)\n            except json.JSONDecodeError:\n                data = None\n        if data is not None:\n            # If we have cached overlap data and new data doesn't include it, merge it in\n            if cached_overlap_data and not include_overlap:\n                details = data.get('details', {})\n                has_overlap = ('section_overlaps' in data or 'consolidation_recommendations' in data or\n                              'section_overlaps' in details or 'consolidation_recommendations' in details)\n                if not has_overlap:\n                    if cached_overlap_in_details:\n                        if 'details' not in data:\n                            data['details'] = {}\n                        if cached_overlap_data.get('section_overlaps'):\n                            data['details']['section_overlaps'] = cached_overlap_data['section_overlaps']\n                        if cached_overlap_data.get('consolidation_recommendations'):\n                            data['details']['consolidation_recommendations'] = cached_overlap_data['consolidation_recommendations']\n                    else:\n                        if cached_overlap_data.get('section_overlaps'):\n                            data['section_overlaps'] = cached_overlap_data['section_overlaps']\n                        if cached_overlap_data.get('consolidation_recommendations'):\n                            data['consolidation_recommendations'] = cached_overlap_data['consolidation_recommendations']\n                    logger.debug(\"Preserved cached overlap analysis data in new results\")\n            result['data'] = data\n            self.results_cache['analyze_documentation'] = data\n            try:\n                save_tool_result('analyze_documentation', 'docs', data, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_documentation result: {e}\")\n            result['issues_found'] = bool(data.get('duplicates') or data.get('placeholders') or data.get('missing'))\n            result['success'] = True\n            result['error'] = ''\n        else:\n            lowered = output.lower() if isinstance(output, str) else ''\n            if not result.get('success') and (\"verbatim duplicate\" in lowered or \"placeholder\" in lowered):\n                result['issues_found'] = True\n                result['success'] = True\n                result['error'] = ''\n        return result\n    \n    def run_analyze_function_registry(self) -> Dict:\n        \"\"\"Run analyze_function_registry with structured JSON handling.\"\"\"\n        logger.info(\"Analyzing function registry...\")\n        result = self.run_script(\"analyze_function_registry\", \"--json\")\n        stderr_output = result.get('error', '')\n        if stderr_output:\n            logger.info(f\"analyze_function_registry stderr output: {stderr_output}\")\n            if 'Traceback' in stderr_output or 'File \"' in stderr_output:\n                logger.error(f\"analyze_function_registry traceback found in stderr:\\n{stderr_output}\")\n            if not result.get('success'):\n                original_error = result.get('error', '')\n                result['error'] = f\"{original_error}\\nStderr: {stderr_output}\" if original_error != stderr_output else stderr_output\n        output = result.get('output', '')\n        data = None\n        if output:\n            try:\n                data = json.loads(output)\n            except json.JSONDecodeError:\n                data = None\n        if data is not None:\n            result['data'] = data\n            try:\n                save_tool_result('analyze_function_registry', 'functions', data, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_function_registry result: {e}\")\n            missing = data.get('missing', {}) if isinstance(data.get('missing'), dict) else data.get('missing')\n            extra = data.get('extra', {}) if isinstance(data.get('extra'), dict) else data.get('extra')\n            errors = data.get('errors') or []\n            missing_count = missing.get('count') if isinstance(missing, dict) else missing\n            extra_count = extra.get('count') if isinstance(extra, dict) else extra\n            result['issues_found'] = bool(missing_count or extra_count or errors)\n            result['success'] = True\n            result['error'] = ''\n        else:\n            lowered = output.lower() if isinstance(output, str) else ''\n            if 'missing from registry' in lowered or 'missing items' in lowered or 'extra functions' in lowered:\n                result['issues_found'] = True\n                result['success'] = True\n                result['error'] = ''\n        return result\n    \n    def run_analyze_module_dependencies(self) -> Dict:\n        \"\"\"Run analyze_module_dependencies and capture dependency drift summary.\"\"\"\n        logger.info(\"Analyzing module dependencies...\")\n        result = self.run_script(\"analyze_module_dependencies\")\n        output = result.get('output', '')\n        summary = self._parse_module_dependency_report(output)\n        if summary:\n            missing_dependencies = summary.get('missing_dependencies', 0)\n            missing_sections = summary.get('missing_sections', [])\n            total_issues = missing_dependencies + len(missing_sections) if isinstance(missing_sections, list) else missing_dependencies\n            standard_format = {\n                'summary': {\n                    'total_issues': total_issues,\n                    'files_affected': 0\n                },\n                'details': summary\n            }\n            result['data'] = standard_format\n            try:\n                save_tool_result('analyze_module_dependencies', 'imports', standard_format, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_module_dependencies result: {e}\")\n            issues = summary.get('missing_dependencies', 0)\n            issues = issues or len(summary.get('missing_sections') or [])\n            result['issues_found'] = bool(issues)\n            if 'success' not in result:\n                result['success'] = True\n            self.module_dependency_summary = summary\n            self.results_cache['analyze_module_dependencies'] = summary\n        return result\n    \n    def run_analyze_functions(self) -> Dict:\n        \"\"\"Run analyze_functions with structured JSON handling.\"\"\"\n        logger.info(\"Analyzing functions...\")\n        args = [\"--json\"]\n        if self.exclusion_config.get('include_tests', False):\n            args.append(\"--include-tests\")\n        if self.exclusion_config.get('include_dev_tools', False):\n            args.append(\"--include-dev-tools\")\n        result = self.run_script(\"analyze_functions\", *args)\n        if result.get('success') and result.get('output'):\n            try:\n                json_data = json.loads(result['output'])\n                result['data'] = json_data\n                if 'analyze_functions' in self.results_cache:\n                    extracted_metrics_raw = self.results_cache['analyze_functions']\n                    if 'details' in extracted_metrics_raw and isinstance(extracted_metrics_raw.get('details'), dict):\n                        extracted_metrics = extracted_metrics_raw['details']\n                    else:\n                        extracted_metrics = extracted_metrics_raw\n                    if 'critical_complexity_examples' in extracted_metrics:\n                        json_data['details']['critical_complexity_examples'] = extracted_metrics['critical_complexity_examples']\n                    if 'high_complexity_examples' in extracted_metrics:\n                        json_data['details']['high_complexity_examples'] = extracted_metrics['high_complexity_examples']\n                    if 'undocumented_examples' in extracted_metrics:\n                        json_data['details']['undocumented_examples'] = extracted_metrics['undocumented_examples']\n                try:\n                    save_tool_result('analyze_functions', 'functions', json_data, project_root=self.project_root)\n                    self.results_cache['analyze_functions'] = json_data\n                except Exception as e:\n                    logger.warning(f\"Failed to save analyze_functions result: {e}\")\n            except (json.JSONDecodeError, ValueError) as e:\n                logger.warning(f\"Failed to parse analyze_functions JSON output: {e}\")\n        return result\n    \n    def run_decision_support(self) -> Dict:\n        \"\"\"Run decision_support with structured JSON handling.\"\"\"\n        logger.info(\"Running decision_support...\")\n        args = []\n        if self.exclusion_config.get('include_tests', False):\n            args.append(\"--include-tests\")\n        if self.exclusion_config.get('include_dev_tools', False):\n            args.append(\"--include-dev-tools\")\n        result = self.run_script(\"decision_support\", *args)\n        self._extract_decision_insights(result)\n        try:\n            data = None\n            if 'decision_support_metrics' in self.results_cache:\n                data = self.results_cache['decision_support_metrics']\n            elif 'decision_support' in self.results_cache:\n                data = self.results_cache['decision_support']\n            elif result.get('data'):\n                data = result.get('data')\n            elif result.get('output'):\n                try:\n                    data = json.loads(result.get('output', ''))\n                except (json.JSONDecodeError, TypeError):\n                    data = {\n                        'success': result.get('success', False),\n                        'output': result.get('output', ''),\n                        'error': result.get('error', ''),\n                        'returncode': result.get('returncode', 0)\n                    }\n            if data is not None:\n                save_tool_result('decision_support', 'reports', data, project_root=self.project_root)\n            else:\n                save_tool_result('decision_support', 'reports', {\n                    'success': result.get('success', False),\n                    'output': result.get('output', '')[:500] if result.get('output') else '',\n                    'returncode': result.get('returncode', 0)\n                }, project_root=self.project_root)\n        except Exception as save_error:\n            logger.error(f\"Failed to save decision_support results: {save_error}\", exc_info=True)\n        return result\n    \n    def run_analyze_function_patterns(self) -> Dict:\n        \"\"\"Run analyze_function_patterns and save results.\"\"\"\n        logger.info(\"Analyzing function patterns...\")\n        try:\n            from development_tools.functions.analyze_function_patterns import analyze_function_patterns\n            from development_tools.functions.analyze_functions import scan_all_python_files\n            actual_functions = scan_all_python_files()\n            patterns = analyze_function_patterns(actual_functions)\n            standard_format = {\n                'summary': {\n                    'total_issues': 0,\n                    'files_affected': 0\n                },\n                'details': patterns\n            }\n            save_tool_result('analyze_function_patterns', 'functions', standard_format, project_root=self.project_root)\n            return {\n                'success': True,\n                'data': standard_format  # Return standard_format to match what was saved\n            }\n        except Exception as e:\n            logger.warning(f\"Failed to run analyze_function_patterns: {e}\")\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    def run_analyze_module_imports(self) -> Dict:\n        \"\"\"Run analyze_module_imports and save results.\"\"\"\n        logger.info(\"Analyzing module imports...\")\n        try:\n            from development_tools.imports.analyze_module_imports import ModuleImportAnalyzer\n            analyzer = ModuleImportAnalyzer(project_root=str(self.project_root))\n            import_data = analyzer.scan_all_python_files()\n            standard_format = {\n                'summary': {\n                    'total_issues': 0,\n                    'files_affected': 0\n                },\n                'details': import_data\n            }\n            save_tool_result('analyze_module_imports', 'imports', standard_format, project_root=self.project_root)\n            return {\n                'success': True,\n                'data': standard_format\n            }\n        except Exception as e:\n            logger.warning(f\"Failed to run analyze_module_imports: {e}\")\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    def run_analyze_dependency_patterns(self) -> Dict:\n        \"\"\"Run analyze_dependency_patterns and save results.\"\"\"\n        logger.info(\"Analyzing dependency patterns...\")\n        try:\n            from development_tools.imports.analyze_module_imports import ModuleImportAnalyzer\n            from development_tools.imports.analyze_dependency_patterns import DependencyPatternAnalyzer\n            import_analyzer = ModuleImportAnalyzer(project_root=str(self.project_root))\n            actual_imports = import_analyzer.scan_all_python_files()\n            pattern_analyzer = DependencyPatternAnalyzer()\n            patterns = pattern_analyzer.analyze_dependency_patterns(actual_imports)\n            standard_format = {\n                'summary': {\n                    'total_issues': 0,\n                    'files_affected': 0\n                },\n                'details': patterns\n            }\n            save_tool_result('analyze_dependency_patterns', 'imports', standard_format, project_root=self.project_root)\n            return {\n                'success': True,\n                'data': standard_format\n            }\n        except Exception as e:\n            logger.warning(f\"Failed to run analyze_dependency_patterns: {e}\")\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    def run_analyze_package_exports(self) -> Dict:\n        \"\"\"Run analyze_package_exports and save results.\"\"\"\n        logger.info(\"Analyzing package exports...\")\n        try:\n            from development_tools.functions.analyze_package_exports import generate_audit_report\n            packages = ['core', 'communication', 'ui', 'tasks', 'ai', 'user']\n            all_reports = {}\n            for package in packages:\n                try:\n                    report = generate_audit_report(package)\n                    if isinstance(report, dict):\n                        for key, value in report.items():\n                            if isinstance(value, set):\n                                report[key] = sorted(value)\n                            elif isinstance(value, dict):\n                                for nested_key, nested_value in value.items():\n                                    if isinstance(nested_value, set):\n                                        value[nested_key] = sorted(nested_value)\n                    all_reports[package] = report\n                except Exception as e:\n                    logger.warning(f\"Failed to audit package {package}: {e}\")\n                    all_reports[package] = {\n                        'package': package,\n                        'error': str(e),\n                        'missing_exports': [],\n                        'potentially_unnecessary': []\n                    }\n            total_missing = sum(len(r.get('missing_exports', [])) for r in all_reports.values())\n            total_unnecessary = sum(len(r.get('potentially_unnecessary', [])) for r in all_reports.values())\n            packages_with_missing = sum(1 for r in all_reports.values() if r.get('missing_exports'))\n            total_issues = total_missing + total_unnecessary\n            result_data = {\n                'summary': {\n                    'total_issues': total_issues,\n                    'files_affected': packages_with_missing\n                },\n                'details': {\n                    'total_missing_exports': total_missing,\n                    'total_unnecessary_exports': total_unnecessary,\n                    'packages_with_missing': packages_with_missing,\n                    'packages': all_reports\n                }\n            }\n            self.results_cache['analyze_package_exports'] = result_data\n            save_tool_result('analyze_package_exports', 'functions', result_data, project_root=self.project_root)\n            return {\n                'success': True,\n                'data': result_data\n            }\n        except Exception as e:\n            logger.warning(f\"Failed to run analyze_package_exports: {e}\")\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    def run_analyze_error_handling(self) -> Dict:\n        \"\"\"Run analyze_error_handling with structured JSON handling.\"\"\"\n        args = [\"--json\"]\n        if self.exclusion_config.get('include_tests', False):\n            args.append(\"--include-tests\")\n        if self.exclusion_config.get('include_dev_tools', False):\n            args.append(\"--include-dev-tools\")\n        result = self.run_script(\"analyze_error_handling\", *args)\n        output = result.get('output', '')\n        stderr_output = result.get('error', '')\n        # Log errors for debugging\n        if stderr_output and not result.get('success', False):\n            logger.warning(f\"analyze_error_handling stderr: {stderr_output[:500]}\")  # Log first 500 chars\n            if 'Traceback' in stderr_output or 'Error' in stderr_output:\n                logger.error(f\"analyze_error_handling error details:\\n{stderr_output}\")\n        data = None\n        if output:\n            try:\n                lines = output.split('\\n')\n                json_start = -1\n                for i, line in enumerate(lines):\n                    if line.strip().startswith('{'):\n                        json_start = i\n                        break\n                if json_start >= 0:\n                    json_output = '\\n'.join(lines[json_start:])\n                    data = json.loads(json_output)\n                else:\n                    data = json.loads(output)\n            except json.JSONDecodeError:\n                data = None\n        # If JSON parsing failed, try loading from standardized output storage\n        # BUT: Only use cached data if the script actually succeeded (returncode == 0)\n        # If the script failed, don't perpetuate stale cached data by saving it back\n        script_succeeded = result.get('success', False) and result.get('returncode') == 0\n        if data is None and script_succeeded:\n            try:\n                from ..output_storage import load_tool_result\n                data = load_tool_result('analyze_error_handling', 'error_handling', project_root=self.project_root)\n            except (OSError, json.JSONDecodeError):\n                data = None\n        if data is not None:\n            result['data'] = data\n            # Only save if we got data from the script output (script succeeded)\n            # Don't save cached data back when script failed - that perpetuates stale data\n            if script_succeeded:\n                try:\n                    save_tool_result('analyze_error_handling', 'error_handling', data, project_root=self.project_root)\n                except Exception as e:\n                    logger.warning(f\"Failed to save analyze_error_handling result: {e}\")\n            coverage = data.get('analyze_error_handling') or data.get('error_handling_coverage', 0)\n            missing_count = data.get('functions_missing_error_handling', 0)\n            result['issues_found'] = coverage < 80 or missing_count > 0\n            # Only mark as successful if script actually succeeded\n            # If we're using cached data from a failed run, keep success=False\n            if script_succeeded:\n                result['success'] = True\n                result['error'] = ''\n        else:\n            lowered = output.lower() if isinstance(output, str) else ''\n            if 'missing error handling' in lowered or 'coverage' in lowered:\n                result['issues_found'] = True\n                result['success'] = True\n                result['error'] = ''\n        return result\n    \n    def run_analyze_documentation_sync(self) -> Dict:\n        \"\"\"Run analyze_documentation_sync with structured data handling.\"\"\"\n        try:\n            if self._run_doc_sync_check('--check'):\n                summary = self.docs_sync_summary or {}\n                all_results = getattr(self, 'docs_sync_results', {}).get('all_results', {})\n                path_drift_files = summary.get('path_drift_files', [])\n                data = {\n                    'summary': {\n                        'total_issues': summary.get('total_issues', 0),\n                        'files_affected': len(path_drift_files) if isinstance(path_drift_files, list) else 0,\n                        'status': summary.get('status', 'UNKNOWN')\n                    },\n                    'details': {\n                        'paired_doc_issues': summary.get('paired_doc_issues', 0),\n                        'path_drift_issues': summary.get('path_drift_issues', 0),\n                        'ascii_compliance_issues': summary.get('ascii_issues', 0),\n                        'heading_numbering_issues': summary.get('heading_numbering_issues', 0),\n                        'missing_address_issues': summary.get('missing_address_issues', 0),\n                        'unconverted_link_issues': summary.get('unconverted_link_issues', 0),\n                        'path_drift_files': path_drift_files,\n                        'paired_docs': all_results.get('paired_docs', {}),\n                        'path_drift': all_results.get('path_drift', {}),\n                        'ascii_compliance': all_results.get('ascii_compliance', {}),\n                        'heading_numbering': all_results.get('heading_numbering', {}),\n                        'missing_addresses': all_results.get('missing_addresses', {}),\n                        'unconverted_links': all_results.get('unconverted_links', {})\n                    }\n                }\n                import io\n                import sys\n                output_buffer = io.StringIO()\n                original_stdout = sys.stdout\n                sys.stdout = output_buffer\n                try:\n                    from development_tools.docs.analyze_documentation_sync import DocumentationSyncChecker\n                    checker = DocumentationSyncChecker()\n                    results = {\n                        'summary': summary,\n                        'paired_docs': all_results.get('paired_docs', {}),\n                        'path_drift': all_results.get('path_drift', {}),\n                        'ascii_compliance': all_results.get('ascii_compliance', {}),\n                        'heading_numbering': all_results.get('heading_numbering', {})\n                    }\n                    checker.print_report(results)\n                    output = output_buffer.getvalue()\n                finally:\n                    sys.stdout = original_stdout\n                try:\n                    save_tool_result('analyze_documentation_sync', 'docs', data, project_root=self.project_root)\n                except Exception as e:\n                    logger.warning(f\"Failed to save analyze_documentation_sync result: {e}\")\n                return {\n                    'success': True,\n                    'output': output,\n                    'error': '',\n                    'returncode': 0,\n                    'data': data\n                }\n            else:\n                return {\n                    'success': False,\n                    'error': 'Documentation sync check failed',\n                    'output': '',\n                    'returncode': 1\n                }\n        except Exception as e:\n            logger.error(f\"Error running documentation sync: {e}\", exc_info=True)\n            return {\n                'success': False,\n                'error': str(e),\n                'output': '',\n                'returncode': 1\n            }\n    \n    def run_analyze_path_drift(self) -> Dict:\n        \"\"\"Run analyze_path_drift with structured data handling.\"\"\"\n        try:\n            from development_tools.docs.analyze_path_drift import PathDriftAnalyzer\n            analyzer = PathDriftAnalyzer()\n            structured_results = analyzer.run_analysis()\n            # run_analysis() always returns standard format with 'summary', 'files', and 'details' keys\n            summary = structured_results.get('summary', {})\n            data = {\n                'files': structured_results.get('files', {}),\n                'total_issues': summary.get('total_issues', 0),\n                'detailed_issues': structured_results.get('details', {}).get('detailed_issues', {})\n            }\n            import io\n            import sys\n            output_buffer = io.StringIO()\n            original_stdout = sys.stdout\n            sys.stdout = output_buffer\n            try:\n                if data['total_issues'] > 0:\n                    print(f\"\\nPath Drift Issues:\")\n                    print(f\"   Total files with issues: {len(data['files'])}\")\n                    print(f\"   Total issues found: {data['total_issues']}\")\n                    print(f\"   Top files with most issues:\")\n                    sorted_files = sorted(data['files'].items(), key=lambda x: x[1], reverse=True)\n                    for doc_file, issue_count in sorted_files[:5]:\n                        print(f\"     {doc_file}: {issue_count} issues\")\n                else:\n                    print(\"\\nNo path drift issues found!\")\n                output = output_buffer.getvalue()\n            finally:\n                sys.stdout = original_stdout\n            try:\n                save_tool_result('analyze_path_drift', 'docs', data, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_path_drift result: {e}\")\n            return {\n                'success': True,\n                'output': output,\n                'error': '',\n                'returncode': 0,\n                'data': data\n            }\n        except Exception as e:\n            logger.error(f\"Error running path drift analyzer: {e}\", exc_info=True)\n            result = self.run_script(\"analyze_path_drift\", '--json')\n            try:\n                def path_drift_converter(file_data: Dict[str, Any]) -> Dict[str, Any]:\n                    files_with_issues = {}\n                    detailed_issues = {}\n                    total_issues = 0\n                    for file_path, file_info in file_data.items():\n                        if isinstance(file_info, dict):\n                            results = file_info.get('results', [])\n                            if results:\n                                files_with_issues[file_path] = len(results)\n                                detailed_issues[file_path] = results\n                                total_issues += len(results)\n                    return {\n                        'files': files_with_issues,\n                        'total_issues': total_issues,\n                        'detailed_issues': detailed_issues\n                    }\n                data = self._load_mtime_cached_tool_results(\n                    'analyze_path_drift',\n                    'docs',\n                    result,\n                    self._parse_path_drift_output,\n                    path_drift_converter\n                )\n                if data:\n                    result['data'] = data\n                    result['success'] = True\n                    result['error'] = ''\n                else:\n                    result['success'] = False\n                    result['error'] = f'Failed to load path drift results: {e}'\n            except Exception as helper_error:\n                logger.debug(f\"Failed to use unified helper for path drift fallback: {helper_error}\")\n                output = result.get('output', '')\n                data = None\n                if output:\n                    try:\n                        data = json.loads(output)\n                    except json.JSONDecodeError:\n                        data = self._parse_path_drift_output(output)\n                if data:\n                    try:\n                        save_tool_result('analyze_path_drift', 'docs', data, project_root=self.project_root)\n                    except Exception as save_error:\n                        logger.warning(f\"Failed to save analyze_path_drift result: {save_error}\")\n                    result['data'] = data\n                    result['success'] = True\n                    result['error'] = ''\n                else:\n                    result['success'] = False\n                    result['error'] = f'Failed to parse path drift output: {e}'\n            return result\n    \n    def run_generate_legacy_reference_report(self) -> Dict:\n        \"\"\"Run generate_legacy_reference_report to create LEGACY_REFERENCE_REPORT.md.\"\"\"\n        logger.info(\"Generating legacy reference report...\")\n        # First, ensure we have legacy reference analysis results\n        legacy_data = None\n        if hasattr(self, 'legacy_cleanup_summary') and self.legacy_cleanup_summary:\n            legacy_data = self.legacy_cleanup_summary\n        else:\n            # Try to load from cache\n            try:\n                legacy_result = self._load_tool_data('analyze_legacy_references', 'legacy', log_source=False)\n                if legacy_result and isinstance(legacy_result, dict):\n                    legacy_data = legacy_result\n            except Exception as e:\n                logger.debug(f\"Failed to load legacy data for report generation: {e}\")\n        \n        if not legacy_data:\n            return {\n                'success': False,\n                'output': '',\n                'error': 'No legacy reference analysis data available. Run analyze_legacy_references first.',\n                'returncode': 1\n            }\n        \n        # Prepare findings file\n        import tempfile\n        import json\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:\n            findings_file = f.name\n            # Extract findings from legacy_data - handle both standard format and old format\n            findings = {}\n            if 'findings' in legacy_data:\n                findings = legacy_data['findings']\n            elif 'details' in legacy_data and 'findings' in legacy_data['details']:\n                findings = legacy_data['details']['findings']\n            elif isinstance(legacy_data, dict):\n                # If no findings key, use the whole structure (might be in old format)\n                findings = legacy_data\n            \n            json.dump(findings, f, indent=2)\n        \n        try:\n            script_path = Path(__file__).resolve().parent.parent.parent / 'legacy' / 'generate_legacy_reference_report.py'\n            output_file = 'development_docs/LEGACY_REFERENCE_REPORT.md'\n            cmd = [sys.executable, str(script_path), '--findings-file', findings_file, '--output-file', output_file]\n            result_proc = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                cwd=str(self.project_root),\n                timeout=300\n            )\n            # Clean up temp file\n            try:\n                Path(findings_file).unlink()\n            except Exception:\n                pass\n            \n            return {\n                'success': result_proc.returncode == 0,\n                'output': result_proc.stdout,\n                'error': result_proc.stderr,\n                'returncode': result_proc.returncode\n            }\n        except subprocess.TimeoutExpired:\n            try:\n                Path(findings_file).unlink()\n            except Exception:\n                pass\n            return {\n                'success': False,\n                'output': '',\n                'error': 'generate_legacy_reference_report timed out after 5 minutes',\n                'returncode': None\n            }\n        except Exception as e:\n            try:\n                Path(findings_file).unlink()\n            except Exception:\n                pass\n            return {\n                'success': False,\n                'output': '',\n                'error': f'generate_legacy_reference_report failed: {e}',\n                'returncode': None\n            }\n    \n    def run_generate_test_coverage_report(self) -> Dict:\n        \"\"\"Run generate_test_coverage_report to generate TEST_COVERAGE_REPORT.md from existing coverage data.\n        \n        This tool generates TEST_COVERAGE_REPORT.md, HTML, and JSON reports from existing coverage.json.\n        It should be run after run_test_coverage has executed tests and collected coverage data.\n        \"\"\"\n        logger.info(\"Generating test coverage report from existing coverage data...\")\n        \n        # Check if coverage.json exists\n        coverage_json = self.project_root / 'development_tools' / 'tests' / 'jsons' / 'coverage.json'\n        if not coverage_json.exists():\n            error_msg = 'coverage.json not found. Run run_test_coverage first to generate coverage data.'\n            logger.warning(f\"Test coverage data not found: {error_msg}\")\n            return {\n                'success': False,\n                'output': '',\n                'error': error_msg,\n                'returncode': 1\n            }\n        \n        # Run the report generator with --update-plan to generate TEST_COVERAGE_REPORT.md\n        try:\n            result = self.run_script('generate_test_coverage_report', '--update-plan', timeout=300)\n            if result.get('success'):\n                test_coverage_report = self.project_root / 'development_docs' / 'TEST_COVERAGE_REPORT.md'\n                if test_coverage_report.exists():\n                    logger.info(f\"Test coverage report generated: {test_coverage_report.relative_to(self.project_root)}\")\n                    return {\n                        'success': True,\n                        'output': f'TEST_COVERAGE_REPORT.md generated successfully',\n                        'error': '',\n                        'returncode': 0\n                    }\n                else:\n                    logger.warning(\"Report generation completed but TEST_COVERAGE_REPORT.md not found\")\n                    return result\n            else:\n                # Log the actual error for debugging\n                error_msg = result.get('error', 'Unknown error')\n                output_msg = result.get('output', '')\n                logger.warning(f\"generate_test_coverage_report failed: {error_msg}\")\n                if output_msg:\n                    logger.debug(f\"Script output: {output_msg[:500]}\")\n                return result\n        except Exception as e:\n            logger.error(f\"Failed to generate test coverage report: {e}\", exc_info=True)\n            return {\n                'success': False,\n                'output': '',\n                'error': f'generate_test_coverage_report failed: {e}',\n                'returncode': None\n            }\n    \n    def run_analyze_legacy_references(self) -> Dict:\n        \"\"\"Run analyze_legacy_references with structured data handling.\"\"\"\n        logger.info(\"Analyzing legacy references...\")\n        try:\n            from development_tools.legacy.analyze_legacy_references import LegacyReferenceAnalyzer\n            analyzer = LegacyReferenceAnalyzer(project_root=str(self.project_root))\n            findings = analyzer.scan_for_legacy_references()\n            total_files = sum(len(files) for files in findings.values())\n            total_markers = sum(len(matches) for files in findings.values() for _, _, matches in files)\n            serializable_findings = {}\n            for pattern_type, file_list in findings.items():\n                serializable_findings[pattern_type] = [\n                    [file_path, content, matches] for file_path, content, matches in file_list\n                ]\n            standard_format = {\n                'summary': {\n                    'total_issues': total_markers,\n                    'files_affected': total_files\n                },\n                'details': {\n                    'findings': serializable_findings,\n                    'files_with_issues': total_files,\n                    'legacy_markers': total_markers,\n                    'report_path': 'development_docs/LEGACY_REFERENCE_REPORT.md'\n                }\n            }\n            try:\n                save_tool_result('analyze_legacy_references', 'legacy', standard_format, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_legacy_references result: {e}\")\n            # Store in legacy_cleanup_summary for report generation\n            self.legacy_cleanup_summary = standard_format\n            # Also store in results_cache\n            if not hasattr(self, 'results_cache'):\n                self.results_cache = {}\n            self.results_cache['analyze_legacy_references'] = standard_format\n            \n            return {\n                'success': True,\n                'output': f\"Found {total_markers} legacy markers in {total_files} files\",\n                'error': '',\n                'returncode': 0,\n                'data': standard_format\n            }\n        except Exception as e:\n            logger.error(f\"Error running legacy references analyzer: {e}\", exc_info=True)\n            return {\n                'success': False,\n                'error': str(e),\n                'output': '',\n                'returncode': 1\n            }\n    \n    def run_analyze_unused_imports(self) -> Dict:\n        \"\"\"Run analyze_unused_imports with structured JSON handling (analysis only).\"\"\"\n        script_path = Path(__file__).resolve().parent.parent.parent / 'imports' / 'analyze_unused_imports.py'\n        # Run with --json flag only (report generation is now separate)\n        cmd = [sys.executable, str(script_path), '--json']\n        try:\n            result_proc = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                cwd=str(self.project_root),\n                timeout=600\n            )\n            result = {\n                'success': result_proc.returncode == 0,\n                'output': result_proc.stdout,\n                'error': result_proc.stderr,\n                'returncode': result_proc.returncode\n            }\n        except subprocess.TimeoutExpired:\n            return {\n                'success': False,\n                'output': '',\n                'error': 'Unused imports checker timed out after 10 minutes',\n                'returncode': None,\n                'issues_found': False\n            }\n        output = result.get('output', '')\n        data = None\n        if output:\n            # Try to find JSON in output (may be mixed with other text)\n            # JSON is printed first, so look for the first complete JSON object\n            output_lines = output.strip().split('\\n')\n            json_start = None\n            \n            # Find the start of JSON (first line with '{')\n            for i, line in enumerate(output_lines):\n                stripped = line.strip()\n                if stripped.startswith('{'):\n                    json_start = i\n                    logger.debug(f\"Found JSON start at line {i}: {stripped[:50]}...\")\n                    break\n            \n            if json_start is not None:\n                # Find the matching closing brace by counting braces\n                # Start with 1 because we found the opening brace\n                brace_count = 1\n                json_end = None\n                for i in range(json_start + 1, len(output_lines)):\n                    line = output_lines[i]\n                    brace_count += line.count('{') - line.count('}')\n                    if brace_count == 0:\n                        json_end = i + 1\n                        logger.debug(f\"Found JSON end at line {i+1}, total lines: {json_end - json_start}\")\n                        break\n                \n                if json_end is not None:\n                    json_output = '\\n'.join(output_lines[json_start:json_end])\n                    try:\n                        data = json.loads(json_output)\n                        logger.info(f\"Successfully parsed JSON output from analyze_unused_imports ({len(str(data))} chars, {len(json_output)} chars raw)\")\n                    except json.JSONDecodeError as e:\n                        logger.warning(f\"Failed to parse JSON output from analyze_unused_imports: {e}\")\n                        logger.debug(f\"JSON section preview (first 500 chars): {json_output[:500]}\")\n                        data = None\n                else:\n                    logger.warning(f\"analyze_unused_imports: Found JSON start at line {json_start} but couldn't find matching closing brace (searched {len(output_lines) - json_start} lines, brace_count ended at {brace_count})\")\n            else:\n                # Try parsing entire output as JSON (fallback)\n                try:\n                    data = json.loads(output)\n                    logger.debug(f\"Successfully parsed JSON output from analyze_unused_imports ({len(str(data))} chars)\")\n                except json.JSONDecodeError as e:\n                    logger.warning(f\"Failed to parse JSON output from analyze_unused_imports: {e}\")\n                    logger.debug(f\"Output preview (first 500 chars): {output[:500]}\")\n                    data = None\n        else:\n            logger.warning(f\"analyze_unused_imports returned empty output (returncode: {result.get('returncode')})\")\n        if data is not None:\n            result['data'] = data\n            self.results_cache['analyze_unused_imports'] = data\n            # Extract total_unused from standard format (summary.total_issues)\n            total_unused = data.get('summary', {}).get('total_issues', 0) if isinstance(data, dict) and 'summary' in data else data.get('total_unused', 0)\n            result['issues_found'] = total_unused > 0\n            result['success'] = True\n            result['error'] = ''\n            try:\n                save_tool_result('analyze_unused_imports', 'imports', data, project_root=self.project_root)\n            except Exception as e:\n                logger.warning(f\"Failed to save analyze_unused_imports result: {e}\")\n                import traceback\n                logger.debug(f\"Traceback: {traceback.format_exc()}\")\n        else:\n            logger.warning(f\"analyze_unused_imports: No data extracted, skipping save_tool_result()\")\n            lowered = output.lower() if isinstance(output, str) else ''\n            if 'unused import' in lowered:\n                result['issues_found'] = True\n                result['success'] = True\n                result['error'] = ''\n        return result\n    \n    def run_generate_unused_imports_report(self) -> Dict:\n        \"\"\"Run generate_unused_imports_report to generate markdown report from analysis results.\"\"\"\n        logger.info(\"Generating unused imports report...\")\n        script_path = Path(__file__).resolve().parent.parent.parent / 'imports' / 'generate_unused_imports_report.py'\n        cmd = [sys.executable, str(script_path), '--project-root', str(self.project_root)]\n        \n        try:\n            result_proc = subprocess.run(\n                cmd,\n                capture_output=True,\n                text=True,\n                cwd=str(self.project_root),\n                timeout=60\n            )\n            result = {\n                'success': result_proc.returncode == 0,\n                'output': result_proc.stdout,\n                'error': result_proc.stderr,\n                'returncode': result_proc.returncode\n            }\n            if result['success']:\n                logger.info(\"Unused imports report generated successfully\")\n            else:\n                logger.warning(f\"Unused imports report generation completed with issues: {result.get('error', 'Unknown error')}\")\n        except subprocess.TimeoutExpired:\n            return {\n                'success': False,\n                'output': '',\n                'error': 'Unused imports report generation timed out after 1 minute',\n                'returncode': None\n            }\n        except Exception as e:\n            logger.error(f\"Error generating unused imports report: {e}\", exc_info=True)\n            return {\n                'success': False,\n                'error': str(e),\n                'output': '',\n                'returncode': 1\n            }\n        return result\n    \n    def run_analyze_error_handling_legacy(self) -> Dict:\n        \"\"\"Run analyze_error_handling with structured JSON handling.\"\"\"\n        args = [\"--json\"]\n\n        if self.exclusion_config.get('include_tests', False):\n\n            args.append(\"--include-tests\")\n\n        if self.exclusion_config.get('include_dev_tools', False):\n\n            args.append(\"--include-dev-tools\")\n\n        result = self.run_script(\"analyze_error_handling\", *args)\n\n        output = result.get('output', '')\n\n        data = None\n\n        if output:\n\n            try:\n\n                lines = output.split('\\n')\n\n                json_start = -1\n\n                for i, line in enumerate(lines):\n\n                    if line.strip().startswith('{'):\n\n                        json_start = i\n\n                        break\n\n                if json_start >= 0:\n\n                    json_output = '\\n'.join(lines[json_start:])\n\n                    data = json.loads(json_output)\n\n                else:\n\n                    data = json.loads(output)\n\n            except json.JSONDecodeError:\n\n                data = None\n\n        # If JSON parsing failed, try loading from standardized output storage\n        if data is None:\n\n            try:\n\n                    from ..output_storage import load_tool_result\n\n                    data = load_tool_result('analyze_error_handling', 'error_handling', project_root=self.project_root)\n\n            except (OSError, json.JSONDecodeError):\n\n                data = None\n\n        if data is not None:\n\n            result['data'] = data\n\n            try:\n\n                save_tool_result('analyze_error_handling', 'error_handling', data, project_root=self.project_root)\n\n            except Exception as e:\n\n                logger.warning(f\"Failed to save analyze_error_handling result: {e}\")\n\n            coverage = data.get('analyze_error_handling') or data.get('error_handling_coverage', 0)\n\n            missing_count = data.get('functions_missing_error_handling', 0)\n\n            result['issues_found'] = coverage < 80 or missing_count > 0\n\n            result['success'] = True\n\n            result['error'] = ''\n\n        else:\n\n            lowered = output.lower() if isinstance(output, str) else ''\n\n            if 'missing error handling' in lowered or 'coverage' in lowered:\n\n                result['issues_found'] = True\n\n                result['success'] = True\n\n                result['error'] = ''\n\n        return result\n\n    \n\n    def run_analyze_documentation_sync_legacy(self) -> Dict:\n\n        \"\"\"Run analyze_documentation_sync with structured data handling.\"\"\"\n\n        try:\n\n            if self._run_doc_sync_check('--check'):\n\n                summary = self.docs_sync_summary or {}\n\n                all_results = getattr(self, 'docs_sync_results', {}).get('all_results', {})\n\n                path_drift_files = summary.get('path_drift_files', [])\n\n                data = {\n\n                    'summary': {\n\n                        'total_issues': summary.get('total_issues', 0),\n\n                        'files_affected': len(path_drift_files) if isinstance(path_drift_files, list) else 0,\n\n                        'status': summary.get('status', 'UNKNOWN')\n\n                    },\n\n                    'details': {\n\n                        'paired_doc_issues': summary.get('paired_doc_issues', 0),\n\n                        'path_drift_issues': summary.get('path_drift_issues', 0),\n\n                        'ascii_compliance_issues': summary.get('ascii_issues', 0),\n\n                        'heading_numbering_issues': summary.get('heading_numbering_issues', 0),\n\n                        'missing_address_issues': summary.get('missing_address_issues', 0),\n\n                        'unconverted_link_issues': summary.get('unconverted_link_issues', 0),\n\n                        'path_drift_files': path_drift_files,\n\n                        'paired_docs': all_results.get('paired_docs', {}),\n\n                        'path_drift': all_results.get('path_drift', {}),\n\n                        'ascii_compliance': all_results.get('ascii_compliance', {}),\n\n                        'heading_numbering': all_results.get('heading_numbering', {}),\n\n                        'missing_addresses': all_results.get('missing_addresses', {}),\n\n                        'unconverted_links': all_results.get('unconverted_links', {})\n\n                    }\n\n                }\n\n                import io\n\n                import sys\n\n                output_buffer = io.StringIO()\n\n                original_stdout = sys.stdout\n\n                sys.stdout = output_buffer\n\n                try:\n\n                    from development_tools.docs.analyze_documentation_sync import DocumentationSyncChecker\n\n                    checker = DocumentationSyncChecker()\n\n                    results = {\n\n                        'summary': summary,\n\n                        'paired_docs': all_results.get('paired_docs', {}),\n\n                        'path_drift': all_results.get('path_drift', {}),\n\n                        'ascii_compliance': all_results.get('ascii_compliance', {}),\n\n                        'heading_numbering': all_results.get('heading_numbering', {})\n\n                    }\n\n                    checker.print_report(results)\n\n                    output = output_buffer.getvalue()\n\n                finally:\n\n                    sys.stdout = original_stdout\n\n                try:\n\n                    save_tool_result('analyze_documentation_sync', 'docs', data, project_root=self.project_root)\n\n                except Exception as e:\n\n                    logger.warning(f\"Failed to save analyze_documentation_sync result: {e}\")\n\n                return {\n\n                    'success': True,\n\n                    'output': output,\n\n                    'error': '',\n\n                    'returncode': 0,\n\n                    'data': data\n\n                }\n\n            else:\n\n                return {\n\n                    'success': False,\n\n                    'error': 'Documentation sync check failed',\n\n                    'output': '',\n\n                    'returncode': 1\n\n                }\n\n        except Exception as e:\n\n            logger.error(f\"Error running documentation sync: {e}\", exc_info=True)\n\n            return {\n\n                'success': False,\n\n                'error': str(e),\n\n                'output': '',\n\n                'returncode': 1\n\n            }\n\n    \n\n    def run_analyze_path_drift_legacy(self) -> Dict:\n\n        \"\"\"Run analyze_path_drift with structured data handling.\"\"\"\n\n        try:\n\n            from development_tools.docs.analyze_path_drift import PathDriftAnalyzer\n\n            analyzer = PathDriftAnalyzer()\n\n            structured_results = analyzer.run_analysis()\n\n            if 'summary' in structured_results:\n\n                summary = structured_results.get('summary', {})\n\n                data = {\n\n                    'files': structured_results.get('files', {}),\n\n                    'total_issues': summary.get('total_issues', 0),\n\n                    'detailed_issues': structured_results.get('details', {}).get('detailed_issues', {})\n\n                }\n\n            else:\n\n                logger.debug(\"analyze_path_drift: Using legacy format (backward compatibility)\")\n\n                data = {\n\n                    'files': structured_results.get('files', {}),\n\n                    'total_issues': structured_results.get('total_issues', 0),\n\n                    'detailed_issues': structured_results.get('detailed_issues', {})\n\n                }\n\n            import io\n\n            import sys\n\n            output_buffer = io.StringIO()\n\n            original_stdout = sys.stdout\n\n            sys.stdout = output_buffer\n\n            try:\n\n                if data['total_issues'] > 0:\n\n                    print(f\"\\nPath Drift Issues:\")\n\n                    print(f\"   Total files with issues: {len(data['files'])}\")\n\n                    print(f\"   Total issues found: {data['total_issues']}\")\n\n                    print(f\"   Top files with most issues:\")\n\n                    sorted_files = sorted(data['files'].items(), key=lambda x: x[1], reverse=True)\n\n                    for doc_file, issue_count in sorted_files[:5]:\n\n                        print(f\"     {doc_file}: {issue_count} issues\")\n\n                else:\n\n                    print(\"\\nNo path drift issues found!\")\n\n                output = output_buffer.getvalue()\n\n            finally:\n\n                sys.stdout = original_stdout\n\n            try:\n\n                save_tool_result('analyze_path_drift', 'docs', data, project_root=self.project_root)\n\n            except Exception as e:\n\n                logger.warning(f\"Failed to save analyze_path_drift result: {e}\")\n\n            return {\n\n                'success': True,\n\n                'output': output,\n\n                'error': '',\n\n                'returncode': 0,\n\n                'data': data\n\n            }\n\n        except Exception as e:\n\n            logger.error(f\"Error running path drift analyzer: {e}\", exc_info=True)\n\n            result = self.run_script(\"analyze_path_drift\", '--json')\n\n            try:\n                def path_drift_converter(file_data: Dict[str, Any]) -> Dict[str, Any]:\n                    files_with_issues = {}\n                    detailed_issues = {}\n                    total_issues = 0\n                    for file_path, file_info in file_data.items():\n                        if isinstance(file_info, dict):\n                            results = file_info.get('results', [])\n                            if results:\n                                files_with_issues[file_path] = len(results)\n                                detailed_issues[file_path] = results\n                                total_issues += len(results)\n                    return {\n                        'files': files_with_issues,\n                        'total_issues': total_issues,\n                        'detailed_issues': detailed_issues\n                    }\n\n                data = self._load_mtime_cached_tool_results(\n                    'analyze_path_drift',\n                    'docs',\n                    result,\n                    self._parse_path_drift_output,\n                    path_drift_converter\n                )\n\n                if data:\n\n                    result['data'] = data\n\n                    result['success'] = True\n\n                    result['error'] = ''\n\n                else:\n\n                    result['success'] = False\n\n                    result['error'] = f'Failed to load path drift results: {e}'\n\n            except Exception as helper_error:\n\n                logger.debug(f\"Failed to use unified helper for path drift fallback: {helper_error}\")\n\n                output = result.get('output', '')\n\n                data = None\n\n                if output:\n\n                    try:\n\n                        data = json.loads(output)\n\n                    except json.JSONDecodeError:\n\n                        data = self._parse_path_drift_output(output)\n\n                if data:\n\n                    try:\n\n                        save_tool_result('analyze_path_drift', 'docs', data, project_root=self.project_root)\n\n                    except Exception as save_error:\n\n                        logger.warning(f\"Failed to save analyze_path_drift result: {save_error}\")\n\n                    result['data'] = data\n\n                    result['success'] = True\n\n                    result['error'] = ''\n\n                else:\n\n                    result['success'] = False\n\n                    result['error'] = f'Failed to parse path drift output: {e}'\n\n            return result\n\n    \n\n    def run_generate_legacy_reference_report_legacy(self) -> Dict:\n\n        \"\"\"Run generate_legacy_reference_report to create LEGACY_REFERENCE_REPORT.md.\"\"\"\n\n        # First, ensure we have legacy reference analysis results\n\n        legacy_data = None\n\n        if hasattr(self, 'legacy_cleanup_summary') and self.legacy_cleanup_summary:\n\n            legacy_data = self.legacy_cleanup_summary\n\n        else:\n\n            # Try to load from cache\n\n            try:\n\n                legacy_result = self._load_tool_data('analyze_legacy_references', 'legacy', log_source=False)\n\n                if legacy_result and isinstance(legacy_result, dict):\n\n                    legacy_data = legacy_result\n\n            except Exception as e:\n\n                logger.debug(f\"Failed to load legacy data for report generation: {e}\")\n\n        \n\n        if not legacy_data:\n\n            return {\n\n                'success': False,\n\n                'output': '',\n\n                'error': 'No legacy reference analysis data available. Run analyze_legacy_references first.',\n\n                'returncode': 1\n\n            }\n\n        \n\n        # Prepare findings file\n\n        import tempfile\n\n        import json\n\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:\n\n            findings_file = f.name\n\n            # Extract findings from legacy_data - handle both standard format and old format\n\n            findings = {}\n\n            if 'findings' in legacy_data:\n\n                findings = legacy_data['findings']\n\n            elif 'details' in legacy_data and 'findings' in legacy_data['details']:\n\n                findings = legacy_data['details']['findings']\n\n            elif isinstance(legacy_data, dict):\n\n                # If no findings key, use the whole structure (might be in old format)\n\n                findings = legacy_data\n\n            \n\n            json.dump(findings, f, indent=2)\n\n        \n\n        try:\n\n            script_path = Path(__file__).resolve().parent.parent.parent / 'legacy' / 'generate_legacy_reference_report.py'\n\n            output_file = 'development_docs/LEGACY_REFERENCE_REPORT.md'\n\n            cmd = [sys.executable, str(script_path), '--findings-file', findings_file, '--output-file', output_file]\n\n            result_proc = subprocess.run(\n\n                cmd,\n\n                capture_output=True,\n\n                text=True,\n\n                cwd=str(self.project_root),\n\n                timeout=300\n\n            )\n\n            # Clean up temp file\n\n            try:\n\n                Path(findings_file).unlink()\n\n            except Exception:\n\n                pass\n\n            \n\n            return {\n\n                'success': result_proc.returncode == 0,\n\n                'output': result_proc.stdout,\n\n                'error': result_proc.stderr,\n\n                'returncode': result_proc.returncode\n\n            }\n\n        except subprocess.TimeoutExpired:\n\n            try:\n\n                Path(findings_file).unlink()\n\n            except Exception:\n\n                pass\n\n            return {\n\n                'success': False,\n\n                'output': '',\n\n                'error': 'generate_legacy_reference_report timed out after 5 minutes',\n\n                'returncode': None\n\n            }\n\n        except Exception as e:\n\n            try:\n\n                Path(findings_file).unlink()\n\n            except Exception:\n\n                pass\n\n            return {\n\n                'success': False,\n\n                'output': '',\n\n                'error': f'generate_legacy_reference_report failed: {e}',\n\n                'returncode': None\n\n            }\n\n    \n\n    def run_analyze_legacy_references_legacy(self) -> Dict:\n\n        \"\"\"Run analyze_legacy_references with structured data handling.\"\"\"\n\n        try:\n\n            from development_tools.legacy.analyze_legacy_references import LegacyReferenceAnalyzer\n\n            analyzer = LegacyReferenceAnalyzer(project_root=str(self.project_root))\n\n            findings = analyzer.scan_for_legacy_references()\n\n            total_files = sum(len(files) for files in findings.values())\n\n            total_markers = sum(len(matches) for files in findings.values() for _, _, matches in files)\n\n            serializable_findings = {}\n\n            for pattern_type, file_list in findings.items():\n\n                serializable_findings[pattern_type] = [\n\n                    [file_path, content, matches] for file_path, content, matches in file_list\n\n                ]\n\n            standard_format = {\n\n                'summary': {\n\n                    'total_issues': total_markers,\n\n                    'files_affected': total_files\n\n                },\n\n                'details': {\n\n                    'findings': serializable_findings,\n\n                    'files_with_issues': total_files,\n\n                    'legacy_markers': total_markers,\n\n                    'report_path': 'development_docs/LEGACY_REFERENCE_REPORT.md'\n\n                }\n\n            }\n\n            try:\n\n                save_tool_result('analyze_legacy_references', 'legacy', standard_format, project_root=self.project_root)\n\n            except Exception as e:\n\n                logger.warning(f\"Failed to save analyze_legacy_references result: {e}\")\n\n            # Store in legacy_cleanup_summary for report generation\n\n            self.legacy_cleanup_summary = standard_format\n\n            # Also store in results_cache\n\n            if not hasattr(self, 'results_cache'):\n\n                self.results_cache = {}\n\n            self.results_cache['analyze_legacy_references'] = standard_format\n\n            \n\n            return {\n\n                'success': True,\n\n                'output': f\"Found {total_markers} legacy markers in {total_files} files\",\n\n                'error': '',\n\n                'returncode': 0,\n\n                'data': standard_format\n\n            }\n\n        except Exception as e:\n\n            logger.error(f\"Error running legacy references analyzer: {e}\", exc_info=True)\n\n            return {\n\n                'success': False,\n\n                'error': str(e),\n\n                'output': '',\n\n                'returncode': 1\n\n            }\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 1312,
                    "line_content": "logger.debug(\"analyze_path_drift: Using legacy format (backward compatibility)\")",
                    "start": 58703,
                    "end": 58725
                  }
                ]
              ],
              [
                "tests\\behavior\\test_account_handler_behavior.py",
                "\"\"\"\nAccount Handler Behavior Tests\n\nTests for communication/command_handlers/account_handler.py focusing on real behavior and side effects.\nThese tests verify that account handlers actually work and produce expected side effects.\n\"\"\"\n\nimport pytest\nfrom unittest.mock import patch, MagicMock, AsyncMock\nfrom tests.test_utilities import TestUserFactory\nfrom core.user_data_handlers import get_user_id_by_identifier\nfrom core.user_data_handlers import get_user_data\nfrom communication.command_handlers.account_handler import (\n    AccountManagementHandler,\n    _generate_confirmation_code,\n    _send_confirmation_code,\n    _pending_link_operations\n)\nfrom communication.command_handlers.shared_types import ParsedCommand\n\n\nclass TestAccountHandlerBehavior:\n    \"\"\"Test account handler real behavior and side effects.\"\"\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_can_handle_recognizes_account_intents(self):\n        \"\"\"Test: Handler recognizes account management intents.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Test valid intents\n        assert handler.can_handle('create_account'), \"Should handle create_account\"\n        assert handler.can_handle('link_account'), \"Should handle link_account\"\n        assert handler.can_handle('check_account_status'), \"Should handle check_account_status\"\n        \n        # Test invalid intents\n        assert not handler.can_handle('unknown_intent'), \"Should not handle unknown intent\"\n        assert not handler.can_handle('task_create'), \"Should not handle non-account intents\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_handle_routes_to_create_account(self, test_data_dir):\n        \"\"\"Test: Handler routes create_account intent correctly.\"\"\"\n        handler = AccountManagementHandler()\n        \n        parsed_command = ParsedCommand(\n            intent='create_account',\n            entities={'username': 'testuser123'},\n            confidence=0.9,\n            original_message='create account'\n        )\n        \n        response = handler.handle('test_channel_id', parsed_command)\n        \n        assert response is not None, \"Should return response\"\n        assert isinstance(response.message, str), \"Should return string message\"\n        # Response may be success or ask for more info\n        assert len(response.message) > 0, \"Should return non-empty message\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_create_account_with_valid_username(self, test_data_dir):\n        \"\"\"Test: Create account with valid username creates user and saves data.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Clear any existing user\n        discord_user_id = \"111222333444555666\"\n        existing_user_id = get_user_id_by_identifier(discord_user_id)\n        if existing_user_id:\n            from core.user_data_manager import delete_user_completely, rebuild_user_index\n            delete_user_completely(existing_user_id, create_backup=False)\n            rebuild_user_index()  # Rebuild index to ensure user is removed\n            # Wait a moment for file system to sync\n            import time\n            time.sleep(0.1)\n        \n        parsed_command = ParsedCommand(\n            intent='create_account',\n            entities={\n                'username': 'newtestuser',\n                'channel_identifier': discord_user_id,\n                'channel_type': 'discord'\n            },\n            confidence=0.9,\n            original_message='create account'\n        )\n        \n        response = handler.handle(discord_user_id, parsed_command)\n        \n        # Assert: Should create account successfully\n        assert response.completed is True, \"Should complete account creation\"\n        assert 'Account created successfully' in response.message, \"Should indicate success\"\n        \n        # Verify user was actually created\n        created_user_id = get_user_id_by_identifier(discord_user_id)\n        assert created_user_id is not None, \"User should be created\"\n        \n        # Verify user data exists\n        user_data = get_user_data(created_user_id, 'account')\n        assert user_data is not None, \"User data should exist\"\n        account_data = user_data.get('account', {})\n        assert account_data.get('internal_username') == 'newtestuser', \"Username should match\"\n        assert account_data.get('discord_user_id') == discord_user_id, \"Discord ID should be set\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_create_account_with_feature_selection(self, test_data_dir):\n        \"\"\"Test: Create account with feature selection parameters sets correct feature flags.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Clear any existing user\n        discord_user_id = \"999888777666555444\"\n        existing_user_id = get_user_id_by_identifier(discord_user_id)\n        if existing_user_id:\n            from core.user_data_manager import delete_user_completely\n            delete_user_completely(existing_user_id, create_backup=False)\n        \n        parsed_command = ParsedCommand(\n            intent='create_account',\n            entities={\n                'username': 'featuretestuser',\n                'channel_identifier': discord_user_id,\n                'channel_type': 'discord',\n                'tasks_enabled': False,\n                'checkins_enabled': True,\n                'messages_enabled': True,\n                'timezone': 'America/New_York'\n            },\n            confidence=0.9,\n            original_message='create account'\n        )\n        \n        response = handler.handle(discord_user_id, parsed_command)\n        \n        # Assert: Should create account successfully\n        assert response.completed is True, \"Should complete account creation\"\n        \n        # Verify user was created\n        created_user_id = get_user_id_by_identifier(discord_user_id)\n        assert created_user_id is not None, \"User should be created\"\n        \n        # Verify feature flags are set correctly\n        user_data = get_user_data(created_user_id, 'account')\n        account_data = user_data.get('account', {})\n        features = account_data.get('features', {})\n        \n        assert features.get('task_management') == 'disabled', \"Task management should be disabled\"\n        assert features.get('checkins') == 'enabled', \"Check-ins should be enabled\"\n        assert features.get('automated_messages') == 'enabled', \"Automated messages should be enabled\"\n        assert account_data.get('timezone') == 'America/New_York', \"Timezone should be set correctly\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_create_account_backward_compatibility_defaults(self, test_data_dir):\n        \"\"\"Test: Create account without feature selection uses backward-compatible defaults.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Clear any existing user\n        discord_user_id = \"888777666555444333\"\n        existing_user_id = get_user_id_by_identifier(discord_user_id)\n        if existing_user_id:\n            from core.user_data_manager import delete_user_completely\n            delete_user_completely(existing_user_id, create_backup=False)\n        \n        # Create account without feature selection parameters (backward compatibility)\n        parsed_command = ParsedCommand(\n            intent='create_account',\n            entities={\n                'username': 'defaulttestuser',\n                'channel_identifier': discord_user_id,\n                'channel_type': 'discord'\n                # No feature selection parameters - should use defaults\n            },\n            confidence=0.9,\n            original_message='create account'\n        )\n        \n        response = handler.handle(discord_user_id, parsed_command)\n        \n        # Assert: Should create account successfully\n        assert response.completed is True, \"Should complete account creation\"\n        \n        # Verify user was created\n        created_user_id = get_user_id_by_identifier(discord_user_id)\n        assert created_user_id is not None, \"User should be created\"\n        \n        # Verify default feature flags (tasks=True, checkins=True, messages=False)\n        user_data = get_user_data(created_user_id, 'account')\n        account_data = user_data.get('account', {})\n        features = account_data.get('features', {})\n        \n        assert features.get('task_management') == 'enabled', \"Task management should default to enabled\"\n        assert features.get('checkins') == 'enabled', \"Check-ins should default to enabled\"\n        assert features.get('automated_messages') == 'disabled', \"Automated messages should default to disabled\"\n        assert account_data.get('timezone') == 'America/Regina', \"Timezone should default to America/Regina\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_handle_create_account_without_username(self, test_data_dir):\n        \"\"\"Test: Create account without username asks for username.\"\"\"\n        handler = AccountManagementHandler()\n        \n        parsed_command = ParsedCommand(\n            intent='create_account',\n            entities={},\n            confidence=0.9,\n            original_message='create account'\n        )\n        \n        response = handler.handle('test_channel_id', parsed_command)\n        \n        # Assert: Should ask for username\n        assert response.completed is False, \"Should not complete without username\"\n        assert 'username' in response.message.lower(), \"Should ask for username\"\n        assert len(response.suggestions) > 0, \"Should provide suggestions\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_handle_create_account_with_short_username(self, test_data_dir):\n        \"\"\"Test: Create account with short username rejects and asks for longer.\"\"\"\n        handler = AccountManagementHandler()\n        \n        parsed_command = ParsedCommand(\n            intent='create_account',\n            entities={'username': 'ab'},\n            confidence=0.9,\n            original_message='create account'\n        )\n        \n        response = handler.handle('test_channel_id', parsed_command)\n        \n        # Assert: Should reject short username\n        assert response.completed is False, \"Should not complete with short username\"\n        assert '3 characters' in response.message, \"Should mention minimum length\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_create_account_with_existing_username(self, test_data_dir):\n        \"\"\"Test: Create account with existing username rejects.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user first\n        existing_username = 'existinguser'\n        TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n        \n        # Rebuild user index to ensure user is discoverable\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        import time\n        time.sleep(0.1)  # Brief delay for index to be written\n        \n        parsed_command = ParsedCommand(\n            intent='create_account',\n            entities={'username': existing_username},\n            confidence=0.9,\n            original_message='create account'\n        )\n        \n        response = handler.handle('test_channel_id', parsed_command)\n        \n        # Assert: Should reject existing username\n        assert response.completed is False, \"Should not complete with existing username\"\n        assert 'already taken' in response.message.lower(), \"Should indicate username taken\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_check_account_status_with_existing_user(self, test_data_dir):\n        \"\"\"Test: Check account status returns account info for existing user.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user\n        discord_user_id = \"222333444555666777\"\n        TestUserFactory.create_discord_user(discord_user_id, test_data_dir=test_data_dir)\n        \n        # Ensure user index is updated (race condition fix)\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        \n        # Retry lookup in case of race conditions\n        import time\n        internal_user_id = None\n        for attempt in range(5):\n            internal_user_id = get_user_id_by_identifier(discord_user_id)\n            if internal_user_id:\n                break\n            if attempt < 4:\n                time.sleep(0.1)\n        \n        assert internal_user_id is not None, \"User should exist\"\n        \n        parsed_command = ParsedCommand(\n            intent='check_account_status',\n            entities={},\n            confidence=0.9,\n            original_message='check account status'\n        )\n        \n        response = handler.handle(discord_user_id, parsed_command)\n        \n        # Assert: Should return account status\n        assert response.completed is True, \"Should complete status check\"\n        assert 'account linked' in response.message.lower(), \"Should indicate account exists\"\n        assert response.rich_data is not None, \"Should include rich data\"\n        assert response.rich_data.get('has_account') is True, \"Should indicate account exists\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_check_account_status_without_user(self, test_data_dir):\n        \"\"\"Test: Check account status indicates no account for new user.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Use non-existent user ID\n        new_user_id = \"999888777666555444\"\n        existing_user_id = get_user_id_by_identifier(new_user_id)\n        if existing_user_id:\n            from core.user_data_manager import delete_user_completely, rebuild_user_index\n            delete_user_completely(existing_user_id, create_backup=False)\n            rebuild_user_index()  # Rebuild index to ensure user is removed\n        \n        parsed_command = ParsedCommand(\n            intent='check_account_status',\n            entities={},\n            confidence=0.9,\n            original_message='check account status'\n        )\n        \n        response = handler.handle(new_user_id, parsed_command)\n        \n        # Assert: Should indicate no account\n        assert response.completed is False, f\"Should indicate no account, but got completed={response.completed}, message={response.message}\"\n        assert 'no mhm account' in response.message.lower() or 'no account found' in response.message.lower(), f\"Should indicate no account found, but got: {response.message}\"\n        assert response.rich_data is not None, \"Should include rich data\"\n        assert response.rich_data.get('has_account') is False, \"Should indicate no account\"\n        assert len(response.suggestions) > 0, \"Should provide suggestions\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_handle_link_account_without_username(self, test_data_dir):\n        \"\"\"Test: Link account without username asks for username.\"\"\"\n        handler = AccountManagementHandler()\n        \n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={},\n            confidence=0.9,\n            original_message='link account'\n        )\n        \n        response = handler.handle('test_channel_id', parsed_command)\n        \n        # Assert: Should ask for username\n        assert response.completed is False, \"Should not complete without username\"\n        assert 'username' in response.message.lower(), \"Should ask for username\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_handle_link_account_with_nonexistent_username(self, test_data_dir):\n        \"\"\"Test: Link account with nonexistent username rejects.\"\"\"\n        handler = AccountManagementHandler()\n        \n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={'username': 'nonexistentuser123'},\n            confidence=0.9,\n            original_message='link account'\n        )\n        \n        response = handler.handle('test_channel_id', parsed_command)\n        \n        # Assert: Should reject nonexistent username\n        assert response.completed is False, \"Should not complete with nonexistent username\"\n        assert 'not found' in response.message.lower(), \"Should indicate username not found\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_link_account_sends_confirmation_code(self, test_data_dir):\n        \"\"\"Test: Link account sends confirmation code for valid username.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user with email\n        existing_username = 'linktestuser'\n        user_id = TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n\n        # Rebuild user index to ensure user is discoverable\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        import time\n        time.sleep(0.1)  # Brief delay for index to be written\n\n        # Add email to user account\n        from core.user_data_handlers import update_user_account\n        update_user_account(user_id, {'email': 'test@example.com'})\n\n        # Clear pending operations\n        _pending_link_operations.clear()\n\n        discord_user_id = \"333444555666777888\"\n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={\n                'username': existing_username,\n                'channel_identifier': discord_user_id,\n                'channel_type': 'discord'\n            },\n            confidence=0.9,\n            original_message='link account'\n        )\n\n        with patch('communication.command_handlers.account_handler._send_confirmation_code') as mock_send:\n            mock_send.return_value = True\n            response = handler.handle(discord_user_id, parsed_command)\n\n        # Assert: Should send confirmation code\n        assert response.completed is False, \"Should not complete without code\"\n        assert 'confirmation code' in response.message.lower(), \"Should mention confirmation code\"\n        assert mock_send.called, \"Should call send confirmation code\"\n        \n        # Verify pending operation was created\n        assert discord_user_id in _pending_link_operations, \"Should create pending operation\"\n        pending = _pending_link_operations[discord_user_id]\n        assert pending['operation_type'] == 'link', \"Should be link operation\"\n        assert pending['username'] == existing_username, \"Should store username\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_link_account_verifies_confirmation_code(self, test_data_dir):\n        \"\"\"Test: Link account verifies confirmation code and links account.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user with email\n        existing_username = 'linkverifyuser'\n        TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n        \n        # Rebuild user index to ensure user is discoverable\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        import time\n        time.sleep(0.1)  # Brief delay for index to be written\n        \n        user_id = get_user_id_by_identifier(existing_username)\n        assert user_id is not None, \"User should be created\"\n        \n        # Add email to user account\n        from core.user_data_handlers import update_user_account\n        update_result = update_user_account(user_id, {'email': 'test@example.com'})\n        assert update_result is True, \"Email should be added\"\n        \n        # Set up pending operation with known code\n        discord_user_id = \"444555666777888999\"\n        confirmation_code = '123456'\n        _pending_link_operations[discord_user_id] = {\n            'operation_type': 'link',\n            'username': existing_username,\n            'user_id': user_id,\n            'confirmation_code': confirmation_code,\n            'channel_type': 'discord'\n        }\n        \n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={\n                'username': existing_username,\n                'confirmation_code': confirmation_code,\n                'channel_identifier': discord_user_id,\n                'channel_type': 'discord'\n            },\n            confidence=0.9,\n            original_message='link account'\n        )\n        \n        with patch('communication.command_handlers.account_handler._send_confirmation_code'):\n            response = handler.handle(discord_user_id, parsed_command)\n        \n        # Assert: Should link account successfully\n        assert response.completed is True, f\"Should complete linking, got: {response.message}\"\n        assert 'linked successfully' in response.message.lower(), \"Should indicate success\"\n        \n        # Verify account was actually linked\n        linked_user_id = get_user_id_by_identifier(discord_user_id)\n        assert linked_user_id == user_id, \"User should be linked\"\n        \n        # Verify pending operation was cleared\n        assert discord_user_id not in _pending_link_operations, \"Should clear pending operation\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_handle_link_account_rejects_invalid_code(self, test_data_dir):\n        \"\"\"Test: Link account rejects invalid confirmation code.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user\n        existing_username = 'linkinvaliduser'\n        user_id = TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n        \n        # Set up pending operation\n        discord_user_id = \"555666777888999000\"\n        _pending_link_operations[discord_user_id] = {\n            'operation_type': 'link',\n            'username': existing_username,\n            'user_id': user_id,\n            'confirmation_code': '123456',\n            'channel_type': 'discord'\n        }\n        \n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={\n                'username': existing_username,\n                'confirmation_code': '999999',  # Wrong code\n                'channel_identifier': discord_user_id,\n                'channel_type': 'discord'\n            },\n            confidence=0.9,\n            original_message='link account'\n        )\n        \n        response = handler.handle(discord_user_id, parsed_command)\n        \n        # Assert: Should reject invalid code\n        assert response.completed is False, \"Should not complete with invalid code\"\n        assert 'invalid' in response.message.lower(), \"Should indicate invalid code\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.no_parallel\n    def test_username_exists_checks_existing_username(self, test_data_dir):\n        \"\"\"Test: Username exists check finds existing username.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create user with known username\n        existing_username = 'existscheckuser'\n        TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n        \n        # Rebuild user index to ensure user is discoverable\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        import time\n        time.sleep(0.1)  # Brief delay for index to be written\n        \n        # Test username exists\n        assert handler._username_exists(existing_username), \"Should find existing username\"\n        assert handler._username_exists(existing_username.upper()), \"Should be case-insensitive\"\n        assert not handler._username_exists('nonexistentuser123'), \"Should not find nonexistent username\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.no_parallel\n    def test_get_user_id_by_username_returns_correct_id(self, test_data_dir):\n        \"\"\"Test: Get user ID by username returns correct user ID.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create user with known username\n        existing_username = 'getiduser'\n        TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n        \n        # Rebuild user index to ensure user is discoverable\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        import time\n        time.sleep(0.1)  # Brief delay for index to be written\n        \n        # Get actual user ID\n        user_id = get_user_id_by_identifier(existing_username)\n        assert user_id is not None, \"User should be created\"\n        \n        # Test get user ID\n        found_id = handler._get_user_id_by_username(existing_username)\n        assert found_id == user_id, \"Should return correct user ID\"\n        \n        found_id_upper = handler._get_user_id_by_username(existing_username.upper())\n        assert found_id_upper == user_id, \"Should be case-insensitive\"\n        \n        found_id_nonexistent = handler._get_user_id_by_username('nonexistentuser123')\n        assert found_id_nonexistent is None, \"Should return None for nonexistent username\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    def test_get_help_returns_help_text(self):\n        \"\"\"Test: Get help returns help text.\"\"\"\n        handler = AccountManagementHandler()\n        \n        help_text = handler.get_help()\n        assert isinstance(help_text, str), \"Should return string\"\n        assert len(help_text) > 0, \"Should return non-empty help text\"\n        assert 'account' in help_text.lower(), \"Should mention account\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    def test_get_examples_returns_example_commands(self):\n        \"\"\"Test: Get examples returns example commands.\"\"\"\n        handler = AccountManagementHandler()\n        \n        examples = handler.get_examples()\n        assert isinstance(examples, list), \"Should return list\"\n        assert len(examples) > 0, \"Should return examples\"\n        assert 'create account' in examples, \"Should include create account\"\n        assert 'link account' in examples, \"Should include link account\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    def test_generate_confirmation_code_creates_6_digit_code(self):\n        \"\"\"Test: Generate confirmation code creates 6-digit code.\"\"\"\n        code = _generate_confirmation_code()\n        \n        assert isinstance(code, str), \"Should return string\"\n        assert len(code) == 6, \"Should be 6 digits\"\n        assert code.isdigit(), \"Should contain only digits\"\n        \n        # Generate multiple codes to ensure randomness\n        codes = [_generate_confirmation_code() for _ in range(10)]\n        assert len(set(codes)) > 1, \"Should generate different codes\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_send_confirmation_code_with_email(self, test_data_dir):\n        \"\"\"Test: Send confirmation code attempts to send when user has email.\"\"\"\n        # Create user with email\n        TestUserFactory.create_basic_user('emailtestuser', test_data_dir=test_data_dir)\n        user_id = get_user_id_by_identifier('emailtestuser')\n        assert user_id is not None, \"User should be created\"\n        \n        from core.user_data_handlers import update_user_account\n        update_user_account(user_id, {'email': 'test@example.com'})\n        \n        # Mock the entire communication manager import path\n        # Since get_communication_manager doesn't exist, we'll patch the import inside the function\n        with patch('communication.command_handlers.account_handler.get_communication_manager', create=True) as mock_get_manager:\n            mock_manager = MagicMock()\n            # Mock both async and sync paths\n            mock_manager.send_message = AsyncMock(return_value=True)\n            mock_manager.send_message_sync.return_value = True\n            mock_get_manager.return_value = mock_manager\n            \n            result = _send_confirmation_code(user_id, '123456', 'discord', 'test_discord_id')\n        \n        # Assert: Should attempt to send (may fail if comm manager not available, but should try)\n        # The function will return False if comm manager is None, True if it succeeds\n        assert isinstance(result, bool), \"Should return boolean result\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_send_confirmation_code_without_email(self, test_data_dir):\n        \"\"\"Test: Send confirmation code fails when user has no email.\"\"\"\n        # Create user without email\n        user_id = TestUserFactory.create_basic_user('noemailuser', test_data_dir=test_data_dir)\n        \n        result = _send_confirmation_code(user_id, '123456', 'discord', 'test_discord_id')\n        \n        # Assert: Should fail without email\n        assert result is False, \"Should fail without email\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_link_account_with_email_channel(self, test_data_dir):\n        \"\"\"Test: Link account works with email channel type.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user with different email (to test linking)\n        existing_username = 'emaillinkuser'\n        user_id = TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n\n        # Rebuild user index to ensure user is discoverable\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        import time\n        time.sleep(0.1)  # Brief delay for index to be written\n\n        # Add different email to user account (not the one we're linking)\n        from core.user_data_handlers import update_user_account\n        update_user_account(user_id, {'email': 'existing@example.com'})\n\n        # Clear pending operations\n        _pending_link_operations.clear()\n\n        # Try to link to new email (different from existing)\n        email_address = 'linktest@example.com'\n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={\n                'username': existing_username,\n                'channel_identifier': email_address,\n                'channel_type': 'email'\n            },\n            confidence=0.9,\n            original_message='link account'\n        )\n\n        response = handler.handle(email_address, parsed_command)\n\n        # Assert: Should reject if email already linked (or proceed if not)\n        # The handler checks if email is already linked to a different account\n        assert response.completed is False, \"Should not complete immediately\"\n        # May reject if already linked, or proceed to confirmation code\n        assert 'already linked' in response.message.lower() or 'confirmation code' in response.message.lower(), \"Should indicate status\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_link_account_with_already_linked_discord(self, test_data_dir):\n        \"\"\"Test: Link account rejects when Discord account already linked to different user.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user with different Discord ID and email\n        existing_username = 'alreadylinkeduser'\n        user_id = TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n\n        # Rebuild user index to ensure user is discoverable\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        import time\n        time.sleep(0.1)  # Brief delay for index to be written\n\n        # Link to different Discord ID and add email (needed for confirmation code)\n        from core.user_data_handlers import update_user_account\n        update_user_account(user_id, {\n            'discord_user_id': '999888777666555444',\n            'email': 'test@example.com'\n        })\n\n        # Try to link to new Discord ID (different from existing)\n        new_discord_id = \"111222333444555666\"\n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={\n                'username': existing_username,\n                'channel_identifier': new_discord_id,\n                'channel_type': 'discord'\n            },\n            confidence=0.9,\n            original_message='link account'\n        )\n\n        # The handler checks if Discord ID is already linked to a different account\n        # However, if the existing Discord ID is the same as the one being linked, it allows it\n        # In this test, we're linking a different Discord ID, so it should check\n        # But the actual behavior may allow linking if the check doesn't match exactly\n        response = handler.handle(new_discord_id, parsed_command)\n\n        # Assert: The handler should either reject or proceed based on the check\n        # If it proceeds, it will try to send confirmation code (which may fail without email)\n        # If it rejects, it will show \"already linked\" message\n        assert response.completed is False, \"Should not complete immediately\"\n        # The response may be \"already linked\" or \"confirmation code sent\" depending on implementation\n        # Both are valid behaviors - the important thing is it doesn't complete without verification\n        assert 'already linked' in response.message.lower() or 'confirmation code' in response.message.lower() or 'could not send' in response.message.lower(), f\"Should indicate status, got: {response.message}\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_link_account_with_already_linked_email(self, test_data_dir):\n        \"\"\"Test: Link account rejects when email already linked to different user.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user with different email\n        existing_username = 'alreadylinkedemailuser'\n        user_id = TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n\n        # Rebuild user index to ensure user is discoverable\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        import time\n        time.sleep(0.1)  # Brief delay for index to be written\n\n        # Link to different email\n        from core.user_data_handlers import update_user_account\n        update_user_account(user_id, {'email': 'existing@example.com'})\n\n        # Try to link to new email\n        new_email = 'newemail@example.com'\n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={\n                'username': existing_username,\n                'channel_identifier': new_email,\n                'channel_type': 'email'\n            },\n            confidence=0.9,\n            original_message='link account'\n        )\n\n        response = handler.handle(new_email, parsed_command)\n\n        # Assert: Should reject already linked account\n        assert response.completed is False, \"Should not complete with already linked account\"\n        assert 'already linked' in response.message.lower(), \"Should indicate already linked\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_link_account_with_invalid_pending_operation(self, test_data_dir):\n        \"\"\"Test: Link account rejects when pending operation doesn't match.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user\n        existing_username = 'invalidpendinguser'\n        user_id = TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n\n        # Rebuild user index to ensure user is discoverable\n        from core.user_data_manager import rebuild_user_index\n        rebuild_user_index()\n        import time\n        time.sleep(0.1)  # Brief delay for index to be written\n\n        # Set up wrong pending operation\n        discord_user_id = \"666777888999000111\"\n        _pending_link_operations[discord_user_id] = {\n            'operation_type': 'wrong_type',  # Wrong type\n            'username': 'wronguser',\n            'user_id': 'wrong_id',\n            'confirmation_code': '123456',\n            'channel_type': 'discord'\n        }\n\n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={\n                'username': existing_username,\n                'confirmation_code': '123456',\n                'channel_identifier': discord_user_id,\n                'channel_type': 'discord'\n            },\n            confidence=0.9,\n            original_message='link account'\n        )\n\n        response = handler.handle(discord_user_id, parsed_command)\n\n        # Assert: Should reject invalid pending operation\n        assert response.completed is False, \"Should not complete with invalid pending operation\"\n        assert 'no pending' in response.message.lower() or 'start over' in response.message.lower(), \"Should indicate need to start over\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    @pytest.mark.no_parallel\n    def test_handle_create_account_with_email_channel(self, test_data_dir):\n        \"\"\"Test: Create account works with email channel type.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Use a unique email address to avoid conflicts\n        email_address = 'newemailuser@example.com'\n        \n        parsed_command = ParsedCommand(\n            intent='create_account',\n            entities={\n                'username': 'newemailuser',\n                'channel_identifier': email_address,\n                'channel_type': 'email'\n            },\n            confidence=0.9,\n            original_message='create account'\n        )\n        \n        response = handler.handle(email_address, parsed_command)\n        \n        # Assert: Should create account successfully\n        assert response.completed is True, \"Should complete account creation\"\n        assert 'Account created successfully' in response.message, \"Should indicate success\"\n        \n        # Verify user was actually created\n        created_user_id = get_user_id_by_identifier(email_address)\n        assert created_user_id is not None, \"User should be created\"\n        \n        # Verify email was set\n        user_data = get_user_data(created_user_id, 'account')\n        account_data = user_data.get('account', {})\n        assert account_data.get('email') == email_address, \"Email should be set\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_handle_create_account_handles_creation_failure(self, test_data_dir):\n        \"\"\"Test: Create account handles creation failure gracefully.\"\"\"\n        handler = AccountManagementHandler()\n        \n        parsed_command = ParsedCommand(\n            intent='create_account',\n            entities={\n                'username': 'failuser',\n                'channel_identifier': 'fail@example.com',\n                'channel_type': 'email'\n            },\n            confidence=0.9,\n            original_message='create account'\n        )\n        \n        # Mock create_new_user to return None (failure)\n        with patch('communication.command_handlers.account_handler.create_new_user') as mock_create:\n            mock_create.return_value = None\n            response = handler.handle('fail@example.com', parsed_command)\n        \n        # Assert: Should handle failure gracefully\n        assert response.completed is False, \"Should not complete on creation failure\"\n        assert 'failed' in response.message.lower() or 'error' in response.message.lower(), \"Should indicate failure\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_handle_link_account_handles_index_update_failure(self, test_data_dir):\n        \"\"\"Test: Link account handles index update failure gracefully.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user with email\n        existing_username = 'indexfailuser'\n        user_id = TestUserFactory.create_basic_user(existing_username, test_data_dir=test_data_dir)\n        \n        # Add email to user account\n        from core.user_data_handlers import update_user_account\n        update_user_account(user_id, {'email': 'test@example.com'})\n        \n        # Set up pending operation\n        discord_user_id = \"777888999000111222\"\n        confirmation_code = '123456'\n        _pending_link_operations[discord_user_id] = {\n            'operation_type': 'link',\n            'username': existing_username,\n            'user_id': user_id,\n            'confirmation_code': confirmation_code,\n            'channel_type': 'discord'\n        }\n        \n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={\n                'username': existing_username,\n                'confirmation_code': confirmation_code,\n                'channel_identifier': discord_user_id,\n                'channel_type': 'discord'\n            },\n            confidence=0.9,\n            original_message='link account'\n        )\n        \n        # Mock update_user_account to succeed, but update_user_index to raise exception\n        with patch('communication.command_handlers.account_handler.update_user_index') as mock_index:\n            mock_index.side_effect = Exception(\"Index update failed\")\n            with patch('communication.command_handlers.account_handler._send_confirmation_code'):\n                # Mock update_user_account to return True (linking succeeds)\n                with patch('communication.command_handlers.account_handler.update_user_account') as mock_update:\n                    mock_update.return_value = True\n                    response = handler.handle(discord_user_id, parsed_command)\n        \n        # Assert: Should still complete linking (index failure is non-critical, but update_user_account failure is)\n        # The actual implementation may fail if update_user_account fails, which is expected\n        assert isinstance(response.completed, bool), \"Should return boolean completion status\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    @pytest.mark.file_io\n    def test_handle_link_account_with_same_discord_id(self, test_data_dir):\n        \"\"\"Test: Link account allows linking when Discord ID matches existing link.\"\"\"\n        handler = AccountManagementHandler()\n        \n        # Create existing user with Discord ID\n        existing_username = 'samediscorduser'\n        discord_user_id = \"888999000111222333\"\n        user_id = TestUserFactory.create_discord_user(discord_user_id, test_data_dir=test_data_dir)\n        \n        # Add email to user account\n        from core.user_data_handlers import update_user_account\n        update_user_account(user_id, {'email': 'test@example.com'})\n        \n        # Try to link same Discord ID (should work - same user)\n        parsed_command = ParsedCommand(\n            intent='link_account',\n            entities={\n                'username': existing_username,\n                'channel_identifier': discord_user_id,\n                'channel_type': 'discord'\n            },\n            confidence=0.9,\n            original_message='link account'\n        )\n        \n        # Should proceed to confirmation code step (not reject)\n        with patch('communication.command_handlers.account_handler._send_confirmation_code') as mock_send:\n            mock_send.return_value = True\n            response = handler.handle(discord_user_id, parsed_command)\n        \n        # Assert: Should proceed (same Discord ID is allowed)\n        # The handler should check username match, not just Discord ID\n        assert response is not None, \"Should return response\"\n        assert isinstance(response.message, str), \"Should return string message\"\n    \n    @pytest.mark.behavior\n    @pytest.mark.communication\n    def test_handle_routes_to_unknown_intent(self):\n        \"\"\"Test: Handler routes unknown intent to error message.\"\"\"\n        handler = AccountManagementHandler()\n        \n        parsed_command = ParsedCommand(\n            intent='unknown_intent',\n            entities={},\n            confidence=0.9,\n            original_message='unknown command'\n        )\n        \n        response = handler.handle('test_user_id', parsed_command)\n        \n        # Assert: Should return error message\n        assert response.completed is True, \"Should complete with error message\"\n        assert 'don\\'t understand' in response.message.lower() or 'try' in response.message.lower(), \"Should provide helpful error\"\n        assert len(response.message) > 0, \"Should return non-empty message\"\n\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 173,
                    "line_content": "# Create account without feature selection parameters (backward compatibility)",
                    "start": 7525,
                    "end": 7547
                  }
                ]
              ],
              [
                "tests\\development_tools\\test_config.py",
                "\"\"\"\nTests for development_tools configuration module.\n\nTests configuration validation, key settings existence, and helper functions.\n\"\"\"\n\nimport pytest\nimport importlib.util\nimport sys\nfrom pathlib import Path\n\n# Import using importlib to handle path issues\nproject_root = Path(__file__).parent.parent.parent\n# Load config through the package system to get proper exports\nconfig_path = project_root / \"development_tools\" / \"config\" / \"__init__.py\"\nif not config_path.exists():\n    # Fallback to config.py if __init__.py doesn't exist\n    config_path = project_root / \"development_tools\" / \"config\" / \"config.py\"\nspec = importlib.util.spec_from_file_location(\"development_tools.config\", config_path)\nconfig = importlib.util.module_from_spec(spec)\nsys.modules[\"development_tools.config\"] = config\nspec.loader.exec_module(config)\n# Also ensure the actual config module is loaded\nif config_path.name == \"__init__.py\":\n    config_config_path = project_root / \"development_tools\" / \"config\" / \"config.py\"\n    if config_config_path.exists() and \"development_tools.config.config\" not in sys.modules:\n        config_config_spec = importlib.util.spec_from_file_location(\"development_tools.config.config\", config_config_path)\n        config_config_module = importlib.util.module_from_spec(config_config_spec)\n        sys.modules[\"development_tools.config.config\"] = config_config_module\n        config_config_spec.loader.exec_module(config_config_module)\n\n\nclass TestConfigKeySettings:\n    \"\"\"Test that key configuration settings exist and have expected structure.\"\"\"\n\n    @pytest.mark.unit\n    @pytest.mark.critical\n    def test_project_root_exists(self):\n        \"\"\"Test that PROJECT_ROOT is defined.\"\"\"\n        assert hasattr(config, 'PROJECT_ROOT')\n        assert isinstance(config.PROJECT_ROOT, str)\n\n    @pytest.mark.unit\n    def test_scan_directories_exists(self):\n        \"\"\"Test that SCAN_DIRECTORIES is defined and is a list.\"\"\"\n        assert hasattr(config, 'SCAN_DIRECTORIES')\n        assert isinstance(config.SCAN_DIRECTORIES, list)\n        # SCAN_DIRECTORIES may be empty by default (requires external config)\n        # Try to load external config and check get_scan_directories()\n        config.load_external_config()\n        scan_dirs = config.get_scan_directories()\n        assert isinstance(scan_dirs, list)\n        # If external config exists, scan_dirs should be populated\n        # If not, it's OK for it to be empty (project-specific config required)\n        if len(scan_dirs) == 0:\n            # Check if external config file exists - if it does, it should have scan_directories\n            import os\n            project_root = config.get_project_root()\n            # Check both new location and old location for backward compatibility\n            config_file = project_root / 'development_tools' / 'config' / 'development_tools_config.json'\n            if not config_file.exists():\n                config_file = project_root / 'development_tools_config.json'\n            if config_file.exists():\n                # Config file exists but scan_dirs is empty - this is a configuration issue\n                pytest.skip(\"External config file exists but scan_directories is empty (configuration issue)\")\n\n    @pytest.mark.unit\n    def test_ai_collaboration_config_exists(self):\n        \"\"\"Test that AI_COLLABORATION config exists and has expected keys.\"\"\"\n        assert hasattr(config, 'AI_COLLABORATION')\n        assert isinstance(config.AI_COLLABORATION, dict)\n        expected_keys = ['concise_output', 'detailed_files', 'actionable_insights', \n                        'context_aware', 'priority_issues', 'integration_mode']\n        for key in expected_keys:\n            assert key in config.AI_COLLABORATION\n\n    @pytest.mark.unit\n    def test_analyze_functions_config_exists(self):\n        \"\"\"Test that FUNCTION_DISCOVERY config exists and has expected keys.\"\"\"\n        assert hasattr(config, 'FUNCTION_DISCOVERY')\n        assert isinstance(config.FUNCTION_DISCOVERY, dict)\n        expected_keys = ['moderate_complexity_threshold', 'high_complexity_threshold',\n                        'critical_complexity_threshold', 'min_docstring_length',\n                        'handler_keywords', 'test_keywords', 'critical_functions']\n        for key in expected_keys:\n            assert key in config.FUNCTION_DISCOVERY\n\n    @pytest.mark.unit\n    def test_validation_config_exists(self):\n        \"\"\"Test that VALIDATION config exists and has expected keys.\"\"\"\n        assert hasattr(config, 'VALIDATION')\n        assert isinstance(config.VALIDATION, dict)\n        expected_keys = ['documentation_coverage_threshold', 'moderate_complexity_warning',\n                        'high_complexity_warning', 'critical_complexity_warning',\n                        'duplicate_function_warning', 'missing_docstring_warning',\n                        'critical_issues_first']\n        for key in expected_keys:\n            assert key in config.VALIDATION\n\n    @pytest.mark.unit\n    def test_audit_config_exists(self):\n        \"\"\"Test that AUDIT config exists and has expected keys.\"\"\"\n        assert hasattr(config, 'AUDIT')\n        assert isinstance(config.AUDIT, dict)\n        expected_keys = ['include_generated_files', 'include_test_files',\n                        'include_legacy_files', 'max_output_lines',\n                        'save_detailed_results', 'generate_summary', 'highlight_issues']\n        for key in expected_keys:\n            assert key in config.AUDIT\n\n    @pytest.mark.unit\n    def test_output_config_exists(self):\n        \"\"\"Test that OUTPUT config exists and has expected keys.\"\"\"\n        assert hasattr(config, 'OUTPUT')\n        assert isinstance(config.OUTPUT, dict)\n        expected_keys = ['use_emojis', 'show_progress', 'color_output',\n                        'detailed_reports', 'concise_format', 'priority_indicators',\n                        'action_items']\n        for key in expected_keys:\n            assert key in config.OUTPUT\n\n    @pytest.mark.unit\n    def test_workflow_config_exists(self):\n        \"\"\"Test that WORKFLOW config exists.\"\"\"\n        assert hasattr(config, 'WORKFLOW')\n        assert isinstance(config.WORKFLOW, dict)\n\n    @pytest.mark.unit\n    def test_documentation_config_exists(self):\n        \"\"\"Test that DOCUMENTATION config exists.\"\"\"\n        assert hasattr(config, 'DOCUMENTATION')\n        assert isinstance(config.DOCUMENTATION, dict)\n\n    @pytest.mark.unit\n    def test_quick_audit_config_exists(self):\n        \"\"\"Test that QUICK_AUDIT config exists.\"\"\"\n        assert hasattr(config, 'QUICK_AUDIT')\n        assert isinstance(config.QUICK_AUDIT, dict)\n\n    @pytest.mark.unit\n    def test_fix_version_sync_config_exists(self):\n        \"\"\"Test that VERSION_SYNC config exists.\"\"\"\n        assert hasattr(config, 'VERSION_SYNC')\n        assert isinstance(config.VERSION_SYNC, dict)\n\n\nclass TestConfigHelperFunctions:\n    \"\"\"Test that helper functions return expected types and structures.\"\"\"\n\n    @pytest.mark.unit\n    def test_get_project_root_returns_path(self):\n        \"\"\"Test that get_project_root() returns a Path object.\"\"\"\n        result = config.get_project_root()\n        assert isinstance(result, Path)\n\n    @pytest.mark.unit\n    def test_get_scan_directories_returns_list(self):\n        \"\"\"Test that get_scan_directories() returns a list.\"\"\"\n        # Load external config to ensure we get the actual configured directories\n        config.load_external_config()\n        result = config.get_scan_directories()\n        assert isinstance(result, list)\n        # If external config exists, scan_dirs should be populated\n        # If not, it's OK for it to be empty (project-specific config required)\n        if len(result) == 0:\n            # Check if external config file exists - if it does, it should have scan_directories\n            import os\n            project_root = config.get_project_root()\n            # Check both new location and old location for backward compatibility\n            config_file = project_root / 'development_tools' / 'config' / 'development_tools_config.json'\n            if not config_file.exists():\n                config_file = project_root / 'development_tools_config.json'\n            if config_file.exists():\n                # Config file exists but scan_dirs is empty - this is a configuration issue\n                pytest.skip(\"External config file exists but scan_directories is empty (configuration issue)\")\n            # If config file doesn't exist, empty list is expected (default behavior)\n\n    @pytest.mark.unit\n    def test_get_analyze_functions_config_returns_dict(self):\n        \"\"\"Test that get_analyze_functions_config() returns a dict.\"\"\"\n        result = config.get_analyze_functions_config()\n        assert isinstance(result, dict)\n        assert 'moderate_complexity_threshold' in result\n\n    @pytest.mark.unit\n    def test_get_validation_config_returns_dict(self):\n        \"\"\"Test that get_validation_config() returns a dict.\"\"\"\n        result = config.get_validation_config()\n        assert isinstance(result, dict)\n        assert 'documentation_coverage_threshold' in result\n\n    @pytest.mark.unit\n    def test_get_audit_config_returns_dict(self):\n        \"\"\"Test that get_audit_config() returns a dict.\"\"\"\n        result = config.get_audit_config()\n        assert isinstance(result, dict)\n        assert 'include_test_files' in result\n\n    @pytest.mark.unit\n    def test_get_output_config_returns_dict(self):\n        \"\"\"Test that get_output_config() returns a dict.\"\"\"\n        result = config.get_output_config()\n        assert isinstance(result, dict)\n        assert 'concise_format' in result\n\n    @pytest.mark.unit\n    def test_get_workflow_config_returns_dict(self):\n        \"\"\"Test that get_workflow_config() returns a dict.\"\"\"\n        result = config.get_workflow_config()\n        assert isinstance(result, dict)\n\n    @pytest.mark.unit\n    def test_get_documentation_config_returns_dict(self):\n        \"\"\"Test that get_documentation_config() returns a dict.\"\"\"\n        result = config.get_documentation_config()\n        assert isinstance(result, dict)\n\n    @pytest.mark.unit\n    def test_get_auto_document_config_returns_dict(self):\n        \"\"\"Test that get_auto_document_config() returns a dict.\"\"\"\n        result = config.get_auto_document_config()\n        assert isinstance(result, dict)\n\n    @pytest.mark.unit\n    def test_get_ai_validation_config_returns_dict(self):\n        \"\"\"Test that get_ai_validation_config() returns a dict.\"\"\"\n        result = config.get_ai_validation_config()\n        assert isinstance(result, dict)\n\n    @pytest.mark.unit\n    def test_get_ai_collaboration_config_returns_dict(self):\n        \"\"\"Test that get_ai_collaboration_config() returns a dict.\"\"\"\n        result = config.get_ai_collaboration_config()\n        assert isinstance(result, dict)\n\n    @pytest.mark.unit\n    def test_get_quick_audit_config_returns_dict(self):\n        \"\"\"Test that get_quick_audit_config() returns a dict.\"\"\"\n        result = config.get_quick_audit_config()\n        assert isinstance(result, dict)\n\n    @pytest.mark.unit\n    def test_get_fix_version_sync_config_returns_dict(self):\n        \"\"\"Test that get_fix_version_sync_config() returns a dict.\"\"\"\n        result = config.get_fix_version_sync_config()\n        assert isinstance(result, dict)\n\n\nclass TestConfigPaths:\n    \"\"\"Test that paths used in tests/fixtures are valid.\"\"\"\n\n    @pytest.mark.unit\n    def test_project_root_path_exists(self):\n        \"\"\"Test that project root path exists.\"\"\"\n        project_root = config.get_project_root()\n        # Resolve relative to current working directory\n        if not project_root.is_absolute():\n            # If relative, check if it exists relative to the test location\n            # Tests run from project root, so relative paths should work\n            assert True  # Path validation is context-dependent\n        else:\n            assert project_root.exists()\n\n    @pytest.mark.unit\n    def test_scan_directories_are_valid_names(self):\n        \"\"\"Test that scan directory names are valid (non-empty strings).\"\"\"\n        # Load external config to ensure we get the actual configured directories\n        config.load_external_config()\n        directories = config.get_scan_directories()\n        # If directories is empty, skip this test (requires external config)\n        if len(directories) == 0:\n            pytest.skip(\"Scan directories is empty (requires external config)\")\n        for directory in directories:\n            assert isinstance(directory, str)\n            assert len(directory) > 0\n            # Should not contain path separators in basic names\n            assert '/' not in directory or directory.startswith('./')\n\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 59,
                    "line_content": "# Check both new location and old location for backward compatibility",
                    "start": 2726,
                    "end": 2748
                  },
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 169,
                    "line_content": "# Check both new location and old location for backward compatibility",
                    "start": 7919,
                    "end": 7941
                  }
                ]
              ],
              [
                "tests\\ui\\test_signal_handler_integration.py",
                "\"\"\"\nSignal handler integration tests for UI components.\n\nThese tests verify that Qt signal handlers are correctly connected and can\naccept the parameters that signals emit. This catches signature mismatches\nthat would otherwise only appear at runtime.\n\nTests cover:\n- Signal handler parameter acceptance\n- Signal connection integrity\n- Import availability for signal-related classes\n- Real signal emission (not just direct method calls)\n\"\"\"\n# Import ensure_qt_runtime - use relative import since we're in tests/ui/\nimport sys\nimport pytest\nfrom pathlib import Path\n\n# Add tests directory to path if needed\ntests_dir = Path(__file__).parent.parent\nif str(tests_dir) not in sys.path:\n    sys.path.insert(0, str(tests_dir))\n\ntry:\n    from conftest import ensure_qt_runtime\n    ensure_qt_runtime()\nexcept ImportError:\n    # Fallback: try absolute import\n    try:\n        from tests.conftest import ensure_qt_runtime\n        ensure_qt_runtime()\n    except ImportError:\n        # If both fail, try to import Qt directly\n        try:\n            from PySide6 import QtWidgets  # noqa: F401\n            from PySide6.QtWidgets import QApplication  # noqa: F401\n        except (ImportError, OSError):\n            pytest.skip(\"Qt runtime not available\", allow_module_level=True)\n\nfrom unittest.mock import patch, MagicMock\nfrom PySide6.QtWidgets import QApplication, QLineEdit\nfrom PySide6.QtCore import Qt\nfrom PySide6.QtTest import QTest\n\n\n@pytest.fixture(scope=\"function\")\ndef qapp():\n    \"\"\"Create QApplication instance for UI testing.\n    \n    Changed from session scope to function scope to prevent Qt object accumulation\n    across tests, which was causing memory leaks in batch runs.\n    \"\"\"\n    app = QApplication.instance()\n    if app is None:\n        app = QApplication([])\n    yield app\n    # Cleanup: Process any pending events and ensure widgets are destroyed\n    app.processEvents()\n    # Note: We don't quit the app here because other tests might need it\n    # The app will be cleaned up when the process exits\n\n\nclass TestAccountCreatorDialogSignalHandlers:\n    \"\"\"Test signal handlers in AccountCreatorDialog accept correct parameters.\"\"\"\n    \n    @pytest.fixture(autouse=True)\n    def stop_channel_monitor_threads(self):\n        \"\"\"Stop any running channel monitor threads to prevent crashes during UI tests.\"\"\"\n        from unittest.mock import patch\n        \n        # Stop any existing channel monitor threads before test\n        try:\n            from communication.core.channel_orchestrator import CommunicationManager\n            # If singleton exists, stop its channel monitor and other background threads\n            if CommunicationManager._instance is not None:\n                instance = CommunicationManager._instance\n                if hasattr(instance, 'channel_monitor'):\n                    instance.channel_monitor.stop_restart_monitor()\n                # Stop email polling loop if it exists\n                if hasattr(instance, '_email_polling_thread') and instance._email_polling_thread:\n                    if hasattr(instance, 'stop_all__stop_email_polling'):\n                        instance.stop_all__stop_email_polling()\n                # Stop retry manager if it exists\n                if hasattr(instance, 'retry_manager') and hasattr(instance.retry_manager, 'stop_retry_thread'):\n                    instance.retry_manager.stop_retry_thread()\n        except Exception:\n            # Ignore errors - components might not exist\n            pass\n        \n        # Patch all thread starters to prevent new threads from starting\n        with patch('communication.core.channel_monitor.ChannelMonitor.start_restart_monitor'), \\\n             patch('communication.core.channel_orchestrator.CommunicationManager.start_all__start_email_polling'), \\\n             patch('communication.core.retry_manager.RetryManager.start_retry_thread'):\n            yield\n        \n        # Cleanup after test\n        try:\n            if CommunicationManager._instance is not None:\n                instance = CommunicationManager._instance\n                if hasattr(instance, 'channel_monitor'):\n                    instance.channel_monitor.stop_restart_monitor()\n                if hasattr(instance, '_email_polling_thread') and instance._email_polling_thread:\n                    if hasattr(instance, 'stop_all__stop_email_polling'):\n                        instance.stop_all__stop_email_polling()\n                if hasattr(instance, 'retry_manager') and hasattr(instance.retry_manager, 'stop_retry_thread'):\n                    instance.retry_manager.stop_retry_thread()\n        except Exception:\n            pass\n    \n    @pytest.fixture\n    def dialog(self, qapp, test_data_dir, mock_config):\n        \"\"\"Create account creation dialog for testing.\"\"\"\n        from ui.dialogs.account_creator_dialog import AccountCreatorDialog\n        from unittest.mock import Mock, patch\n        \n        # Patch channel monitor to prevent threads from starting\n        with patch('communication.core.channel_monitor.ChannelMonitor.start_restart_monitor'):\n            # Create a mock communication manager\n            mock_comm_manager = Mock()\n            mock_comm_manager.get_active_channels.return_value = ['email', 'discord']\n            \n            dialog = AccountCreatorDialog(parent=None, communication_manager=mock_comm_manager)\n            yield dialog\n            dialog.close()\n            dialog.deleteLater()\n    \n    @pytest.mark.ui\n    @pytest.mark.user_management\n    @pytest.mark.regression\n    @pytest.mark.critical\n    def test_username_textChanged_signal_handler_accepts_parameter(self, dialog):\n        \"\"\"Test that on_username_changed accepts text parameter from textChanged signal.\n        \n        This test would catch the signature mismatch where the handler only\n        accepted 'self' but the signal emits a text parameter.\n        \"\"\"\n        # Arrange - Get the line edit widget\n        username_edit = dialog.ui.lineEdit_username\n        assert username_edit is not None, \"Username line edit should exist\"\n        \n        # Verify the handler can accept the parameter that textChanged signal emits\n        # textChanged signal emits (str) - the new text\n        # This would fail with TypeError if signature was wrong (e.g., only accepts self)\n        try:\n            # Simulate what the signal would do - call handler with text parameter\n            dialog.on_username_changed(\"testtext\")\n        except TypeError as e:\n            if \"takes\" in str(e) and \"positional argument\" in str(e):\n                pytest.fail(\n                    f\"Handler signature mismatch: on_username_changed should accept text parameter \"\n                    f\"from textChanged signal, but got TypeError: {e}\"\n                )\n            raise\n        \n        # Also verify it works when called with no parameter (backward compatibility)\n        try:\n            dialog.on_username_changed()\n        except TypeError as e:\n            pytest.fail(f\"Handler should also work with no parameter (default), but got TypeError: {e}\")\n        \n        # Verify signal is actually connected and works\n        initial_username = dialog.username\n        username_edit.setText(\"newusername\")\n        QApplication.processEvents()\n        # The handler should have updated the username\n        assert dialog.username == \"newusername\", \"Signal handler should update username when text changes\"\n    \n    @pytest.mark.ui\n    @pytest.mark.user_management\n    @pytest.mark.regression\n    @pytest.mark.critical\n    def test_preferred_name_textChanged_signal_handler_accepts_parameter(self, dialog):\n        \"\"\"Test that on_preferred_name_changed accepts text parameter from textChanged signal.\"\"\"\n        # Arrange\n        preferred_name_edit = dialog.ui.lineEdit_preferred_name\n        assert preferred_name_edit is not None, \"Preferred name line edit should exist\"\n        \n        # Verify handler accepts parameter that textChanged signal emits\n        try:\n            dialog.on_preferred_name_changed(\"testname\")\n        except TypeError as e:\n            if \"takes\" in str(e) and \"positional argument\" in str(e):\n                pytest.fail(\n                    f\"Handler signature mismatch: on_preferred_name_changed should accept text parameter \"\n                    f\"from textChanged signal, but got TypeError: {e}\"\n                )\n            raise\n        \n        # Verify it works with no parameter too\n        try:\n            dialog.on_preferred_name_changed()\n        except TypeError as e:\n            pytest.fail(f\"Handler should also work with no parameter (default), but got TypeError: {e}\")\n        \n        # Verify signal connection works\n        preferred_name_edit.setText(\"New Name\")\n        QApplication.processEvents()\n        assert dialog.preferred_name == \"New Name\", \"Signal handler should update preferred name\"\n    \n    @pytest.mark.ui\n    @pytest.mark.user_management\n    @pytest.mark.regression\n    def test_account_creator_dialog_imports_available(self, dialog):\n        \"\"\"Test that all required PySide6 imports are available.\n        \n        This catches missing imports like QSizePolicy, QDialogButtonBox, Qt\n        that are used in setup methods.\n        \"\"\"\n        # Test that setup methods can run without NameError\n        try:\n            # These methods use QSizePolicy, QDialogButtonBox, Qt\n            dialog.setup_feature_group_boxes()\n            dialog.setup_connections()\n        except NameError as e:\n            pytest.fail(f\"Missing import detected: {e}\")\n        except Exception as e:\n            # Other exceptions are OK (e.g., widget not found), but NameError indicates missing import\n            if \"not defined\" in str(e) or \"NameError\" in str(type(e).__name__):\n                pytest.fail(f\"Missing import detected: {e}\")\n\n\nclass TestDynamicListFieldSignalHandlers:\n    \"\"\"Test signal handlers in DynamicListField accept correct parameters.\"\"\"\n    \n    @pytest.fixture\n    def widget(self, qapp):\n        \"\"\"Create DynamicListField for testing.\"\"\"\n        from ui.widgets.dynamic_list_field import DynamicListField\n        \n        widget = DynamicListField(\n            parent=None,\n            preset_label=\"Test Item\",\n            editable=True,\n            checked=False\n        )\n        yield widget\n        widget.close()\n        widget.deleteLater()\n    \n    @pytest.mark.ui\n    @pytest.mark.regression\n    @pytest.mark.critical\n    def test_textEdited_signal_handler_accepts_parameter(self, widget):\n        \"\"\"Test that on_text_changed accepts text parameter from textEdited signal.\n        \n        This test would catch the signature mismatch where the handler only\n        accepted 'self' but textEdited signal emits a text parameter.\n        \"\"\"\n        # Arrange\n        line_edit = widget.ui.lineEdit_dynamic_list_field\n        assert line_edit is not None, \"Line edit should exist\"\n        \n        # Verify handler accepts parameter that textEdited signal emits\n        # textEdited signal emits (str) - the new text\n        try:\n            widget.on_text_changed(\"testtext\")\n        except TypeError as e:\n            if \"takes\" in str(e) and \"positional argument\" in str(e):\n                pytest.fail(\n                    f\"Handler signature mismatch: on_text_changed should accept text parameter \"\n                    f\"from textEdited signal, but got TypeError: {e}\"\n                )\n            raise\n        \n        # Verify it works with no parameter too\n        try:\n            widget.on_text_changed()\n        except TypeError as e:\n            pytest.fail(f\"Handler should also work with no parameter (default), but got TypeError: {e}\")\n    \n    @pytest.mark.ui\n    @pytest.mark.regression\n    @pytest.mark.critical\n    def test_checkbox_toggled_signal_handler_accepts_parameter(self, widget):\n        \"\"\"Test that on_checkbox_toggled accepts checked parameter from toggled signal.\n        \n        This test would catch the signature mismatch where the handler only\n        accepted 'self' but toggled signal emits a boolean parameter.\n        \"\"\"\n        # Arrange\n        checkbox = widget.ui.checkBox__dynamic_list_field\n        assert checkbox is not None, \"Checkbox should exist\"\n        \n        # Verify handler accepts parameter that toggled signal emits\n        # toggled signal emits (bool) - the new checked state\n        try:\n            widget.on_checkbox_toggled(True)\n            widget.on_checkbox_toggled(False)\n        except TypeError as e:\n            if \"takes\" in str(e) and \"positional argument\" in str(e):\n                pytest.fail(\n                    f\"Handler signature mismatch: on_checkbox_toggled should accept checked parameter \"\n                    f\"from toggled signal, but got TypeError: {e}\"\n                )\n            raise\n        \n        # Verify it works with no parameter too\n        try:\n            widget.on_checkbox_toggled()\n        except TypeError as e:\n            pytest.fail(f\"Handler should also work with no parameter (default), but got TypeError: {e}\")\n\n\nclass TestUISignalConnectionIntegrity:\n    \"\"\"Test that signal connections are properly set up and handlers work.\"\"\"\n    \n    @pytest.fixture(autouse=True)\n    def stop_channel_monitor_threads(self):\n        \"\"\"Stop any running channel monitor threads to prevent crashes during UI tests.\"\"\"\n        from unittest.mock import patch\n        \n        # Stop any existing channel monitor threads before test\n        try:\n            from communication.core.channel_orchestrator import CommunicationManager\n            # If singleton exists, stop its channel monitor and other background threads\n            if CommunicationManager._instance is not None:\n                instance = CommunicationManager._instance\n                if hasattr(instance, 'channel_monitor'):\n                    instance.channel_monitor.stop_restart_monitor()\n                # Stop email polling loop if it exists\n                if hasattr(instance, '_email_polling_thread') and instance._email_polling_thread:\n                    if hasattr(instance, 'stop_all__stop_email_polling'):\n                        instance.stop_all__stop_email_polling()\n                # Stop retry manager if it exists\n                if hasattr(instance, 'retry_manager') and hasattr(instance.retry_manager, 'stop_retry_thread'):\n                    instance.retry_manager.stop_retry_thread()\n        except Exception:\n            # Ignore errors - components might not exist\n            pass\n        \n        # Patch all thread starters to prevent new threads from starting\n        with patch('communication.core.channel_monitor.ChannelMonitor.start_restart_monitor'), \\\n             patch('communication.core.channel_orchestrator.CommunicationManager.start_all__start_email_polling'), \\\n             patch('communication.core.retry_manager.RetryManager.start_retry_thread'):\n            yield\n        \n        # Cleanup after test\n        try:\n            if CommunicationManager._instance is not None:\n                instance = CommunicationManager._instance\n                if hasattr(instance, 'channel_monitor'):\n                    instance.channel_monitor.stop_restart_monitor()\n                if hasattr(instance, '_email_polling_thread') and instance._email_polling_thread:\n                    if hasattr(instance, 'stop_all__stop_email_polling'):\n                        instance.stop_all__stop_email_polling()\n                if hasattr(instance, 'retry_manager') and hasattr(instance.retry_manager, 'stop_retry_thread'):\n                    instance.retry_manager.stop_retry_thread()\n        except Exception:\n            pass\n    \n    @pytest.mark.ui\n    @pytest.mark.regression\n    @pytest.mark.critical\n    @pytest.mark.no_parallel\n    def test_signal_handlers_dont_raise_on_signal_emission(self, qapp, test_data_dir, mock_config):\n        \"\"\"Test that signal handlers don't raise exceptions when signals fire.\n        \n        This is a broader integration test that verifies the actual signal\n        connections work end-to-end without errors being swallowed.\n        \"\"\"\n        from ui.dialogs.account_creator_dialog import AccountCreatorDialog\n        from communication.core.channel_orchestrator import CommunicationManager\n        from unittest.mock import patch\n        \n        # Capture any exceptions that occur during signal handling\n        exceptions_caught = []\n        \n        def exception_handler(exc_type, exc_value, exc_traceback):\n            exceptions_caught.append((exc_type, exc_value))\n        \n        import sys\n        original_excepthook = sys.excepthook\n        sys.excepthook = exception_handler\n        \n        try:\n            from unittest.mock import Mock\n            # Patch channel monitor to prevent threads from starting\n            with patch('communication.core.channel_monitor.ChannelMonitor.start_restart_monitor'):\n                mock_comm_manager = Mock()\n                mock_comm_manager.get_active_channels.return_value = ['email', 'discord']\n                dialog = AccountCreatorDialog(parent=None, communication_manager=mock_comm_manager)\n            \n            # Trigger signals that should call handlers\n            dialog.ui.lineEdit_username.setText(\"testuser\")\n            QApplication.processEvents()\n            \n            dialog.ui.lineEdit_preferred_name.setText(\"Test Name\")\n            QApplication.processEvents()\n            \n            # Check for TypeErrors which indicate signature mismatches\n            type_errors = [e for e in exceptions_caught if e[0] == TypeError]\n            if type_errors:\n                error_messages = [str(e[1]) for e in type_errors]\n                # Filter for signature-related errors\n                signature_errors = [msg for msg in error_messages if \"takes\" in msg and \"positional argument\" in msg]\n                if signature_errors:\n                    pytest.fail(\n                        f\"Signal handlers raised TypeError (likely signature mismatch): {signature_errors}\"\n                    )\n                dialog.close()\n                dialog.deleteLater()\n        finally:\n            sys.excepthook = original_excepthook\n    \n    @pytest.mark.ui\n    @pytest.mark.regression\n    @pytest.mark.no_parallel\n    def test_dynamic_list_field_signals_dont_raise(self, qapp):\n        \"\"\"Test that DynamicListField signal handlers don't raise exceptions.\"\"\"\n        from ui.widgets.dynamic_list_field import DynamicListField\n        \n        exceptions_caught = []\n        \n        def exception_handler(exc_type, exc_value, exc_traceback):\n            exceptions_caught.append((exc_type, exc_value))\n        \n        import sys\n        original_excepthook = sys.excepthook\n        sys.excepthook = exception_handler\n        \n        try:\n            widget = DynamicListField(\n                parent=None,\n                preset_label=\"Test\",\n                editable=True,\n                checked=False\n            )\n            \n            # Trigger signals\n            widget.ui.lineEdit_dynamic_list_field.setText(\"test\")\n            QApplication.processEvents()\n            \n            widget.ui.checkBox__dynamic_list_field.setChecked(True)\n            QApplication.processEvents()\n            \n            # Check for TypeErrors\n            type_errors = [e for e in exceptions_caught if e[0] == TypeError]\n            if type_errors:\n                error_messages = [str(e[1]) for e in type_errors]\n                pytest.fail(\n                    f\"Signal handlers raised TypeError: {error_messages}\"\n                )\n            \n            widget.close()\n            widget.deleteLater()\n        finally:\n            sys.excepthook = original_excepthook\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 155,
                    "line_content": "# Also verify it works when called with no parameter (backward compatibility)",
                    "start": 6836,
                    "end": 6858
                  }
                ]
              ],
              [
                "tests\\data\\tmpneut7le6\\demo_project\\legacy_code.py",
                "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\nNote: This file is in tests/fixtures/ and is excluded from main project scans,\nbut is intentionally scanned in test contexts to verify legacy detection works.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
                [
                  {
                    "pattern": "(?i)backward compatibility",
                    "match": "backward compatibility",
                    "line": 9,
                    "line_content": "# LEGACY COMPATIBILITY: This function is kept for backward compatibility",
                    "start": 353,
                    "end": 375
                  }
                ]
              ]
            ],
            "old_bot_directory": [
              [
                "tests\\data\\tmpneut7le6\\demo_project\\legacy_code.py",
                "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\nNote: This file is in tests/fixtures/ and is excluded from main project scans,\nbut is intentionally scanned in test contexts to verify legacy detection works.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
                [
                  {
                    "pattern": "bot/",
                    "match": "bot/",
                    "line": 23,
                    "line_content": "old_path = \"bot/communication/old_file.py\"",
                    "start": 779,
                    "end": 783
                  },
                  {
                    "pattern": "from\\s+bot\\.",
                    "match": "from bot.",
                    "line": 17,
                    "line_content": "# from bot.communication import old_module  # noqa: F401",
                    "start": 596,
                    "end": 605
                  }
                ]
              ]
            ],
            "old_import_paths": [
              [
                "tests\\data\\tmpneut7le6\\demo_project\\legacy_code.py",
                "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\nNote: This file is in tests/fixtures/ and is excluded from main project scans,\nbut is intentionally scanned in test contexts to verify legacy detection works.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
                [
                  {
                    "pattern": "from\\s+bot\\.communication",
                    "match": "from bot.communication",
                    "line": 17,
                    "line_content": "# from bot.communication import old_module  # noqa: F401",
                    "start": 596,
                    "end": 618
                  }
                ]
              ]
            ],
            "historical_references": [
              [
                "tests\\data\\tmpneut7le6\\demo_project\\legacy_code.py",
                "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\nNote: This file is in tests/fixtures/ and is excluded from main project scans,\nbut is intentionally scanned in test contexts to verify legacy detection works.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
                [
                  {
                    "pattern": "bot/communication",
                    "match": "bot/communication",
                    "line": 23,
                    "line_content": "old_path = \"bot/communication/old_file.py\"",
                    "start": 779,
                    "end": 796
                  }
                ]
              ]
            ],
            "deprecated_functions": [
              [
                "tests\\data\\tmpneut7le6\\demo_project\\legacy_code.py",
                "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\nNote: This file is in tests/fixtures/ and is excluded from main project scans,\nbut is intentionally scanned in test contexts to verify legacy detection works.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
                [
                  {
                    "pattern": "LegacyChannelWrapper",
                    "match": "LegacyChannelWrapper",
                    "line": 27,
                    "line_content": "class LegacyChannelWrapper:",
                    "start": 838,
                    "end": 858
                  },
                  {
                    "pattern": "_create_legacy_channel_access\\(",
                    "match": "_create_legacy_channel_access(",
                    "line": 32,
                    "line_content": "def _create_legacy_channel_access():",
                    "start": 907,
                    "end": 937
                  }
                ]
              ]
            ],
            "legacy_compatibility_markers": [
              [
                "tests\\data\\tmpneut7le6\\demo_project\\legacy_code.py",
                "\"\"\"\nLegacy code module for testing legacy reference cleanup.\n\nThis module contains legacy patterns that should be detected and cleaned up.\nNote: This file is in tests/fixtures/ and is excluded from main project scans,\nbut is intentionally scanned in test contexts to verify legacy detection works.\n\"\"\"\n\n# LEGACY COMPATIBILITY: This function is kept for backward compatibility\ndef legacy_function():\n    \"\"\"Legacy function that should be detected.\"\"\"\n    pass\n\n\n# LEGACY COMPATIBILITY: Old import pattern\n# Note: This is commented out to avoid import errors, but the pattern is still detectable\n# from bot.communication import old_module  # noqa: F401\n\n\ndef uses_legacy_pattern():\n    \"\"\"Function that uses legacy patterns.\"\"\"\n    # Reference to old bot directory\n    old_path = \"bot/communication/old_file.py\"\n    return old_path\n\n\nclass LegacyChannelWrapper:\n    \"\"\"Legacy wrapper class.\"\"\"\n    pass\n\n\ndef _create_legacy_channel_access():\n    \"\"\"Legacy channel access function.\"\"\"\n    pass\n\n",
                [
                  {
                    "pattern": "# LEGACY COMPATIBILITY:",
                    "match": "# LEGACY COMPATIBILITY:",
                    "line": 9,
                    "line_content": "# LEGACY COMPATIBILITY: This function is kept for backward compatibility",
                    "start": 303,
                    "end": 326
                  },
                  {
                    "pattern": "# LEGACY COMPATIBILITY:",
                    "match": "# LEGACY COMPATIBILITY:",
                    "line": 15,
                    "line_content": "# LEGACY COMPATIBILITY: Old import pattern",
                    "start": 461,
                    "end": 484
                  }
                ]
              ]
            ]
          },
          "files_with_issues": 41,
          "legacy_markers": 73,
          "report_path": "development_docs/LEGACY_REFERENCE_REPORT.md"
        }
      },
      "timestamp": "2026-01-18T01:50:30"
    },
    "analyze_config": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 0,
          "files_affected": 0
        },
        "details": {
          "tools_analysis": {
            "run_development_tools.py": {
              "imports_config": true,
              "uses_config_functions": false,
              "hardcoded_values": [],
              "config_functions_used": [],
              "issues": [],
              "is_wrapper_script": false,
              "is_entry_point_script": true
            },
            "run_dev_tools.py": {
              "imports_config": true,
              "uses_config_functions": false,
              "hardcoded_values": [],
              "config_functions_used": [],
              "issues": [],
              "is_wrapper_script": true,
              "is_entry_point_script": false
            },
            "__init__.py": {
              "imports_config": true,
              "uses_config_functions": false,
              "hardcoded_values": [],
              "config_functions_used": [],
              "issues": [],
              "is_wrapper_script": false,
              "is_entry_point_script": false
            }
          },
          "validation": {
            "scan_directories_exist": [
              "ai",
              "communication",
              "core",
              "tasks",
              "tests",
              "ui",
              "user"
            ],
            "missing_directories": [],
            "config_structure_valid": true,
            "issues": []
          },
          "completeness": {
            "sections_complete": true,
            "missing_fields": [],
            "recommendations": []
          },
          "recommendations": [],
          "summary": {
            "tools_using_config": 3,
            "total_tools": 3,
            "config_valid": true,
            "config_complete": true,
            "total_recommendations": 0
          }
        }
      },
      "timestamp": "2026-01-18T01:52:21"
    },
    "analyze_ai_work": {
      "success": true,
      "data": {
        "summary": {
          "total_issues": 0,
          "files_affected": 0,
          "status": "GOOD"
        },
        "details": {
          "output": "Validation Type: documentation\n\nDocumentation File: N/A\nFile Exists: True\nCoverage: 100.0%\n\nOVERALL ASSESSMENT:\nGOOD - Documentation covers most items",
          "work_type": "documentation"
        }
      },
      "timestamp": "2026-01-18T01:46:27"
    },
    "analyze_system_signals": {
      "success": true,
      "data": {
        "timestamp": "2026-01-18T01:46:26.493990",
        "system_health": {
          "overall_status": "CRITICAL",
          "core_files": {
            "run_mhm.py": "OK",
            "core/service.py": "OK",
            "core/config.py": "OK"
          },
          "key_directories": {
            ".": "OK",
            "ai": "OK",
            "communication": "OK",
            "core": "OK",
            "tasks": "OK",
            "tests": "OK",
            "ui": "OK",
            "user": "OK"
          },
          "last_audit": "2026-01-18T01:40:21.534553",
          "audit_freshness": "<1 hour",
          "test_coverage_status": "Unknown",
          "documentation_sync_status": "Minor Issues",
          "health_indicators": [
            "Audit data is very recent (<1 hour)"
          ],
          "recommendations": [
            "Run `python development_tools/run_development_tools.py doc-sync` to check details",
            "Review logs/errors.log for recent error details"
          ],
          "warnings": [],
          "errors": [],
          "severity_levels": {
            "INFO": [
              "Documentation has 1 minor sync issue(s)"
            ],
            "WARNING": [],
            "CRITICAL": [
              "Recent errors detected in logs/errors.log (last hour)"
            ]
          }
        },
        "recent_activity": {
          "recent_changes": [
            "communication/command_handlers/schedule_handler.py",
            "tasks/task_management.py",
            "core/schedule_management.py",
            "tests/ai/test_ai_functionality_manual.py",
            "communication/message_processing/conversation_flow_manager.py",
            "tests/unit/test_interaction_handlers_helpers.py",
            "tests/behavior/test_task_error_handling.py",
            "ui/generate_ui_files.py",
            "development_tools/imports/generate_unused_imports_report.py",
            "tests/behavior/test_task_behavior.py"
          ],
          "git_status": "Modified",
          "last_commit": null,
          "uncommitted_changes": true
        },
        "critical_alerts": [
          "CRITICAL: Recent errors in logs/errors.log"
        ],
        "performance_indicators": {
          "log_file_sizes": {
            "ai.log": "0.0MB",
            "ai_dev_tools.log": "0.0MB",
            "app.log": "0.0MB",
            "communication_manager.log": "0.0MB",
            "discord.log": "0.0MB",
            "email.log": "0.0MB",
            "errors.log": "0.0MB",
            "file_ops.log": "0.0MB",
            "message.log": "0.0MB",
            "scheduler.log": "0.2MB",
            "test_consolidated.log": "0.0MB",
            "ui.log": "0.0MB",
            "user_activity.log": "0.0MB"
          },
          "cache_status": "Unknown",
          "memory_usage": "Unknown"
        }
      },
      "timestamp": "2026-01-18T01:46:26"
    },
    "decision_support": {
      "success": true,
      "data": {
        "total_functions": 1505,
        "moderate_complexity": 155,
        "high_complexity": 146,
        "critical_complexity": 141,
        "undocumented": 5,
        "decision_support_items": 5,
        "decision_support_sample": [
          "[COMPLEXITY] Functions needing attention: 442",
          "[CRITICAL] Critical Complexity (>199 nodes): 141",
          "[DOC] Undocumented Handlers: 5",
          "[DUPE] Duplicate Function Names: 97",
          "- [CRITICAL] Refactor critical complexity functions immediately."
        ],
        "critical_complexity_examples": [
          {
            "name": "_detect_mode",
            "function": "_detect_mode",
            "file": "chatbot.py",
            "complexity": 355
          },
          {
            "name": "generate_response",
            "function": "generate_response",
            "file": "chatbot.py",
            "complexity": 781
          },
          {
            "name": "generate_personalized_message",
            "function": "generate_personalized_message",
            "file": "chatbot.py",
            "complexity": 206
          },
          {
            "name": "generate_contextual_response",
            "function": "generate_contextual_response",
            "file": "chatbot.py",
            "complexity": 657
          },
          {
            "name": "_clean_system_prompt_leaks",
            "function": "_clean_system_prompt_leaks",
            "file": "chatbot.py",
            "complexity": 309
          },
          {
            "name": "__init__",
            "function": "__init__",
            "file": "chatbot.py",
            "complexity": 122
          },
          {
            "name": "_make_cache_key_inputs",
            "function": "_make_cache_key_inputs",
            "file": "chatbot.py",
            "complexity": 180
          },
          {
            "name": "_call_lm_studio_api",
            "function": "_call_lm_studio_api",
            "file": "chatbot.py",
            "complexity": 184
          },
          {
            "name": "_detect_resource_constraints",
            "function": "_detect_resource_constraints",
            "file": "chatbot.py",
            "complexity": 101
          },
          {
            "name": "_smart_truncate_response",
            "function": "_smart_truncate_response",
            "file": "chatbot.py",
            "complexity": 141
          },
          {
            "name": "__init__",
            "function": "__init__",
            "file": "cache_manager.py",
            "complexity": 75
          },
          {
            "name": "_generate_key",
            "function": "_generate_key",
            "file": "cache_manager.py",
            "complexity": 57
          },
          {
            "name": "_cleanup_lru",
            "function": "_cleanup_lru",
            "file": "cache_manager.py",
            "complexity": 90
          },
          {
            "name": "_validate_email",
            "function": "_validate_email",
            "file": "schemas.py",
            "complexity": 0
          },
          {
            "name": "_validate_timezone",
            "function": "_validate_timezone",
            "file": "schemas.py",
            "complexity": 0
          },
          {
            "name": "on_save",
            "function": "on_save",
            "file": "ui_app_qt.py",
            "complexity": 0
          },
          {
            "name": "on_personalization_save",
            "function": "on_personalization_save",
            "file": "account_creator_dialog.py",
            "complexity": 0
          },
          {
            "name": "set_contact_info",
            "function": "set_contact_info",
            "file": "channel_selection_widget.py",
            "complexity": 0
          }
        ]
      },
      "timestamp": "2026-01-18T01:46:30"
    },
    "quick_status": {
      "success": true,
      "data": {
        "timestamp": "2026-01-18T01:46:31.030224",
        "system_health": {
          "core_files": {
            "run_mhm.py": "OK",
            "core/service.py": "OK",
            "core/config.py": "OK"
          },
          "key_directories": {
            "ai": "OK",
            "communication": "OK",
            "core": "OK",
            "tasks": "OK",
            "tests": "OK",
            "ui": "OK",
            "user": "OK"
          },
          "overall_status": "OK"
        },
        "documentation_status": {
          "coverage": "Unknown",
          "recent_audit": "2026-01-17T06:08:29.836650",
          "key_files": {
            "README.md": "OK",
            "ai_development_docs/AI_CHANGELOG.md": "OK",
            "development_docs/CHANGELOG_DETAIL.md": "OK",
            "TODO.md": "OK",
            "development_docs/FUNCTION_REGISTRY_DETAIL.md": "OK",
            "development_docs/MODULE_DEPENDENCIES_DETAIL.md": "OK"
          }
        },
        "critical_issues": [],
        "action_items": [],
        "recent_activity": {
          "last_audit": "2026-01-17T06:08:29.836650",
          "recent_changes": [
            "tasks/task_management.py",
            "core/user_data_validation.py",
            "core/user_data_manager.py",
            "core/user_data_handlers.py",
            "core/time_utilities.py",
            "core/tags.py",
            "core/service_utilities.py",
            "core/service.py",
            "core/scheduler.py",
            "core/schedule_utilities.py",
            "core/schedule_management.py",
            "core/response_tracking.py",
            "core/message_management.py",
            "core/file_operations.py",
            "core/error_handling.py"
          ]
        }
      },
      "timestamp": "2026-01-18T01:46:31"
    },
    "system_signals": {
      "success": true,
      "data": {
        "timestamp": "2025-12-26T18:04:12.976250",
        "system_health": {
          "overall_status": "OK",
          "core_files": {
            "run_mhm.py": "OK",
            "core/service.py": "OK",
            "core/config.py": "OK"
          },
          "key_directories": {
            ".": "OK",
            "ai": "OK",
            "communication": "OK",
            "core": "OK",
            "tasks": "OK",
            "ui": "OK",
            "user": "OK",
            "tests": "OK"
          },
          "last_audit": "2025-12-26T18:01:24.185219",
          "warnings": [],
          "errors": []
        },
        "recent_activity": {
          "recent_changes": [
            "development_tools/run_development_tools.py",
            "tests/development_tools/test_analyze_module_dependencies.py",
            "tests/development_tools/test_analyze_unused_imports.py",
            "tests/development_tools/test_generate_directory_tree.py",
            "communication/core/welcome_manager.py",
            "tests/development_tools/test_decision_support.py",
            "tests/unit/test_command_registry.py",
            "development_tools/imports/generate_module_dependencies.py",
            "tests/behavior/test_webhook_server_behavior.py",
            "tests/development_tools/test_fix_documentation.py"
          ],
          "git_status": "Modified",
          "last_commit": null,
          "uncommitted_changes": true
        },
        "critical_alerts": [],
        "performance_indicators": {
          "log_file_sizes": {
            "ai.log": "0.0MB",
            "ai_dev_tools.log": "0.2MB",
            "app.log": "0.1MB",
            "communication_manager.log": "0.0MB",
            "discord.log": "0.0MB",
            "email.log": "0.0MB",
            "errors.log": "0.0MB",
            "file_ops.log": "0.0MB",
            "message.log": "0.0MB",
            "scheduler.log": "0.1MB",
            "ui.log": "0.0MB",
            "user_activity.log": "0.0MB"
          },
          "cache_status": "Unknown",
          "memory_usage": "Unknown"
        }
      },
      "timestamp": "2025-12-26T18:04:13.195820"
    },
    "decision_support_metrics": {
      "success": true,
      "data": {
        "total_functions": 1505,
        "moderate_complexity": 155,
        "high_complexity": 146,
        "critical_complexity": 141,
        "undocumented": 5,
        "decision_support_items": 5,
        "decision_support_sample": [
          "[COMPLEXITY] Functions needing attention: 442",
          "[CRITICAL] Critical Complexity (>199 nodes): 141",
          "[DOC] Undocumented Handlers: 5",
          "[DUPE] Duplicate Function Names: 97",
          "- [CRITICAL] Refactor critical complexity functions immediately."
        ],
        "critical_complexity_examples": [
          {
            "name": "_detect_mode",
            "function": "_detect_mode",
            "file": "chatbot.py",
            "complexity": 355
          },
          {
            "name": "generate_response",
            "function": "generate_response",
            "file": "chatbot.py",
            "complexity": 781
          },
          {
            "name": "generate_personalized_message",
            "function": "generate_personalized_message",
            "file": "chatbot.py",
            "complexity": 206
          },
          {
            "name": "generate_contextual_response",
            "function": "generate_contextual_response",
            "file": "chatbot.py",
            "complexity": 657
          },
          {
            "name": "_clean_system_prompt_leaks",
            "function": "_clean_system_prompt_leaks",
            "file": "chatbot.py",
            "complexity": 309
          },
          {
            "name": "__init__",
            "function": "__init__",
            "file": "chatbot.py",
            "complexity": 122
          },
          {
            "name": "_make_cache_key_inputs",
            "function": "_make_cache_key_inputs",
            "file": "chatbot.py",
            "complexity": 180
          },
          {
            "name": "_call_lm_studio_api",
            "function": "_call_lm_studio_api",
            "file": "chatbot.py",
            "complexity": 184
          },
          {
            "name": "_detect_resource_constraints",
            "function": "_detect_resource_constraints",
            "file": "chatbot.py",
            "complexity": 101
          },
          {
            "name": "_smart_truncate_response",
            "function": "_smart_truncate_response",
            "file": "chatbot.py",
            "complexity": 141
          },
          {
            "name": "__init__",
            "function": "__init__",
            "file": "cache_manager.py",
            "complexity": 75
          },
          {
            "name": "_generate_key",
            "function": "_generate_key",
            "file": "cache_manager.py",
            "complexity": 57
          },
          {
            "name": "_cleanup_lru",
            "function": "_cleanup_lru",
            "file": "cache_manager.py",
            "complexity": 90
          },
          {
            "name": "_validate_email",
            "function": "_validate_email",
            "file": "schemas.py",
            "complexity": 0
          },
          {
            "name": "_validate_timezone",
            "function": "_validate_timezone",
            "file": "schemas.py",
            "complexity": 0
          },
          {
            "name": "on_save",
            "function": "on_save",
            "file": "ui_app_qt.py",
            "complexity": 0
          },
          {
            "name": "on_personalization_save",
            "function": "on_personalization_save",
            "file": "account_creator_dialog.py",
            "complexity": 0
          },
          {
            "name": "set_contact_info",
            "function": "set_contact_info",
            "file": "channel_selection_widget.py",
            "complexity": 0
          }
        ]
      },
      "timestamp": "2026-01-18T01:55:43.498939"
    }
  }
}