#!/usr/bin/env python3
# TOOL_TIER: core
# TOOL_PORTABILITY: portable

"""
Missing Address Analyzer

This script checks for documentation files missing file addresses in their headers.
Configuration is loaded from external config file (development_tools_config.json)
if available, making this tool portable across different projects.

Usage:
    python docs/analyze_missing_addresses.py
"""

import re
import argparse
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
from collections import defaultdict

# Add project root to path for core module imports
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from core.logger import get_component_logger

# Handle both relative and absolute imports
if __name__ != '__main__' and __package__ and '.' in __package__:
    from .. import config
else:
    from development_tools import config

# Load external config on module import (if not already loaded)
try:
    if hasattr(config, 'load_external_config'):
        config.load_external_config()
except (AttributeError, ImportError):
    pass

logger = get_component_logger("development_tools")


class MissingAddressAnalyzer:
    """Analyzes documentation files for missing file addresses."""
    
    def __init__(self, project_root: Optional[str] = None, config_path: Optional[str] = None, use_cache: bool = True):
        """
        Initialize missing address analyzer.
        
        Args:
            project_root: Root directory of the project
            config_path: Optional path to external config file
            use_cache: Whether to use mtime-based caching for file results
        """
        # Load external config if provided
        if config_path:
            config.load_external_config(config_path)
        else:
            config.load_external_config()
        
        # Use provided project_root or get from config
        if project_root:
            self.project_root = Path(project_root).resolve()
        else:
            self.project_root = Path(config.get_project_root()).resolve()
        
        # Caching - use standardized storage
        from development_tools.shared.mtime_cache import MtimeFileCache
        cache_file = self.project_root / "development_tools" / "docs" / ".missing_addresses_cache.json"  # Legacy fallback
        self.cache = MtimeFileCache(cache_file, self.project_root, use_cache=use_cache,
                                    tool_name='analyze_missing_addresses', domain='docs')
    
    def _is_generated_file(self, file_path: Path) -> bool:
        """Check if a file is generated (should not be edited)."""
        from development_tools.shared.standard_exclusions import ALL_GENERATED_FILES
        # Check if file is in ALL_GENERATED_FILES list
        try:
            rel_path = str(file_path.relative_to(self.project_root)).replace('\\', '/')
            if rel_path in ALL_GENERATED_FILES:
                return True
        except ValueError:
            pass
        
        # Check if file has "Generated" marker in first few lines
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_lines = ''.join(f.readlines()[:10])
                if '**Generated**' in first_lines or 'Generated:' in first_lines or 'Generated by:' in first_lines:
                    return True
        except Exception:
            pass
        
        return False
    
    def check_missing_addresses(self) -> Dict[str, List[str]]:
        """
        Check for documentation files missing file addresses.
        
        Excludes:
        - Generated files
        - .cursor/ directory files (plans, rules, etc. - they have their own metadata format)
        
        Returns:
            Dictionary mapping file paths to lists of issues
        """
        missing_addresses = defaultdict(list)
        
        # Find all documentation files (same logic as fix_add_addresses)
        files_to_check = []
        for ext in ['*.md', '*.mdc']:
            files_to_check.extend(self.project_root.rglob(ext))
        
        ignore_dirs = {'venv', '.venv', '__pycache__', '.git', 'node_modules', '.pytest_cache', 'coverage_html', 'archive'}
        from development_tools.shared.standard_exclusions import ALL_GENERATED_FILES
        generated_files = set(ALL_GENERATED_FILES)
        
        for file_path in files_to_check:
            parts = file_path.parts
            if any(ignore in parts for ignore in ignore_dirs):
                continue
            try:
                rel_path = file_path.relative_to(self.project_root)
                rel_path_str = str(rel_path).replace('\\', '/')
                if rel_path_str in generated_files:
                    continue
                
                # Skip .cursor/ directory files (plans, rules, etc.) - they have their own metadata format
                if '.cursor' in parts:
                    continue
                
                if 'tests' in rel_path.parts:
                    tests_index = rel_path.parts.index('tests')
                    if tests_index < len(rel_path.parts) - 2:
                        continue
            except ValueError:
                continue
            
            # Check cache first
            cached_issues = self.cache.get_cached(file_path)
            if cached_issues is not None:
                if cached_issues:
                    missing_addresses[str(rel_path)] = cached_issues
                continue
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                has_address = bool(re.search(r'^>\s*\*\*File\*\*:\s*`', content[:2000], re.MULTILINE))
                file_issues = []
                if not has_address:
                    file_issues.append("Missing file address in header")
                
                # Cache results
                self.cache.cache_results(file_path, file_issues)
                if file_issues:
                    missing_addresses[str(rel_path)] = file_issues
            except Exception as e:
                file_issues = [f"Error reading file: {e}"]
                self.cache.cache_results(file_path, file_issues)
                missing_addresses[str(rel_path)] = file_issues
        
        # Save cache
        self.cache.save_cache()
        
        return missing_addresses
    
    def run_analysis(self) -> Dict[str, Any]:
        """
        Run missing addresses analysis and return results in standard format.
        
        Returns:
            Dictionary with standard format structure:
            {
                "summary": {
                    "total_issues": int,
                    "files_affected": int,
                    "status": str
                },
                "files": {
                    "file_path": issue_count
                },
                "details": {
                    "detailed_issues": Dict[str, List[str]]
                }
            }
        """
        missing_addresses = self.check_missing_addresses()
        
        # Convert to standard format
        files = {}
        total_issues = 0
        for file_path, issues in missing_addresses.items():
            issue_count = len(issues)
            files[file_path] = issue_count
            total_issues += issue_count
        
        # Determine status
        if total_issues == 0:
            status = "CLEAN"
        elif total_issues < 5:
            status = "NEEDS_ATTENTION"
        else:
            status = "CRITICAL"
        
        return {
            'summary': {
                'total_issues': total_issues,
                'files_affected': len(files),
                'status': status
            },
            'files': files,
            'details': {
                'detailed_issues': dict(missing_addresses)
            }
        }


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Check for documentation files missing file addresses")
    parser.add_argument('--json', action='store_true', help='Output results as JSON in standard format')
    
    args = parser.parse_args()
    
    analyzer = MissingAddressAnalyzer()
    
    # Use run_analysis() to get standard format
    if args.json:
        import json
        results = analyzer.run_analysis()
        print(json.dumps(results, indent=2))
        return 0
    
    # Otherwise use check_missing_addresses() for human-readable output
    results = analyzer.check_missing_addresses()
    
    # Print results
    if results:
        print(f"\nMissing Address Issues:")
        print(f"   Total files missing addresses: {len(results)}")
        print(f"   Total issues found: {sum(len(issues) for issues in results.values())}")
        print(f"   Files missing addresses:")
        for doc_file, issues in results.items():
            print(f"     {doc_file}: {len(issues)} issues")
            for issue in issues[:3]:
                clean_issue = issue.encode('ascii', 'ignore').decode('ascii')
                print(f"       - {clean_issue}")
    else:
        print("\nAll documentation files have file addresses!")
    
    return 0 if not results else 1


if __name__ == "__main__":
    sys.exit(main())

